==> Preparing data..
==> Building model..
DataParallel(
  (module): ResNet(
    (conv1): InputConv2dLSQ(
      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=2, is_activation=False)
            (quan_a): LSQQuantizer (bit=2, is_activation=True)
          )
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=2, is_activation=False)
            (quan_a): LSQQuantizer (bit=2, is_activation=True)
          )
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=2, is_activation=False)
            (quan_a): LSQQuantizer (bit=2, is_activation=True)
          )
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): LinearLSQ(
      in_features=512, out_features=10, bias=True
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=True)
    )
  )
)
==> Initializing from checkpoint..

Epoch: 0
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
[Train] Epoch= 0  BatchID= 0 Loss: 2.122 | Acc: 24.219% (31/128)
[Train] Epoch= 0  BatchID= 10 Loss: 1.592 | Acc: 47.727% (672/1408)
[Train] Epoch= 0  BatchID= 20 Loss: 1.225 | Acc: 60.268% (1620/2688)
[Train] Epoch= 0  BatchID= 30 Loss: 1.018 | Acc: 66.633% (2644/3968)
[Train] Epoch= 0  BatchID= 40 Loss: 0.884 | Acc: 71.189% (3736/5248)
[Train] Epoch= 0  BatchID= 50 Loss: 0.788 | Acc: 74.234% (4846/6528)
[Train] Epoch= 0  BatchID= 60 Loss: 0.726 | Acc: 76.217% (5951/7808)
[Train] Epoch= 0  BatchID= 70 Loss: 0.679 | Acc: 77.784% (7069/9088)
[Test] Epoch= 0  BatchID= 0 Loss: 0.253 | Acc: 93.750% (120/128)
[Test] Epoch= 0  BatchID= 10 Loss: 0.570 | Acc: 81.534% (1148/1408)
[Test] Epoch= 0  BatchID= 20 Loss: 0.584 | Acc: 81.771% (2198/2688)
[Test] Epoch= 0  BatchID= 30 Loss: 0.575 | Acc: 82.217% (3227/3925)
Saving..
Best accuracy:  82.21656050955414

Epoch: 1
[Train] Epoch= 1  BatchID= 0 Loss: 0.309 | Acc: 90.625% (116/128)
[Train] Epoch= 1  BatchID= 10 Loss: 0.356 | Acc: 89.702% (1263/1408)
[Train] Epoch= 1  BatchID= 20 Loss: 0.361 | Acc: 89.100% (2395/2688)
[Train] Epoch= 1  BatchID= 30 Loss: 0.351 | Acc: 89.113% (3536/3968)
[Train] Epoch= 1  BatchID= 40 Loss: 0.343 | Acc: 89.405% (4692/5248)
[Train] Epoch= 1  BatchID= 50 Loss: 0.346 | Acc: 89.384% (5835/6528)
[Train] Epoch= 1  BatchID= 60 Loss: 0.344 | Acc: 89.331% (6975/7808)
[Train] Epoch= 1  BatchID= 70 Loss: 0.342 | Acc: 89.294% (8115/9088)
[Test] Epoch= 1  BatchID= 0 Loss: 0.224 | Acc: 94.531% (121/128)
[Test] Epoch= 1  BatchID= 10 Loss: 0.416 | Acc: 87.997% (1239/1408)
[Test] Epoch= 1  BatchID= 20 Loss: 0.402 | Acc: 88.170% (2370/2688)
[Test] Epoch= 1  BatchID= 30 Loss: 0.460 | Acc: 86.675% (3402/3925)
Saving..
Best accuracy:  86.67515923566879

Epoch: 2
[Train] Epoch= 2  BatchID= 0 Loss: 0.349 | Acc: 91.406% (117/128)
[Train] Epoch= 2  BatchID= 10 Loss: 0.281 | Acc: 91.193% (1284/1408)
[Train] Epoch= 2  BatchID= 20 Loss: 0.288 | Acc: 90.737% (2439/2688)
[Train] Epoch= 2  BatchID= 30 Loss: 0.289 | Acc: 90.801% (3603/3968)
[Train] Epoch= 2  BatchID= 40 Loss: 0.291 | Acc: 90.682% (4759/5248)
[Train] Epoch= 2  BatchID= 50 Loss: 0.290 | Acc: 90.640% (5917/6528)
[Train] Epoch= 2  BatchID= 60 Loss: 0.292 | Acc: 90.676% (7080/7808)
[Train] Epoch= 2  BatchID= 70 Loss: 0.293 | Acc: 90.614% (8235/9088)
[Test] Epoch= 2  BatchID= 0 Loss: 0.174 | Acc: 96.094% (123/128)
[Test] Epoch= 2  BatchID= 10 Loss: 0.416 | Acc: 86.790% (1222/1408)
[Test] Epoch= 2  BatchID= 20 Loss: 0.416 | Acc: 86.942% (2337/2688)
[Test] Epoch= 2  BatchID= 30 Loss: 0.443 | Acc: 86.242% (3385/3925)

Epoch: 3
[Train] Epoch= 3  BatchID= 0 Loss: 0.279 | Acc: 91.406% (117/128)
[Train] Epoch= 3  BatchID= 10 Loss: 0.255 | Acc: 91.335% (1286/1408)
[Train] Epoch= 3  BatchID= 20 Loss: 0.272 | Acc: 90.923% (2444/2688)
[Train] Epoch= 3  BatchID= 30 Loss: 0.283 | Acc: 90.650% (3597/3968)
[Train] Epoch= 3  BatchID= 40 Loss: 0.280 | Acc: 90.911% (4771/5248)
[Train] Epoch= 3  BatchID= 50 Loss: 0.287 | Acc: 90.640% (5917/6528)
[Train] Epoch= 3  BatchID= 60 Loss: 0.286 | Acc: 90.612% (7075/7808)
[Train] Epoch= 3  BatchID= 70 Loss: 0.285 | Acc: 90.636% (8237/9088)
[Test] Epoch= 3  BatchID= 0 Loss: 0.129 | Acc: 96.875% (124/128)
[Test] Epoch= 3  BatchID= 10 Loss: 0.426 | Acc: 87.003% (1225/1408)
[Test] Epoch= 3  BatchID= 20 Loss: 0.393 | Acc: 87.463% (2351/2688)
[Test] Epoch= 3  BatchID= 30 Loss: 0.438 | Acc: 86.624% (3400/3925)

Epoch: 4
[Train] Epoch= 4  BatchID= 0 Loss: 0.249 | Acc: 92.969% (119/128)
[Train] Epoch= 4  BatchID= 10 Loss: 0.265 | Acc: 90.980% (1281/1408)
[Train] Epoch= 4  BatchID= 20 Loss: 0.273 | Acc: 90.997% (2446/2688)
[Train] Epoch= 4  BatchID= 30 Loss: 0.275 | Acc: 90.902% (3607/3968)
[Train] Epoch= 4  BatchID= 40 Loss: 0.270 | Acc: 91.254% (4789/5248)
[Train] Epoch= 4  BatchID= 50 Loss: 0.267 | Acc: 91.437% (5969/6528)
[Train] Epoch= 4  BatchID= 60 Loss: 0.268 | Acc: 91.381% (7135/7808)
[Train] Epoch= 4  BatchID= 70 Loss: 0.268 | Acc: 91.494% (8315/9088)
[Test] Epoch= 4  BatchID= 0 Loss: 0.137 | Acc: 96.094% (123/128)
[Test] Epoch= 4  BatchID= 10 Loss: 0.404 | Acc: 87.713% (1235/1408)
[Test] Epoch= 4  BatchID= 20 Loss: 0.412 | Acc: 87.314% (2347/2688)
[Test] Epoch= 4  BatchID= 30 Loss: 0.427 | Acc: 87.236% (3424/3925)
Saving..
Best accuracy:  87.23566878980891

Epoch: 5
[Train] Epoch= 5  BatchID= 0 Loss: 0.321 | Acc: 85.156% (109/128)
[Train] Epoch= 5  BatchID= 10 Loss: 0.320 | Acc: 88.494% (1246/1408)
[Train] Epoch= 5  BatchID= 20 Loss: 0.306 | Acc: 89.286% (2400/2688)
[Train] Epoch= 5  BatchID= 30 Loss: 0.283 | Acc: 90.549% (3593/3968)
[Train] Epoch= 5  BatchID= 40 Loss: 0.274 | Acc: 90.777% (4764/5248)
[Train] Epoch= 5  BatchID= 50 Loss: 0.274 | Acc: 90.916% (5935/6528)
[Train] Epoch= 5  BatchID= 60 Loss: 0.272 | Acc: 91.060% (7110/7808)
[Train] Epoch= 5  BatchID= 70 Loss: 0.268 | Acc: 91.186% (8287/9088)
[Test] Epoch= 5  BatchID= 0 Loss: 0.143 | Acc: 97.656% (125/128)
[Test] Epoch= 5  BatchID= 10 Loss: 0.350 | Acc: 88.636% (1248/1408)
[Test] Epoch= 5  BatchID= 20 Loss: 0.344 | Acc: 88.839% (2388/2688)
[Test] Epoch= 5  BatchID= 30 Loss: 0.384 | Acc: 87.924% (3451/3925)
Saving..
Best accuracy:  87.92356687898089

Epoch: 6
[Train] Epoch= 6  BatchID= 0 Loss: 0.205 | Acc: 93.750% (120/128)
[Train] Epoch= 6  BatchID= 10 Loss: 0.259 | Acc: 91.832% (1293/1408)
[Train] Epoch= 6  BatchID= 20 Loss: 0.261 | Acc: 91.778% (2467/2688)
[Train] Epoch= 6  BatchID= 30 Loss: 0.263 | Acc: 91.860% (3645/3968)
[Train] Epoch= 6  BatchID= 40 Loss: 0.259 | Acc: 91.978% (4827/5248)
[Train] Epoch= 6  BatchID= 50 Loss: 0.270 | Acc: 91.422% (5968/6528)
[Train] Epoch= 6  BatchID= 60 Loss: 0.273 | Acc: 91.265% (7126/7808)
[Train] Epoch= 6  BatchID= 70 Loss: 0.268 | Acc: 91.406% (8307/9088)
[Test] Epoch= 6  BatchID= 0 Loss: 0.162 | Acc: 96.094% (123/128)
[Test] Epoch= 6  BatchID= 10 Loss: 0.357 | Acc: 89.205% (1256/1408)
[Test] Epoch= 6  BatchID= 20 Loss: 0.351 | Acc: 89.100% (2395/2688)
[Test] Epoch= 6  BatchID= 30 Loss: 0.423 | Acc: 87.236% (3424/3925)

Epoch: 7
[Train] Epoch= 7  BatchID= 0 Loss: 0.192 | Acc: 94.531% (121/128)
[Train] Epoch= 7  BatchID= 10 Loss: 0.230 | Acc: 92.614% (1304/1408)
[Train] Epoch= 7  BatchID= 20 Loss: 0.237 | Acc: 92.299% (2481/2688)
[Train] Epoch= 7  BatchID= 30 Loss: 0.239 | Acc: 92.288% (3662/3968)
[Train] Epoch= 7  BatchID= 40 Loss: 0.243 | Acc: 92.149% (4836/5248)
[Train] Epoch= 7  BatchID= 50 Loss: 0.243 | Acc: 92.188% (6018/6528)
[Train] Epoch= 7  BatchID= 60 Loss: 0.243 | Acc: 92.149% (7195/7808)
[Train] Epoch= 7  BatchID= 70 Loss: 0.245 | Acc: 92.044% (8365/9088)
[Test] Epoch= 7  BatchID= 0 Loss: 0.181 | Acc: 94.531% (121/128)
[Test] Epoch= 7  BatchID= 10 Loss: 0.401 | Acc: 87.713% (1235/1408)
[Test] Epoch= 7  BatchID= 20 Loss: 0.364 | Acc: 88.988% (2392/2688)
[Test] Epoch= 7  BatchID= 30 Loss: 0.408 | Acc: 87.745% (3444/3925)

Epoch: 8
[Train] Epoch= 8  BatchID= 0 Loss: 0.241 | Acc: 92.969% (119/128)
[Train] Epoch= 8  BatchID= 10 Loss: 0.256 | Acc: 92.259% (1299/1408)
[Train] Epoch= 8  BatchID= 20 Loss: 0.250 | Acc: 92.262% (2480/2688)
[Train] Epoch= 8  BatchID= 30 Loss: 0.251 | Acc: 92.087% (3654/3968)
[Train] Epoch= 8  BatchID= 40 Loss: 0.259 | Acc: 91.978% (4827/5248)
[Train] Epoch= 8  BatchID= 50 Loss: 0.252 | Acc: 92.096% (6012/6528)
[Train] Epoch= 8  BatchID= 60 Loss: 0.246 | Acc: 92.277% (7205/7808)
[Train] Epoch= 8  BatchID= 70 Loss: 0.247 | Acc: 92.265% (8385/9088)
[Test] Epoch= 8  BatchID= 0 Loss: 0.160 | Acc: 96.094% (123/128)
[Test] Epoch= 8  BatchID= 10 Loss: 0.324 | Acc: 89.347% (1258/1408)
[Test] Epoch= 8  BatchID= 20 Loss: 0.340 | Acc: 89.025% (2393/2688)
[Test] Epoch= 8  BatchID= 30 Loss: 0.388 | Acc: 88.051% (3456/3925)
Saving..
Best accuracy:  88.05095541401273

Epoch: 9
[Train] Epoch= 9  BatchID= 0 Loss: 0.172 | Acc: 96.875% (124/128)
[Train] Epoch= 9  BatchID= 10 Loss: 0.215 | Acc: 93.679% (1319/1408)
[Train] Epoch= 9  BatchID= 20 Loss: 0.216 | Acc: 93.527% (2514/2688)
[Train] Epoch= 9  BatchID= 30 Loss: 0.232 | Acc: 92.969% (3689/3968)
[Train] Epoch= 9  BatchID= 40 Loss: 0.233 | Acc: 92.835% (4872/5248)
[Train] Epoch= 9  BatchID= 50 Loss: 0.233 | Acc: 92.831% (6060/6528)
[Train] Epoch= 9  BatchID= 60 Loss: 0.236 | Acc: 92.725% (7240/7808)
[Train] Epoch= 9  BatchID= 70 Loss: 0.241 | Acc: 92.639% (8419/9088)
[Test] Epoch= 9  BatchID= 0 Loss: 0.124 | Acc: 97.656% (125/128)
[Test] Epoch= 9  BatchID= 10 Loss: 0.323 | Acc: 89.702% (1263/1408)
[Test] Epoch= 9  BatchID= 20 Loss: 0.355 | Acc: 88.802% (2387/2688)
[Test] Epoch= 9  BatchID= 30 Loss: 0.407 | Acc: 87.389% (3430/3925)

Epoch: 10
[Train] Epoch= 10  BatchID= 0 Loss: 0.138 | Acc: 96.094% (123/128)
[Train] Epoch= 10  BatchID= 10 Loss: 0.222 | Acc: 93.253% (1313/1408)
[Train] Epoch= 10  BatchID= 20 Loss: 0.214 | Acc: 93.452% (2512/2688)
[Train] Epoch= 10  BatchID= 30 Loss: 0.242 | Acc: 92.440% (3668/3968)
[Train] Epoch= 10  BatchID= 40 Loss: 0.240 | Acc: 92.511% (4855/5248)
[Train] Epoch= 10  BatchID= 50 Loss: 0.243 | Acc: 92.417% (6033/6528)
[Train] Epoch= 10  BatchID= 60 Loss: 0.246 | Acc: 92.290% (7206/7808)
[Train] Epoch= 10  BatchID= 70 Loss: 0.246 | Acc: 92.342% (8392/9088)
[Test] Epoch= 10  BatchID= 0 Loss: 0.127 | Acc: 97.656% (125/128)
[Test] Epoch= 10  BatchID= 10 Loss: 0.368 | Acc: 89.418% (1259/1408)
[Test] Epoch= 10  BatchID= 20 Loss: 0.370 | Acc: 89.472% (2405/2688)
[Test] Epoch= 10  BatchID= 30 Loss: 0.395 | Acc: 88.841% (3487/3925)
Saving..
Best accuracy:  88.8407643312102

Epoch: 11
[Train] Epoch= 11  BatchID= 0 Loss: 0.163 | Acc: 95.312% (122/128)
[Train] Epoch= 11  BatchID= 10 Loss: 0.223 | Acc: 92.685% (1305/1408)
[Train] Epoch= 11  BatchID= 20 Loss: 0.234 | Acc: 92.783% (2494/2688)
[Train] Epoch= 11  BatchID= 30 Loss: 0.230 | Acc: 92.918% (3687/3968)
[Train] Epoch= 11  BatchID= 40 Loss: 0.230 | Acc: 93.083% (4885/5248)
[Train] Epoch= 11  BatchID= 50 Loss: 0.229 | Acc: 93.045% (6074/6528)
[Train] Epoch= 11  BatchID= 60 Loss: 0.230 | Acc: 92.905% (7254/7808)
[Train] Epoch= 11  BatchID= 70 Loss: 0.232 | Acc: 92.826% (8436/9088)
[Test] Epoch= 11  BatchID= 0 Loss: 0.120 | Acc: 96.875% (124/128)
[Test] Epoch= 11  BatchID= 10 Loss: 0.404 | Acc: 88.281% (1243/1408)
[Test] Epoch= 11  BatchID= 20 Loss: 0.416 | Acc: 87.909% (2363/2688)
[Test] Epoch= 11  BatchID= 30 Loss: 0.419 | Acc: 87.541% (3436/3925)

Epoch: 12
[Train] Epoch= 12  BatchID= 0 Loss: 0.227 | Acc: 93.750% (120/128)
[Train] Epoch= 12  BatchID= 10 Loss: 0.227 | Acc: 93.395% (1315/1408)
[Train] Epoch= 12  BatchID= 20 Loss: 0.238 | Acc: 93.155% (2504/2688)
[Train] Epoch= 12  BatchID= 30 Loss: 0.229 | Acc: 93.221% (3699/3968)
[Train] Epoch= 12  BatchID= 40 Loss: 0.231 | Acc: 93.064% (4884/5248)
[Train] Epoch= 12  BatchID= 50 Loss: 0.232 | Acc: 93.061% (6075/6528)
[Train] Epoch= 12  BatchID= 60 Loss: 0.230 | Acc: 93.148% (7273/7808)
[Train] Epoch= 12  BatchID= 70 Loss: 0.233 | Acc: 92.969% (8449/9088)
[Test] Epoch= 12  BatchID= 0 Loss: 0.120 | Acc: 97.656% (125/128)
[Test] Epoch= 12  BatchID= 10 Loss: 0.358 | Acc: 89.702% (1263/1408)
[Test] Epoch= 12  BatchID= 20 Loss: 0.336 | Acc: 90.327% (2428/2688)
[Test] Epoch= 12  BatchID= 30 Loss: 0.395 | Acc: 88.331% (3467/3925)

Epoch: 13
[Train] Epoch= 13  BatchID= 0 Loss: 0.152 | Acc: 96.094% (123/128)
[Train] Epoch= 13  BatchID= 10 Loss: 0.221 | Acc: 93.040% (1310/1408)
[Train] Epoch= 13  BatchID= 20 Loss: 0.236 | Acc: 92.708% (2492/2688)
[Train] Epoch= 13  BatchID= 30 Loss: 0.234 | Acc: 92.792% (3682/3968)
[Train] Epoch= 13  BatchID= 40 Loss: 0.225 | Acc: 93.026% (4882/5248)
[Train] Epoch= 13  BatchID= 50 Loss: 0.230 | Acc: 92.800% (6058/6528)
[Train] Epoch= 13  BatchID= 60 Loss: 0.233 | Acc: 92.725% (7240/7808)
[Train] Epoch= 13  BatchID= 70 Loss: 0.238 | Acc: 92.551% (8411/9088)
[Test] Epoch= 13  BatchID= 0 Loss: 0.144 | Acc: 96.875% (124/128)
[Test] Epoch= 13  BatchID= 10 Loss: 0.353 | Acc: 89.489% (1260/1408)
[Test] Epoch= 13  BatchID= 20 Loss: 0.342 | Acc: 89.583% (2408/2688)
[Test] Epoch= 13  BatchID= 30 Loss: 0.395 | Acc: 88.025% (3455/3925)

Epoch: 14
[Train] Epoch= 14  BatchID= 0 Loss: 0.178 | Acc: 94.531% (121/128)
[Train] Epoch= 14  BatchID= 10 Loss: 0.214 | Acc: 93.963% (1323/1408)
[Train] Epoch= 14  BatchID= 20 Loss: 0.215 | Acc: 93.601% (2516/2688)
[Train] Epoch= 14  BatchID= 30 Loss: 0.210 | Acc: 93.725% (3719/3968)
[Train] Epoch= 14  BatchID= 40 Loss: 0.217 | Acc: 93.216% (4892/5248)
[Train] Epoch= 14  BatchID= 50 Loss: 0.228 | Acc: 92.770% (6056/6528)
[Train] Epoch= 14  BatchID= 60 Loss: 0.228 | Acc: 92.802% (7246/7808)
[Train] Epoch= 14  BatchID= 70 Loss: 0.228 | Acc: 92.826% (8436/9088)
[Test] Epoch= 14  BatchID= 0 Loss: 0.185 | Acc: 96.094% (123/128)
[Test] Epoch= 14  BatchID= 10 Loss: 0.367 | Acc: 88.778% (1250/1408)
[Test] Epoch= 14  BatchID= 20 Loss: 0.359 | Acc: 89.100% (2395/2688)
[Test] Epoch= 14  BatchID= 30 Loss: 0.375 | Acc: 88.739% (3483/3925)

Epoch: 15
[Train] Epoch= 15  BatchID= 0 Loss: 0.223 | Acc: 95.312% (122/128)
[Train] Epoch= 15  BatchID= 10 Loss: 0.225 | Acc: 92.116% (1297/1408)
[Train] Epoch= 15  BatchID= 20 Loss: 0.221 | Acc: 92.411% (2484/2688)
[Train] Epoch= 15  BatchID= 30 Loss: 0.227 | Acc: 92.036% (3652/3968)
[Train] Epoch= 15  BatchID= 40 Loss: 0.224 | Acc: 92.435% (4851/5248)
[Train] Epoch= 15  BatchID= 50 Loss: 0.228 | Acc: 92.402% (6032/6528)
[Train] Epoch= 15  BatchID= 60 Loss: 0.233 | Acc: 92.444% (7218/7808)
[Train] Epoch= 15  BatchID= 70 Loss: 0.232 | Acc: 92.496% (8406/9088)
[Test] Epoch= 15  BatchID= 0 Loss: 0.131 | Acc: 96.875% (124/128)
[Test] Epoch= 15  BatchID= 10 Loss: 0.395 | Acc: 87.997% (1239/1408)
[Test] Epoch= 15  BatchID= 20 Loss: 0.365 | Acc: 88.951% (2391/2688)
[Test] Epoch= 15  BatchID= 30 Loss: 0.416 | Acc: 87.389% (3430/3925)

Epoch: 16
[Train] Epoch= 16  BatchID= 0 Loss: 0.207 | Acc: 94.531% (121/128)
[Train] Epoch= 16  BatchID= 10 Loss: 0.225 | Acc: 92.898% (1308/1408)
[Train] Epoch= 16  BatchID= 20 Loss: 0.225 | Acc: 92.857% (2496/2688)
[Train] Epoch= 16  BatchID= 30 Loss: 0.225 | Acc: 92.969% (3689/3968)
[Train] Epoch= 16  BatchID= 40 Loss: 0.223 | Acc: 93.007% (4881/5248)
[Train] Epoch= 16  BatchID= 50 Loss: 0.222 | Acc: 92.923% (6066/6528)
[Train] Epoch= 16  BatchID= 60 Loss: 0.224 | Acc: 92.918% (7255/7808)
[Train] Epoch= 16  BatchID= 70 Loss: 0.223 | Acc: 92.980% (8450/9088)
[Test] Epoch= 16  BatchID= 0 Loss: 0.153 | Acc: 96.094% (123/128)
[Test] Epoch= 16  BatchID= 10 Loss: 0.323 | Acc: 89.773% (1264/1408)
[Test] Epoch= 16  BatchID= 20 Loss: 0.336 | Acc: 89.695% (2411/2688)
[Test] Epoch= 16  BatchID= 30 Loss: 0.391 | Acc: 88.127% (3459/3925)

Epoch: 17
[Train] Epoch= 17  BatchID= 0 Loss: 0.230 | Acc: 92.969% (119/128)
[Train] Epoch= 17  BatchID= 10 Loss: 0.210 | Acc: 93.395% (1315/1408)
[Train] Epoch= 17  BatchID= 20 Loss: 0.217 | Acc: 93.118% (2503/2688)
[Train] Epoch= 17  BatchID= 30 Loss: 0.223 | Acc: 92.566% (3673/3968)
[Train] Epoch= 17  BatchID= 40 Loss: 0.228 | Acc: 92.530% (4856/5248)
[Train] Epoch= 17  BatchID= 50 Loss: 0.227 | Acc: 92.647% (6048/6528)
[Train] Epoch= 17  BatchID= 60 Loss: 0.231 | Acc: 92.533% (7225/7808)
[Train] Epoch= 17  BatchID= 70 Loss: 0.227 | Acc: 92.727% (8427/9088)
[Test] Epoch= 17  BatchID= 0 Loss: 0.140 | Acc: 96.875% (124/128)
[Test] Epoch= 17  BatchID= 10 Loss: 0.363 | Acc: 88.849% (1251/1408)
[Test] Epoch= 17  BatchID= 20 Loss: 0.364 | Acc: 88.802% (2387/2688)
[Test] Epoch= 17  BatchID= 30 Loss: 0.385 | Acc: 88.127% (3459/3925)

Epoch: 18
[Train] Epoch= 18  BatchID= 0 Loss: 0.230 | Acc: 91.406% (117/128)
[Train] Epoch= 18  BatchID= 10 Loss: 0.218 | Acc: 93.253% (1313/1408)
[Train] Epoch= 18  BatchID= 20 Loss: 0.212 | Acc: 93.155% (2504/2688)
[Train] Epoch= 18  BatchID= 30 Loss: 0.215 | Acc: 93.019% (3691/3968)
[Train] Epoch= 18  BatchID= 40 Loss: 0.215 | Acc: 93.026% (4882/5248)
[Train] Epoch= 18  BatchID= 50 Loss: 0.216 | Acc: 93.076% (6076/6528)
[Train] Epoch= 18  BatchID= 60 Loss: 0.214 | Acc: 93.212% (7278/7808)
[Train] Epoch= 18  BatchID= 70 Loss: 0.214 | Acc: 93.134% (8464/9088)
[Test] Epoch= 18  BatchID= 0 Loss: 0.186 | Acc: 95.312% (122/128)
[Test] Epoch= 18  BatchID= 10 Loss: 0.368 | Acc: 88.494% (1246/1408)
[Test] Epoch= 18  BatchID= 20 Loss: 0.352 | Acc: 88.951% (2391/2688)
[Test] Epoch= 18  BatchID= 30 Loss: 0.389 | Acc: 87.975% (3453/3925)

Epoch: 19
[Train] Epoch= 19  BatchID= 0 Loss: 0.276 | Acc: 91.406% (117/128)
[Train] Epoch= 19  BatchID= 10 Loss: 0.204 | Acc: 93.892% (1322/1408)
[Train] Epoch= 19  BatchID= 20 Loss: 0.202 | Acc: 93.973% (2526/2688)
[Train] Epoch= 19  BatchID= 30 Loss: 0.205 | Acc: 93.800% (3722/3968)
[Train] Epoch= 19  BatchID= 40 Loss: 0.208 | Acc: 93.636% (4914/5248)
[Train] Epoch= 19  BatchID= 50 Loss: 0.213 | Acc: 93.581% (6109/6528)
[Train] Epoch= 19  BatchID= 60 Loss: 0.212 | Acc: 93.609% (7309/7808)
[Train] Epoch= 19  BatchID= 70 Loss: 0.213 | Acc: 93.453% (8493/9088)
[Test] Epoch= 19  BatchID= 0 Loss: 0.165 | Acc: 96.094% (123/128)
[Test] Epoch= 19  BatchID= 10 Loss: 0.366 | Acc: 89.134% (1255/1408)
[Test] Epoch= 19  BatchID= 20 Loss: 0.350 | Acc: 89.583% (2408/2688)
[Test] Epoch= 19  BatchID= 30 Loss: 0.407 | Acc: 88.102% (3458/3925)

Epoch: 20
[Train] Epoch= 20  BatchID= 0 Loss: 0.201 | Acc: 92.969% (119/128)
[Train] Epoch= 20  BatchID= 10 Loss: 0.222 | Acc: 92.685% (1305/1408)
[Train] Epoch= 20  BatchID= 20 Loss: 0.208 | Acc: 93.266% (2507/2688)
[Train] Epoch= 20  BatchID= 30 Loss: 0.218 | Acc: 93.044% (3692/3968)
[Train] Epoch= 20  BatchID= 40 Loss: 0.214 | Acc: 93.293% (4896/5248)
[Train] Epoch= 20  BatchID= 50 Loss: 0.215 | Acc: 93.306% (6091/6528)
[Train] Epoch= 20  BatchID= 60 Loss: 0.220 | Acc: 93.110% (7270/7808)
[Train] Epoch= 20  BatchID= 70 Loss: 0.218 | Acc: 93.134% (8464/9088)
[Test] Epoch= 20  BatchID= 0 Loss: 0.094 | Acc: 97.656% (125/128)
[Test] Epoch= 20  BatchID= 10 Loss: 0.353 | Acc: 88.494% (1246/1408)
[Test] Epoch= 20  BatchID= 20 Loss: 0.349 | Acc: 89.025% (2393/2688)
[Test] Epoch= 20  BatchID= 30 Loss: 0.387 | Acc: 88.051% (3456/3925)

Epoch: 21
[Train] Epoch= 21  BatchID= 0 Loss: 0.235 | Acc: 91.406% (117/128)
[Train] Epoch= 21  BatchID= 10 Loss: 0.217 | Acc: 93.111% (1311/1408)
[Train] Epoch= 21  BatchID= 20 Loss: 0.214 | Acc: 93.192% (2505/2688)
[Train] Epoch= 21  BatchID= 30 Loss: 0.217 | Acc: 93.120% (3695/3968)
[Train] Epoch= 21  BatchID= 40 Loss: 0.217 | Acc: 93.121% (4887/5248)
[Train] Epoch= 21  BatchID= 50 Loss: 0.212 | Acc: 93.490% (6103/6528)
[Train] Epoch= 21  BatchID= 60 Loss: 0.209 | Acc: 93.494% (7300/7808)
[Train] Epoch= 21  BatchID= 70 Loss: 0.208 | Acc: 93.497% (8497/9088)
[Test] Epoch= 21  BatchID= 0 Loss: 0.175 | Acc: 96.094% (123/128)
[Test] Epoch= 21  BatchID= 10 Loss: 0.371 | Acc: 88.281% (1243/1408)
[Test] Epoch= 21  BatchID= 20 Loss: 0.369 | Acc: 88.393% (2376/2688)
[Test] Epoch= 21  BatchID= 30 Loss: 0.401 | Acc: 87.720% (3443/3925)

Epoch: 22
[Train] Epoch= 22  BatchID= 0 Loss: 0.243 | Acc: 94.531% (121/128)
[Train] Epoch= 22  BatchID= 10 Loss: 0.217 | Acc: 94.034% (1324/1408)
[Train] Epoch= 22  BatchID= 20 Loss: 0.219 | Acc: 93.564% (2515/2688)
[Train] Epoch= 22  BatchID= 30 Loss: 0.220 | Acc: 93.473% (3709/3968)
[Train] Epoch= 22  BatchID= 40 Loss: 0.220 | Acc: 93.445% (4904/5248)
[Train] Epoch= 22  BatchID= 50 Loss: 0.216 | Acc: 93.444% (6100/6528)
[Train] Epoch= 22  BatchID= 60 Loss: 0.215 | Acc: 93.443% (7296/7808)
[Train] Epoch= 22  BatchID= 70 Loss: 0.211 | Acc: 93.596% (8506/9088)
[Test] Epoch= 22  BatchID= 0 Loss: 0.172 | Acc: 94.531% (121/128)
[Test] Epoch= 22  BatchID= 10 Loss: 0.382 | Acc: 88.281% (1243/1408)
[Test] Epoch= 22  BatchID= 20 Loss: 0.376 | Acc: 88.430% (2377/2688)
[Test] Epoch= 22  BatchID= 30 Loss: 0.416 | Acc: 87.541% (3436/3925)

Epoch: 23
[Train] Epoch= 23  BatchID= 0 Loss: 0.185 | Acc: 95.312% (122/128)
[Train] Epoch= 23  BatchID= 10 Loss: 0.223 | Acc: 92.685% (1305/1408)
[Train] Epoch= 23  BatchID= 20 Loss: 0.219 | Acc: 92.932% (2498/2688)
[Train] Epoch= 23  BatchID= 30 Loss: 0.214 | Acc: 93.322% (3703/3968)
[Train] Epoch= 23  BatchID= 40 Loss: 0.216 | Acc: 93.255% (4894/5248)
[Train] Epoch= 23  BatchID= 50 Loss: 0.214 | Acc: 93.244% (6087/6528)
[Train] Epoch= 23  BatchID= 60 Loss: 0.209 | Acc: 93.443% (7296/7808)
[Train] Epoch= 23  BatchID= 70 Loss: 0.210 | Acc: 93.530% (8500/9088)
[Test] Epoch= 23  BatchID= 0 Loss: 0.066 | Acc: 98.438% (126/128)
[Test] Epoch= 23  BatchID= 10 Loss: 0.344 | Acc: 89.560% (1261/1408)
[Test] Epoch= 23  BatchID= 20 Loss: 0.345 | Acc: 89.695% (2411/2688)
[Test] Epoch= 23  BatchID= 30 Loss: 0.381 | Acc: 88.611% (3478/3925)

Epoch: 24
[Train] Epoch= 24  BatchID= 0 Loss: 0.251 | Acc: 92.969% (119/128)
[Train] Epoch= 24  BatchID= 10 Loss: 0.182 | Acc: 94.176% (1326/1408)
[Train] Epoch= 24  BatchID= 20 Loss: 0.185 | Acc: 94.308% (2535/2688)
[Train] Epoch= 24  BatchID= 30 Loss: 0.188 | Acc: 94.229% (3739/3968)
[Train] Epoch= 24  BatchID= 40 Loss: 0.194 | Acc: 94.093% (4938/5248)
[Train] Epoch= 24  BatchID= 50 Loss: 0.197 | Acc: 93.857% (6127/6528)
[Train] Epoch= 24  BatchID= 60 Loss: 0.197 | Acc: 93.776% (7322/7808)
[Train] Epoch= 24  BatchID= 70 Loss: 0.195 | Acc: 93.871% (8531/9088)
[Test] Epoch= 24  BatchID= 0 Loss: 0.183 | Acc: 93.750% (120/128)
[Test] Epoch= 24  BatchID= 10 Loss: 0.379 | Acc: 88.565% (1247/1408)
[Test] Epoch= 24  BatchID= 20 Loss: 0.360 | Acc: 89.137% (2396/2688)
[Test] Epoch= 24  BatchID= 30 Loss: 0.395 | Acc: 88.255% (3464/3925)

Epoch: 25
[Train] Epoch= 25  BatchID= 0 Loss: 0.210 | Acc: 96.094% (123/128)
[Train] Epoch= 25  BatchID= 10 Loss: 0.206 | Acc: 93.821% (1321/1408)
[Train] Epoch= 25  BatchID= 20 Loss: 0.193 | Acc: 94.010% (2527/2688)
[Train] Epoch= 25  BatchID= 30 Loss: 0.193 | Acc: 93.750% (3720/3968)
[Train] Epoch= 25  BatchID= 40 Loss: 0.206 | Acc: 93.331% (4898/5248)
[Train] Epoch= 25  BatchID= 50 Loss: 0.204 | Acc: 93.490% (6103/6528)
[Train] Epoch= 25  BatchID= 60 Loss: 0.203 | Acc: 93.635% (7311/7808)
[Train] Epoch= 25  BatchID= 70 Loss: 0.206 | Acc: 93.607% (8507/9088)
[Test] Epoch= 25  BatchID= 0 Loss: 0.113 | Acc: 96.875% (124/128)
[Test] Epoch= 25  BatchID= 10 Loss: 0.387 | Acc: 87.855% (1237/1408)
[Test] Epoch= 25  BatchID= 20 Loss: 0.353 | Acc: 89.211% (2398/2688)
[Test] Epoch= 25  BatchID= 30 Loss: 0.370 | Acc: 88.611% (3478/3925)

Epoch: 26
[Train] Epoch= 26  BatchID= 0 Loss: 0.195 | Acc: 95.312% (122/128)
[Train] Epoch= 26  BatchID= 10 Loss: 0.169 | Acc: 95.241% (1341/1408)
[Train] Epoch= 26  BatchID= 20 Loss: 0.179 | Acc: 94.643% (2544/2688)
[Train] Epoch= 26  BatchID= 30 Loss: 0.188 | Acc: 93.977% (3729/3968)
[Train] Epoch= 26  BatchID= 40 Loss: 0.190 | Acc: 93.941% (4930/5248)
[Train] Epoch= 26  BatchID= 50 Loss: 0.192 | Acc: 93.949% (6133/6528)
[Train] Epoch= 26  BatchID= 60 Loss: 0.192 | Acc: 94.006% (7340/7808)
[Train] Epoch= 26  BatchID= 70 Loss: 0.194 | Acc: 93.970% (8540/9088)
[Test] Epoch= 26  BatchID= 0 Loss: 0.162 | Acc: 95.312% (122/128)
[Test] Epoch= 26  BatchID= 10 Loss: 0.390 | Acc: 89.276% (1257/1408)
[Test] Epoch= 26  BatchID= 20 Loss: 0.358 | Acc: 89.658% (2410/2688)
[Test] Epoch= 26  BatchID= 30 Loss: 0.385 | Acc: 88.866% (3488/3925)
Saving..
Best accuracy:  88.86624203821655

Epoch: 27
[Train] Epoch= 27  BatchID= 0 Loss: 0.127 | Acc: 95.312% (122/128)
[Train] Epoch= 27  BatchID= 10 Loss: 0.180 | Acc: 93.821% (1321/1408)
[Train] Epoch= 27  BatchID= 20 Loss: 0.180 | Acc: 94.196% (2532/2688)
[Train] Epoch= 27  BatchID= 30 Loss: 0.179 | Acc: 94.304% (3742/3968)
[Train] Epoch= 27  BatchID= 40 Loss: 0.182 | Acc: 94.245% (4946/5248)
[Train] Epoch= 27  BatchID= 50 Loss: 0.181 | Acc: 94.225% (6151/6528)
[Train] Epoch= 27  BatchID= 60 Loss: 0.186 | Acc: 94.134% (7350/7808)
[Train] Epoch= 27  BatchID= 70 Loss: 0.187 | Acc: 94.168% (8558/9088)
[Test] Epoch= 27  BatchID= 0 Loss: 0.170 | Acc: 95.312% (122/128)
[Test] Epoch= 27  BatchID= 10 Loss: 0.365 | Acc: 89.062% (1254/1408)
[Test] Epoch= 27  BatchID= 20 Loss: 0.342 | Acc: 89.621% (2409/2688)
[Test] Epoch= 27  BatchID= 30 Loss: 0.414 | Acc: 87.618% (3439/3925)

Epoch: 28
[Train] Epoch= 28  BatchID= 0 Loss: 0.124 | Acc: 95.312% (122/128)
[Train] Epoch= 28  BatchID= 10 Loss: 0.193 | Acc: 93.963% (1323/1408)
[Train] Epoch= 28  BatchID= 20 Loss: 0.200 | Acc: 94.085% (2529/2688)
[Train] Epoch= 28  BatchID= 30 Loss: 0.197 | Acc: 93.952% (3728/3968)
[Train] Epoch= 28  BatchID= 40 Loss: 0.202 | Acc: 93.826% (4924/5248)
[Train] Epoch= 28  BatchID= 50 Loss: 0.205 | Acc: 93.612% (6111/6528)
[Train] Epoch= 28  BatchID= 60 Loss: 0.203 | Acc: 93.750% (7320/7808)
[Train] Epoch= 28  BatchID= 70 Loss: 0.202 | Acc: 93.717% (8517/9088)
[Test] Epoch= 28  BatchID= 0 Loss: 0.189 | Acc: 94.531% (121/128)
[Test] Epoch= 28  BatchID= 10 Loss: 0.364 | Acc: 88.849% (1251/1408)
[Test] Epoch= 28  BatchID= 20 Loss: 0.360 | Acc: 88.951% (2391/2688)
[Test] Epoch= 28  BatchID= 30 Loss: 0.383 | Acc: 88.382% (3469/3925)

Epoch: 29
[Train] Epoch= 29  BatchID= 0 Loss: 0.116 | Acc: 98.438% (126/128)
[Train] Epoch= 29  BatchID= 10 Loss: 0.197 | Acc: 94.034% (1324/1408)
[Train] Epoch= 29  BatchID= 20 Loss: 0.197 | Acc: 93.973% (2526/2688)
[Train] Epoch= 29  BatchID= 30 Loss: 0.199 | Acc: 93.700% (3718/3968)
[Train] Epoch= 29  BatchID= 40 Loss: 0.199 | Acc: 93.617% (4913/5248)
[Train] Epoch= 29  BatchID= 50 Loss: 0.203 | Acc: 93.520% (6105/6528)
[Train] Epoch= 29  BatchID= 60 Loss: 0.205 | Acc: 93.468% (7298/7808)
[Train] Epoch= 29  BatchID= 70 Loss: 0.205 | Acc: 93.442% (8492/9088)
[Test] Epoch= 29  BatchID= 0 Loss: 0.106 | Acc: 96.875% (124/128)
[Test] Epoch= 29  BatchID= 10 Loss: 0.392 | Acc: 88.423% (1245/1408)
[Test] Epoch= 29  BatchID= 20 Loss: 0.367 | Acc: 88.876% (2389/2688)
[Test] Epoch= 29  BatchID= 30 Loss: 0.375 | Acc: 88.433% (3471/3925)

Epoch: 30
[Train] Epoch= 30  BatchID= 0 Loss: 0.187 | Acc: 94.531% (121/128)
[Train] Epoch= 30  BatchID= 10 Loss: 0.206 | Acc: 93.608% (1318/1408)
[Train] Epoch= 30  BatchID= 20 Loss: 0.192 | Acc: 94.010% (2527/2688)
[Train] Epoch= 30  BatchID= 30 Loss: 0.205 | Acc: 93.448% (3708/3968)
[Train] Epoch= 30  BatchID= 40 Loss: 0.203 | Acc: 93.502% (4907/5248)
[Train] Epoch= 30  BatchID= 50 Loss: 0.205 | Acc: 93.474% (6102/6528)
[Train] Epoch= 30  BatchID= 60 Loss: 0.204 | Acc: 93.519% (7302/7808)
[Train] Epoch= 30  BatchID= 70 Loss: 0.201 | Acc: 93.618% (8508/9088)
[Test] Epoch= 30  BatchID= 0 Loss: 0.085 | Acc: 97.656% (125/128)
[Test] Epoch= 30  BatchID= 10 Loss: 0.322 | Acc: 89.915% (1266/1408)
[Test] Epoch= 30  BatchID= 20 Loss: 0.325 | Acc: 89.881% (2416/2688)
[Test] Epoch= 30  BatchID= 30 Loss: 0.365 | Acc: 89.019% (3494/3925)
Saving..
Best accuracy:  89.01910828025478

Epoch: 31
[Train] Epoch= 31  BatchID= 0 Loss: 0.097 | Acc: 96.875% (124/128)
[Train] Epoch= 31  BatchID= 10 Loss: 0.175 | Acc: 94.389% (1329/1408)
[Train] Epoch= 31  BatchID= 20 Loss: 0.182 | Acc: 94.159% (2531/2688)
[Train] Epoch= 31  BatchID= 30 Loss: 0.192 | Acc: 94.128% (3735/3968)
[Train] Epoch= 31  BatchID= 40 Loss: 0.190 | Acc: 94.284% (4948/5248)
[Train] Epoch= 31  BatchID= 50 Loss: 0.184 | Acc: 94.409% (6163/6528)
[Train] Epoch= 31  BatchID= 60 Loss: 0.190 | Acc: 94.045% (7343/7808)
[Train] Epoch= 31  BatchID= 70 Loss: 0.190 | Acc: 94.014% (8544/9088)
[Test] Epoch= 31  BatchID= 0 Loss: 0.095 | Acc: 96.875% (124/128)
[Test] Epoch= 31  BatchID= 10 Loss: 0.392 | Acc: 88.068% (1240/1408)
[Test] Epoch= 31  BatchID= 20 Loss: 0.336 | Acc: 89.695% (2411/2688)
[Test] Epoch= 31  BatchID= 30 Loss: 0.369 | Acc: 88.790% (3485/3925)

Epoch: 32
[Train] Epoch= 32  BatchID= 0 Loss: 0.113 | Acc: 96.875% (124/128)
[Train] Epoch= 32  BatchID= 10 Loss: 0.147 | Acc: 95.099% (1339/1408)
[Train] Epoch= 32  BatchID= 20 Loss: 0.161 | Acc: 94.680% (2545/2688)
[Train] Epoch= 32  BatchID= 30 Loss: 0.172 | Acc: 94.355% (3744/3968)
[Train] Epoch= 32  BatchID= 40 Loss: 0.180 | Acc: 94.074% (4937/5248)
[Train] Epoch= 32  BatchID= 50 Loss: 0.182 | Acc: 93.964% (6134/6528)
[Train] Epoch= 32  BatchID= 60 Loss: 0.183 | Acc: 94.096% (7347/7808)
[Train] Epoch= 32  BatchID= 70 Loss: 0.182 | Acc: 94.135% (8555/9088)
[Test] Epoch= 32  BatchID= 0 Loss: 0.073 | Acc: 97.656% (125/128)
[Test] Epoch= 32  BatchID= 10 Loss: 0.340 | Acc: 89.489% (1260/1408)
[Test] Epoch= 32  BatchID= 20 Loss: 0.345 | Acc: 89.211% (2398/2688)
[Test] Epoch= 32  BatchID= 30 Loss: 0.366 | Acc: 88.535% (3475/3925)

Epoch: 33
[Train] Epoch= 33  BatchID= 0 Loss: 0.207 | Acc: 92.969% (119/128)
[Train] Epoch= 33  BatchID= 10 Loss: 0.179 | Acc: 93.537% (1317/1408)
[Train] Epoch= 33  BatchID= 20 Loss: 0.176 | Acc: 94.010% (2527/2688)
[Train] Epoch= 33  BatchID= 30 Loss: 0.191 | Acc: 93.599% (3714/3968)
[Train] Epoch= 33  BatchID= 40 Loss: 0.199 | Acc: 93.445% (4904/5248)
[Train] Epoch= 33  BatchID= 50 Loss: 0.200 | Acc: 93.444% (6100/6528)
[Train] Epoch= 33  BatchID= 60 Loss: 0.201 | Acc: 93.545% (7304/7808)
[Train] Epoch= 33  BatchID= 70 Loss: 0.201 | Acc: 93.464% (8494/9088)
[Test] Epoch= 33  BatchID= 0 Loss: 0.131 | Acc: 96.875% (124/128)
[Test] Epoch= 33  BatchID= 10 Loss: 0.365 | Acc: 88.920% (1252/1408)
[Test] Epoch= 33  BatchID= 20 Loss: 0.345 | Acc: 89.360% (2402/2688)
[Test] Epoch= 33  BatchID= 30 Loss: 0.371 | Acc: 88.637% (3479/3925)

Epoch: 34
[Train] Epoch= 34  BatchID= 0 Loss: 0.136 | Acc: 96.094% (123/128)
[Train] Epoch= 34  BatchID= 10 Loss: 0.187 | Acc: 94.531% (1331/1408)
[Train] Epoch= 34  BatchID= 20 Loss: 0.189 | Acc: 93.973% (2526/2688)
[Train] Epoch= 34  BatchID= 30 Loss: 0.186 | Acc: 94.002% (3730/3968)
[Train] Epoch= 34  BatchID= 40 Loss: 0.187 | Acc: 94.017% (4934/5248)
[Train] Epoch= 34  BatchID= 50 Loss: 0.186 | Acc: 94.133% (6145/6528)
[Train] Epoch= 34  BatchID= 60 Loss: 0.186 | Acc: 94.083% (7346/7808)
[Train] Epoch= 34  BatchID= 70 Loss: 0.183 | Acc: 94.223% (8563/9088)
[Test] Epoch= 34  BatchID= 0 Loss: 0.134 | Acc: 96.875% (124/128)
[Test] Epoch= 34  BatchID= 10 Loss: 0.347 | Acc: 89.062% (1254/1408)
[Test] Epoch= 34  BatchID= 20 Loss: 0.340 | Acc: 89.658% (2410/2688)
[Test] Epoch= 34  BatchID= 30 Loss: 0.372 | Acc: 88.968% (3492/3925)

Epoch: 35
[Train] Epoch= 35  BatchID= 0 Loss: 0.207 | Acc: 91.406% (117/128)
[Train] Epoch= 35  BatchID= 10 Loss: 0.177 | Acc: 94.815% (1335/1408)
[Train] Epoch= 35  BatchID= 20 Loss: 0.182 | Acc: 94.568% (2542/2688)
[Train] Epoch= 35  BatchID= 30 Loss: 0.179 | Acc: 94.531% (3751/3968)
[Train] Epoch= 35  BatchID= 40 Loss: 0.183 | Acc: 94.284% (4948/5248)
[Train] Epoch= 35  BatchID= 50 Loss: 0.191 | Acc: 94.026% (6138/6528)
[Train] Epoch= 35  BatchID= 60 Loss: 0.192 | Acc: 94.032% (7342/7808)
[Train] Epoch= 35  BatchID= 70 Loss: 0.194 | Acc: 94.036% (8546/9088)
[Test] Epoch= 35  BatchID= 0 Loss: 0.081 | Acc: 98.438% (126/128)
[Test] Epoch= 35  BatchID= 10 Loss: 0.354 | Acc: 89.205% (1256/1408)
[Test] Epoch= 35  BatchID= 20 Loss: 0.336 | Acc: 89.881% (2416/2688)
[Test] Epoch= 35  BatchID= 30 Loss: 0.374 | Acc: 88.815% (3486/3925)

Epoch: 36
[Train] Epoch= 36  BatchID= 0 Loss: 0.157 | Acc: 95.312% (122/128)
[Train] Epoch= 36  BatchID= 10 Loss: 0.170 | Acc: 94.815% (1335/1408)
[Train] Epoch= 36  BatchID= 20 Loss: 0.178 | Acc: 94.457% (2539/2688)
[Train] Epoch= 36  BatchID= 30 Loss: 0.174 | Acc: 94.481% (3749/3968)
[Train] Epoch= 36  BatchID= 40 Loss: 0.174 | Acc: 94.493% (4959/5248)
[Train] Epoch= 36  BatchID= 50 Loss: 0.174 | Acc: 94.562% (6173/6528)
[Train] Epoch= 36  BatchID= 60 Loss: 0.178 | Acc: 94.403% (7371/7808)
[Train] Epoch= 36  BatchID= 70 Loss: 0.178 | Acc: 94.564% (8594/9088)
[Test] Epoch= 36  BatchID= 0 Loss: 0.130 | Acc: 96.094% (123/128)
[Test] Epoch= 36  BatchID= 10 Loss: 0.351 | Acc: 89.418% (1259/1408)
[Test] Epoch= 36  BatchID= 20 Loss: 0.306 | Acc: 90.811% (2441/2688)
[Test] Epoch= 36  BatchID= 30 Loss: 0.358 | Acc: 89.325% (3506/3925)
Saving..
Best accuracy:  89.32484076433121

Epoch: 37
[Train] Epoch= 37  BatchID= 0 Loss: 0.095 | Acc: 96.875% (124/128)
[Train] Epoch= 37  BatchID= 10 Loss: 0.153 | Acc: 95.312% (1342/1408)
[Train] Epoch= 37  BatchID= 20 Loss: 0.173 | Acc: 94.680% (2545/2688)
[Train] Epoch= 37  BatchID= 30 Loss: 0.177 | Acc: 94.456% (3748/3968)
[Train] Epoch= 37  BatchID= 40 Loss: 0.174 | Acc: 94.607% (4965/5248)
[Train] Epoch= 37  BatchID= 50 Loss: 0.176 | Acc: 94.593% (6175/6528)
[Train] Epoch= 37  BatchID= 60 Loss: 0.178 | Acc: 94.480% (7377/7808)
[Train] Epoch= 37  BatchID= 70 Loss: 0.180 | Acc: 94.432% (8582/9088)
[Test] Epoch= 37  BatchID= 0 Loss: 0.096 | Acc: 97.656% (125/128)
[Test] Epoch= 37  BatchID= 10 Loss: 0.347 | Acc: 89.631% (1262/1408)
[Test] Epoch= 37  BatchID= 20 Loss: 0.340 | Acc: 89.621% (2409/2688)
[Test] Epoch= 37  BatchID= 30 Loss: 0.368 | Acc: 88.815% (3486/3925)

Epoch: 38
[Train] Epoch= 38  BatchID= 0 Loss: 0.136 | Acc: 95.312% (122/128)
[Train] Epoch= 38  BatchID= 10 Loss: 0.163 | Acc: 94.531% (1331/1408)
[Train] Epoch= 38  BatchID= 20 Loss: 0.176 | Acc: 94.159% (2531/2688)
[Train] Epoch= 38  BatchID= 30 Loss: 0.171 | Acc: 94.531% (3751/3968)
[Train] Epoch= 38  BatchID= 40 Loss: 0.166 | Acc: 94.836% (4977/5248)
[Train] Epoch= 38  BatchID= 50 Loss: 0.169 | Acc: 94.761% (6186/6528)
[Train] Epoch= 38  BatchID= 60 Loss: 0.175 | Acc: 94.557% (7383/7808)
[Train] Epoch= 38  BatchID= 70 Loss: 0.178 | Acc: 94.388% (8578/9088)
[Test] Epoch= 38  BatchID= 0 Loss: 0.092 | Acc: 97.656% (125/128)
[Test] Epoch= 38  BatchID= 10 Loss: 0.327 | Acc: 89.560% (1261/1408)
[Test] Epoch= 38  BatchID= 20 Loss: 0.308 | Acc: 90.365% (2429/2688)
[Test] Epoch= 38  BatchID= 30 Loss: 0.364 | Acc: 89.019% (3494/3925)

Epoch: 39
[Train] Epoch= 39  BatchID= 0 Loss: 0.274 | Acc: 89.844% (115/128)
[Train] Epoch= 39  BatchID= 10 Loss: 0.187 | Acc: 93.537% (1317/1408)
[Train] Epoch= 39  BatchID= 20 Loss: 0.192 | Acc: 93.824% (2522/2688)
[Train] Epoch= 39  BatchID= 30 Loss: 0.189 | Acc: 93.926% (3727/3968)
[Train] Epoch= 39  BatchID= 40 Loss: 0.186 | Acc: 94.036% (4935/5248)
[Train] Epoch= 39  BatchID= 50 Loss: 0.188 | Acc: 94.056% (6140/6528)
[Train] Epoch= 39  BatchID= 60 Loss: 0.186 | Acc: 94.173% (7353/7808)
[Train] Epoch= 39  BatchID= 70 Loss: 0.185 | Acc: 94.278% (8568/9088)
[Test] Epoch= 39  BatchID= 0 Loss: 0.103 | Acc: 96.875% (124/128)
[Test] Epoch= 39  BatchID= 10 Loss: 0.365 | Acc: 90.270% (1271/1408)
[Test] Epoch= 39  BatchID= 20 Loss: 0.333 | Acc: 90.551% (2434/2688)
[Test] Epoch= 39  BatchID= 30 Loss: 0.368 | Acc: 89.478% (3512/3925)
Saving..
Best accuracy:  89.47770700636943

Epoch: 40
[Train] Epoch= 40  BatchID= 0 Loss: 0.110 | Acc: 96.875% (124/128)
[Train] Epoch= 40  BatchID= 10 Loss: 0.179 | Acc: 94.389% (1329/1408)
[Train] Epoch= 40  BatchID= 20 Loss: 0.170 | Acc: 94.643% (2544/2688)
[Train] Epoch= 40  BatchID= 30 Loss: 0.177 | Acc: 94.430% (3747/3968)
[Train] Epoch= 40  BatchID= 40 Loss: 0.178 | Acc: 94.436% (4956/5248)
[Train] Epoch= 40  BatchID= 50 Loss: 0.176 | Acc: 94.424% (6164/6528)
[Train] Epoch= 40  BatchID= 60 Loss: 0.178 | Acc: 94.378% (7369/7808)
[Train] Epoch= 40  BatchID= 70 Loss: 0.177 | Acc: 94.388% (8578/9088)
[Test] Epoch= 40  BatchID= 0 Loss: 0.121 | Acc: 96.875% (124/128)
[Test] Epoch= 40  BatchID= 10 Loss: 0.356 | Acc: 89.418% (1259/1408)
[Test] Epoch= 40  BatchID= 20 Loss: 0.325 | Acc: 89.918% (2417/2688)
[Test] Epoch= 40  BatchID= 30 Loss: 0.353 | Acc: 89.197% (3501/3925)

Epoch: 41
[Train] Epoch= 41  BatchID= 0 Loss: 0.204 | Acc: 92.969% (119/128)
[Train] Epoch= 41  BatchID= 10 Loss: 0.168 | Acc: 94.886% (1336/1408)
[Train] Epoch= 41  BatchID= 20 Loss: 0.182 | Acc: 94.345% (2536/2688)
[Train] Epoch= 41  BatchID= 30 Loss: 0.184 | Acc: 94.430% (3747/3968)
[Train] Epoch= 41  BatchID= 40 Loss: 0.183 | Acc: 94.493% (4959/5248)
[Train] Epoch= 41  BatchID= 50 Loss: 0.184 | Acc: 94.393% (6162/6528)
[Train] Epoch= 41  BatchID= 60 Loss: 0.186 | Acc: 94.314% (7364/7808)
[Train] Epoch= 41  BatchID= 70 Loss: 0.184 | Acc: 94.322% (8572/9088)
[Test] Epoch= 41  BatchID= 0 Loss: 0.154 | Acc: 95.312% (122/128)
[Test] Epoch= 41  BatchID= 10 Loss: 0.365 | Acc: 89.489% (1260/1408)
[Test] Epoch= 41  BatchID= 20 Loss: 0.337 | Acc: 89.881% (2416/2688)
[Test] Epoch= 41  BatchID= 30 Loss: 0.369 | Acc: 88.713% (3482/3925)

Epoch: 42
[Train] Epoch= 42  BatchID= 0 Loss: 0.156 | Acc: 95.312% (122/128)
[Train] Epoch= 42  BatchID= 10 Loss: 0.166 | Acc: 94.957% (1337/1408)
[Train] Epoch= 42  BatchID= 20 Loss: 0.170 | Acc: 95.164% (2558/2688)
[Train] Epoch= 42  BatchID= 30 Loss: 0.165 | Acc: 95.237% (3779/3968)
[Train] Epoch= 42  BatchID= 40 Loss: 0.171 | Acc: 94.874% (4979/5248)
[Train] Epoch= 42  BatchID= 50 Loss: 0.177 | Acc: 94.608% (6176/6528)
[Train] Epoch= 42  BatchID= 60 Loss: 0.176 | Acc: 94.621% (7388/7808)
[Train] Epoch= 42  BatchID= 70 Loss: 0.176 | Acc: 94.619% (8599/9088)
[Test] Epoch= 42  BatchID= 0 Loss: 0.140 | Acc: 96.094% (123/128)
[Test] Epoch= 42  BatchID= 10 Loss: 0.382 | Acc: 89.062% (1254/1408)
[Test] Epoch= 42  BatchID= 20 Loss: 0.339 | Acc: 90.104% (2422/2688)
[Test] Epoch= 42  BatchID= 30 Loss: 0.369 | Acc: 89.223% (3502/3925)

Epoch: 43
[Train] Epoch= 43  BatchID= 0 Loss: 0.245 | Acc: 90.625% (116/128)
[Train] Epoch= 43  BatchID= 10 Loss: 0.194 | Acc: 94.105% (1325/1408)
[Train] Epoch= 43  BatchID= 20 Loss: 0.176 | Acc: 94.643% (2544/2688)
[Train] Epoch= 43  BatchID= 30 Loss: 0.176 | Acc: 94.582% (3753/3968)
[Train] Epoch= 43  BatchID= 40 Loss: 0.179 | Acc: 94.417% (4955/5248)
[Train] Epoch= 43  BatchID= 50 Loss: 0.180 | Acc: 94.363% (6160/6528)
[Train] Epoch= 43  BatchID= 60 Loss: 0.178 | Acc: 94.429% (7373/7808)
[Train] Epoch= 43  BatchID= 70 Loss: 0.180 | Acc: 94.421% (8581/9088)
[Test] Epoch= 43  BatchID= 0 Loss: 0.114 | Acc: 96.875% (124/128)
[Test] Epoch= 43  BatchID= 10 Loss: 0.337 | Acc: 89.986% (1267/1408)
[Test] Epoch= 43  BatchID= 20 Loss: 0.332 | Acc: 90.141% (2423/2688)
[Test] Epoch= 43  BatchID= 30 Loss: 0.376 | Acc: 88.790% (3485/3925)

Epoch: 44
[Train] Epoch= 44  BatchID= 0 Loss: 0.185 | Acc: 94.531% (121/128)
[Train] Epoch= 44  BatchID= 10 Loss: 0.155 | Acc: 95.170% (1340/1408)
[Train] Epoch= 44  BatchID= 20 Loss: 0.173 | Acc: 94.643% (2544/2688)
[Train] Epoch= 44  BatchID= 30 Loss: 0.176 | Acc: 94.682% (3757/3968)
[Train] Epoch= 44  BatchID= 40 Loss: 0.172 | Acc: 94.950% (4983/5248)
[Train] Epoch= 44  BatchID= 50 Loss: 0.170 | Acc: 94.960% (6199/6528)
[Train] Epoch= 44  BatchID= 60 Loss: 0.171 | Acc: 94.864% (7407/7808)
[Train] Epoch= 44  BatchID= 70 Loss: 0.174 | Acc: 94.729% (8609/9088)
[Test] Epoch= 44  BatchID= 0 Loss: 0.092 | Acc: 97.656% (125/128)
[Test] Epoch= 44  BatchID= 10 Loss: 0.378 | Acc: 88.849% (1251/1408)
[Test] Epoch= 44  BatchID= 20 Loss: 0.351 | Acc: 89.397% (2403/2688)
[Test] Epoch= 44  BatchID= 30 Loss: 0.365 | Acc: 89.070% (3496/3925)

Epoch: 45
[Train] Epoch= 45  BatchID= 0 Loss: 0.188 | Acc: 92.969% (119/128)
[Train] Epoch= 45  BatchID= 10 Loss: 0.142 | Acc: 95.739% (1348/1408)
[Train] Epoch= 45  BatchID= 20 Loss: 0.156 | Acc: 95.238% (2560/2688)
[Train] Epoch= 45  BatchID= 30 Loss: 0.157 | Acc: 95.287% (3781/3968)
[Train] Epoch= 45  BatchID= 40 Loss: 0.165 | Acc: 95.046% (4988/5248)
[Train] Epoch= 45  BatchID= 50 Loss: 0.170 | Acc: 94.853% (6192/6528)
[Train] Epoch= 45  BatchID= 60 Loss: 0.166 | Acc: 95.018% (7419/7808)
[Train] Epoch= 45  BatchID= 70 Loss: 0.165 | Acc: 94.938% (8628/9088)
[Test] Epoch= 45  BatchID= 0 Loss: 0.136 | Acc: 96.875% (124/128)
[Test] Epoch= 45  BatchID= 10 Loss: 0.337 | Acc: 90.412% (1273/1408)
[Test] Epoch= 45  BatchID= 20 Loss: 0.309 | Acc: 90.551% (2434/2688)
[Test] Epoch= 45  BatchID= 30 Loss: 0.371 | Acc: 88.841% (3487/3925)

Epoch: 46
[Train] Epoch= 46  BatchID= 0 Loss: 0.209 | Acc: 92.969% (119/128)
[Train] Epoch= 46  BatchID= 10 Loss: 0.165 | Acc: 94.673% (1333/1408)
[Train] Epoch= 46  BatchID= 20 Loss: 0.167 | Acc: 94.680% (2545/2688)
[Train] Epoch= 46  BatchID= 30 Loss: 0.164 | Acc: 94.884% (3765/3968)
[Train] Epoch= 46  BatchID= 40 Loss: 0.168 | Acc: 94.722% (4971/5248)
[Train] Epoch= 46  BatchID= 50 Loss: 0.164 | Acc: 94.838% (6191/6528)
[Train] Epoch= 46  BatchID= 60 Loss: 0.163 | Acc: 94.826% (7404/7808)
[Train] Epoch= 46  BatchID= 70 Loss: 0.164 | Acc: 94.773% (8613/9088)
[Test] Epoch= 46  BatchID= 0 Loss: 0.107 | Acc: 97.656% (125/128)
[Test] Epoch= 46  BatchID= 10 Loss: 0.355 | Acc: 89.489% (1260/1408)
[Test] Epoch= 46  BatchID= 20 Loss: 0.310 | Acc: 90.699% (2438/2688)
[Test] Epoch= 46  BatchID= 30 Loss: 0.349 | Acc: 89.656% (3519/3925)
Saving..
Best accuracy:  89.65605095541402

Epoch: 47
[Train] Epoch= 47  BatchID= 0 Loss: 0.080 | Acc: 96.875% (124/128)
[Train] Epoch= 47  BatchID= 10 Loss: 0.159 | Acc: 95.241% (1341/1408)
[Train] Epoch= 47  BatchID= 20 Loss: 0.169 | Acc: 95.052% (2555/2688)
[Train] Epoch= 47  BatchID= 30 Loss: 0.162 | Acc: 95.237% (3779/3968)
[Train] Epoch= 47  BatchID= 40 Loss: 0.162 | Acc: 95.141% (4993/5248)
[Train] Epoch= 47  BatchID= 50 Loss: 0.159 | Acc: 95.037% (6204/6528)
[Train] Epoch= 47  BatchID= 60 Loss: 0.162 | Acc: 94.967% (7415/7808)
[Train] Epoch= 47  BatchID= 70 Loss: 0.161 | Acc: 94.905% (8625/9088)
[Test] Epoch= 47  BatchID= 0 Loss: 0.137 | Acc: 96.094% (123/128)
[Test] Epoch= 47  BatchID= 10 Loss: 0.350 | Acc: 89.631% (1262/1408)
[Test] Epoch= 47  BatchID= 20 Loss: 0.317 | Acc: 90.513% (2433/2688)
[Test] Epoch= 47  BatchID= 30 Loss: 0.370 | Acc: 89.223% (3502/3925)

Epoch: 48
[Train] Epoch= 48  BatchID= 0 Loss: 0.104 | Acc: 99.219% (127/128)
[Train] Epoch= 48  BatchID= 10 Loss: 0.158 | Acc: 95.455% (1344/1408)
[Train] Epoch= 48  BatchID= 20 Loss: 0.173 | Acc: 94.866% (2550/2688)
[Train] Epoch= 48  BatchID= 30 Loss: 0.167 | Acc: 95.086% (3773/3968)
[Train] Epoch= 48  BatchID= 40 Loss: 0.172 | Acc: 94.874% (4979/5248)
[Train] Epoch= 48  BatchID= 50 Loss: 0.168 | Acc: 95.067% (6206/6528)
[Train] Epoch= 48  BatchID= 60 Loss: 0.172 | Acc: 94.877% (7408/7808)
[Train] Epoch= 48  BatchID= 70 Loss: 0.171 | Acc: 94.806% (8616/9088)
[Test] Epoch= 48  BatchID= 0 Loss: 0.115 | Acc: 96.094% (123/128)
[Test] Epoch= 48  BatchID= 10 Loss: 0.339 | Acc: 89.702% (1263/1408)
[Test] Epoch= 48  BatchID= 20 Loss: 0.296 | Acc: 90.737% (2439/2688)
[Test] Epoch= 48  BatchID= 30 Loss: 0.349 | Acc: 89.350% (3507/3925)

Epoch: 49
[Train] Epoch= 49  BatchID= 0 Loss: 0.183 | Acc: 93.750% (120/128)
[Train] Epoch= 49  BatchID= 10 Loss: 0.152 | Acc: 95.384% (1343/1408)
[Train] Epoch= 49  BatchID= 20 Loss: 0.166 | Acc: 95.015% (2554/2688)
[Train] Epoch= 49  BatchID= 30 Loss: 0.159 | Acc: 95.237% (3779/3968)
[Train] Epoch= 49  BatchID= 40 Loss: 0.164 | Acc: 94.989% (4985/5248)
[Train] Epoch= 49  BatchID= 50 Loss: 0.162 | Acc: 94.991% (6201/6528)
[Train] Epoch= 49  BatchID= 60 Loss: 0.158 | Acc: 95.120% (7427/7808)
[Train] Epoch= 49  BatchID= 70 Loss: 0.161 | Acc: 95.070% (8640/9088)
[Test] Epoch= 49  BatchID= 0 Loss: 0.135 | Acc: 96.094% (123/128)
[Test] Epoch= 49  BatchID= 10 Loss: 0.350 | Acc: 89.347% (1258/1408)
[Test] Epoch= 49  BatchID= 20 Loss: 0.312 | Acc: 90.030% (2420/2688)
[Test] Epoch= 49  BatchID= 30 Loss: 0.356 | Acc: 88.917% (3490/3925)

Epoch: 50
[Train] Epoch= 50  BatchID= 0 Loss: 0.137 | Acc: 96.094% (123/128)
[Train] Epoch= 50  BatchID= 10 Loss: 0.183 | Acc: 94.389% (1329/1408)
[Train] Epoch= 50  BatchID= 20 Loss: 0.168 | Acc: 94.866% (2550/2688)
[Train] Epoch= 50  BatchID= 30 Loss: 0.169 | Acc: 95.060% (3772/3968)
[Train] Epoch= 50  BatchID= 40 Loss: 0.172 | Acc: 94.817% (4976/5248)
[Train] Epoch= 50  BatchID= 50 Loss: 0.164 | Acc: 95.037% (6204/6528)
[Train] Epoch= 50  BatchID= 60 Loss: 0.162 | Acc: 95.069% (7423/7808)
[Train] Epoch= 50  BatchID= 70 Loss: 0.162 | Acc: 94.982% (8632/9088)
[Test] Epoch= 50  BatchID= 0 Loss: 0.122 | Acc: 97.656% (125/128)
[Test] Epoch= 50  BatchID= 10 Loss: 0.323 | Acc: 90.483% (1274/1408)
[Test] Epoch= 50  BatchID= 20 Loss: 0.301 | Acc: 90.551% (2434/2688)
[Test] Epoch= 50  BatchID= 30 Loss: 0.353 | Acc: 89.554% (3515/3925)

Epoch: 51
[Train] Epoch= 51  BatchID= 0 Loss: 0.198 | Acc: 93.750% (120/128)
[Train] Epoch= 51  BatchID= 10 Loss: 0.172 | Acc: 94.460% (1330/1408)
[Train] Epoch= 51  BatchID= 20 Loss: 0.162 | Acc: 94.717% (2546/2688)
[Train] Epoch= 51  BatchID= 30 Loss: 0.171 | Acc: 94.355% (3744/3968)
[Train] Epoch= 51  BatchID= 40 Loss: 0.167 | Acc: 94.512% (4960/5248)
[Train] Epoch= 51  BatchID= 50 Loss: 0.167 | Acc: 94.623% (6177/6528)
[Train] Epoch= 51  BatchID= 60 Loss: 0.167 | Acc: 94.698% (7394/7808)
[Train] Epoch= 51  BatchID= 70 Loss: 0.164 | Acc: 94.938% (8628/9088)
[Test] Epoch= 51  BatchID= 0 Loss: 0.091 | Acc: 96.875% (124/128)
[Test] Epoch= 51  BatchID= 10 Loss: 0.318 | Acc: 90.696% (1277/1408)
[Test] Epoch= 51  BatchID= 20 Loss: 0.306 | Acc: 90.662% (2437/2688)
[Test] Epoch= 51  BatchID= 30 Loss: 0.355 | Acc: 89.631% (3518/3925)

Epoch: 52
[Train] Epoch= 52  BatchID= 0 Loss: 0.197 | Acc: 96.094% (123/128)
[Train] Epoch= 52  BatchID= 10 Loss: 0.176 | Acc: 94.744% (1334/1408)
[Train] Epoch= 52  BatchID= 20 Loss: 0.170 | Acc: 94.903% (2551/2688)
[Train] Epoch= 52  BatchID= 30 Loss: 0.166 | Acc: 94.859% (3764/3968)
[Train] Epoch= 52  BatchID= 40 Loss: 0.168 | Acc: 94.874% (4979/5248)
[Train] Epoch= 52  BatchID= 50 Loss: 0.166 | Acc: 94.899% (6195/6528)
[Train] Epoch= 52  BatchID= 60 Loss: 0.164 | Acc: 94.941% (7413/7808)
[Train] Epoch= 52  BatchID= 70 Loss: 0.162 | Acc: 94.883% (8623/9088)
[Test] Epoch= 52  BatchID= 0 Loss: 0.129 | Acc: 96.875% (124/128)
[Test] Epoch= 52  BatchID= 10 Loss: 0.348 | Acc: 89.347% (1258/1408)
[Test] Epoch= 52  BatchID= 20 Loss: 0.306 | Acc: 90.699% (2438/2688)
[Test] Epoch= 52  BatchID= 30 Loss: 0.353 | Acc: 89.580% (3516/3925)

Epoch: 53
[Train] Epoch= 53  BatchID= 0 Loss: 0.118 | Acc: 96.875% (124/128)
[Train] Epoch= 53  BatchID= 10 Loss: 0.170 | Acc: 94.744% (1334/1408)
[Train] Epoch= 53  BatchID= 20 Loss: 0.163 | Acc: 95.126% (2557/2688)
[Train] Epoch= 53  BatchID= 30 Loss: 0.153 | Acc: 95.312% (3782/3968)
[Train] Epoch= 53  BatchID= 40 Loss: 0.154 | Acc: 95.332% (5003/5248)
[Train] Epoch= 53  BatchID= 50 Loss: 0.151 | Acc: 95.404% (6228/6528)
[Train] Epoch= 53  BatchID= 60 Loss: 0.155 | Acc: 95.300% (7441/7808)
[Train] Epoch= 53  BatchID= 70 Loss: 0.156 | Acc: 95.257% (8657/9088)
[Test] Epoch= 53  BatchID= 0 Loss: 0.099 | Acc: 96.875% (124/128)
[Test] Epoch= 53  BatchID= 10 Loss: 0.341 | Acc: 89.631% (1262/1408)
[Test] Epoch= 53  BatchID= 20 Loss: 0.314 | Acc: 90.365% (2429/2688)
[Test] Epoch= 53  BatchID= 30 Loss: 0.344 | Acc: 89.732% (3522/3925)
Saving..
Best accuracy:  89.73248407643312

Epoch: 54
[Train] Epoch= 54  BatchID= 0 Loss: 0.154 | Acc: 96.094% (123/128)
[Train] Epoch= 54  BatchID= 10 Loss: 0.177 | Acc: 94.247% (1327/1408)
[Train] Epoch= 54  BatchID= 20 Loss: 0.166 | Acc: 94.568% (2542/2688)
[Train] Epoch= 54  BatchID= 30 Loss: 0.166 | Acc: 94.556% (3752/3968)
[Train] Epoch= 54  BatchID= 40 Loss: 0.169 | Acc: 94.550% (4962/5248)
[Train] Epoch= 54  BatchID= 50 Loss: 0.168 | Acc: 94.608% (6176/6528)
[Train] Epoch= 54  BatchID= 60 Loss: 0.163 | Acc: 94.723% (7396/7808)
[Train] Epoch= 54  BatchID= 70 Loss: 0.166 | Acc: 94.696% (8606/9088)
[Test] Epoch= 54  BatchID= 0 Loss: 0.122 | Acc: 96.875% (124/128)
[Test] Epoch= 54  BatchID= 10 Loss: 0.310 | Acc: 90.980% (1281/1408)
[Test] Epoch= 54  BatchID= 20 Loss: 0.295 | Acc: 90.923% (2444/2688)
[Test] Epoch= 54  BatchID= 30 Loss: 0.364 | Acc: 89.197% (3501/3925)

Epoch: 55
[Train] Epoch= 55  BatchID= 0 Loss: 0.262 | Acc: 95.312% (122/128)
[Train] Epoch= 55  BatchID= 10 Loss: 0.135 | Acc: 96.307% (1356/1408)
[Train] Epoch= 55  BatchID= 20 Loss: 0.136 | Acc: 96.057% (2582/2688)
[Train] Epoch= 55  BatchID= 30 Loss: 0.145 | Acc: 95.716% (3798/3968)
[Train] Epoch= 55  BatchID= 40 Loss: 0.147 | Acc: 95.713% (5023/5248)
[Train] Epoch= 55  BatchID= 50 Loss: 0.149 | Acc: 95.650% (6244/6528)
[Train] Epoch= 55  BatchID= 60 Loss: 0.152 | Acc: 95.428% (7451/7808)
[Train] Epoch= 55  BatchID= 70 Loss: 0.148 | Acc: 95.533% (8682/9088)
[Test] Epoch= 55  BatchID= 0 Loss: 0.110 | Acc: 97.656% (125/128)
[Test] Epoch= 55  BatchID= 10 Loss: 0.355 | Acc: 89.134% (1255/1408)
[Test] Epoch= 55  BatchID= 20 Loss: 0.323 | Acc: 90.067% (2421/2688)
[Test] Epoch= 55  BatchID= 30 Loss: 0.355 | Acc: 89.350% (3507/3925)

Epoch: 56
[Train] Epoch= 56  BatchID= 0 Loss: 0.124 | Acc: 96.094% (123/128)
[Train] Epoch= 56  BatchID= 10 Loss: 0.133 | Acc: 96.378% (1357/1408)
[Train] Epoch= 56  BatchID= 20 Loss: 0.136 | Acc: 95.796% (2575/2688)
[Train] Epoch= 56  BatchID= 30 Loss: 0.139 | Acc: 95.539% (3791/3968)
[Train] Epoch= 56  BatchID= 40 Loss: 0.142 | Acc: 95.522% (5013/5248)
[Train] Epoch= 56  BatchID= 50 Loss: 0.143 | Acc: 95.374% (6226/6528)
[Train] Epoch= 56  BatchID= 60 Loss: 0.148 | Acc: 95.287% (7440/7808)
[Train] Epoch= 56  BatchID= 70 Loss: 0.151 | Acc: 95.224% (8654/9088)
[Test] Epoch= 56  BatchID= 0 Loss: 0.139 | Acc: 96.875% (124/128)
[Test] Epoch= 56  BatchID= 10 Loss: 0.313 | Acc: 90.199% (1270/1408)
[Test] Epoch= 56  BatchID= 20 Loss: 0.297 | Acc: 90.588% (2435/2688)
[Test] Epoch= 56  BatchID= 30 Loss: 0.344 | Acc: 89.529% (3514/3925)

Epoch: 57
[Train] Epoch= 57  BatchID= 0 Loss: 0.114 | Acc: 96.875% (124/128)
[Train] Epoch= 57  BatchID= 10 Loss: 0.160 | Acc: 95.170% (1340/1408)
[Train] Epoch= 57  BatchID= 20 Loss: 0.161 | Acc: 95.201% (2559/2688)
[Train] Epoch= 57  BatchID= 30 Loss: 0.155 | Acc: 95.262% (3780/3968)
[Train] Epoch= 57  BatchID= 40 Loss: 0.164 | Acc: 95.008% (4986/5248)
[Train] Epoch= 57  BatchID= 50 Loss: 0.162 | Acc: 94.930% (6197/6528)
[Train] Epoch= 57  BatchID= 60 Loss: 0.159 | Acc: 95.120% (7427/7808)
[Train] Epoch= 57  BatchID= 70 Loss: 0.155 | Acc: 95.202% (8652/9088)
[Test] Epoch= 57  BatchID= 0 Loss: 0.121 | Acc: 97.656% (125/128)
[Test] Epoch= 57  BatchID= 10 Loss: 0.343 | Acc: 89.915% (1266/1408)
[Test] Epoch= 57  BatchID= 20 Loss: 0.309 | Acc: 90.551% (2434/2688)
[Test] Epoch= 57  BatchID= 30 Loss: 0.357 | Acc: 89.580% (3516/3925)

Epoch: 58
[Train] Epoch= 58  BatchID= 0 Loss: 0.120 | Acc: 95.312% (122/128)
[Train] Epoch= 58  BatchID= 10 Loss: 0.153 | Acc: 95.526% (1345/1408)
[Train] Epoch= 58  BatchID= 20 Loss: 0.157 | Acc: 95.089% (2556/2688)
[Train] Epoch= 58  BatchID= 30 Loss: 0.154 | Acc: 95.338% (3783/3968)
[Train] Epoch= 58  BatchID= 40 Loss: 0.152 | Acc: 95.484% (5011/5248)
[Train] Epoch= 58  BatchID= 50 Loss: 0.152 | Acc: 95.512% (6235/6528)
[Train] Epoch= 58  BatchID= 60 Loss: 0.151 | Acc: 95.441% (7452/7808)
[Train] Epoch= 58  BatchID= 70 Loss: 0.154 | Acc: 95.312% (8662/9088)
[Test] Epoch= 58  BatchID= 0 Loss: 0.128 | Acc: 96.875% (124/128)
[Test] Epoch= 58  BatchID= 10 Loss: 0.344 | Acc: 89.773% (1264/1408)
[Test] Epoch= 58  BatchID= 20 Loss: 0.313 | Acc: 90.216% (2425/2688)
[Test] Epoch= 58  BatchID= 30 Loss: 0.356 | Acc: 89.121% (3498/3925)

Epoch: 59
[Train] Epoch= 59  BatchID= 0 Loss: 0.167 | Acc: 95.312% (122/128)
[Train] Epoch= 59  BatchID= 10 Loss: 0.149 | Acc: 95.526% (1345/1408)
[Train] Epoch= 59  BatchID= 20 Loss: 0.147 | Acc: 95.722% (2573/2688)
[Train] Epoch= 59  BatchID= 30 Loss: 0.141 | Acc: 95.892% (3805/3968)
[Train] Epoch= 59  BatchID= 40 Loss: 0.142 | Acc: 95.884% (5032/5248)
[Train] Epoch= 59  BatchID= 50 Loss: 0.147 | Acc: 95.726% (6249/6528)
[Train] Epoch= 59  BatchID= 60 Loss: 0.155 | Acc: 95.402% (7449/7808)
[Train] Epoch= 59  BatchID= 70 Loss: 0.156 | Acc: 95.324% (8663/9088)
[Test] Epoch= 59  BatchID= 0 Loss: 0.160 | Acc: 96.875% (124/128)
[Test] Epoch= 59  BatchID= 10 Loss: 0.366 | Acc: 88.991% (1253/1408)
[Test] Epoch= 59  BatchID= 20 Loss: 0.322 | Acc: 90.179% (2424/2688)
[Test] Epoch= 59  BatchID= 30 Loss: 0.353 | Acc: 89.401% (3509/3925)

Epoch: 60
[Train] Epoch= 60  BatchID= 0 Loss: 0.187 | Acc: 95.312% (122/128)
[Train] Epoch= 60  BatchID= 10 Loss: 0.139 | Acc: 96.023% (1352/1408)
[Train] Epoch= 60  BatchID= 20 Loss: 0.139 | Acc: 95.908% (2578/2688)
[Train] Epoch= 60  BatchID= 30 Loss: 0.143 | Acc: 95.565% (3792/3968)
[Train] Epoch= 60  BatchID= 40 Loss: 0.138 | Acc: 95.694% (5022/5248)
[Train] Epoch= 60  BatchID= 50 Loss: 0.141 | Acc: 95.588% (6240/6528)
[Train] Epoch= 60  BatchID= 60 Loss: 0.144 | Acc: 95.543% (7460/7808)
[Train] Epoch= 60  BatchID= 70 Loss: 0.142 | Acc: 95.621% (8690/9088)
[Test] Epoch= 60  BatchID= 0 Loss: 0.111 | Acc: 97.656% (125/128)
[Test] Epoch= 60  BatchID= 10 Loss: 0.341 | Acc: 89.560% (1261/1408)
[Test] Epoch= 60  BatchID= 20 Loss: 0.312 | Acc: 90.513% (2433/2688)
[Test] Epoch= 60  BatchID= 30 Loss: 0.339 | Acc: 89.682% (3520/3925)

Epoch: 61
[Train] Epoch= 61  BatchID= 0 Loss: 0.146 | Acc: 95.312% (122/128)
[Train] Epoch= 61  BatchID= 10 Loss: 0.149 | Acc: 95.312% (1342/1408)
[Train] Epoch= 61  BatchID= 20 Loss: 0.145 | Acc: 95.759% (2574/2688)
[Train] Epoch= 61  BatchID= 30 Loss: 0.146 | Acc: 95.590% (3793/3968)
[Train] Epoch= 61  BatchID= 40 Loss: 0.149 | Acc: 95.446% (5009/5248)
[Train] Epoch= 61  BatchID= 50 Loss: 0.146 | Acc: 95.650% (6244/6528)
[Train] Epoch= 61  BatchID= 60 Loss: 0.150 | Acc: 95.441% (7452/7808)
[Train] Epoch= 61  BatchID= 70 Loss: 0.151 | Acc: 95.445% (8674/9088)
[Test] Epoch= 61  BatchID= 0 Loss: 0.128 | Acc: 97.656% (125/128)
[Test] Epoch= 61  BatchID= 10 Loss: 0.337 | Acc: 89.773% (1264/1408)
[Test] Epoch= 61  BatchID= 20 Loss: 0.317 | Acc: 90.141% (2423/2688)
[Test] Epoch= 61  BatchID= 30 Loss: 0.351 | Acc: 89.503% (3513/3925)

Epoch: 62
[Train] Epoch= 62  BatchID= 0 Loss: 0.103 | Acc: 98.438% (126/128)
[Train] Epoch= 62  BatchID= 10 Loss: 0.143 | Acc: 95.881% (1350/1408)
[Train] Epoch= 62  BatchID= 20 Loss: 0.161 | Acc: 95.275% (2561/2688)
[Train] Epoch= 62  BatchID= 30 Loss: 0.160 | Acc: 95.136% (3775/3968)
[Train] Epoch= 62  BatchID= 40 Loss: 0.160 | Acc: 94.989% (4985/5248)
[Train] Epoch= 62  BatchID= 50 Loss: 0.157 | Acc: 95.037% (6204/6528)
[Train] Epoch= 62  BatchID= 60 Loss: 0.153 | Acc: 95.236% (7436/7808)
[Train] Epoch= 62  BatchID= 70 Loss: 0.151 | Acc: 95.357% (8666/9088)
[Test] Epoch= 62  BatchID= 0 Loss: 0.132 | Acc: 96.875% (124/128)
[Test] Epoch= 62  BatchID= 10 Loss: 0.361 | Acc: 88.991% (1253/1408)
[Test] Epoch= 62  BatchID= 20 Loss: 0.314 | Acc: 90.179% (2424/2688)
[Test] Epoch= 62  BatchID= 30 Loss: 0.345 | Acc: 89.478% (3512/3925)

Epoch: 63
[Train] Epoch= 63  BatchID= 0 Loss: 0.117 | Acc: 97.656% (125/128)
[Train] Epoch= 63  BatchID= 10 Loss: 0.141 | Acc: 96.094% (1353/1408)
[Train] Epoch= 63  BatchID= 20 Loss: 0.156 | Acc: 95.350% (2563/2688)
[Train] Epoch= 63  BatchID= 30 Loss: 0.156 | Acc: 95.338% (3783/3968)
[Train] Epoch= 63  BatchID= 40 Loss: 0.152 | Acc: 95.503% (5012/5248)
[Train] Epoch= 63  BatchID= 50 Loss: 0.149 | Acc: 95.619% (6242/6528)
[Train] Epoch= 63  BatchID= 60 Loss: 0.148 | Acc: 95.569% (7462/7808)
[Train] Epoch= 63  BatchID= 70 Loss: 0.149 | Acc: 95.434% (8673/9088)
[Test] Epoch= 63  BatchID= 0 Loss: 0.140 | Acc: 96.875% (124/128)
[Test] Epoch= 63  BatchID= 10 Loss: 0.327 | Acc: 89.844% (1265/1408)
[Test] Epoch= 63  BatchID= 20 Loss: 0.296 | Acc: 90.848% (2442/2688)
[Test] Epoch= 63  BatchID= 30 Loss: 0.334 | Acc: 89.809% (3525/3925)
Saving..
Best accuracy:  89.80891719745223

Epoch: 64
[Train] Epoch= 64  BatchID= 0 Loss: 0.096 | Acc: 96.875% (124/128)
[Train] Epoch= 64  BatchID= 10 Loss: 0.139 | Acc: 95.810% (1349/1408)
[Train] Epoch= 64  BatchID= 20 Loss: 0.157 | Acc: 95.387% (2564/2688)
[Train] Epoch= 64  BatchID= 30 Loss: 0.146 | Acc: 95.615% (3794/3968)
[Train] Epoch= 64  BatchID= 40 Loss: 0.148 | Acc: 95.484% (5011/5248)
[Train] Epoch= 64  BatchID= 50 Loss: 0.145 | Acc: 95.466% (6232/6528)
[Train] Epoch= 64  BatchID= 60 Loss: 0.144 | Acc: 95.556% (7461/7808)
[Train] Epoch= 64  BatchID= 70 Loss: 0.141 | Acc: 95.632% (8691/9088)
[Test] Epoch= 64  BatchID= 0 Loss: 0.128 | Acc: 96.875% (124/128)
[Test] Epoch= 64  BatchID= 10 Loss: 0.348 | Acc: 89.489% (1260/1408)
[Test] Epoch= 64  BatchID= 20 Loss: 0.311 | Acc: 90.737% (2439/2688)
[Test] Epoch= 64  BatchID= 30 Loss: 0.341 | Acc: 89.834% (3526/3925)
Saving..
Best accuracy:  89.8343949044586

Epoch: 65
[Train] Epoch= 65  BatchID= 0 Loss: 0.119 | Acc: 96.875% (124/128)
[Train] Epoch= 65  BatchID= 10 Loss: 0.142 | Acc: 95.739% (1348/1408)
[Train] Epoch= 65  BatchID= 20 Loss: 0.139 | Acc: 95.685% (2572/2688)
[Train] Epoch= 65  BatchID= 30 Loss: 0.139 | Acc: 95.917% (3806/3968)
[Train] Epoch= 65  BatchID= 40 Loss: 0.142 | Acc: 95.922% (5034/5248)
[Train] Epoch= 65  BatchID= 50 Loss: 0.141 | Acc: 96.002% (6267/6528)
[Train] Epoch= 65  BatchID= 60 Loss: 0.141 | Acc: 95.940% (7491/7808)
[Train] Epoch= 65  BatchID= 70 Loss: 0.142 | Acc: 95.841% (8710/9088)
[Test] Epoch= 65  BatchID= 0 Loss: 0.110 | Acc: 96.875% (124/128)
[Test] Epoch= 65  BatchID= 10 Loss: 0.330 | Acc: 89.489% (1260/1408)
[Test] Epoch= 65  BatchID= 20 Loss: 0.302 | Acc: 90.699% (2438/2688)
[Test] Epoch= 65  BatchID= 30 Loss: 0.335 | Acc: 89.758% (3523/3925)

Epoch: 66
[Train] Epoch= 66  BatchID= 0 Loss: 0.142 | Acc: 96.094% (123/128)
[Train] Epoch= 66  BatchID= 10 Loss: 0.124 | Acc: 96.023% (1352/1408)
[Train] Epoch= 66  BatchID= 20 Loss: 0.129 | Acc: 95.945% (2579/2688)
[Train] Epoch= 66  BatchID= 30 Loss: 0.135 | Acc: 95.842% (3803/3968)
[Train] Epoch= 66  BatchID= 40 Loss: 0.138 | Acc: 95.827% (5029/5248)
[Train] Epoch= 66  BatchID= 50 Loss: 0.139 | Acc: 95.787% (6253/6528)
[Train] Epoch= 66  BatchID= 60 Loss: 0.141 | Acc: 95.633% (7467/7808)
[Train] Epoch= 66  BatchID= 70 Loss: 0.140 | Acc: 95.610% (8689/9088)
[Test] Epoch= 66  BatchID= 0 Loss: 0.131 | Acc: 96.875% (124/128)
[Test] Epoch= 66  BatchID= 10 Loss: 0.328 | Acc: 89.986% (1267/1408)
[Test] Epoch= 66  BatchID= 20 Loss: 0.300 | Acc: 90.885% (2443/2688)
[Test] Epoch= 66  BatchID= 30 Loss: 0.343 | Acc: 89.860% (3527/3925)
Saving..
Best accuracy:  89.85987261146497

Epoch: 67
[Train] Epoch= 67  BatchID= 0 Loss: 0.192 | Acc: 92.969% (119/128)
[Train] Epoch= 67  BatchID= 10 Loss: 0.171 | Acc: 95.028% (1338/1408)
[Train] Epoch= 67  BatchID= 20 Loss: 0.153 | Acc: 95.424% (2565/2688)
[Train] Epoch= 67  BatchID= 30 Loss: 0.160 | Acc: 95.237% (3779/3968)
[Train] Epoch= 67  BatchID= 40 Loss: 0.150 | Acc: 95.503% (5012/5248)
[Train] Epoch= 67  BatchID= 50 Loss: 0.152 | Acc: 95.374% (6226/6528)
[Train] Epoch= 67  BatchID= 60 Loss: 0.152 | Acc: 95.287% (7440/7808)
[Train] Epoch= 67  BatchID= 70 Loss: 0.151 | Acc: 95.390% (8669/9088)
[Test] Epoch= 67  BatchID= 0 Loss: 0.130 | Acc: 97.656% (125/128)
[Test] Epoch= 67  BatchID= 10 Loss: 0.345 | Acc: 89.347% (1258/1408)
[Test] Epoch= 67  BatchID= 20 Loss: 0.305 | Acc: 90.588% (2435/2688)
[Test] Epoch= 67  BatchID= 30 Loss: 0.345 | Acc: 89.401% (3509/3925)

Epoch: 68
[Train] Epoch= 68  BatchID= 0 Loss: 0.048 | Acc: 99.219% (127/128)
[Train] Epoch= 68  BatchID= 10 Loss: 0.129 | Acc: 96.236% (1355/1408)
[Train] Epoch= 68  BatchID= 20 Loss: 0.128 | Acc: 96.131% (2584/2688)
[Train] Epoch= 68  BatchID= 30 Loss: 0.136 | Acc: 95.741% (3799/3968)
[Train] Epoch= 68  BatchID= 40 Loss: 0.145 | Acc: 95.427% (5008/5248)
[Train] Epoch= 68  BatchID= 50 Loss: 0.142 | Acc: 95.604% (6241/6528)
[Train] Epoch= 68  BatchID= 60 Loss: 0.139 | Acc: 95.684% (7471/7808)
[Train] Epoch= 68  BatchID= 70 Loss: 0.142 | Acc: 95.643% (8692/9088)
[Test] Epoch= 68  BatchID= 0 Loss: 0.161 | Acc: 96.875% (124/128)
[Test] Epoch= 68  BatchID= 10 Loss: 0.349 | Acc: 89.844% (1265/1408)
[Test] Epoch= 68  BatchID= 20 Loss: 0.311 | Acc: 90.811% (2441/2688)
[Test] Epoch= 68  BatchID= 30 Loss: 0.343 | Acc: 89.860% (3527/3925)

Epoch: 69
[Train] Epoch= 69  BatchID= 0 Loss: 0.161 | Acc: 94.531% (121/128)
[Train] Epoch= 69  BatchID= 10 Loss: 0.137 | Acc: 96.165% (1354/1408)
[Train] Epoch= 69  BatchID= 20 Loss: 0.136 | Acc: 96.094% (2583/2688)
[Train] Epoch= 69  BatchID= 30 Loss: 0.139 | Acc: 95.968% (3808/3968)
[Train] Epoch= 69  BatchID= 40 Loss: 0.138 | Acc: 96.113% (5044/5248)
[Train] Epoch= 69  BatchID= 50 Loss: 0.143 | Acc: 95.895% (6260/6528)
[Train] Epoch= 69  BatchID= 60 Loss: 0.144 | Acc: 95.838% (7483/7808)
[Train] Epoch= 69  BatchID= 70 Loss: 0.141 | Acc: 95.918% (8717/9088)
[Test] Epoch= 69  BatchID= 0 Loss: 0.137 | Acc: 96.875% (124/128)
[Test] Epoch= 69  BatchID= 10 Loss: 0.328 | Acc: 90.199% (1270/1408)
[Test] Epoch= 69  BatchID= 20 Loss: 0.299 | Acc: 90.551% (2434/2688)
[Test] Epoch= 69  BatchID= 30 Loss: 0.336 | Acc: 89.682% (3520/3925)

Epoch: 70
[Train] Epoch= 70  BatchID= 0 Loss: 0.143 | Acc: 95.312% (122/128)
[Train] Epoch= 70  BatchID= 10 Loss: 0.135 | Acc: 95.952% (1351/1408)
[Train] Epoch= 70  BatchID= 20 Loss: 0.129 | Acc: 96.019% (2581/2688)
[Train] Epoch= 70  BatchID= 30 Loss: 0.126 | Acc: 96.018% (3810/3968)
[Train] Epoch= 70  BatchID= 40 Loss: 0.128 | Acc: 95.922% (5034/5248)
[Train] Epoch= 70  BatchID= 50 Loss: 0.130 | Acc: 95.849% (6257/6528)
[Train] Epoch= 70  BatchID= 60 Loss: 0.133 | Acc: 95.825% (7482/7808)
[Train] Epoch= 70  BatchID= 70 Loss: 0.132 | Acc: 95.863% (8712/9088)
[Test] Epoch= 70  BatchID= 0 Loss: 0.123 | Acc: 97.656% (125/128)
[Test] Epoch= 70  BatchID= 10 Loss: 0.309 | Acc: 90.341% (1272/1408)
[Test] Epoch= 70  BatchID= 20 Loss: 0.299 | Acc: 90.625% (2436/2688)
[Test] Epoch= 70  BatchID= 30 Loss: 0.343 | Acc: 89.707% (3521/3925)

Epoch: 71
[Train] Epoch= 71  BatchID= 0 Loss: 0.177 | Acc: 94.531% (121/128)
[Train] Epoch= 71  BatchID= 10 Loss: 0.137 | Acc: 95.739% (1348/1408)
[Train] Epoch= 71  BatchID= 20 Loss: 0.142 | Acc: 95.275% (2561/2688)
[Train] Epoch= 71  BatchID= 30 Loss: 0.140 | Acc: 95.489% (3789/3968)
[Train] Epoch= 71  BatchID= 40 Loss: 0.137 | Acc: 95.713% (5023/5248)
[Train] Epoch= 71  BatchID= 50 Loss: 0.135 | Acc: 95.833% (6256/6528)
[Train] Epoch= 71  BatchID= 60 Loss: 0.132 | Acc: 95.953% (7492/7808)
[Train] Epoch= 71  BatchID= 70 Loss: 0.133 | Acc: 95.907% (8716/9088)
[Test] Epoch= 71  BatchID= 0 Loss: 0.131 | Acc: 96.875% (124/128)
[Test] Epoch= 71  BatchID= 10 Loss: 0.301 | Acc: 90.696% (1277/1408)
[Test] Epoch= 71  BatchID= 20 Loss: 0.286 | Acc: 91.295% (2454/2688)
[Test] Epoch= 71  BatchID= 30 Loss: 0.336 | Acc: 89.987% (3532/3925)
Saving..
Best accuracy:  89.98726114649682

Epoch: 72
[Train] Epoch= 72  BatchID= 0 Loss: 0.142 | Acc: 95.312% (122/128)
[Train] Epoch= 72  BatchID= 10 Loss: 0.138 | Acc: 95.241% (1341/1408)
[Train] Epoch= 72  BatchID= 20 Loss: 0.133 | Acc: 95.536% (2568/2688)
[Train] Epoch= 72  BatchID= 30 Loss: 0.129 | Acc: 95.741% (3799/3968)
[Train] Epoch= 72  BatchID= 40 Loss: 0.134 | Acc: 95.560% (5015/5248)
[Train] Epoch= 72  BatchID= 50 Loss: 0.134 | Acc: 95.650% (6244/6528)
[Train] Epoch= 72  BatchID= 60 Loss: 0.135 | Acc: 95.684% (7471/7808)
[Train] Epoch= 72  BatchID= 70 Loss: 0.137 | Acc: 95.687% (8696/9088)
[Test] Epoch= 72  BatchID= 0 Loss: 0.146 | Acc: 96.875% (124/128)
[Test] Epoch= 72  BatchID= 10 Loss: 0.322 | Acc: 89.915% (1266/1408)
[Test] Epoch= 72  BatchID= 20 Loss: 0.299 | Acc: 90.811% (2441/2688)
[Test] Epoch= 72  BatchID= 30 Loss: 0.337 | Acc: 89.758% (3523/3925)

Epoch: 73
[Train] Epoch= 73  BatchID= 0 Loss: 0.134 | Acc: 96.875% (124/128)
[Train] Epoch= 73  BatchID= 10 Loss: 0.123 | Acc: 96.591% (1360/1408)
[Train] Epoch= 73  BatchID= 20 Loss: 0.123 | Acc: 96.429% (2592/2688)
[Train] Epoch= 73  BatchID= 30 Loss: 0.127 | Acc: 95.993% (3809/3968)
[Train] Epoch= 73  BatchID= 40 Loss: 0.124 | Acc: 96.037% (5040/5248)
[Train] Epoch= 73  BatchID= 50 Loss: 0.123 | Acc: 96.048% (6270/6528)
[Train] Epoch= 73  BatchID= 60 Loss: 0.127 | Acc: 95.966% (7493/7808)
[Train] Epoch= 73  BatchID= 70 Loss: 0.130 | Acc: 95.940% (8719/9088)
[Test] Epoch= 73  BatchID= 0 Loss: 0.108 | Acc: 96.875% (124/128)
[Test] Epoch= 73  BatchID= 10 Loss: 0.348 | Acc: 89.773% (1264/1408)
[Test] Epoch= 73  BatchID= 20 Loss: 0.314 | Acc: 90.439% (2431/2688)
[Test] Epoch= 73  BatchID= 30 Loss: 0.340 | Acc: 89.783% (3524/3925)

Epoch: 74
[Train] Epoch= 74  BatchID= 0 Loss: 0.092 | Acc: 98.438% (126/128)
[Train] Epoch= 74  BatchID= 10 Loss: 0.154 | Acc: 95.170% (1340/1408)
[Train] Epoch= 74  BatchID= 20 Loss: 0.155 | Acc: 95.164% (2558/2688)
[Train] Epoch= 74  BatchID= 30 Loss: 0.144 | Acc: 95.565% (3792/3968)
[Train] Epoch= 74  BatchID= 40 Loss: 0.145 | Acc: 95.579% (5016/5248)
[Train] Epoch= 74  BatchID= 50 Loss: 0.143 | Acc: 95.726% (6249/6528)
[Train] Epoch= 74  BatchID= 60 Loss: 0.141 | Acc: 95.697% (7472/7808)
[Train] Epoch= 74  BatchID= 70 Loss: 0.145 | Acc: 95.621% (8690/9088)
[Test] Epoch= 74  BatchID= 0 Loss: 0.108 | Acc: 97.656% (125/128)
[Test] Epoch= 74  BatchID= 10 Loss: 0.312 | Acc: 90.341% (1272/1408)
[Test] Epoch= 74  BatchID= 20 Loss: 0.298 | Acc: 90.997% (2446/2688)
[Test] Epoch= 74  BatchID= 30 Loss: 0.343 | Acc: 89.809% (3525/3925)

Epoch: 75
[Train] Epoch= 75  BatchID= 0 Loss: 0.087 | Acc: 98.438% (126/128)
[Train] Epoch= 75  BatchID= 10 Loss: 0.144 | Acc: 96.094% (1353/1408)
[Train] Epoch= 75  BatchID= 20 Loss: 0.133 | Acc: 96.131% (2584/2688)
[Train] Epoch= 75  BatchID= 30 Loss: 0.137 | Acc: 95.917% (3806/3968)
[Train] Epoch= 75  BatchID= 40 Loss: 0.140 | Acc: 95.655% (5020/5248)
[Train] Epoch= 75  BatchID= 50 Loss: 0.136 | Acc: 95.818% (6255/6528)
[Train] Epoch= 75  BatchID= 60 Loss: 0.134 | Acc: 95.902% (7488/7808)
[Train] Epoch= 75  BatchID= 70 Loss: 0.135 | Acc: 95.841% (8710/9088)
[Test] Epoch= 75  BatchID= 0 Loss: 0.123 | Acc: 96.875% (124/128)
[Test] Epoch= 75  BatchID= 10 Loss: 0.317 | Acc: 90.341% (1272/1408)
[Test] Epoch= 75  BatchID= 20 Loss: 0.298 | Acc: 90.662% (2437/2688)
[Test] Epoch= 75  BatchID= 30 Loss: 0.335 | Acc: 89.656% (3519/3925)

Epoch: 76
[Train] Epoch= 76  BatchID= 0 Loss: 0.155 | Acc: 93.750% (120/128)
[Train] Epoch= 76  BatchID= 10 Loss: 0.138 | Acc: 95.597% (1346/1408)
[Train] Epoch= 76  BatchID= 20 Loss: 0.122 | Acc: 96.243% (2587/2688)
[Train] Epoch= 76  BatchID= 30 Loss: 0.120 | Acc: 96.094% (3813/3968)
[Train] Epoch= 76  BatchID= 40 Loss: 0.124 | Acc: 96.037% (5040/5248)
[Train] Epoch= 76  BatchID= 50 Loss: 0.130 | Acc: 95.879% (6259/6528)
[Train] Epoch= 76  BatchID= 60 Loss: 0.131 | Acc: 95.914% (7489/7808)
[Train] Epoch= 76  BatchID= 70 Loss: 0.135 | Acc: 95.863% (8712/9088)
[Test] Epoch= 76  BatchID= 0 Loss: 0.105 | Acc: 96.875% (124/128)
[Test] Epoch= 76  BatchID= 10 Loss: 0.313 | Acc: 90.057% (1268/1408)
[Test] Epoch= 76  BatchID= 20 Loss: 0.293 | Acc: 90.960% (2445/2688)
[Test] Epoch= 76  BatchID= 30 Loss: 0.332 | Acc: 90.013% (3533/3925)
Saving..
Best accuracy:  90.01273885350318

Epoch: 77
[Train] Epoch= 77  BatchID= 0 Loss: 0.112 | Acc: 98.438% (126/128)
[Train] Epoch= 77  BatchID= 10 Loss: 0.132 | Acc: 96.449% (1358/1408)
[Train] Epoch= 77  BatchID= 20 Loss: 0.143 | Acc: 95.945% (2579/2688)
[Train] Epoch= 77  BatchID= 30 Loss: 0.148 | Acc: 95.892% (3805/3968)
[Train] Epoch= 77  BatchID= 40 Loss: 0.140 | Acc: 95.903% (5033/5248)
[Train] Epoch= 77  BatchID= 50 Loss: 0.139 | Acc: 95.910% (6261/6528)
[Train] Epoch= 77  BatchID= 60 Loss: 0.134 | Acc: 96.068% (7501/7808)
[Train] Epoch= 77  BatchID= 70 Loss: 0.133 | Acc: 96.094% (8733/9088)
[Test] Epoch= 77  BatchID= 0 Loss: 0.118 | Acc: 97.656% (125/128)
[Test] Epoch= 77  BatchID= 10 Loss: 0.339 | Acc: 89.773% (1264/1408)
[Test] Epoch= 77  BatchID= 20 Loss: 0.309 | Acc: 90.774% (2440/2688)
[Test] Epoch= 77  BatchID= 30 Loss: 0.341 | Acc: 90.013% (3533/3925)

Epoch: 78
[Train] Epoch= 78  BatchID= 0 Loss: 0.147 | Acc: 96.094% (123/128)
[Train] Epoch= 78  BatchID= 10 Loss: 0.128 | Acc: 96.236% (1355/1408)
[Train] Epoch= 78  BatchID= 20 Loss: 0.129 | Acc: 96.205% (2586/2688)
[Train] Epoch= 78  BatchID= 30 Loss: 0.125 | Acc: 96.094% (3813/3968)
[Train] Epoch= 78  BatchID= 40 Loss: 0.135 | Acc: 95.789% (5027/5248)
[Train] Epoch= 78  BatchID= 50 Loss: 0.133 | Acc: 95.787% (6253/6528)
[Train] Epoch= 78  BatchID= 60 Loss: 0.132 | Acc: 95.927% (7490/7808)
[Train] Epoch= 78  BatchID= 70 Loss: 0.131 | Acc: 95.929% (8718/9088)
[Test] Epoch= 78  BatchID= 0 Loss: 0.133 | Acc: 96.094% (123/128)
[Test] Epoch= 78  BatchID= 10 Loss: 0.334 | Acc: 89.986% (1267/1408)
[Test] Epoch= 78  BatchID= 20 Loss: 0.309 | Acc: 90.699% (2438/2688)
[Test] Epoch= 78  BatchID= 30 Loss: 0.347 | Acc: 89.783% (3524/3925)

Epoch: 79
[Train] Epoch= 79  BatchID= 0 Loss: 0.109 | Acc: 96.875% (124/128)
[Train] Epoch= 79  BatchID= 10 Loss: 0.111 | Acc: 96.236% (1355/1408)
[Train] Epoch= 79  BatchID= 20 Loss: 0.126 | Acc: 95.871% (2577/2688)
[Train] Epoch= 79  BatchID= 30 Loss: 0.122 | Acc: 96.069% (3812/3968)
[Train] Epoch= 79  BatchID= 40 Loss: 0.126 | Acc: 95.998% (5038/5248)
[Train] Epoch= 79  BatchID= 50 Loss: 0.129 | Acc: 96.017% (6268/6528)
[Train] Epoch= 79  BatchID= 60 Loss: 0.127 | Acc: 96.107% (7504/7808)
[Train] Epoch= 79  BatchID= 70 Loss: 0.126 | Acc: 96.171% (8740/9088)
[Test] Epoch= 79  BatchID= 0 Loss: 0.121 | Acc: 96.875% (124/128)
[Test] Epoch= 79  BatchID= 10 Loss: 0.335 | Acc: 90.128% (1269/1408)
[Test] Epoch= 79  BatchID= 20 Loss: 0.300 | Acc: 90.811% (2441/2688)
[Test] Epoch= 79  BatchID= 30 Loss: 0.340 | Acc: 89.885% (3528/3925)

Epoch: 80
[Train] Epoch= 80  BatchID= 0 Loss: 0.039 | Acc: 99.219% (127/128)
[Train] Epoch= 80  BatchID= 10 Loss: 0.119 | Acc: 96.236% (1355/1408)
[Train] Epoch= 80  BatchID= 20 Loss: 0.115 | Acc: 96.429% (2592/2688)
[Train] Epoch= 80  BatchID= 30 Loss: 0.122 | Acc: 96.119% (3814/3968)
[Train] Epoch= 80  BatchID= 40 Loss: 0.130 | Acc: 95.884% (5032/5248)
[Train] Epoch= 80  BatchID= 50 Loss: 0.131 | Acc: 96.032% (6269/6528)
[Train] Epoch= 80  BatchID= 60 Loss: 0.134 | Acc: 96.043% (7499/7808)
[Train] Epoch= 80  BatchID= 70 Loss: 0.134 | Acc: 96.072% (8731/9088)
[Test] Epoch= 80  BatchID= 0 Loss: 0.106 | Acc: 97.656% (125/128)
[Test] Epoch= 80  BatchID= 10 Loss: 0.321 | Acc: 89.986% (1267/1408)
[Test] Epoch= 80  BatchID= 20 Loss: 0.298 | Acc: 90.699% (2438/2688)
[Test] Epoch= 80  BatchID= 30 Loss: 0.334 | Acc: 89.631% (3518/3925)

Epoch: 81
[Train] Epoch= 81  BatchID= 0 Loss: 0.193 | Acc: 91.406% (117/128)
[Train] Epoch= 81  BatchID= 10 Loss: 0.135 | Acc: 95.312% (1342/1408)
[Train] Epoch= 81  BatchID= 20 Loss: 0.138 | Acc: 95.461% (2566/2688)
[Train] Epoch= 81  BatchID= 30 Loss: 0.146 | Acc: 95.439% (3787/3968)
[Train] Epoch= 81  BatchID= 40 Loss: 0.146 | Acc: 95.389% (5006/5248)
[Train] Epoch= 81  BatchID= 50 Loss: 0.141 | Acc: 95.481% (6233/6528)
[Train] Epoch= 81  BatchID= 60 Loss: 0.143 | Acc: 95.389% (7448/7808)
[Train] Epoch= 81  BatchID= 70 Loss: 0.142 | Acc: 95.379% (8668/9088)
[Test] Epoch= 81  BatchID= 0 Loss: 0.116 | Acc: 96.875% (124/128)
[Test] Epoch= 81  BatchID= 10 Loss: 0.332 | Acc: 90.270% (1271/1408)
[Test] Epoch= 81  BatchID= 20 Loss: 0.299 | Acc: 91.109% (2449/2688)
[Test] Epoch= 81  BatchID= 30 Loss: 0.343 | Acc: 89.809% (3525/3925)

Epoch: 82
[Train] Epoch= 82  BatchID= 0 Loss: 0.176 | Acc: 92.188% (118/128)
[Train] Epoch= 82  BatchID= 10 Loss: 0.119 | Acc: 95.739% (1348/1408)
[Train] Epoch= 82  BatchID= 20 Loss: 0.117 | Acc: 95.982% (2580/2688)
[Train] Epoch= 82  BatchID= 30 Loss: 0.128 | Acc: 95.766% (3800/3968)
[Train] Epoch= 82  BatchID= 40 Loss: 0.128 | Acc: 95.846% (5030/5248)
[Train] Epoch= 82  BatchID= 50 Loss: 0.124 | Acc: 96.048% (6270/6528)
[Train] Epoch= 82  BatchID= 60 Loss: 0.125 | Acc: 96.030% (7498/7808)
[Train] Epoch= 82  BatchID= 70 Loss: 0.127 | Acc: 95.962% (8721/9088)
[Test] Epoch= 82  BatchID= 0 Loss: 0.122 | Acc: 97.656% (125/128)
[Test] Epoch= 82  BatchID= 10 Loss: 0.328 | Acc: 90.199% (1270/1408)
[Test] Epoch= 82  BatchID= 20 Loss: 0.306 | Acc: 90.737% (2439/2688)
[Test] Epoch= 82  BatchID= 30 Loss: 0.342 | Acc: 89.682% (3520/3925)

Epoch: 83
[Train] Epoch= 83  BatchID= 0 Loss: 0.046 | Acc: 99.219% (127/128)
[Train] Epoch= 83  BatchID= 10 Loss: 0.116 | Acc: 96.591% (1360/1408)
[Train] Epoch= 83  BatchID= 20 Loss: 0.121 | Acc: 96.615% (2597/2688)
[Train] Epoch= 83  BatchID= 30 Loss: 0.125 | Acc: 96.371% (3824/3968)
[Train] Epoch= 83  BatchID= 40 Loss: 0.132 | Acc: 96.132% (5045/5248)
[Train] Epoch= 83  BatchID= 50 Loss: 0.132 | Acc: 96.155% (6277/6528)
[Train] Epoch= 83  BatchID= 60 Loss: 0.135 | Acc: 96.030% (7498/7808)
[Train] Epoch= 83  BatchID= 70 Loss: 0.135 | Acc: 95.995% (8724/9088)
[Test] Epoch= 83  BatchID= 0 Loss: 0.132 | Acc: 96.875% (124/128)
[Test] Epoch= 83  BatchID= 10 Loss: 0.325 | Acc: 90.270% (1271/1408)
[Test] Epoch= 83  BatchID= 20 Loss: 0.295 | Acc: 91.071% (2448/2688)
[Test] Epoch= 83  BatchID= 30 Loss: 0.341 | Acc: 89.554% (3515/3925)

Epoch: 84
[Train] Epoch= 84  BatchID= 0 Loss: 0.140 | Acc: 94.531% (121/128)
[Train] Epoch= 84  BatchID= 10 Loss: 0.125 | Acc: 96.094% (1353/1408)
[Train] Epoch= 84  BatchID= 20 Loss: 0.124 | Acc: 95.982% (2580/2688)
[Train] Epoch= 84  BatchID= 30 Loss: 0.133 | Acc: 95.716% (3798/3968)
[Train] Epoch= 84  BatchID= 40 Loss: 0.131 | Acc: 95.884% (5032/5248)
[Train] Epoch= 84  BatchID= 50 Loss: 0.138 | Acc: 95.726% (6249/6528)
[Train] Epoch= 84  BatchID= 60 Loss: 0.134 | Acc: 95.838% (7483/7808)
[Train] Epoch= 84  BatchID= 70 Loss: 0.134 | Acc: 95.852% (8711/9088)
[Test] Epoch= 84  BatchID= 0 Loss: 0.114 | Acc: 97.656% (125/128)
[Test] Epoch= 84  BatchID= 10 Loss: 0.321 | Acc: 90.128% (1269/1408)
[Test] Epoch= 84  BatchID= 20 Loss: 0.297 | Acc: 90.811% (2441/2688)
[Test] Epoch= 84  BatchID= 30 Loss: 0.334 | Acc: 89.962% (3531/3925)

Epoch: 85
[Train] Epoch= 85  BatchID= 0 Loss: 0.129 | Acc: 96.094% (123/128)
[Train] Epoch= 85  BatchID= 10 Loss: 0.146 | Acc: 95.739% (1348/1408)
[Train] Epoch= 85  BatchID= 20 Loss: 0.141 | Acc: 95.722% (2573/2688)
[Train] Epoch= 85  BatchID= 30 Loss: 0.140 | Acc: 95.766% (3800/3968)
[Train] Epoch= 85  BatchID= 40 Loss: 0.141 | Acc: 95.770% (5026/5248)
[Train] Epoch= 85  BatchID= 50 Loss: 0.138 | Acc: 95.849% (6257/6528)
[Train] Epoch= 85  BatchID= 60 Loss: 0.138 | Acc: 95.863% (7485/7808)
[Train] Epoch= 85  BatchID= 70 Loss: 0.135 | Acc: 95.907% (8716/9088)
[Test] Epoch= 85  BatchID= 0 Loss: 0.113 | Acc: 96.875% (124/128)
[Test] Epoch= 85  BatchID= 10 Loss: 0.335 | Acc: 89.702% (1263/1408)
[Test] Epoch= 85  BatchID= 20 Loss: 0.299 | Acc: 90.997% (2446/2688)
[Test] Epoch= 85  BatchID= 30 Loss: 0.343 | Acc: 89.834% (3526/3925)

Epoch: 86
[Train] Epoch= 86  BatchID= 0 Loss: 0.077 | Acc: 97.656% (125/128)
[Train] Epoch= 86  BatchID= 10 Loss: 0.132 | Acc: 96.165% (1354/1408)
[Train] Epoch= 86  BatchID= 20 Loss: 0.145 | Acc: 95.685% (2572/2688)
[Train] Epoch= 86  BatchID= 30 Loss: 0.148 | Acc: 95.464% (3788/3968)
[Train] Epoch= 86  BatchID= 40 Loss: 0.146 | Acc: 95.636% (5019/5248)
[Train] Epoch= 86  BatchID= 50 Loss: 0.145 | Acc: 95.680% (6246/6528)
[Train] Epoch= 86  BatchID= 60 Loss: 0.142 | Acc: 95.748% (7476/7808)
[Train] Epoch= 86  BatchID= 70 Loss: 0.141 | Acc: 95.808% (8707/9088)
[Test] Epoch= 86  BatchID= 0 Loss: 0.104 | Acc: 97.656% (125/128)
[Test] Epoch= 86  BatchID= 10 Loss: 0.329 | Acc: 90.199% (1270/1408)
[Test] Epoch= 86  BatchID= 20 Loss: 0.300 | Acc: 90.960% (2445/2688)
[Test] Epoch= 86  BatchID= 30 Loss: 0.342 | Acc: 89.936% (3530/3925)

Epoch: 87
[Train] Epoch= 87  BatchID= 0 Loss: 0.248 | Acc: 92.969% (119/128)
[Train] Epoch= 87  BatchID= 10 Loss: 0.131 | Acc: 96.236% (1355/1408)
[Train] Epoch= 87  BatchID= 20 Loss: 0.123 | Acc: 96.354% (2590/2688)
[Train] Epoch= 87  BatchID= 30 Loss: 0.133 | Acc: 96.119% (3814/3968)
[Train] Epoch= 87  BatchID= 40 Loss: 0.138 | Acc: 95.827% (5029/5248)
[Train] Epoch= 87  BatchID= 50 Loss: 0.134 | Acc: 95.925% (6262/6528)
[Train] Epoch= 87  BatchID= 60 Loss: 0.134 | Acc: 95.914% (7489/7808)
[Train] Epoch= 87  BatchID= 70 Loss: 0.134 | Acc: 95.940% (8719/9088)
[Test] Epoch= 87  BatchID= 0 Loss: 0.104 | Acc: 96.875% (124/128)
[Test] Epoch= 87  BatchID= 10 Loss: 0.327 | Acc: 90.128% (1269/1408)
[Test] Epoch= 87  BatchID= 20 Loss: 0.302 | Acc: 90.774% (2440/2688)
[Test] Epoch= 87  BatchID= 30 Loss: 0.334 | Acc: 89.885% (3528/3925)

Epoch: 88
[Train] Epoch= 88  BatchID= 0 Loss: 0.097 | Acc: 97.656% (125/128)
[Train] Epoch= 88  BatchID= 10 Loss: 0.134 | Acc: 95.739% (1348/1408)
[Train] Epoch= 88  BatchID= 20 Loss: 0.120 | Acc: 96.168% (2585/2688)
[Train] Epoch= 88  BatchID= 30 Loss: 0.122 | Acc: 96.371% (3824/3968)
[Train] Epoch= 88  BatchID= 40 Loss: 0.116 | Acc: 96.627% (5071/5248)
[Train] Epoch= 88  BatchID= 50 Loss: 0.119 | Acc: 96.599% (6306/6528)
[Train] Epoch= 88  BatchID= 60 Loss: 0.121 | Acc: 96.529% (7537/7808)
[Train] Epoch= 88  BatchID= 70 Loss: 0.121 | Acc: 96.523% (8772/9088)
[Test] Epoch= 88  BatchID= 0 Loss: 0.121 | Acc: 96.875% (124/128)
[Test] Epoch= 88  BatchID= 10 Loss: 0.330 | Acc: 89.631% (1262/1408)
[Test] Epoch= 88  BatchID= 20 Loss: 0.301 | Acc: 90.774% (2440/2688)
[Test] Epoch= 88  BatchID= 30 Loss: 0.337 | Acc: 89.732% (3522/3925)

Epoch: 89
[Train] Epoch= 89  BatchID= 0 Loss: 0.106 | Acc: 96.094% (123/128)
[Train] Epoch= 89  BatchID= 10 Loss: 0.136 | Acc: 95.739% (1348/1408)
[Train] Epoch= 89  BatchID= 20 Loss: 0.129 | Acc: 95.908% (2578/2688)
[Train] Epoch= 89  BatchID= 30 Loss: 0.134 | Acc: 95.867% (3804/3968)
[Train] Epoch= 89  BatchID= 40 Loss: 0.136 | Acc: 95.922% (5034/5248)
[Train] Epoch= 89  BatchID= 50 Loss: 0.133 | Acc: 95.941% (6263/6528)
[Train] Epoch= 89  BatchID= 60 Loss: 0.133 | Acc: 95.940% (7491/7808)
[Train] Epoch= 89  BatchID= 70 Loss: 0.133 | Acc: 95.918% (8717/9088)
[Test] Epoch= 89  BatchID= 0 Loss: 0.115 | Acc: 97.656% (125/128)
[Test] Epoch= 89  BatchID= 10 Loss: 0.316 | Acc: 90.270% (1271/1408)
[Test] Epoch= 89  BatchID= 20 Loss: 0.302 | Acc: 90.699% (2438/2688)
[Test] Epoch= 89  BatchID= 30 Loss: 0.340 | Acc: 89.783% (3524/3925)
==> Preparing data..
==> Building model..
DataParallel(
  (module): ResNet(
    (conv1): InputConv2dLSQ(
      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=2, is_activation=False)
            (quan_a): LSQQuantizer (bit=2, is_activation=True)
          )
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=2, is_activation=False)
            (quan_a): LSQQuantizer (bit=2, is_activation=True)
          )
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=2, is_activation=False)
            (quan_a): LSQQuantizer (bit=2, is_activation=True)
          )
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): LinearLSQ(
      in_features=512, out_features=10, bias=True
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=True)
    )
  )
)
==> Initializing from checkpoint..

Epoch: 0
==> Preparing data..
==> Building model..
DataParallel(
  (module): ResNet(
    (conv1): InputConv2dLSQ(
      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=2, is_activation=False)
            (quan_a): LSQQuantizer (bit=2, is_activation=True)
          )
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=2, is_activation=False)
            (quan_a): LSQQuantizer (bit=2, is_activation=True)
          )
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=2, is_activation=False)
            (quan_a): LSQQuantizer (bit=2, is_activation=True)
          )
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=2, is_activation=False)
          (quan_a): LSQQuantizer (bit=2, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): LinearLSQ(
      in_features=512, out_features=10, bias=True
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=True)
    )
  )
)
==> Initializing from checkpoint..

Epoch: 0
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
[Train] Epoch= 0  BatchID= 0 Loss: 1.853 | Acc: 32.031% (41/128)
[Train] Epoch= 0  BatchID= 10 Loss: 1.579 | Acc: 45.739% (644/1408)
[Train] Epoch= 0  BatchID= 20 Loss: 1.224 | Acc: 58.854% (1582/2688)
[Train] Epoch= 0  BatchID= 30 Loss: 1.013 | Acc: 66.053% (2621/3968)
[Train] Epoch= 0  BatchID= 40 Loss: 0.879 | Acc: 70.941% (3723/5248)
[Train] Epoch= 0  BatchID= 50 Loss: 0.795 | Acc: 73.698% (4811/6528)
[Train] Epoch= 0  BatchID= 60 Loss: 0.737 | Acc: 75.628% (5905/7808)
[Train] Epoch= 0  BatchID= 70 Loss: 0.691 | Acc: 77.003% (6998/9088)
[Test] Epoch= 0  BatchID= 0 Loss: 0.236 | Acc: 92.188% (118/128)
[Test] Epoch= 0  BatchID= 10 Loss: 0.630 | Acc: 80.398% (1132/1408)
[Test] Epoch= 0  BatchID= 20 Loss: 0.649 | Acc: 80.283% (2158/2688)
[Test] Epoch= 0  BatchID= 30 Loss: 0.587 | Acc: 81.987% (3218/3925)
Saving..
Best accuracy:  81.98726114649682

Epoch: 1
[Train] Epoch= 1  BatchID= 0 Loss: 0.336 | Acc: 92.969% (119/128)
[Train] Epoch= 1  BatchID= 10 Loss: 0.356 | Acc: 88.920% (1252/1408)
[Train] Epoch= 1  BatchID= 20 Loss: 0.351 | Acc: 88.951% (2391/2688)
[Train] Epoch= 1  BatchID= 30 Loss: 0.344 | Acc: 88.911% (3528/3968)
[Train] Epoch= 1  BatchID= 40 Loss: 0.343 | Acc: 88.948% (4668/5248)
[Train] Epoch= 1  BatchID= 50 Loss: 0.338 | Acc: 89.185% (5822/6528)
[Train] Epoch= 1  BatchID= 60 Loss: 0.333 | Acc: 89.216% (6966/7808)
[Train] Epoch= 1  BatchID= 70 Loss: 0.332 | Acc: 89.415% (8126/9088)
[Test] Epoch= 1  BatchID= 0 Loss: 0.127 | Acc: 95.312% (122/128)
[Test] Epoch= 1  BatchID= 10 Loss: 0.462 | Acc: 85.653% (1206/1408)
[Test] Epoch= 1  BatchID= 20 Loss: 0.418 | Acc: 86.830% (2334/2688)
[Test] Epoch= 1  BatchID= 30 Loss: 0.461 | Acc: 85.783% (3367/3925)
Saving..
Best accuracy:  85.78343949044586

Epoch: 2
[Train] Epoch= 2  BatchID= 0 Loss: 0.244 | Acc: 92.188% (118/128)
[Train] Epoch= 2  BatchID= 10 Loss: 0.300 | Acc: 90.554% (1275/1408)
[Train] Epoch= 2  BatchID= 20 Loss: 0.301 | Acc: 90.699% (2438/2688)
[Train] Epoch= 2  BatchID= 30 Loss: 0.303 | Acc: 90.423% (3588/3968)
[Train] Epoch= 2  BatchID= 40 Loss: 0.295 | Acc: 90.682% (4759/5248)
[Train] Epoch= 2  BatchID= 50 Loss: 0.294 | Acc: 90.794% (5927/6528)
[Train] Epoch= 2  BatchID= 60 Loss: 0.288 | Acc: 90.996% (7105/7808)
[Train] Epoch= 2  BatchID= 70 Loss: 0.294 | Acc: 90.724% (8245/9088)
[Test] Epoch= 2  BatchID= 0 Loss: 0.156 | Acc: 95.312% (122/128)
[Test] Epoch= 2  BatchID= 10 Loss: 0.457 | Acc: 85.866% (1209/1408)
[Test] Epoch= 2  BatchID= 20 Loss: 0.444 | Acc: 86.607% (2328/2688)
[Test] Epoch= 2  BatchID= 30 Loss: 0.452 | Acc: 86.573% (3398/3925)
Saving..
Best accuracy:  86.5732484076433

Epoch: 3
[Train] Epoch= 3  BatchID= 0 Loss: 0.366 | Acc: 90.625% (116/128)
[Train] Epoch= 3  BatchID= 10 Loss: 0.259 | Acc: 92.472% (1302/1408)
[Train] Epoch= 3  BatchID= 20 Loss: 0.277 | Acc: 91.927% (2471/2688)
[Train] Epoch= 3  BatchID= 30 Loss: 0.289 | Acc: 91.154% (3617/3968)
[Train] Epoch= 3  BatchID= 40 Loss: 0.290 | Acc: 90.816% (4766/5248)
[Train] Epoch= 3  BatchID= 50 Loss: 0.286 | Acc: 90.763% (5925/6528)
[Train] Epoch= 3  BatchID= 60 Loss: 0.283 | Acc: 90.753% (7086/7808)
[Train] Epoch= 3  BatchID= 70 Loss: 0.284 | Acc: 90.922% (8263/9088)
[Test] Epoch= 3  BatchID= 0 Loss: 0.133 | Acc: 96.875% (124/128)
[Test] Epoch= 3  BatchID= 10 Loss: 0.345 | Acc: 89.844% (1265/1408)
[Test] Epoch= 3  BatchID= 20 Loss: 0.356 | Acc: 89.174% (2397/2688)
[Test] Epoch= 3  BatchID= 30 Loss: 0.415 | Acc: 87.898% (3450/3925)
Saving..
Best accuracy:  87.89808917197452

Epoch: 4
[Train] Epoch= 4  BatchID= 0 Loss: 0.226 | Acc: 92.969% (119/128)
[Train] Epoch= 4  BatchID= 10 Loss: 0.270 | Acc: 90.980% (1281/1408)
[Train] Epoch= 4  BatchID= 20 Loss: 0.263 | Acc: 91.443% (2458/2688)
[Train] Epoch= 4  BatchID= 30 Loss: 0.255 | Acc: 91.709% (3639/3968)
[Train] Epoch= 4  BatchID= 40 Loss: 0.252 | Acc: 91.959% (4826/5248)
[Train] Epoch= 4  BatchID= 50 Loss: 0.257 | Acc: 91.927% (6001/6528)
[Train] Epoch= 4  BatchID= 60 Loss: 0.264 | Acc: 91.675% (7158/7808)
[Train] Epoch= 4  BatchID= 70 Loss: 0.268 | Acc: 91.549% (8320/9088)
[Test] Epoch= 4  BatchID= 0 Loss: 0.159 | Acc: 94.531% (121/128)
[Test] Epoch= 4  BatchID= 10 Loss: 0.350 | Acc: 88.778% (1250/1408)
[Test] Epoch= 4  BatchID= 20 Loss: 0.363 | Acc: 88.579% (2381/2688)
[Test] Epoch= 4  BatchID= 30 Loss: 0.424 | Acc: 87.134% (3420/3925)

Epoch: 5
[Train] Epoch= 5  BatchID= 0 Loss: 0.235 | Acc: 92.969% (119/128)
[Train] Epoch= 5  BatchID= 10 Loss: 0.254 | Acc: 91.903% (1294/1408)
[Train] Epoch= 5  BatchID= 20 Loss: 0.259 | Acc: 91.704% (2465/2688)
[Train] Epoch= 5  BatchID= 30 Loss: 0.259 | Acc: 91.583% (3634/3968)
[Train] Epoch= 5  BatchID= 40 Loss: 0.265 | Acc: 91.463% (4800/5248)
[Train] Epoch= 5  BatchID= 50 Loss: 0.257 | Acc: 91.912% (6000/6528)
[Train] Epoch= 5  BatchID= 60 Loss: 0.260 | Acc: 91.765% (7165/7808)
[Train] Epoch= 5  BatchID= 70 Loss: 0.265 | Acc: 91.615% (8326/9088)
[Test] Epoch= 5  BatchID= 0 Loss: 0.182 | Acc: 95.312% (122/128)
[Test] Epoch= 5  BatchID= 10 Loss: 0.442 | Acc: 85.227% (1200/1408)
[Test] Epoch= 5  BatchID= 20 Loss: 0.445 | Acc: 85.751% (2305/2688)
[Test] Epoch= 5  BatchID= 30 Loss: 0.445 | Acc: 85.911% (3372/3925)

Epoch: 6
[Train] Epoch= 6  BatchID= 0 Loss: 0.336 | Acc: 89.844% (115/128)
[Train] Epoch= 6  BatchID= 10 Loss: 0.292 | Acc: 90.980% (1281/1408)
[Train] Epoch= 6  BatchID= 20 Loss: 0.277 | Acc: 91.257% (2453/2688)
[Train] Epoch= 6  BatchID= 30 Loss: 0.268 | Acc: 91.205% (3619/3968)
[Train] Epoch= 6  BatchID= 40 Loss: 0.265 | Acc: 91.425% (4798/5248)
[Train] Epoch= 6  BatchID= 50 Loss: 0.264 | Acc: 91.483% (5972/6528)
[Train] Epoch= 6  BatchID= 60 Loss: 0.264 | Acc: 91.522% (7146/7808)
[Train] Epoch= 6  BatchID= 70 Loss: 0.263 | Acc: 91.615% (8326/9088)
[Test] Epoch= 6  BatchID= 0 Loss: 0.183 | Acc: 95.312% (122/128)
[Test] Epoch= 6  BatchID= 10 Loss: 0.378 | Acc: 88.352% (1244/1408)
[Test] Epoch= 6  BatchID= 20 Loss: 0.404 | Acc: 87.835% (2361/2688)
[Test] Epoch= 6  BatchID= 30 Loss: 0.417 | Acc: 87.592% (3438/3925)

Epoch: 7
[Train] Epoch= 7  BatchID= 0 Loss: 0.202 | Acc: 93.750% (120/128)
[Train] Epoch= 7  BatchID= 10 Loss: 0.243 | Acc: 91.974% (1295/1408)
[Train] Epoch= 7  BatchID= 20 Loss: 0.242 | Acc: 91.964% (2472/2688)
[Train] Epoch= 7  BatchID= 30 Loss: 0.248 | Acc: 92.061% (3653/3968)
[Train] Epoch= 7  BatchID= 40 Loss: 0.251 | Acc: 91.959% (4826/5248)
[Train] Epoch= 7  BatchID= 50 Loss: 0.254 | Acc: 91.789% (5992/6528)
[Train] Epoch= 7  BatchID= 60 Loss: 0.248 | Acc: 92.008% (7184/7808)
[Train] Epoch= 7  BatchID= 70 Loss: 0.247 | Acc: 92.000% (8361/9088)
[Test] Epoch= 7  BatchID= 0 Loss: 0.166 | Acc: 95.312% (122/128)
[Test] Epoch= 7  BatchID= 10 Loss: 0.384 | Acc: 88.494% (1246/1408)
[Test] Epoch= 7  BatchID= 20 Loss: 0.393 | Acc: 88.504% (2379/2688)
[Test] Epoch= 7  BatchID= 30 Loss: 0.425 | Acc: 87.643% (3440/3925)

Epoch: 8
[Train] Epoch= 8  BatchID= 0 Loss: 0.249 | Acc: 92.188% (118/128)
[Train] Epoch= 8  BatchID= 10 Loss: 0.242 | Acc: 92.827% (1307/1408)
[Train] Epoch= 8  BatchID= 20 Loss: 0.244 | Acc: 92.708% (2492/2688)
[Train] Epoch= 8  BatchID= 30 Loss: 0.237 | Acc: 92.792% (3682/3968)
[Train] Epoch= 8  BatchID= 40 Loss: 0.239 | Acc: 92.645% (4862/5248)
[Train] Epoch= 8  BatchID= 50 Loss: 0.236 | Acc: 92.785% (6057/6528)
[Train] Epoch= 8  BatchID= 60 Loss: 0.235 | Acc: 92.841% (7249/7808)
[Train] Epoch= 8  BatchID= 70 Loss: 0.236 | Acc: 92.804% (8434/9088)
[Test] Epoch= 8  BatchID= 0 Loss: 0.245 | Acc: 96.094% (123/128)
[Test] Epoch= 8  BatchID= 10 Loss: 0.380 | Acc: 88.565% (1247/1408)
[Test] Epoch= 8  BatchID= 20 Loss: 0.374 | Acc: 88.467% (2378/2688)
[Test] Epoch= 8  BatchID= 30 Loss: 0.406 | Acc: 87.592% (3438/3925)

Epoch: 9
[Train] Epoch= 9  BatchID= 0 Loss: 0.256 | Acc: 90.625% (116/128)
[Train] Epoch= 9  BatchID= 10 Loss: 0.230 | Acc: 93.324% (1314/1408)
[Train] Epoch= 9  BatchID= 20 Loss: 0.240 | Acc: 92.634% (2490/2688)
[Train] Epoch= 9  BatchID= 30 Loss: 0.233 | Acc: 92.843% (3684/3968)
[Train] Epoch= 9  BatchID= 40 Loss: 0.236 | Acc: 92.759% (4868/5248)
[Train] Epoch= 9  BatchID= 50 Loss: 0.234 | Acc: 92.831% (6060/6528)
[Train] Epoch= 9  BatchID= 60 Loss: 0.235 | Acc: 92.572% (7228/7808)
[Train] Epoch= 9  BatchID= 70 Loss: 0.237 | Acc: 92.430% (8400/9088)
[Test] Epoch= 9  BatchID= 0 Loss: 0.131 | Acc: 96.094% (123/128)
[Test] Epoch= 9  BatchID= 10 Loss: 0.357 | Acc: 88.494% (1246/1408)
[Test] Epoch= 9  BatchID= 20 Loss: 0.373 | Acc: 88.430% (2377/2688)
[Test] Epoch= 9  BatchID= 30 Loss: 0.402 | Acc: 87.567% (3437/3925)

Epoch: 10
[Train] Epoch= 10  BatchID= 0 Loss: 0.257 | Acc: 91.406% (117/128)
[Train] Epoch= 10  BatchID= 10 Loss: 0.235 | Acc: 92.685% (1305/1408)
[Train] Epoch= 10  BatchID= 20 Loss: 0.240 | Acc: 92.262% (2480/2688)
[Train] Epoch= 10  BatchID= 30 Loss: 0.237 | Acc: 92.314% (3663/3968)
[Train] Epoch= 10  BatchID= 40 Loss: 0.242 | Acc: 92.168% (4837/5248)
[Train] Epoch= 10  BatchID= 50 Loss: 0.246 | Acc: 92.065% (6010/6528)
[Train] Epoch= 10  BatchID= 60 Loss: 0.243 | Acc: 92.111% (7192/7808)
[Train] Epoch= 10  BatchID= 70 Loss: 0.249 | Acc: 91.934% (8355/9088)
[Test] Epoch= 10  BatchID= 0 Loss: 0.114 | Acc: 96.875% (124/128)
[Test] Epoch= 10  BatchID= 10 Loss: 0.352 | Acc: 88.636% (1248/1408)
[Test] Epoch= 10  BatchID= 20 Loss: 0.358 | Acc: 88.690% (2384/2688)
[Test] Epoch= 10  BatchID= 30 Loss: 0.396 | Acc: 87.796% (3446/3925)

Epoch: 11
[Train] Epoch= 11  BatchID= 0 Loss: 0.236 | Acc: 91.406% (117/128)
[Train] Epoch= 11  BatchID= 10 Loss: 0.217 | Acc: 93.324% (1314/1408)
[Train] Epoch= 11  BatchID= 20 Loss: 0.215 | Acc: 93.229% (2506/2688)
[Train] Epoch= 11  BatchID= 30 Loss: 0.222 | Acc: 92.969% (3689/3968)
[Train] Epoch= 11  BatchID= 40 Loss: 0.230 | Acc: 92.740% (4867/5248)
[Train] Epoch= 11  BatchID= 50 Loss: 0.227 | Acc: 92.816% (6059/6528)
[Train] Epoch= 11  BatchID= 60 Loss: 0.229 | Acc: 92.777% (7244/7808)
[Train] Epoch= 11  BatchID= 70 Loss: 0.229 | Acc: 92.760% (8430/9088)
[Test] Epoch= 11  BatchID= 0 Loss: 0.122 | Acc: 96.094% (123/128)
[Test] Epoch= 11  BatchID= 10 Loss: 0.341 | Acc: 89.134% (1255/1408)
[Test] Epoch= 11  BatchID= 20 Loss: 0.324 | Acc: 89.807% (2414/2688)
[Test] Epoch= 11  BatchID= 30 Loss: 0.409 | Acc: 87.694% (3442/3925)

Epoch: 12
[Train] Epoch= 12  BatchID= 0 Loss: 0.212 | Acc: 93.750% (120/128)
[Train] Epoch= 12  BatchID= 10 Loss: 0.229 | Acc: 92.614% (1304/1408)
[Train] Epoch= 12  BatchID= 20 Loss: 0.221 | Acc: 92.857% (2496/2688)
[Train] Epoch= 12  BatchID= 30 Loss: 0.228 | Acc: 92.944% (3688/3968)
[Train] Epoch= 12  BatchID= 40 Loss: 0.231 | Acc: 92.740% (4867/5248)
[Train] Epoch= 12  BatchID= 50 Loss: 0.226 | Acc: 92.862% (6062/6528)
[Train] Epoch= 12  BatchID= 60 Loss: 0.235 | Acc: 92.597% (7230/7808)
[Train] Epoch= 12  BatchID= 70 Loss: 0.235 | Acc: 92.496% (8406/9088)
[Test] Epoch= 12  BatchID= 0 Loss: 0.164 | Acc: 96.094% (123/128)
[Test] Epoch= 12  BatchID= 10 Loss: 0.436 | Acc: 86.506% (1218/1408)
[Test] Epoch= 12  BatchID= 20 Loss: 0.391 | Acc: 88.244% (2372/2688)
[Test] Epoch= 12  BatchID= 30 Loss: 0.433 | Acc: 87.185% (3422/3925)

Epoch: 13
[Train] Epoch= 13  BatchID= 0 Loss: 0.123 | Acc: 96.094% (123/128)
[Train] Epoch= 13  BatchID= 10 Loss: 0.220 | Acc: 93.679% (1319/1408)
[Train] Epoch= 13  BatchID= 20 Loss: 0.231 | Acc: 93.155% (2504/2688)
[Train] Epoch= 13  BatchID= 30 Loss: 0.230 | Acc: 93.095% (3694/3968)
[Train] Epoch= 13  BatchID= 40 Loss: 0.231 | Acc: 93.083% (4885/5248)
[Train] Epoch= 13  BatchID= 50 Loss: 0.231 | Acc: 93.045% (6074/6528)
[Train] Epoch= 13  BatchID= 60 Loss: 0.228 | Acc: 93.071% (7267/7808)
[Train] Epoch= 13  BatchID= 70 Loss: 0.230 | Acc: 92.947% (8447/9088)
[Test] Epoch= 13  BatchID= 0 Loss: 0.142 | Acc: 96.094% (123/128)
[Test] Epoch= 13  BatchID= 10 Loss: 0.370 | Acc: 88.565% (1247/1408)
[Test] Epoch= 13  BatchID= 20 Loss: 0.350 | Acc: 89.137% (2396/2688)
[Test] Epoch= 13  BatchID= 30 Loss: 0.374 | Acc: 88.611% (3478/3925)
Saving..
Best accuracy:  88.61146496815287

Epoch: 14
[Train] Epoch= 14  BatchID= 0 Loss: 0.263 | Acc: 90.625% (116/128)
[Train] Epoch= 14  BatchID= 10 Loss: 0.223 | Acc: 92.827% (1307/1408)
[Train] Epoch= 14  BatchID= 20 Loss: 0.217 | Acc: 93.378% (2510/2688)
[Train] Epoch= 14  BatchID= 30 Loss: 0.220 | Acc: 93.322% (3703/3968)
[Train] Epoch= 14  BatchID= 40 Loss: 0.221 | Acc: 93.140% (4888/5248)
[Train] Epoch= 14  BatchID= 50 Loss: 0.218 | Acc: 93.214% (6085/6528)
[Train] Epoch= 14  BatchID= 60 Loss: 0.217 | Acc: 93.186% (7276/7808)
[Train] Epoch= 14  BatchID= 70 Loss: 0.218 | Acc: 93.156% (8466/9088)
[Test] Epoch= 14  BatchID= 0 Loss: 0.179 | Acc: 96.094% (123/128)
[Test] Epoch= 14  BatchID= 10 Loss: 0.332 | Acc: 89.986% (1267/1408)
[Test] Epoch= 14  BatchID= 20 Loss: 0.378 | Acc: 88.579% (2381/2688)
[Test] Epoch= 14  BatchID= 30 Loss: 0.401 | Acc: 87.694% (3442/3925)

Epoch: 15
[Train] Epoch= 15  BatchID= 0 Loss: 0.157 | Acc: 92.969% (119/128)
[Train] Epoch= 15  BatchID= 10 Loss: 0.199 | Acc: 93.537% (1317/1408)
[Train] Epoch= 15  BatchID= 20 Loss: 0.222 | Acc: 93.043% (2501/2688)
[Train] Epoch= 15  BatchID= 30 Loss: 0.210 | Acc: 93.523% (3711/3968)
[Train] Epoch= 15  BatchID= 40 Loss: 0.210 | Acc: 93.559% (4910/5248)
[Train] Epoch= 15  BatchID= 50 Loss: 0.215 | Acc: 93.306% (6091/6528)
[Train] Epoch= 15  BatchID= 60 Loss: 0.225 | Acc: 92.943% (7257/7808)
[Train] Epoch= 15  BatchID= 70 Loss: 0.224 | Acc: 92.991% (8451/9088)
[Test] Epoch= 15  BatchID= 0 Loss: 0.173 | Acc: 97.656% (125/128)
[Test] Epoch= 15  BatchID= 10 Loss: 0.362 | Acc: 89.631% (1262/1408)
[Test] Epoch= 15  BatchID= 20 Loss: 0.348 | Acc: 89.546% (2407/2688)
[Test] Epoch= 15  BatchID= 30 Loss: 0.375 | Acc: 88.408% (3470/3925)

Epoch: 16
[Train] Epoch= 16  BatchID= 0 Loss: 0.271 | Acc: 90.625% (116/128)
[Train] Epoch= 16  BatchID= 10 Loss: 0.236 | Acc: 92.543% (1303/1408)
[Train] Epoch= 16  BatchID= 20 Loss: 0.224 | Acc: 92.969% (2499/2688)
[Train] Epoch= 16  BatchID= 30 Loss: 0.231 | Acc: 92.792% (3682/3968)
[Train] Epoch= 16  BatchID= 40 Loss: 0.225 | Acc: 93.083% (4885/5248)
[Train] Epoch= 16  BatchID= 50 Loss: 0.229 | Acc: 92.969% (6069/6528)
[Train] Epoch= 16  BatchID= 60 Loss: 0.230 | Acc: 92.879% (7252/7808)
[Train] Epoch= 16  BatchID= 70 Loss: 0.230 | Acc: 92.914% (8444/9088)
[Test] Epoch= 16  BatchID= 0 Loss: 0.135 | Acc: 96.875% (124/128)
[Test] Epoch= 16  BatchID= 10 Loss: 0.379 | Acc: 88.423% (1245/1408)
[Test] Epoch= 16  BatchID= 20 Loss: 0.356 | Acc: 89.435% (2404/2688)
[Test] Epoch= 16  BatchID= 30 Loss: 0.382 | Acc: 88.611% (3478/3925)

Epoch: 17
[Train] Epoch= 17  BatchID= 0 Loss: 0.303 | Acc: 92.188% (118/128)
[Train] Epoch= 17  BatchID= 10 Loss: 0.186 | Acc: 94.460% (1330/1408)
[Train] Epoch= 17  BatchID= 20 Loss: 0.197 | Acc: 93.899% (2524/2688)
[Train] Epoch= 17  BatchID= 30 Loss: 0.196 | Acc: 93.800% (3722/3968)
[Train] Epoch= 17  BatchID= 40 Loss: 0.197 | Acc: 93.864% (4926/5248)
[Train] Epoch= 17  BatchID= 50 Loss: 0.201 | Acc: 93.765% (6121/6528)
[Train] Epoch= 17  BatchID= 60 Loss: 0.203 | Acc: 93.712% (7317/7808)
[Train] Epoch= 17  BatchID= 70 Loss: 0.211 | Acc: 93.552% (8502/9088)
[Test] Epoch= 17  BatchID= 0 Loss: 0.123 | Acc: 96.875% (124/128)
[Test] Epoch= 17  BatchID= 10 Loss: 0.383 | Acc: 88.139% (1241/1408)
[Test] Epoch= 17  BatchID= 20 Loss: 0.362 | Acc: 88.839% (2388/2688)
[Test] Epoch= 17  BatchID= 30 Loss: 0.389 | Acc: 88.459% (3472/3925)

Epoch: 18
[Train] Epoch= 18  BatchID= 0 Loss: 0.190 | Acc: 94.531% (121/128)
[Train] Epoch= 18  BatchID= 10 Loss: 0.207 | Acc: 93.963% (1323/1408)
[Train] Epoch= 18  BatchID= 20 Loss: 0.216 | Acc: 93.713% (2519/2688)
[Train] Epoch= 18  BatchID= 30 Loss: 0.210 | Acc: 93.775% (3721/3968)
[Train] Epoch= 18  BatchID= 40 Loss: 0.214 | Acc: 93.521% (4908/5248)
[Train] Epoch= 18  BatchID= 50 Loss: 0.209 | Acc: 93.627% (6112/6528)
[Train] Epoch= 18  BatchID= 60 Loss: 0.208 | Acc: 93.673% (7314/7808)
[Train] Epoch= 18  BatchID= 70 Loss: 0.209 | Acc: 93.585% (8505/9088)
[Test] Epoch= 18  BatchID= 0 Loss: 0.184 | Acc: 95.312% (122/128)
[Test] Epoch= 18  BatchID= 10 Loss: 0.345 | Acc: 88.991% (1253/1408)
[Test] Epoch= 18  BatchID= 20 Loss: 0.359 | Acc: 89.062% (2394/2688)
[Test] Epoch= 18  BatchID= 30 Loss: 0.395 | Acc: 88.102% (3458/3925)

Epoch: 19
[Train] Epoch= 19  BatchID= 0 Loss: 0.216 | Acc: 92.969% (119/128)
[Train] Epoch= 19  BatchID= 10 Loss: 0.214 | Acc: 93.253% (1313/1408)
[Train] Epoch= 19  BatchID= 20 Loss: 0.214 | Acc: 93.601% (2516/2688)
[Train] Epoch= 19  BatchID= 30 Loss: 0.226 | Acc: 93.044% (3692/3968)
[Train] Epoch= 19  BatchID= 40 Loss: 0.225 | Acc: 93.159% (4889/5248)
[Train] Epoch= 19  BatchID= 50 Loss: 0.223 | Acc: 93.076% (6076/6528)
[Train] Epoch= 19  BatchID= 60 Loss: 0.218 | Acc: 93.225% (7279/7808)
[Train] Epoch= 19  BatchID= 70 Loss: 0.218 | Acc: 93.222% (8472/9088)
[Test] Epoch= 19  BatchID= 0 Loss: 0.207 | Acc: 95.312% (122/128)
[Test] Epoch= 19  BatchID= 10 Loss: 0.337 | Acc: 88.991% (1253/1408)
[Test] Epoch= 19  BatchID= 20 Loss: 0.344 | Acc: 88.988% (2392/2688)
[Test] Epoch= 19  BatchID= 30 Loss: 0.378 | Acc: 88.484% (3473/3925)

Epoch: 20
[Train] Epoch= 20  BatchID= 0 Loss: 0.180 | Acc: 95.312% (122/128)
[Train] Epoch= 20  BatchID= 10 Loss: 0.215 | Acc: 92.543% (1303/1408)
[Train] Epoch= 20  BatchID= 20 Loss: 0.216 | Acc: 92.969% (2499/2688)
[Train] Epoch= 20  BatchID= 30 Loss: 0.221 | Acc: 92.742% (3680/3968)
[Train] Epoch= 20  BatchID= 40 Loss: 0.219 | Acc: 92.969% (4879/5248)
[Train] Epoch= 20  BatchID= 50 Loss: 0.214 | Acc: 93.153% (6081/6528)
[Train] Epoch= 20  BatchID= 60 Loss: 0.214 | Acc: 93.186% (7276/7808)
[Train] Epoch= 20  BatchID= 70 Loss: 0.210 | Acc: 93.332% (8482/9088)
[Test] Epoch= 20  BatchID= 0 Loss: 0.143 | Acc: 96.094% (123/128)
[Test] Epoch= 20  BatchID= 10 Loss: 0.337 | Acc: 89.062% (1254/1408)
[Test] Epoch= 20  BatchID= 20 Loss: 0.352 | Acc: 89.137% (2396/2688)
[Test] Epoch= 20  BatchID= 30 Loss: 0.390 | Acc: 88.229% (3463/3925)

Epoch: 21
[Train] Epoch= 21  BatchID= 0 Loss: 0.286 | Acc: 92.969% (119/128)
[Train] Epoch= 21  BatchID= 10 Loss: 0.211 | Acc: 94.815% (1335/1408)
[Train] Epoch= 21  BatchID= 20 Loss: 0.199 | Acc: 94.606% (2543/2688)
[Train] Epoch= 21  BatchID= 30 Loss: 0.205 | Acc: 94.078% (3733/3968)
[Train] Epoch= 21  BatchID= 40 Loss: 0.208 | Acc: 93.788% (4922/5248)
[Train] Epoch= 21  BatchID= 50 Loss: 0.209 | Acc: 93.627% (6112/6528)
[Train] Epoch= 21  BatchID= 60 Loss: 0.211 | Acc: 93.558% (7305/7808)
[Train] Epoch= 21  BatchID= 70 Loss: 0.211 | Acc: 93.453% (8493/9088)
[Test] Epoch= 21  BatchID= 0 Loss: 0.128 | Acc: 96.875% (124/128)
[Test] Epoch= 21  BatchID= 10 Loss: 0.376 | Acc: 87.642% (1234/1408)
[Test] Epoch= 21  BatchID= 20 Loss: 0.347 | Acc: 89.025% (2393/2688)
[Test] Epoch= 21  BatchID= 30 Loss: 0.368 | Acc: 88.586% (3477/3925)

Epoch: 22
[Train] Epoch= 22  BatchID= 0 Loss: 0.200 | Acc: 93.750% (120/128)
[Train] Epoch= 22  BatchID= 10 Loss: 0.203 | Acc: 94.176% (1326/1408)
[Train] Epoch= 22  BatchID= 20 Loss: 0.196 | Acc: 94.234% (2533/2688)
[Train] Epoch= 22  BatchID= 30 Loss: 0.197 | Acc: 93.926% (3727/3968)
[Train] Epoch= 22  BatchID= 40 Loss: 0.196 | Acc: 93.960% (4931/5248)
[Train] Epoch= 22  BatchID= 50 Loss: 0.201 | Acc: 93.827% (6125/6528)
[Train] Epoch= 22  BatchID= 60 Loss: 0.198 | Acc: 93.916% (7333/7808)
[Train] Epoch= 22  BatchID= 70 Loss: 0.203 | Acc: 93.838% (8528/9088)
[Test] Epoch= 22  BatchID= 0 Loss: 0.138 | Acc: 96.094% (123/128)
[Test] Epoch= 22  BatchID= 10 Loss: 0.324 | Acc: 89.915% (1266/1408)
[Test] Epoch= 22  BatchID= 20 Loss: 0.328 | Acc: 89.844% (2415/2688)
[Test] Epoch= 22  BatchID= 30 Loss: 0.366 | Acc: 88.688% (3481/3925)
Saving..
Best accuracy:  88.68789808917198

Epoch: 23
[Train] Epoch= 23  BatchID= 0 Loss: 0.132 | Acc: 96.875% (124/128)
[Train] Epoch= 23  BatchID= 10 Loss: 0.192 | Acc: 93.963% (1323/1408)
[Train] Epoch= 23  BatchID= 20 Loss: 0.197 | Acc: 93.713% (2519/2688)
[Train] Epoch= 23  BatchID= 30 Loss: 0.195 | Acc: 93.876% (3725/3968)
[Train] Epoch= 23  BatchID= 40 Loss: 0.198 | Acc: 93.788% (4922/5248)
[Train] Epoch= 23  BatchID= 50 Loss: 0.200 | Acc: 93.689% (6116/6528)
[Train] Epoch= 23  BatchID= 60 Loss: 0.198 | Acc: 93.801% (7324/7808)
[Train] Epoch= 23  BatchID= 70 Loss: 0.199 | Acc: 93.717% (8517/9088)
[Test] Epoch= 23  BatchID= 0 Loss: 0.133 | Acc: 96.094% (123/128)
[Test] Epoch= 23  BatchID= 10 Loss: 0.376 | Acc: 88.281% (1243/1408)
[Test] Epoch= 23  BatchID= 20 Loss: 0.357 | Acc: 88.765% (2386/2688)
[Test] Epoch= 23  BatchID= 30 Loss: 0.381 | Acc: 88.127% (3459/3925)

Epoch: 24
[Train] Epoch= 24  BatchID= 0 Loss: 0.121 | Acc: 96.875% (124/128)
[Train] Epoch= 24  BatchID= 10 Loss: 0.204 | Acc: 93.608% (1318/1408)
[Train] Epoch= 24  BatchID= 20 Loss: 0.211 | Acc: 93.155% (2504/2688)
[Train] Epoch= 24  BatchID= 30 Loss: 0.210 | Acc: 93.296% (3702/3968)
[Train] Epoch= 24  BatchID= 40 Loss: 0.203 | Acc: 93.598% (4912/5248)
[Train] Epoch= 24  BatchID= 50 Loss: 0.205 | Acc: 93.612% (6111/6528)
[Train] Epoch= 24  BatchID= 60 Loss: 0.205 | Acc: 93.635% (7311/7808)
[Train] Epoch= 24  BatchID= 70 Loss: 0.205 | Acc: 93.618% (8508/9088)
[Test] Epoch= 24  BatchID= 0 Loss: 0.136 | Acc: 96.094% (123/128)
[Test] Epoch= 24  BatchID= 10 Loss: 0.338 | Acc: 89.773% (1264/1408)
[Test] Epoch= 24  BatchID= 20 Loss: 0.331 | Acc: 90.067% (2421/2688)
[Test] Epoch= 24  BatchID= 30 Loss: 0.372 | Acc: 88.866% (3488/3925)
Saving..
Best accuracy:  88.86624203821655

Epoch: 25
[Train] Epoch= 25  BatchID= 0 Loss: 0.183 | Acc: 93.750% (120/128)
[Train] Epoch= 25  BatchID= 10 Loss: 0.175 | Acc: 94.744% (1334/1408)
[Train] Epoch= 25  BatchID= 20 Loss: 0.185 | Acc: 94.271% (2534/2688)
[Train] Epoch= 25  BatchID= 30 Loss: 0.205 | Acc: 93.775% (3721/3968)
[Train] Epoch= 25  BatchID= 40 Loss: 0.209 | Acc: 93.445% (4904/5248)
[Train] Epoch= 25  BatchID= 50 Loss: 0.210 | Acc: 93.367% (6095/6528)
[Train] Epoch= 25  BatchID= 60 Loss: 0.211 | Acc: 93.340% (7288/7808)
[Train] Epoch= 25  BatchID= 70 Loss: 0.209 | Acc: 93.420% (8490/9088)
[Test] Epoch= 25  BatchID= 0 Loss: 0.112 | Acc: 97.656% (125/128)
[Test] Epoch= 25  BatchID= 10 Loss: 0.307 | Acc: 90.412% (1273/1408)
[Test] Epoch= 25  BatchID= 20 Loss: 0.332 | Acc: 90.030% (2420/2688)
[Test] Epoch= 25  BatchID= 30 Loss: 0.370 | Acc: 88.917% (3490/3925)
Saving..
Best accuracy:  88.9171974522293

Epoch: 26
[Train] Epoch= 26  BatchID= 0 Loss: 0.129 | Acc: 94.531% (121/128)
[Train] Epoch= 26  BatchID= 10 Loss: 0.177 | Acc: 94.176% (1326/1408)
[Train] Epoch= 26  BatchID= 20 Loss: 0.195 | Acc: 93.638% (2517/2688)
[Train] Epoch= 26  BatchID= 30 Loss: 0.194 | Acc: 93.725% (3719/3968)
[Train] Epoch= 26  BatchID= 40 Loss: 0.199 | Acc: 93.540% (4909/5248)
[Train] Epoch= 26  BatchID= 50 Loss: 0.206 | Acc: 93.428% (6099/6528)
[Train] Epoch= 26  BatchID= 60 Loss: 0.211 | Acc: 93.174% (7275/7808)
[Train] Epoch= 26  BatchID= 70 Loss: 0.212 | Acc: 93.288% (8478/9088)
[Test] Epoch= 26  BatchID= 0 Loss: 0.121 | Acc: 96.875% (124/128)
[Test] Epoch= 26  BatchID= 10 Loss: 0.372 | Acc: 88.068% (1240/1408)
[Test] Epoch= 26  BatchID= 20 Loss: 0.357 | Acc: 88.839% (2388/2688)
[Test] Epoch= 26  BatchID= 30 Loss: 0.370 | Acc: 88.662% (3480/3925)

Epoch: 27
[Train] Epoch= 27  BatchID= 0 Loss: 0.274 | Acc: 92.188% (118/128)
[Train] Epoch= 27  BatchID= 10 Loss: 0.223 | Acc: 93.182% (1312/1408)
[Train] Epoch= 27  BatchID= 20 Loss: 0.214 | Acc: 93.378% (2510/2688)
[Train] Epoch= 27  BatchID= 30 Loss: 0.216 | Acc: 93.170% (3697/3968)
[Train] Epoch= 27  BatchID= 40 Loss: 0.208 | Acc: 93.445% (4904/5248)
[Train] Epoch= 27  BatchID= 50 Loss: 0.207 | Acc: 93.490% (6103/6528)
[Train] Epoch= 27  BatchID= 60 Loss: 0.211 | Acc: 93.481% (7299/7808)
[Train] Epoch= 27  BatchID= 70 Loss: 0.213 | Acc: 93.519% (8499/9088)
[Test] Epoch= 27  BatchID= 0 Loss: 0.185 | Acc: 95.312% (122/128)
[Test] Epoch= 27  BatchID= 10 Loss: 0.348 | Acc: 89.347% (1258/1408)
[Test] Epoch= 27  BatchID= 20 Loss: 0.328 | Acc: 90.030% (2420/2688)
[Test] Epoch= 27  BatchID= 30 Loss: 0.375 | Acc: 88.561% (3476/3925)

Epoch: 28
[Train] Epoch= 28  BatchID= 0 Loss: 0.219 | Acc: 93.750% (120/128)
[Train] Epoch= 28  BatchID= 10 Loss: 0.204 | Acc: 93.821% (1321/1408)
[Train] Epoch= 28  BatchID= 20 Loss: 0.210 | Acc: 93.862% (2523/2688)
[Train] Epoch= 28  BatchID= 30 Loss: 0.208 | Acc: 93.800% (3722/3968)
[Train] Epoch= 28  BatchID= 40 Loss: 0.207 | Acc: 93.693% (4917/5248)
[Train] Epoch= 28  BatchID= 50 Loss: 0.204 | Acc: 93.842% (6126/6528)
[Train] Epoch= 28  BatchID= 60 Loss: 0.205 | Acc: 93.814% (7325/7808)
[Train] Epoch= 28  BatchID= 70 Loss: 0.206 | Acc: 93.629% (8509/9088)
[Test] Epoch= 28  BatchID= 0 Loss: 0.151 | Acc: 96.875% (124/128)
[Test] Epoch= 28  BatchID= 10 Loss: 0.328 | Acc: 89.489% (1260/1408)
[Test] Epoch= 28  BatchID= 20 Loss: 0.318 | Acc: 89.844% (2415/2688)
[Test] Epoch= 28  BatchID= 30 Loss: 0.372 | Acc: 88.357% (3468/3925)

Epoch: 29
[Train] Epoch= 29  BatchID= 0 Loss: 0.124 | Acc: 96.875% (124/128)
[Train] Epoch= 29  BatchID= 10 Loss: 0.171 | Acc: 95.170% (1340/1408)
[Train] Epoch= 29  BatchID= 20 Loss: 0.185 | Acc: 94.457% (2539/2688)
[Train] Epoch= 29  BatchID= 30 Loss: 0.186 | Acc: 94.052% (3732/3968)
[Train] Epoch= 29  BatchID= 40 Loss: 0.185 | Acc: 94.131% (4940/5248)
[Train] Epoch= 29  BatchID= 50 Loss: 0.187 | Acc: 93.980% (6135/6528)
[Train] Epoch= 29  BatchID= 60 Loss: 0.188 | Acc: 93.916% (7333/7808)
[Train] Epoch= 29  BatchID= 70 Loss: 0.190 | Acc: 93.838% (8528/9088)
[Test] Epoch= 29  BatchID= 0 Loss: 0.119 | Acc: 98.438% (126/128)
[Test] Epoch= 29  BatchID= 10 Loss: 0.353 | Acc: 88.849% (1251/1408)
[Test] Epoch= 29  BatchID= 20 Loss: 0.347 | Acc: 89.137% (2396/2688)
[Test] Epoch= 29  BatchID= 30 Loss: 0.362 | Acc: 88.739% (3483/3925)

Epoch: 30
[Train] Epoch= 30  BatchID= 0 Loss: 0.261 | Acc: 91.406% (117/128)
[Train] Epoch= 30  BatchID= 10 Loss: 0.197 | Acc: 94.460% (1330/1408)
[Train] Epoch= 30  BatchID= 20 Loss: 0.195 | Acc: 94.382% (2537/2688)
[Train] Epoch= 30  BatchID= 30 Loss: 0.194 | Acc: 94.229% (3739/3968)
[Train] Epoch= 30  BatchID= 40 Loss: 0.196 | Acc: 94.036% (4935/5248)
[Train] Epoch= 30  BatchID= 50 Loss: 0.199 | Acc: 93.873% (6128/6528)
[Train] Epoch= 30  BatchID= 60 Loss: 0.200 | Acc: 93.776% (7322/7808)
[Train] Epoch= 30  BatchID= 70 Loss: 0.194 | Acc: 93.992% (8542/9088)
[Test] Epoch= 30  BatchID= 0 Loss: 0.168 | Acc: 95.312% (122/128)
[Test] Epoch= 30  BatchID= 10 Loss: 0.318 | Acc: 90.128% (1269/1408)
[Test] Epoch= 30  BatchID= 20 Loss: 0.330 | Acc: 90.104% (2422/2688)
[Test] Epoch= 30  BatchID= 30 Loss: 0.368 | Acc: 88.866% (3488/3925)

Epoch: 31
[Train] Epoch= 31  BatchID= 0 Loss: 0.233 | Acc: 91.406% (117/128)
[Train] Epoch= 31  BatchID= 10 Loss: 0.176 | Acc: 94.318% (1328/1408)
[Train] Epoch= 31  BatchID= 20 Loss: 0.193 | Acc: 94.048% (2528/2688)
[Train] Epoch= 31  BatchID= 30 Loss: 0.192 | Acc: 94.380% (3745/3968)
[Train] Epoch= 31  BatchID= 40 Loss: 0.191 | Acc: 94.303% (4949/5248)
[Train] Epoch= 31  BatchID= 50 Loss: 0.193 | Acc: 94.271% (6154/6528)
[Train] Epoch= 31  BatchID= 60 Loss: 0.195 | Acc: 94.262% (7360/7808)
[Train] Epoch= 31  BatchID= 70 Loss: 0.193 | Acc: 94.157% (8557/9088)
[Test] Epoch= 31  BatchID= 0 Loss: 0.149 | Acc: 96.094% (123/128)
[Test] Epoch= 31  BatchID= 10 Loss: 0.398 | Acc: 88.139% (1241/1408)
[Test] Epoch= 31  BatchID= 20 Loss: 0.354 | Acc: 89.137% (2396/2688)
[Test] Epoch= 31  BatchID= 30 Loss: 0.385 | Acc: 88.229% (3463/3925)

Epoch: 32
[Train] Epoch= 32  BatchID= 0 Loss: 0.169 | Acc: 96.094% (123/128)
[Train] Epoch= 32  BatchID= 10 Loss: 0.169 | Acc: 95.170% (1340/1408)
[Train] Epoch= 32  BatchID= 20 Loss: 0.176 | Acc: 94.680% (2545/2688)
[Train] Epoch= 32  BatchID= 30 Loss: 0.181 | Acc: 94.456% (3748/3968)
[Train] Epoch= 32  BatchID= 40 Loss: 0.181 | Acc: 94.455% (4957/5248)
[Train] Epoch= 32  BatchID= 50 Loss: 0.182 | Acc: 94.363% (6160/6528)
[Train] Epoch= 32  BatchID= 60 Loss: 0.181 | Acc: 94.224% (7357/7808)
[Train] Epoch= 32  BatchID= 70 Loss: 0.183 | Acc: 94.190% (8560/9088)
[Test] Epoch= 32  BatchID= 0 Loss: 0.148 | Acc: 96.875% (124/128)
[Test] Epoch= 32  BatchID= 10 Loss: 0.347 | Acc: 89.489% (1260/1408)
[Test] Epoch= 32  BatchID= 20 Loss: 0.341 | Acc: 89.695% (2411/2688)
[Test] Epoch= 32  BatchID= 30 Loss: 0.373 | Acc: 88.739% (3483/3925)

Epoch: 33
[Train] Epoch= 33  BatchID= 0 Loss: 0.132 | Acc: 96.094% (123/128)
[Train] Epoch= 33  BatchID= 10 Loss: 0.197 | Acc: 93.324% (1314/1408)
[Train] Epoch= 33  BatchID= 20 Loss: 0.188 | Acc: 94.308% (2535/2688)
[Train] Epoch= 33  BatchID= 30 Loss: 0.186 | Acc: 94.229% (3739/3968)
[Train] Epoch= 33  BatchID= 40 Loss: 0.182 | Acc: 94.455% (4957/5248)
[Train] Epoch= 33  BatchID= 50 Loss: 0.180 | Acc: 94.501% (6169/6528)
[Train] Epoch= 33  BatchID= 60 Loss: 0.181 | Acc: 94.570% (7384/7808)
[Train] Epoch= 33  BatchID= 70 Loss: 0.182 | Acc: 94.531% (8591/9088)
[Test] Epoch= 33  BatchID= 0 Loss: 0.175 | Acc: 96.094% (123/128)
[Test] Epoch= 33  BatchID= 10 Loss: 0.358 | Acc: 89.205% (1256/1408)
[Test] Epoch= 33  BatchID= 20 Loss: 0.342 | Acc: 89.807% (2414/2688)
[Test] Epoch= 33  BatchID= 30 Loss: 0.371 | Acc: 88.815% (3486/3925)

Epoch: 34
[Train] Epoch= 34  BatchID= 0 Loss: 0.126 | Acc: 95.312% (122/128)
[Train] Epoch= 34  BatchID= 10 Loss: 0.160 | Acc: 95.241% (1341/1408)
[Train] Epoch= 34  BatchID= 20 Loss: 0.173 | Acc: 94.866% (2550/2688)
[Train] Epoch= 34  BatchID= 30 Loss: 0.183 | Acc: 94.456% (3748/3968)
[Train] Epoch= 34  BatchID= 40 Loss: 0.180 | Acc: 94.493% (4959/5248)
[Train] Epoch= 34  BatchID= 50 Loss: 0.184 | Acc: 94.256% (6153/6528)
[Train] Epoch= 34  BatchID= 60 Loss: 0.186 | Acc: 94.275% (7361/7808)
[Train] Epoch= 34  BatchID= 70 Loss: 0.187 | Acc: 94.267% (8567/9088)
[Test] Epoch= 34  BatchID= 0 Loss: 0.170 | Acc: 95.312% (122/128)
[Test] Epoch= 34  BatchID= 10 Loss: 0.297 | Acc: 91.122% (1283/1408)
[Test] Epoch= 34  BatchID= 20 Loss: 0.304 | Acc: 90.625% (2436/2688)
[Test] Epoch= 34  BatchID= 30 Loss: 0.365 | Acc: 88.866% (3488/3925)

Epoch: 35
[Train] Epoch= 35  BatchID= 0 Loss: 0.124 | Acc: 95.312% (122/128)
[Train] Epoch= 35  BatchID= 10 Loss: 0.164 | Acc: 94.886% (1336/1408)
[Train] Epoch= 35  BatchID= 20 Loss: 0.183 | Acc: 94.271% (2534/2688)
[Train] Epoch= 35  BatchID= 30 Loss: 0.183 | Acc: 94.229% (3739/3968)
[Train] Epoch= 35  BatchID= 40 Loss: 0.189 | Acc: 93.998% (4933/5248)
[Train] Epoch= 35  BatchID= 50 Loss: 0.187 | Acc: 94.026% (6138/6528)
[Train] Epoch= 35  BatchID= 60 Loss: 0.185 | Acc: 94.057% (7344/7808)
[Train] Epoch= 35  BatchID= 70 Loss: 0.185 | Acc: 94.113% (8553/9088)
[Test] Epoch= 35  BatchID= 0 Loss: 0.167 | Acc: 95.312% (122/128)
[Test] Epoch= 35  BatchID= 10 Loss: 0.375 | Acc: 88.210% (1242/1408)
[Test] Epoch= 35  BatchID= 20 Loss: 0.339 | Acc: 89.360% (2402/2688)
[Test] Epoch= 35  BatchID= 30 Loss: 0.377 | Acc: 88.331% (3467/3925)

Epoch: 36
[Train] Epoch= 36  BatchID= 0 Loss: 0.214 | Acc: 92.188% (118/128)
[Train] Epoch= 36  BatchID= 10 Loss: 0.211 | Acc: 93.537% (1317/1408)
[Train] Epoch= 36  BatchID= 20 Loss: 0.200 | Acc: 93.601% (2516/2688)
[Train] Epoch= 36  BatchID= 30 Loss: 0.203 | Acc: 93.624% (3715/3968)
[Train] Epoch= 36  BatchID= 40 Loss: 0.198 | Acc: 93.655% (4915/5248)
[Train] Epoch= 36  BatchID= 50 Loss: 0.197 | Acc: 93.750% (6120/6528)
[Train] Epoch= 36  BatchID= 60 Loss: 0.191 | Acc: 94.019% (7341/7808)
[Train] Epoch= 36  BatchID= 70 Loss: 0.191 | Acc: 94.036% (8546/9088)
[Test] Epoch= 36  BatchID= 0 Loss: 0.130 | Acc: 96.875% (124/128)
[Test] Epoch= 36  BatchID= 10 Loss: 0.343 | Acc: 89.560% (1261/1408)
[Test] Epoch= 36  BatchID= 20 Loss: 0.334 | Acc: 89.993% (2419/2688)
[Test] Epoch= 36  BatchID= 30 Loss: 0.370 | Acc: 89.197% (3501/3925)
Saving..
Best accuracy:  89.19745222929936

Epoch: 37
[Train] Epoch= 37  BatchID= 0 Loss: 0.186 | Acc: 92.969% (119/128)
[Train] Epoch= 37  BatchID= 10 Loss: 0.173 | Acc: 94.389% (1329/1408)
[Train] Epoch= 37  BatchID= 20 Loss: 0.171 | Acc: 94.680% (2545/2688)
[Train] Epoch= 37  BatchID= 30 Loss: 0.170 | Acc: 94.682% (3757/3968)
[Train] Epoch= 37  BatchID= 40 Loss: 0.179 | Acc: 94.417% (4955/5248)
[Train] Epoch= 37  BatchID= 50 Loss: 0.179 | Acc: 94.393% (6162/6528)
[Train] Epoch= 37  BatchID= 60 Loss: 0.183 | Acc: 94.403% (7371/7808)
[Train] Epoch= 37  BatchID= 70 Loss: 0.188 | Acc: 94.212% (8562/9088)
[Test] Epoch= 37  BatchID= 0 Loss: 0.143 | Acc: 95.312% (122/128)
[Test] Epoch= 37  BatchID= 10 Loss: 0.323 | Acc: 90.057% (1268/1408)
[Test] Epoch= 37  BatchID= 20 Loss: 0.302 | Acc: 90.699% (2438/2688)
[Test] Epoch= 37  BatchID= 30 Loss: 0.360 | Acc: 88.917% (3490/3925)

Epoch: 38
[Train] Epoch= 38  BatchID= 0 Loss: 0.192 | Acc: 93.750% (120/128)
[Train] Epoch= 38  BatchID= 10 Loss: 0.186 | Acc: 93.892% (1322/1408)
[Train] Epoch= 38  BatchID= 20 Loss: 0.198 | Acc: 93.527% (2514/2688)
[Train] Epoch= 38  BatchID= 30 Loss: 0.193 | Acc: 93.674% (3717/3968)
[Train] Epoch= 38  BatchID= 40 Loss: 0.188 | Acc: 94.036% (4935/5248)
[Train] Epoch= 38  BatchID= 50 Loss: 0.187 | Acc: 94.210% (6150/6528)
[Train] Epoch= 38  BatchID= 60 Loss: 0.181 | Acc: 94.352% (7367/7808)
[Train] Epoch= 38  BatchID= 70 Loss: 0.182 | Acc: 94.311% (8571/9088)
[Test] Epoch= 38  BatchID= 0 Loss: 0.221 | Acc: 93.750% (120/128)
[Test] Epoch= 38  BatchID= 10 Loss: 0.391 | Acc: 87.642% (1234/1408)
[Test] Epoch= 38  BatchID= 20 Loss: 0.360 | Acc: 88.653% (2383/2688)
[Test] Epoch= 38  BatchID= 30 Loss: 0.375 | Acc: 88.280% (3465/3925)

Epoch: 39
[Train] Epoch= 39  BatchID= 0 Loss: 0.150 | Acc: 95.312% (122/128)
[Train] Epoch= 39  BatchID= 10 Loss: 0.186 | Acc: 94.247% (1327/1408)
[Train] Epoch= 39  BatchID= 20 Loss: 0.182 | Acc: 94.531% (2541/2688)
[Train] Epoch= 39  BatchID= 30 Loss: 0.175 | Acc: 94.481% (3749/3968)
[Train] Epoch= 39  BatchID= 40 Loss: 0.179 | Acc: 94.474% (4958/5248)
[Train] Epoch= 39  BatchID= 50 Loss: 0.175 | Acc: 94.638% (6178/6528)
[Train] Epoch= 39  BatchID= 60 Loss: 0.175 | Acc: 94.634% (7389/7808)
[Train] Epoch= 39  BatchID= 70 Loss: 0.176 | Acc: 94.575% (8595/9088)
[Test] Epoch= 39  BatchID= 0 Loss: 0.163 | Acc: 95.312% (122/128)
[Test] Epoch= 39  BatchID= 10 Loss: 0.361 | Acc: 89.347% (1258/1408)
[Test] Epoch= 39  BatchID= 20 Loss: 0.364 | Acc: 89.918% (2417/2688)
[Test] Epoch= 39  BatchID= 30 Loss: 0.388 | Acc: 89.146% (3499/3925)

Epoch: 40
[Train] Epoch= 40  BatchID= 0 Loss: 0.169 | Acc: 93.750% (120/128)
[Train] Epoch= 40  BatchID= 10 Loss: 0.183 | Acc: 94.531% (1331/1408)
[Train] Epoch= 40  BatchID= 20 Loss: 0.169 | Acc: 94.829% (2549/2688)
[Train] Epoch= 40  BatchID= 30 Loss: 0.174 | Acc: 94.556% (3752/3968)
[Train] Epoch= 40  BatchID= 40 Loss: 0.171 | Acc: 94.703% (4970/5248)
[Train] Epoch= 40  BatchID= 50 Loss: 0.173 | Acc: 94.638% (6178/6528)
[Train] Epoch= 40  BatchID= 60 Loss: 0.169 | Acc: 94.787% (7401/7808)
[Train] Epoch= 40  BatchID= 70 Loss: 0.168 | Acc: 94.916% (8626/9088)
[Test] Epoch= 40  BatchID= 0 Loss: 0.123 | Acc: 96.875% (124/128)
[Test] Epoch= 40  BatchID= 10 Loss: 0.317 | Acc: 89.773% (1264/1408)
[Test] Epoch= 40  BatchID= 20 Loss: 0.320 | Acc: 89.658% (2410/2688)
[Test] Epoch= 40  BatchID= 30 Loss: 0.349 | Acc: 89.121% (3498/3925)

Epoch: 41
[Train] Epoch= 41  BatchID= 0 Loss: 0.169 | Acc: 95.312% (122/128)
[Train] Epoch= 41  BatchID= 10 Loss: 0.147 | Acc: 96.023% (1352/1408)
[Train] Epoch= 41  BatchID= 20 Loss: 0.162 | Acc: 95.461% (2566/2688)
[Train] Epoch= 41  BatchID= 30 Loss: 0.165 | Acc: 95.388% (3785/3968)
[Train] Epoch= 41  BatchID= 40 Loss: 0.176 | Acc: 94.874% (4979/5248)
[Train] Epoch= 41  BatchID= 50 Loss: 0.173 | Acc: 94.914% (6196/6528)
[Train] Epoch= 41  BatchID= 60 Loss: 0.176 | Acc: 94.800% (7402/7808)
[Train] Epoch= 41  BatchID= 70 Loss: 0.170 | Acc: 95.004% (8634/9088)
[Test] Epoch= 41  BatchID= 0 Loss: 0.144 | Acc: 94.531% (121/128)
[Test] Epoch= 41  BatchID= 10 Loss: 0.297 | Acc: 91.477% (1288/1408)
[Test] Epoch= 41  BatchID= 20 Loss: 0.308 | Acc: 91.034% (2447/2688)
[Test] Epoch= 41  BatchID= 30 Loss: 0.371 | Acc: 89.350% (3507/3925)
Saving..
Best accuracy:  89.35031847133757

Epoch: 42
[Train] Epoch= 42  BatchID= 0 Loss: 0.105 | Acc: 94.531% (121/128)
[Train] Epoch= 42  BatchID= 10 Loss: 0.167 | Acc: 94.176% (1326/1408)
[Train] Epoch= 42  BatchID= 20 Loss: 0.159 | Acc: 95.052% (2555/2688)
[Train] Epoch= 42  BatchID= 30 Loss: 0.160 | Acc: 95.086% (3773/3968)
[Train] Epoch= 42  BatchID= 40 Loss: 0.171 | Acc: 94.855% (4978/5248)
[Train] Epoch= 42  BatchID= 50 Loss: 0.174 | Acc: 94.730% (6184/6528)
[Train] Epoch= 42  BatchID= 60 Loss: 0.170 | Acc: 94.877% (7408/7808)
[Train] Epoch= 42  BatchID= 70 Loss: 0.171 | Acc: 94.905% (8625/9088)
[Test] Epoch= 42  BatchID= 0 Loss: 0.185 | Acc: 96.094% (123/128)
[Test] Epoch= 42  BatchID= 10 Loss: 0.310 | Acc: 90.199% (1270/1408)
[Test] Epoch= 42  BatchID= 20 Loss: 0.326 | Acc: 89.472% (2405/2688)
[Test] Epoch= 42  BatchID= 30 Loss: 0.360 | Acc: 88.994% (3493/3925)

Epoch: 43
[Train] Epoch= 43  BatchID= 0 Loss: 0.139 | Acc: 97.656% (125/128)
[Train] Epoch= 43  BatchID= 10 Loss: 0.170 | Acc: 94.957% (1337/1408)
[Train] Epoch= 43  BatchID= 20 Loss: 0.172 | Acc: 94.680% (2545/2688)
[Train] Epoch= 43  BatchID= 30 Loss: 0.170 | Acc: 94.632% (3755/3968)
[Train] Epoch= 43  BatchID= 40 Loss: 0.167 | Acc: 94.817% (4976/5248)
[Train] Epoch= 43  BatchID= 50 Loss: 0.174 | Acc: 94.531% (6171/6528)
[Train] Epoch= 43  BatchID= 60 Loss: 0.171 | Acc: 94.570% (7384/7808)
[Train] Epoch= 43  BatchID= 70 Loss: 0.171 | Acc: 94.575% (8595/9088)
[Test] Epoch= 43  BatchID= 0 Loss: 0.190 | Acc: 94.531% (121/128)
[Test] Epoch= 43  BatchID= 10 Loss: 0.366 | Acc: 88.849% (1251/1408)
[Test] Epoch= 43  BatchID= 20 Loss: 0.342 | Acc: 89.583% (2408/2688)
[Test] Epoch= 43  BatchID= 30 Loss: 0.365 | Acc: 89.248% (3503/3925)

Epoch: 44
[Train] Epoch= 44  BatchID= 0 Loss: 0.155 | Acc: 95.312% (122/128)
[Train] Epoch= 44  BatchID= 10 Loss: 0.172 | Acc: 95.028% (1338/1408)
[Train] Epoch= 44  BatchID= 20 Loss: 0.187 | Acc: 94.568% (2542/2688)
[Train] Epoch= 44  BatchID= 30 Loss: 0.190 | Acc: 94.380% (3745/3968)
[Train] Epoch= 44  BatchID= 40 Loss: 0.180 | Acc: 94.722% (4971/5248)
[Train] Epoch= 44  BatchID= 50 Loss: 0.179 | Acc: 94.730% (6184/6528)
[Train] Epoch= 44  BatchID= 60 Loss: 0.178 | Acc: 94.736% (7397/7808)
[Train] Epoch= 44  BatchID= 70 Loss: 0.175 | Acc: 94.762% (8612/9088)
[Test] Epoch= 44  BatchID= 0 Loss: 0.141 | Acc: 96.875% (124/128)
[Test] Epoch= 44  BatchID= 10 Loss: 0.320 | Acc: 90.341% (1272/1408)
[Test] Epoch= 44  BatchID= 20 Loss: 0.325 | Acc: 90.067% (2421/2688)
[Test] Epoch= 44  BatchID= 30 Loss: 0.353 | Acc: 89.427% (3510/3925)
Saving..
Best accuracy:  89.4267515923567

Epoch: 45
[Train] Epoch= 45  BatchID= 0 Loss: 0.228 | Acc: 93.750% (120/128)
[Train] Epoch= 45  BatchID= 10 Loss: 0.163 | Acc: 95.312% (1342/1408)
[Train] Epoch= 45  BatchID= 20 Loss: 0.161 | Acc: 95.350% (2563/2688)
[Train] Epoch= 45  BatchID= 30 Loss: 0.164 | Acc: 95.035% (3771/3968)
[Train] Epoch= 45  BatchID= 40 Loss: 0.165 | Acc: 95.027% (4987/5248)
[Train] Epoch= 45  BatchID= 50 Loss: 0.170 | Acc: 94.807% (6189/6528)
[Train] Epoch= 45  BatchID= 60 Loss: 0.170 | Acc: 94.813% (7403/7808)
[Train] Epoch= 45  BatchID= 70 Loss: 0.173 | Acc: 94.707% (8607/9088)
[Test] Epoch= 45  BatchID= 0 Loss: 0.155 | Acc: 96.094% (123/128)
[Test] Epoch= 45  BatchID= 10 Loss: 0.322 | Acc: 89.560% (1261/1408)
[Test] Epoch= 45  BatchID= 20 Loss: 0.318 | Acc: 89.918% (2417/2688)
[Test] Epoch= 45  BatchID= 30 Loss: 0.357 | Acc: 89.146% (3499/3925)

Epoch: 46
[Train] Epoch= 46  BatchID= 0 Loss: 0.135 | Acc: 96.875% (124/128)
[Train] Epoch= 46  BatchID= 10 Loss: 0.170 | Acc: 94.602% (1332/1408)
[Train] Epoch= 46  BatchID= 20 Loss: 0.162 | Acc: 95.015% (2554/2688)
[Train] Epoch= 46  BatchID= 30 Loss: 0.162 | Acc: 95.086% (3773/3968)
[Train] Epoch= 46  BatchID= 40 Loss: 0.164 | Acc: 94.950% (4983/5248)
[Train] Epoch= 46  BatchID= 50 Loss: 0.165 | Acc: 95.052% (6205/6528)
[Train] Epoch= 46  BatchID= 60 Loss: 0.166 | Acc: 94.941% (7413/7808)
[Train] Epoch= 46  BatchID= 70 Loss: 0.167 | Acc: 94.883% (8623/9088)
[Test] Epoch= 46  BatchID= 0 Loss: 0.125 | Acc: 96.094% (123/128)
[Test] Epoch= 46  BatchID= 10 Loss: 0.322 | Acc: 90.270% (1271/1408)
[Test] Epoch= 46  BatchID= 20 Loss: 0.310 | Acc: 90.476% (2432/2688)
[Test] Epoch= 46  BatchID= 30 Loss: 0.364 | Acc: 89.299% (3505/3925)

Epoch: 47
[Train] Epoch= 47  BatchID= 0 Loss: 0.170 | Acc: 94.531% (121/128)
[Train] Epoch= 47  BatchID= 10 Loss: 0.157 | Acc: 94.673% (1333/1408)
[Train] Epoch= 47  BatchID= 20 Loss: 0.173 | Acc: 94.606% (2543/2688)
[Train] Epoch= 47  BatchID= 30 Loss: 0.174 | Acc: 94.456% (3748/3968)
[Train] Epoch= 47  BatchID= 40 Loss: 0.169 | Acc: 94.588% (4964/5248)
[Train] Epoch= 47  BatchID= 50 Loss: 0.170 | Acc: 94.577% (6174/6528)
[Train] Epoch= 47  BatchID= 60 Loss: 0.170 | Acc: 94.749% (7398/7808)
[Train] Epoch= 47  BatchID= 70 Loss: 0.167 | Acc: 94.806% (8616/9088)
[Test] Epoch= 47  BatchID= 0 Loss: 0.100 | Acc: 96.875% (124/128)
[Test] Epoch= 47  BatchID= 10 Loss: 0.338 | Acc: 89.986% (1267/1408)
[Test] Epoch= 47  BatchID= 20 Loss: 0.342 | Acc: 89.955% (2418/2688)
[Test] Epoch= 47  BatchID= 30 Loss: 0.351 | Acc: 89.732% (3522/3925)
Saving..
Best accuracy:  89.73248407643312

Epoch: 48
[Train] Epoch= 48  BatchID= 0 Loss: 0.101 | Acc: 96.875% (124/128)
[Train] Epoch= 48  BatchID= 10 Loss: 0.179 | Acc: 94.176% (1326/1408)
[Train] Epoch= 48  BatchID= 20 Loss: 0.171 | Acc: 94.457% (2539/2688)
[Train] Epoch= 48  BatchID= 30 Loss: 0.172 | Acc: 94.607% (3754/3968)
[Train] Epoch= 48  BatchID= 40 Loss: 0.174 | Acc: 94.646% (4967/5248)
[Train] Epoch= 48  BatchID= 50 Loss: 0.170 | Acc: 94.945% (6198/6528)
[Train] Epoch= 48  BatchID= 60 Loss: 0.169 | Acc: 94.877% (7408/7808)
[Train] Epoch= 48  BatchID= 70 Loss: 0.170 | Acc: 94.828% (8618/9088)
[Test] Epoch= 48  BatchID= 0 Loss: 0.105 | Acc: 96.875% (124/128)
[Test] Epoch= 48  BatchID= 10 Loss: 0.343 | Acc: 88.991% (1253/1408)
[Test] Epoch= 48  BatchID= 20 Loss: 0.333 | Acc: 89.249% (2399/2688)
[Test] Epoch= 48  BatchID= 30 Loss: 0.355 | Acc: 88.943% (3491/3925)

Epoch: 49
[Train] Epoch= 49  BatchID= 0 Loss: 0.060 | Acc: 99.219% (127/128)
[Train] Epoch= 49  BatchID= 10 Loss: 0.142 | Acc: 95.668% (1347/1408)
[Train] Epoch= 49  BatchID= 20 Loss: 0.162 | Acc: 95.164% (2558/2688)
[Train] Epoch= 49  BatchID= 30 Loss: 0.163 | Acc: 94.960% (3768/3968)
[Train] Epoch= 49  BatchID= 40 Loss: 0.170 | Acc: 94.722% (4971/5248)
[Train] Epoch= 49  BatchID= 50 Loss: 0.171 | Acc: 94.684% (6181/6528)
[Train] Epoch= 49  BatchID= 60 Loss: 0.170 | Acc: 94.749% (7398/7808)
[Train] Epoch= 49  BatchID= 70 Loss: 0.171 | Acc: 94.762% (8612/9088)
[Test] Epoch= 49  BatchID= 0 Loss: 0.127 | Acc: 96.875% (124/128)
[Test] Epoch= 49  BatchID= 10 Loss: 0.354 | Acc: 89.134% (1255/1408)
[Test] Epoch= 49  BatchID= 20 Loss: 0.332 | Acc: 89.993% (2419/2688)
[Test] Epoch= 49  BatchID= 30 Loss: 0.355 | Acc: 89.070% (3496/3925)

Epoch: 50
[Train] Epoch= 50  BatchID= 0 Loss: 0.205 | Acc: 92.969% (119/128)
[Train] Epoch= 50  BatchID= 10 Loss: 0.170 | Acc: 94.886% (1336/1408)
[Train] Epoch= 50  BatchID= 20 Loss: 0.166 | Acc: 95.015% (2554/2688)
[Train] Epoch= 50  BatchID= 30 Loss: 0.166 | Acc: 94.834% (3763/3968)
[Train] Epoch= 50  BatchID= 40 Loss: 0.163 | Acc: 95.046% (4988/5248)
[Train] Epoch= 50  BatchID= 50 Loss: 0.166 | Acc: 94.991% (6201/6528)
[Train] Epoch= 50  BatchID= 60 Loss: 0.166 | Acc: 94.980% (7416/7808)
[Train] Epoch= 50  BatchID= 70 Loss: 0.164 | Acc: 95.004% (8634/9088)
[Test] Epoch= 50  BatchID= 0 Loss: 0.145 | Acc: 95.312% (122/128)
[Test] Epoch= 50  BatchID= 10 Loss: 0.322 | Acc: 90.554% (1275/1408)
[Test] Epoch= 50  BatchID= 20 Loss: 0.315 | Acc: 90.588% (2435/2688)
[Test] Epoch= 50  BatchID= 30 Loss: 0.361 | Acc: 89.350% (3507/3925)

Epoch: 51
[Train] Epoch= 51  BatchID= 0 Loss: 0.174 | Acc: 95.312% (122/128)
[Train] Epoch= 51  BatchID= 10 Loss: 0.158 | Acc: 95.597% (1346/1408)
[Train] Epoch= 51  BatchID= 20 Loss: 0.150 | Acc: 95.908% (2578/2688)
[Train] Epoch= 51  BatchID= 30 Loss: 0.162 | Acc: 95.439% (3787/3968)
[Train] Epoch= 51  BatchID= 40 Loss: 0.155 | Acc: 95.408% (5007/5248)
[Train] Epoch= 51  BatchID= 50 Loss: 0.157 | Acc: 95.512% (6235/6528)
[Train] Epoch= 51  BatchID= 60 Loss: 0.156 | Acc: 95.338% (7444/7808)
[Train] Epoch= 51  BatchID= 70 Loss: 0.159 | Acc: 95.324% (8663/9088)
[Test] Epoch= 51  BatchID= 0 Loss: 0.111 | Acc: 96.875% (124/128)
[Test] Epoch= 51  BatchID= 10 Loss: 0.351 | Acc: 89.276% (1257/1408)
[Test] Epoch= 51  BatchID= 20 Loss: 0.341 | Acc: 89.732% (2412/2688)
[Test] Epoch= 51  BatchID= 30 Loss: 0.353 | Acc: 89.452% (3511/3925)

Epoch: 52
[Train] Epoch= 52  BatchID= 0 Loss: 0.242 | Acc: 91.406% (117/128)
[Train] Epoch= 52  BatchID= 10 Loss: 0.159 | Acc: 95.668% (1347/1408)
[Train] Epoch= 52  BatchID= 20 Loss: 0.149 | Acc: 95.685% (2572/2688)
[Train] Epoch= 52  BatchID= 30 Loss: 0.158 | Acc: 95.161% (3776/3968)
[Train] Epoch= 52  BatchID= 40 Loss: 0.158 | Acc: 95.255% (4999/5248)
[Train] Epoch= 52  BatchID= 50 Loss: 0.159 | Acc: 95.236% (6217/6528)
[Train] Epoch= 52  BatchID= 60 Loss: 0.159 | Acc: 95.312% (7442/7808)
[Train] Epoch= 52  BatchID= 70 Loss: 0.158 | Acc: 95.301% (8661/9088)
[Test] Epoch= 52  BatchID= 0 Loss: 0.122 | Acc: 96.875% (124/128)
[Test] Epoch= 52  BatchID= 10 Loss: 0.304 | Acc: 90.625% (1276/1408)
[Test] Epoch= 52  BatchID= 20 Loss: 0.302 | Acc: 90.737% (2439/2688)
[Test] Epoch= 52  BatchID= 30 Loss: 0.354 | Acc: 89.401% (3509/3925)

Epoch: 53
[Train] Epoch= 53  BatchID= 0 Loss: 0.238 | Acc: 94.531% (121/128)
[Train] Epoch= 53  BatchID= 10 Loss: 0.164 | Acc: 94.744% (1334/1408)
[Train] Epoch= 53  BatchID= 20 Loss: 0.172 | Acc: 94.978% (2553/2688)
[Train] Epoch= 53  BatchID= 30 Loss: 0.173 | Acc: 94.783% (3761/3968)
[Train] Epoch= 53  BatchID= 40 Loss: 0.167 | Acc: 94.989% (4985/5248)
[Train] Epoch= 53  BatchID= 50 Loss: 0.161 | Acc: 95.221% (6216/6528)
[Train] Epoch= 53  BatchID= 60 Loss: 0.161 | Acc: 95.300% (7441/7808)
[Train] Epoch= 53  BatchID= 70 Loss: 0.160 | Acc: 95.312% (8662/9088)
[Test] Epoch= 53  BatchID= 0 Loss: 0.125 | Acc: 96.094% (123/128)
[Test] Epoch= 53  BatchID= 10 Loss: 0.346 | Acc: 89.134% (1255/1408)
[Test] Epoch= 53  BatchID= 20 Loss: 0.326 | Acc: 89.881% (2416/2688)
[Test] Epoch= 53  BatchID= 30 Loss: 0.364 | Acc: 89.019% (3494/3925)

Epoch: 54
[Train] Epoch= 54  BatchID= 0 Loss: 0.151 | Acc: 95.312% (122/128)
[Train] Epoch= 54  BatchID= 10 Loss: 0.151 | Acc: 95.241% (1341/1408)
[Train] Epoch= 54  BatchID= 20 Loss: 0.155 | Acc: 95.052% (2555/2688)
[Train] Epoch= 54  BatchID= 30 Loss: 0.156 | Acc: 95.111% (3774/3968)
[Train] Epoch= 54  BatchID= 40 Loss: 0.151 | Acc: 95.255% (4999/5248)
[Train] Epoch= 54  BatchID= 50 Loss: 0.154 | Acc: 95.098% (6208/6528)
[Train] Epoch= 54  BatchID= 60 Loss: 0.152 | Acc: 95.159% (7430/7808)
[Train] Epoch= 54  BatchID= 70 Loss: 0.154 | Acc: 95.048% (8638/9088)
[Test] Epoch= 54  BatchID= 0 Loss: 0.123 | Acc: 95.312% (122/128)
[Test] Epoch= 54  BatchID= 10 Loss: 0.342 | Acc: 89.276% (1257/1408)
[Test] Epoch= 54  BatchID= 20 Loss: 0.332 | Acc: 89.695% (2411/2688)
[Test] Epoch= 54  BatchID= 30 Loss: 0.360 | Acc: 89.070% (3496/3925)

Epoch: 55
[Train] Epoch= 55  BatchID= 0 Loss: 0.212 | Acc: 92.188% (118/128)
[Train] Epoch= 55  BatchID= 10 Loss: 0.168 | Acc: 95.170% (1340/1408)
[Train] Epoch= 55  BatchID= 20 Loss: 0.160 | Acc: 95.275% (2561/2688)
[Train] Epoch= 55  BatchID= 30 Loss: 0.165 | Acc: 95.212% (3778/3968)
[Train] Epoch= 55  BatchID= 40 Loss: 0.165 | Acc: 95.141% (4993/5248)
[Train] Epoch= 55  BatchID= 50 Loss: 0.170 | Acc: 95.021% (6203/6528)
[Train] Epoch= 55  BatchID= 60 Loss: 0.167 | Acc: 95.056% (7422/7808)
[Train] Epoch= 55  BatchID= 70 Loss: 0.164 | Acc: 95.147% (8647/9088)
[Test] Epoch= 55  BatchID= 0 Loss: 0.090 | Acc: 96.875% (124/128)
[Test] Epoch= 55  BatchID= 10 Loss: 0.348 | Acc: 90.199% (1270/1408)
[Test] Epoch= 55  BatchID= 20 Loss: 0.324 | Acc: 90.365% (2429/2688)
[Test] Epoch= 55  BatchID= 30 Loss: 0.352 | Acc: 89.529% (3514/3925)

Epoch: 56
[Train] Epoch= 56  BatchID= 0 Loss: 0.169 | Acc: 93.750% (120/128)
[Train] Epoch= 56  BatchID= 10 Loss: 0.156 | Acc: 95.952% (1351/1408)
[Train] Epoch= 56  BatchID= 20 Loss: 0.157 | Acc: 95.164% (2558/2688)
[Train] Epoch= 56  BatchID= 30 Loss: 0.156 | Acc: 95.010% (3770/3968)
[Train] Epoch= 56  BatchID= 40 Loss: 0.154 | Acc: 95.065% (4989/5248)
[Train] Epoch= 56  BatchID= 50 Loss: 0.154 | Acc: 95.098% (6208/6528)
[Train] Epoch= 56  BatchID= 60 Loss: 0.154 | Acc: 95.248% (7437/7808)
[Train] Epoch= 56  BatchID= 70 Loss: 0.156 | Acc: 95.180% (8650/9088)
[Test] Epoch= 56  BatchID= 0 Loss: 0.148 | Acc: 95.312% (122/128)
[Test] Epoch= 56  BatchID= 10 Loss: 0.315 | Acc: 89.773% (1264/1408)
[Test] Epoch= 56  BatchID= 20 Loss: 0.317 | Acc: 90.141% (2423/2688)
[Test] Epoch= 56  BatchID= 30 Loss: 0.372 | Acc: 88.892% (3489/3925)

Epoch: 57
[Train] Epoch= 57  BatchID= 0 Loss: 0.154 | Acc: 94.531% (121/128)
[Train] Epoch= 57  BatchID= 10 Loss: 0.139 | Acc: 95.739% (1348/1408)
[Train] Epoch= 57  BatchID= 20 Loss: 0.140 | Acc: 95.796% (2575/2688)
[Train] Epoch= 57  BatchID= 30 Loss: 0.146 | Acc: 95.439% (3787/3968)
[Train] Epoch= 57  BatchID= 40 Loss: 0.145 | Acc: 95.427% (5008/5248)
[Train] Epoch= 57  BatchID= 50 Loss: 0.144 | Acc: 95.496% (6234/6528)
[Train] Epoch= 57  BatchID= 60 Loss: 0.143 | Acc: 95.581% (7463/7808)
[Train] Epoch= 57  BatchID= 70 Loss: 0.148 | Acc: 95.423% (8672/9088)
[Test] Epoch= 57  BatchID= 0 Loss: 0.127 | Acc: 96.094% (123/128)
[Test] Epoch= 57  BatchID= 10 Loss: 0.357 | Acc: 89.205% (1256/1408)
[Test] Epoch= 57  BatchID= 20 Loss: 0.336 | Acc: 89.844% (2415/2688)
[Test] Epoch= 57  BatchID= 30 Loss: 0.349 | Acc: 89.605% (3517/3925)

Epoch: 58
[Train] Epoch= 58  BatchID= 0 Loss: 0.131 | Acc: 94.531% (121/128)
[Train] Epoch= 58  BatchID= 10 Loss: 0.156 | Acc: 95.597% (1346/1408)
[Train] Epoch= 58  BatchID= 20 Loss: 0.139 | Acc: 95.908% (2578/2688)
[Train] Epoch= 58  BatchID= 30 Loss: 0.139 | Acc: 95.842% (3803/3968)
[Train] Epoch= 58  BatchID= 40 Loss: 0.143 | Acc: 95.713% (5023/5248)
[Train] Epoch= 58  BatchID= 50 Loss: 0.150 | Acc: 95.512% (6235/6528)
[Train] Epoch= 58  BatchID= 60 Loss: 0.149 | Acc: 95.479% (7455/7808)
[Train] Epoch= 58  BatchID= 70 Loss: 0.148 | Acc: 95.588% (8687/9088)
[Test] Epoch= 58  BatchID= 0 Loss: 0.136 | Acc: 96.094% (123/128)
[Test] Epoch= 58  BatchID= 10 Loss: 0.317 | Acc: 90.128% (1269/1408)
[Test] Epoch= 58  BatchID= 20 Loss: 0.318 | Acc: 90.588% (2435/2688)
[Test] Epoch= 58  BatchID= 30 Loss: 0.343 | Acc: 89.860% (3527/3925)
Saving..
Best accuracy:  89.85987261146497

Epoch: 59
[Train] Epoch= 59  BatchID= 0 Loss: 0.140 | Acc: 95.312% (122/128)
[Train] Epoch= 59  BatchID= 10 Loss: 0.153 | Acc: 95.028% (1338/1408)
[Train] Epoch= 59  BatchID= 20 Loss: 0.141 | Acc: 95.610% (2570/2688)
[Train] Epoch= 59  BatchID= 30 Loss: 0.147 | Acc: 95.464% (3788/3968)
[Train] Epoch= 59  BatchID= 40 Loss: 0.150 | Acc: 95.370% (5005/5248)
[Train] Epoch= 59  BatchID= 50 Loss: 0.151 | Acc: 95.328% (6223/6528)
[Train] Epoch= 59  BatchID= 60 Loss: 0.152 | Acc: 95.300% (7441/7808)
[Train] Epoch= 59  BatchID= 70 Loss: 0.150 | Acc: 95.346% (8665/9088)
[Test] Epoch= 59  BatchID= 0 Loss: 0.131 | Acc: 96.094% (123/128)
[Test] Epoch= 59  BatchID= 10 Loss: 0.311 | Acc: 89.915% (1266/1408)
[Test] Epoch= 59  BatchID= 20 Loss: 0.310 | Acc: 90.551% (2434/2688)
[Test] Epoch= 59  BatchID= 30 Loss: 0.356 | Acc: 89.605% (3517/3925)

Epoch: 60
[Train] Epoch= 60  BatchID= 0 Loss: 0.195 | Acc: 94.531% (121/128)
[Train] Epoch= 60  BatchID= 10 Loss: 0.153 | Acc: 95.668% (1347/1408)
[Train] Epoch= 60  BatchID= 20 Loss: 0.145 | Acc: 95.796% (2575/2688)
[Train] Epoch= 60  BatchID= 30 Loss: 0.149 | Acc: 95.590% (3793/3968)
[Train] Epoch= 60  BatchID= 40 Loss: 0.152 | Acc: 95.427% (5008/5248)
[Train] Epoch= 60  BatchID= 50 Loss: 0.148 | Acc: 95.512% (6235/6528)
[Train] Epoch= 60  BatchID= 60 Loss: 0.149 | Acc: 95.402% (7449/7808)
[Train] Epoch= 60  BatchID= 70 Loss: 0.150 | Acc: 95.301% (8661/9088)
[Test] Epoch= 60  BatchID= 0 Loss: 0.134 | Acc: 95.312% (122/128)
[Test] Epoch= 60  BatchID= 10 Loss: 0.333 | Acc: 90.057% (1268/1408)
[Test] Epoch= 60  BatchID= 20 Loss: 0.319 | Acc: 90.365% (2429/2688)
[Test] Epoch= 60  BatchID= 30 Loss: 0.346 | Acc: 89.809% (3525/3925)

Epoch: 61
[Train] Epoch= 61  BatchID= 0 Loss: 0.109 | Acc: 96.875% (124/128)
[Train] Epoch= 61  BatchID= 10 Loss: 0.158 | Acc: 95.028% (1338/1408)
[Train] Epoch= 61  BatchID= 20 Loss: 0.137 | Acc: 95.871% (2577/2688)
[Train] Epoch= 61  BatchID= 30 Loss: 0.145 | Acc: 95.615% (3794/3968)
[Train] Epoch= 61  BatchID= 40 Loss: 0.143 | Acc: 95.694% (5022/5248)
[Train] Epoch= 61  BatchID= 50 Loss: 0.140 | Acc: 95.833% (6256/6528)
[Train] Epoch= 61  BatchID= 60 Loss: 0.145 | Acc: 95.569% (7462/7808)
[Train] Epoch= 61  BatchID= 70 Loss: 0.145 | Acc: 95.588% (8687/9088)
[Test] Epoch= 61  BatchID= 0 Loss: 0.127 | Acc: 95.312% (122/128)
[Test] Epoch= 61  BatchID= 10 Loss: 0.333 | Acc: 89.489% (1260/1408)
[Test] Epoch= 61  BatchID= 20 Loss: 0.323 | Acc: 89.732% (2412/2688)
[Test] Epoch= 61  BatchID= 30 Loss: 0.356 | Acc: 89.299% (3505/3925)

Epoch: 62
[Train] Epoch= 62  BatchID= 0 Loss: 0.158 | Acc: 93.750% (120/128)
[Train] Epoch= 62  BatchID= 10 Loss: 0.166 | Acc: 95.312% (1342/1408)
[Train] Epoch= 62  BatchID= 20 Loss: 0.139 | Acc: 96.057% (2582/2688)
[Train] Epoch= 62  BatchID= 30 Loss: 0.148 | Acc: 95.741% (3799/3968)
[Train] Epoch= 62  BatchID= 40 Loss: 0.145 | Acc: 95.751% (5025/5248)
[Train] Epoch= 62  BatchID= 50 Loss: 0.146 | Acc: 95.711% (6248/6528)
[Train] Epoch= 62  BatchID= 60 Loss: 0.147 | Acc: 95.697% (7472/7808)
[Train] Epoch= 62  BatchID= 70 Loss: 0.148 | Acc: 95.632% (8691/9088)
[Test] Epoch= 62  BatchID= 0 Loss: 0.165 | Acc: 95.312% (122/128)
[Test] Epoch= 62  BatchID= 10 Loss: 0.348 | Acc: 89.560% (1261/1408)
[Test] Epoch= 62  BatchID= 20 Loss: 0.323 | Acc: 90.104% (2422/2688)
[Test] Epoch= 62  BatchID= 30 Loss: 0.363 | Acc: 89.172% (3500/3925)

Epoch: 63
[Train] Epoch= 63  BatchID= 0 Loss: 0.159 | Acc: 96.094% (123/128)
[Train] Epoch= 63  BatchID= 10 Loss: 0.161 | Acc: 95.099% (1339/1408)
[Train] Epoch= 63  BatchID= 20 Loss: 0.153 | Acc: 95.499% (2567/2688)
[Train] Epoch= 63  BatchID= 30 Loss: 0.153 | Acc: 95.439% (3787/3968)
[Train] Epoch= 63  BatchID= 40 Loss: 0.152 | Acc: 95.351% (5004/5248)
[Train] Epoch= 63  BatchID= 50 Loss: 0.151 | Acc: 95.389% (6227/6528)
[Train] Epoch= 63  BatchID= 60 Loss: 0.150 | Acc: 95.453% (7453/7808)
[Train] Epoch= 63  BatchID= 70 Loss: 0.147 | Acc: 95.522% (8681/9088)
[Test] Epoch= 63  BatchID= 0 Loss: 0.137 | Acc: 96.875% (124/128)
[Test] Epoch= 63  BatchID= 10 Loss: 0.310 | Acc: 90.838% (1279/1408)
[Test] Epoch= 63  BatchID= 20 Loss: 0.309 | Acc: 90.811% (2441/2688)
[Test] Epoch= 63  BatchID= 30 Loss: 0.358 | Acc: 89.656% (3519/3925)

Epoch: 64
[Train] Epoch= 64  BatchID= 0 Loss: 0.180 | Acc: 92.969% (119/128)
[Train] Epoch= 64  BatchID= 10 Loss: 0.144 | Acc: 95.952% (1351/1408)
[Train] Epoch= 64  BatchID= 20 Loss: 0.142 | Acc: 96.168% (2585/2688)
[Train] Epoch= 64  BatchID= 30 Loss: 0.144 | Acc: 95.917% (3806/3968)
[Train] Epoch= 64  BatchID= 40 Loss: 0.148 | Acc: 95.694% (5022/5248)
[Train] Epoch= 64  BatchID= 50 Loss: 0.148 | Acc: 95.680% (6246/6528)
[Train] Epoch= 64  BatchID= 60 Loss: 0.148 | Acc: 95.581% (7463/7808)
[Train] Epoch= 64  BatchID= 70 Loss: 0.145 | Acc: 95.599% (8688/9088)
[Test] Epoch= 64  BatchID= 0 Loss: 0.114 | Acc: 96.094% (123/128)
[Test] Epoch= 64  BatchID= 10 Loss: 0.327 | Acc: 89.844% (1265/1408)
[Test] Epoch= 64  BatchID= 20 Loss: 0.318 | Acc: 90.104% (2422/2688)
[Test] Epoch= 64  BatchID= 30 Loss: 0.348 | Acc: 89.707% (3521/3925)

Epoch: 65
[Train] Epoch= 65  BatchID= 0 Loss: 0.049 | Acc: 98.438% (126/128)
[Train] Epoch= 65  BatchID= 10 Loss: 0.116 | Acc: 96.733% (1362/1408)
[Train] Epoch= 65  BatchID= 20 Loss: 0.138 | Acc: 95.871% (2577/2688)
[Train] Epoch= 65  BatchID= 30 Loss: 0.141 | Acc: 95.766% (3800/3968)
[Train] Epoch= 65  BatchID= 40 Loss: 0.140 | Acc: 95.827% (5029/5248)
[Train] Epoch= 65  BatchID= 50 Loss: 0.146 | Acc: 95.542% (6237/6528)
[Train] Epoch= 65  BatchID= 60 Loss: 0.143 | Acc: 95.671% (7470/7808)
[Train] Epoch= 65  BatchID= 70 Loss: 0.144 | Acc: 95.555% (8684/9088)
[Test] Epoch= 65  BatchID= 0 Loss: 0.102 | Acc: 97.656% (125/128)
[Test] Epoch= 65  BatchID= 10 Loss: 0.338 | Acc: 89.915% (1266/1408)
[Test] Epoch= 65  BatchID= 20 Loss: 0.319 | Acc: 90.551% (2434/2688)
[Test] Epoch= 65  BatchID= 30 Loss: 0.347 | Acc: 89.911% (3529/3925)
Saving..
Best accuracy:  89.91082802547771

Epoch: 66
[Train] Epoch= 66  BatchID= 0 Loss: 0.147 | Acc: 95.312% (122/128)
[Train] Epoch= 66  BatchID= 10 Loss: 0.130 | Acc: 95.810% (1349/1408)
[Train] Epoch= 66  BatchID= 20 Loss: 0.136 | Acc: 95.685% (2572/2688)
[Train] Epoch= 66  BatchID= 30 Loss: 0.139 | Acc: 95.741% (3799/3968)
[Train] Epoch= 66  BatchID= 40 Loss: 0.139 | Acc: 95.751% (5025/5248)
[Train] Epoch= 66  BatchID= 50 Loss: 0.140 | Acc: 95.604% (6241/6528)
[Train] Epoch= 66  BatchID= 60 Loss: 0.146 | Acc: 95.402% (7449/7808)
[Train] Epoch= 66  BatchID= 70 Loss: 0.149 | Acc: 95.368% (8667/9088)
[Test] Epoch= 66  BatchID= 0 Loss: 0.149 | Acc: 96.094% (123/128)
[Test] Epoch= 66  BatchID= 10 Loss: 0.308 | Acc: 90.483% (1274/1408)
[Test] Epoch= 66  BatchID= 20 Loss: 0.314 | Acc: 90.327% (2428/2688)
[Test] Epoch= 66  BatchID= 30 Loss: 0.348 | Acc: 89.732% (3522/3925)

Epoch: 67
[Train] Epoch= 67  BatchID= 0 Loss: 0.171 | Acc: 96.094% (123/128)
[Train] Epoch= 67  BatchID= 10 Loss: 0.162 | Acc: 94.957% (1337/1408)
[Train] Epoch= 67  BatchID= 20 Loss: 0.153 | Acc: 95.312% (2562/2688)
[Train] Epoch= 67  BatchID= 30 Loss: 0.147 | Acc: 95.413% (3786/3968)
[Train] Epoch= 67  BatchID= 40 Loss: 0.146 | Acc: 95.484% (5011/5248)
[Train] Epoch= 67  BatchID= 50 Loss: 0.145 | Acc: 95.542% (6237/6528)
[Train] Epoch= 67  BatchID= 60 Loss: 0.142 | Acc: 95.684% (7471/7808)
[Train] Epoch= 67  BatchID= 70 Loss: 0.140 | Acc: 95.841% (8710/9088)
[Test] Epoch= 67  BatchID= 0 Loss: 0.116 | Acc: 97.656% (125/128)
[Test] Epoch= 67  BatchID= 10 Loss: 0.343 | Acc: 89.631% (1262/1408)
[Test] Epoch= 67  BatchID= 20 Loss: 0.324 | Acc: 90.253% (2426/2688)
[Test] Epoch= 67  BatchID= 30 Loss: 0.350 | Acc: 89.809% (3525/3925)

Epoch: 68
[Train] Epoch= 68  BatchID= 0 Loss: 0.176 | Acc: 94.531% (121/128)
[Train] Epoch= 68  BatchID= 10 Loss: 0.147 | Acc: 95.384% (1343/1408)
[Train] Epoch= 68  BatchID= 20 Loss: 0.148 | Acc: 95.796% (2575/2688)
[Train] Epoch= 68  BatchID= 30 Loss: 0.138 | Acc: 96.069% (3812/3968)
[Train] Epoch= 68  BatchID= 40 Loss: 0.137 | Acc: 95.998% (5038/5248)
[Train] Epoch= 68  BatchID= 50 Loss: 0.141 | Acc: 95.879% (6259/6528)
[Train] Epoch= 68  BatchID= 60 Loss: 0.138 | Acc: 95.876% (7486/7808)
[Train] Epoch= 68  BatchID= 70 Loss: 0.138 | Acc: 95.852% (8711/9088)
[Test] Epoch= 68  BatchID= 0 Loss: 0.118 | Acc: 96.875% (124/128)
[Test] Epoch= 68  BatchID= 10 Loss: 0.342 | Acc: 89.844% (1265/1408)
[Test] Epoch= 68  BatchID= 20 Loss: 0.311 | Acc: 90.662% (2437/2688)
[Test] Epoch= 68  BatchID= 30 Loss: 0.342 | Acc: 90.064% (3535/3925)
Saving..
Best accuracy:  90.06369426751593

Epoch: 69
[Train] Epoch= 69  BatchID= 0 Loss: 0.207 | Acc: 92.969% (119/128)
[Train] Epoch= 69  BatchID= 10 Loss: 0.164 | Acc: 94.318% (1328/1408)
[Train] Epoch= 69  BatchID= 20 Loss: 0.138 | Acc: 95.164% (2558/2688)
[Train] Epoch= 69  BatchID= 30 Loss: 0.137 | Acc: 95.413% (3786/3968)
[Train] Epoch= 69  BatchID= 40 Loss: 0.139 | Acc: 95.446% (5009/5248)
[Train] Epoch= 69  BatchID= 50 Loss: 0.136 | Acc: 95.512% (6235/6528)
[Train] Epoch= 69  BatchID= 60 Loss: 0.134 | Acc: 95.581% (7463/7808)
[Train] Epoch= 69  BatchID= 70 Loss: 0.137 | Acc: 95.544% (8683/9088)
[Test] Epoch= 69  BatchID= 0 Loss: 0.132 | Acc: 96.094% (123/128)
[Test] Epoch= 69  BatchID= 10 Loss: 0.309 | Acc: 90.483% (1274/1408)
[Test] Epoch= 69  BatchID= 20 Loss: 0.310 | Acc: 90.588% (2435/2688)
[Test] Epoch= 69  BatchID= 30 Loss: 0.344 | Acc: 89.783% (3524/3925)

Epoch: 70
[Train] Epoch= 70  BatchID= 0 Loss: 0.156 | Acc: 94.531% (121/128)
[Train] Epoch= 70  BatchID= 10 Loss: 0.154 | Acc: 95.384% (1343/1408)
[Train] Epoch= 70  BatchID= 20 Loss: 0.142 | Acc: 95.833% (2576/2688)
[Train] Epoch= 70  BatchID= 30 Loss: 0.146 | Acc: 95.489% (3789/3968)
[Train] Epoch= 70  BatchID= 40 Loss: 0.145 | Acc: 95.503% (5012/5248)
[Train] Epoch= 70  BatchID= 50 Loss: 0.144 | Acc: 95.527% (6236/6528)
[Train] Epoch= 70  BatchID= 60 Loss: 0.145 | Acc: 95.517% (7458/7808)
[Train] Epoch= 70  BatchID= 70 Loss: 0.147 | Acc: 95.500% (8679/9088)
[Test] Epoch= 70  BatchID= 0 Loss: 0.105 | Acc: 96.875% (124/128)
[Test] Epoch= 70  BatchID= 10 Loss: 0.313 | Acc: 90.483% (1274/1408)
[Test] Epoch= 70  BatchID= 20 Loss: 0.313 | Acc: 90.476% (2432/2688)
[Test] Epoch= 70  BatchID= 30 Loss: 0.345 | Acc: 89.682% (3520/3925)

Epoch: 71
[Train] Epoch= 71  BatchID= 0 Loss: 0.150 | Acc: 96.094% (123/128)
[Train] Epoch= 71  BatchID= 10 Loss: 0.140 | Acc: 95.455% (1344/1408)
[Train] Epoch= 71  BatchID= 20 Loss: 0.137 | Acc: 95.536% (2568/2688)
[Train] Epoch= 71  BatchID= 30 Loss: 0.139 | Acc: 95.539% (3791/3968)
[Train] Epoch= 71  BatchID= 40 Loss: 0.146 | Acc: 95.236% (4998/5248)
[Train] Epoch= 71  BatchID= 50 Loss: 0.144 | Acc: 95.420% (6229/6528)
[Train] Epoch= 71  BatchID= 60 Loss: 0.140 | Acc: 95.569% (7462/7808)
[Train] Epoch= 71  BatchID= 70 Loss: 0.139 | Acc: 95.599% (8688/9088)
[Test] Epoch= 71  BatchID= 0 Loss: 0.115 | Acc: 96.875% (124/128)
[Test] Epoch= 71  BatchID= 10 Loss: 0.327 | Acc: 89.702% (1263/1408)
[Test] Epoch= 71  BatchID= 20 Loss: 0.312 | Acc: 90.141% (2423/2688)
[Test] Epoch= 71  BatchID= 30 Loss: 0.333 | Acc: 89.860% (3527/3925)

Epoch: 72
[Train] Epoch= 72  BatchID= 0 Loss: 0.131 | Acc: 96.094% (123/128)
[Train] Epoch= 72  BatchID= 10 Loss: 0.140 | Acc: 95.668% (1347/1408)
[Train] Epoch= 72  BatchID= 20 Loss: 0.132 | Acc: 96.057% (2582/2688)
[Train] Epoch= 72  BatchID= 30 Loss: 0.138 | Acc: 95.741% (3799/3968)
[Train] Epoch= 72  BatchID= 40 Loss: 0.141 | Acc: 95.789% (5027/5248)
[Train] Epoch= 72  BatchID= 50 Loss: 0.139 | Acc: 95.849% (6257/6528)
[Train] Epoch= 72  BatchID= 60 Loss: 0.141 | Acc: 95.774% (7478/7808)
[Train] Epoch= 72  BatchID= 70 Loss: 0.143 | Acc: 95.698% (8697/9088)
[Test] Epoch= 72  BatchID= 0 Loss: 0.141 | Acc: 96.094% (123/128)
[Test] Epoch= 72  BatchID= 10 Loss: 0.313 | Acc: 90.057% (1268/1408)
[Test] Epoch= 72  BatchID= 20 Loss: 0.310 | Acc: 90.625% (2436/2688)
[Test] Epoch= 72  BatchID= 30 Loss: 0.338 | Acc: 89.860% (3527/3925)

Epoch: 73
[Train] Epoch= 73  BatchID= 0 Loss: 0.149 | Acc: 94.531% (121/128)
[Train] Epoch= 73  BatchID= 10 Loss: 0.140 | Acc: 95.810% (1349/1408)
[Train] Epoch= 73  BatchID= 20 Loss: 0.131 | Acc: 95.945% (2579/2688)
[Train] Epoch= 73  BatchID= 30 Loss: 0.135 | Acc: 95.892% (3805/3968)
[Train] Epoch= 73  BatchID= 40 Loss: 0.142 | Acc: 95.675% (5021/5248)
[Train] Epoch= 73  BatchID= 50 Loss: 0.141 | Acc: 95.818% (6255/6528)
[Train] Epoch= 73  BatchID= 60 Loss: 0.141 | Acc: 95.761% (7477/7808)
[Train] Epoch= 73  BatchID= 70 Loss: 0.142 | Acc: 95.764% (8703/9088)
[Test] Epoch= 73  BatchID= 0 Loss: 0.119 | Acc: 97.656% (125/128)
[Test] Epoch= 73  BatchID= 10 Loss: 0.305 | Acc: 90.341% (1272/1408)
[Test] Epoch= 73  BatchID= 20 Loss: 0.301 | Acc: 90.699% (2438/2688)
[Test] Epoch= 73  BatchID= 30 Loss: 0.350 | Acc: 89.783% (3524/3925)

Epoch: 74
[Train] Epoch= 74  BatchID= 0 Loss: 0.156 | Acc: 94.531% (121/128)
[Train] Epoch= 74  BatchID= 10 Loss: 0.135 | Acc: 96.094% (1353/1408)
[Train] Epoch= 74  BatchID= 20 Loss: 0.132 | Acc: 96.131% (2584/2688)
[Train] Epoch= 74  BatchID= 30 Loss: 0.128 | Acc: 96.195% (3817/3968)
[Train] Epoch= 74  BatchID= 40 Loss: 0.129 | Acc: 96.075% (5042/5248)
[Train] Epoch= 74  BatchID= 50 Loss: 0.130 | Acc: 96.078% (6272/6528)
[Train] Epoch= 74  BatchID= 60 Loss: 0.130 | Acc: 96.068% (7501/7808)
[Train] Epoch= 74  BatchID= 70 Loss: 0.131 | Acc: 96.072% (8731/9088)
[Test] Epoch= 74  BatchID= 0 Loss: 0.116 | Acc: 96.875% (124/128)
[Test] Epoch= 74  BatchID= 10 Loss: 0.323 | Acc: 90.270% (1271/1408)
[Test] Epoch= 74  BatchID= 20 Loss: 0.312 | Acc: 90.588% (2435/2688)
[Test] Epoch= 74  BatchID= 30 Loss: 0.340 | Acc: 90.089% (3536/3925)
Saving..
Best accuracy:  90.08917197452229

Epoch: 75
[Train] Epoch= 75  BatchID= 0 Loss: 0.145 | Acc: 94.531% (121/128)
[Train] Epoch= 75  BatchID= 10 Loss: 0.127 | Acc: 95.739% (1348/1408)
[Train] Epoch= 75  BatchID= 20 Loss: 0.132 | Acc: 95.573% (2569/2688)
[Train] Epoch= 75  BatchID= 30 Loss: 0.132 | Acc: 95.640% (3795/3968)
[Train] Epoch= 75  BatchID= 40 Loss: 0.136 | Acc: 95.522% (5013/5248)
[Train] Epoch= 75  BatchID= 50 Loss: 0.141 | Acc: 95.450% (6231/6528)
[Train] Epoch= 75  BatchID= 60 Loss: 0.141 | Acc: 95.530% (7459/7808)
[Train] Epoch= 75  BatchID= 70 Loss: 0.142 | Acc: 95.577% (8686/9088)
[Test] Epoch= 75  BatchID= 0 Loss: 0.123 | Acc: 96.875% (124/128)
[Test] Epoch= 75  BatchID= 10 Loss: 0.302 | Acc: 90.483% (1274/1408)
[Test] Epoch= 75  BatchID= 20 Loss: 0.306 | Acc: 90.662% (2437/2688)
[Test] Epoch= 75  BatchID= 30 Loss: 0.337 | Acc: 90.191% (3540/3925)
Saving..
Best accuracy:  90.19108280254777

Epoch: 76
[Train] Epoch= 76  BatchID= 0 Loss: 0.118 | Acc: 96.094% (123/128)
[Train] Epoch= 76  BatchID= 10 Loss: 0.142 | Acc: 95.739% (1348/1408)
[Train] Epoch= 76  BatchID= 20 Loss: 0.142 | Acc: 95.610% (2570/2688)
[Train] Epoch= 76  BatchID= 30 Loss: 0.141 | Acc: 95.665% (3796/3968)
[Train] Epoch= 76  BatchID= 40 Loss: 0.138 | Acc: 95.846% (5030/5248)
[Train] Epoch= 76  BatchID= 50 Loss: 0.138 | Acc: 95.879% (6259/6528)
[Train] Epoch= 76  BatchID= 60 Loss: 0.136 | Acc: 95.966% (7493/7808)
[Train] Epoch= 76  BatchID= 70 Loss: 0.138 | Acc: 95.874% (8713/9088)
[Test] Epoch= 76  BatchID= 0 Loss: 0.137 | Acc: 96.094% (123/128)
[Test] Epoch= 76  BatchID= 10 Loss: 0.338 | Acc: 89.915% (1266/1408)
[Test] Epoch= 76  BatchID= 20 Loss: 0.320 | Acc: 90.625% (2436/2688)
[Test] Epoch= 76  BatchID= 30 Loss: 0.345 | Acc: 90.013% (3533/3925)

Epoch: 77
[Train] Epoch= 77  BatchID= 0 Loss: 0.112 | Acc: 96.094% (123/128)
[Train] Epoch= 77  BatchID= 10 Loss: 0.134 | Acc: 95.455% (1344/1408)
[Train] Epoch= 77  BatchID= 20 Loss: 0.139 | Acc: 95.647% (2571/2688)
[Train] Epoch= 77  BatchID= 30 Loss: 0.137 | Acc: 95.766% (3800/3968)
[Train] Epoch= 77  BatchID= 40 Loss: 0.138 | Acc: 95.598% (5017/5248)
[Train] Epoch= 77  BatchID= 50 Loss: 0.138 | Acc: 95.680% (6246/6528)
[Train] Epoch= 77  BatchID= 60 Loss: 0.137 | Acc: 95.799% (7480/7808)
[Train] Epoch= 77  BatchID= 70 Loss: 0.136 | Acc: 95.907% (8716/9088)
[Test] Epoch= 77  BatchID= 0 Loss: 0.136 | Acc: 97.656% (125/128)
[Test] Epoch= 77  BatchID= 10 Loss: 0.321 | Acc: 90.696% (1277/1408)
[Test] Epoch= 77  BatchID= 20 Loss: 0.311 | Acc: 90.699% (2438/2688)
[Test] Epoch= 77  BatchID= 30 Loss: 0.337 | Acc: 89.987% (3532/3925)

Epoch: 78
[Train] Epoch= 78  BatchID= 0 Loss: 0.078 | Acc: 98.438% (126/128)
[Train] Epoch= 78  BatchID= 10 Loss: 0.110 | Acc: 96.875% (1364/1408)
[Train] Epoch= 78  BatchID= 20 Loss: 0.122 | Acc: 96.354% (2590/2688)
[Train] Epoch= 78  BatchID= 30 Loss: 0.134 | Acc: 95.842% (3803/3968)
[Train] Epoch= 78  BatchID= 40 Loss: 0.132 | Acc: 95.960% (5036/5248)
[Train] Epoch= 78  BatchID= 50 Loss: 0.133 | Acc: 95.956% (6264/6528)
[Train] Epoch= 78  BatchID= 60 Loss: 0.136 | Acc: 95.902% (7488/7808)
[Train] Epoch= 78  BatchID= 70 Loss: 0.135 | Acc: 95.951% (8720/9088)
[Test] Epoch= 78  BatchID= 0 Loss: 0.133 | Acc: 96.094% (123/128)
[Test] Epoch= 78  BatchID= 10 Loss: 0.323 | Acc: 90.128% (1269/1408)
[Test] Epoch= 78  BatchID= 20 Loss: 0.313 | Acc: 90.588% (2435/2688)
[Test] Epoch= 78  BatchID= 30 Loss: 0.346 | Acc: 89.809% (3525/3925)

Epoch: 79
[Train] Epoch= 79  BatchID= 0 Loss: 0.140 | Acc: 97.656% (125/128)
[Train] Epoch= 79  BatchID= 10 Loss: 0.130 | Acc: 96.449% (1358/1408)
[Train] Epoch= 79  BatchID= 20 Loss: 0.140 | Acc: 96.094% (2583/2688)
[Train] Epoch= 79  BatchID= 30 Loss: 0.135 | Acc: 96.094% (3813/3968)
[Train] Epoch= 79  BatchID= 40 Loss: 0.135 | Acc: 95.922% (5034/5248)
[Train] Epoch= 79  BatchID= 50 Loss: 0.137 | Acc: 95.833% (6256/6528)
[Train] Epoch= 79  BatchID= 60 Loss: 0.133 | Acc: 95.991% (7495/7808)
[Train] Epoch= 79  BatchID= 70 Loss: 0.133 | Acc: 96.006% (8725/9088)
[Test] Epoch= 79  BatchID= 0 Loss: 0.115 | Acc: 96.094% (123/128)
[Test] Epoch= 79  BatchID= 10 Loss: 0.313 | Acc: 90.057% (1268/1408)
[Test] Epoch= 79  BatchID= 20 Loss: 0.305 | Acc: 90.439% (2431/2688)
[Test] Epoch= 79  BatchID= 30 Loss: 0.343 | Acc: 89.631% (3518/3925)

Epoch: 80
[Train] Epoch= 80  BatchID= 0 Loss: 0.145 | Acc: 96.094% (123/128)
[Train] Epoch= 80  BatchID= 10 Loss: 0.137 | Acc: 95.455% (1344/1408)
[Train] Epoch= 80  BatchID= 20 Loss: 0.142 | Acc: 95.610% (2570/2688)
[Train] Epoch= 80  BatchID= 30 Loss: 0.139 | Acc: 95.791% (3801/3968)
[Train] Epoch= 80  BatchID= 40 Loss: 0.139 | Acc: 95.846% (5030/5248)
[Train] Epoch= 80  BatchID= 50 Loss: 0.136 | Acc: 95.849% (6257/6528)
[Train] Epoch= 80  BatchID= 60 Loss: 0.139 | Acc: 95.722% (7474/7808)
[Train] Epoch= 80  BatchID= 70 Loss: 0.137 | Acc: 95.841% (8710/9088)
[Test] Epoch= 80  BatchID= 0 Loss: 0.132 | Acc: 96.875% (124/128)
[Test] Epoch= 80  BatchID= 10 Loss: 0.309 | Acc: 90.270% (1271/1408)
[Test] Epoch= 80  BatchID= 20 Loss: 0.305 | Acc: 90.588% (2435/2688)
[Test] Epoch= 80  BatchID= 30 Loss: 0.340 | Acc: 89.936% (3530/3925)

Epoch: 81
[Train] Epoch= 81  BatchID= 0 Loss: 0.054 | Acc: 99.219% (127/128)
[Train] Epoch= 81  BatchID= 10 Loss: 0.147 | Acc: 95.455% (1344/1408)
[Train] Epoch= 81  BatchID= 20 Loss: 0.148 | Acc: 95.350% (2563/2688)
[Train] Epoch= 81  BatchID= 30 Loss: 0.145 | Acc: 95.312% (3782/3968)
[Train] Epoch= 81  BatchID= 40 Loss: 0.139 | Acc: 95.465% (5010/5248)
[Train] Epoch= 81  BatchID= 50 Loss: 0.140 | Acc: 95.512% (6235/6528)
[Train] Epoch= 81  BatchID= 60 Loss: 0.142 | Acc: 95.505% (7457/7808)
[Train] Epoch= 81  BatchID= 70 Loss: 0.140 | Acc: 95.665% (8694/9088)
[Test] Epoch= 81  BatchID= 0 Loss: 0.125 | Acc: 96.875% (124/128)
[Test] Epoch= 81  BatchID= 10 Loss: 0.325 | Acc: 90.341% (1272/1408)
[Test] Epoch= 81  BatchID= 20 Loss: 0.310 | Acc: 90.774% (2440/2688)
[Test] Epoch= 81  BatchID= 30 Loss: 0.343 | Acc: 90.115% (3537/3925)

Epoch: 82
[Train] Epoch= 82  BatchID= 0 Loss: 0.179 | Acc: 96.094% (123/128)
[Train] Epoch= 82  BatchID= 10 Loss: 0.129 | Acc: 96.378% (1357/1408)
[Train] Epoch= 82  BatchID= 20 Loss: 0.125 | Acc: 96.503% (2594/2688)
[Train] Epoch= 82  BatchID= 30 Loss: 0.122 | Acc: 96.472% (3828/3968)
[Train] Epoch= 82  BatchID= 40 Loss: 0.118 | Acc: 96.646% (5072/5248)
[Train] Epoch= 82  BatchID= 50 Loss: 0.118 | Acc: 96.706% (6313/6528)
[Train] Epoch= 82  BatchID= 60 Loss: 0.122 | Acc: 96.568% (7540/7808)
[Train] Epoch= 82  BatchID= 70 Loss: 0.124 | Acc: 96.534% (8773/9088)
[Test] Epoch= 82  BatchID= 0 Loss: 0.098 | Acc: 97.656% (125/128)
[Test] Epoch= 82  BatchID= 10 Loss: 0.328 | Acc: 89.773% (1264/1408)
[Test] Epoch= 82  BatchID= 20 Loss: 0.312 | Acc: 90.365% (2429/2688)
[Test] Epoch= 82  BatchID= 30 Loss: 0.349 | Acc: 89.605% (3517/3925)

Epoch: 83
[Train] Epoch= 83  BatchID= 0 Loss: 0.170 | Acc: 95.312% (122/128)
[Train] Epoch= 83  BatchID= 10 Loss: 0.145 | Acc: 95.384% (1343/1408)
[Train] Epoch= 83  BatchID= 20 Loss: 0.137 | Acc: 95.647% (2571/2688)
[Train] Epoch= 83  BatchID= 30 Loss: 0.135 | Acc: 95.766% (3800/3968)
[Train] Epoch= 83  BatchID= 40 Loss: 0.135 | Acc: 95.770% (5026/5248)
[Train] Epoch= 83  BatchID= 50 Loss: 0.132 | Acc: 95.849% (6257/6528)
[Train] Epoch= 83  BatchID= 60 Loss: 0.135 | Acc: 95.722% (7474/7808)
[Train] Epoch= 83  BatchID= 70 Loss: 0.137 | Acc: 95.665% (8694/9088)
[Test] Epoch= 83  BatchID= 0 Loss: 0.118 | Acc: 96.875% (124/128)
[Test] Epoch= 83  BatchID= 10 Loss: 0.327 | Acc: 89.844% (1265/1408)
[Test] Epoch= 83  BatchID= 20 Loss: 0.312 | Acc: 90.439% (2431/2688)
[Test] Epoch= 83  BatchID= 30 Loss: 0.347 | Acc: 89.732% (3522/3925)

Epoch: 84
[Train] Epoch= 84  BatchID= 0 Loss: 0.180 | Acc: 96.094% (123/128)
[Train] Epoch= 84  BatchID= 10 Loss: 0.129 | Acc: 96.236% (1355/1408)
[Train] Epoch= 84  BatchID= 20 Loss: 0.125 | Acc: 96.354% (2590/2688)
[Train] Epoch= 84  BatchID= 30 Loss: 0.132 | Acc: 96.094% (3813/3968)
[Train] Epoch= 84  BatchID= 40 Loss: 0.130 | Acc: 96.227% (5050/5248)
[Train] Epoch= 84  BatchID= 50 Loss: 0.132 | Acc: 96.201% (6280/6528)
[Train] Epoch= 84  BatchID= 60 Loss: 0.134 | Acc: 96.043% (7499/7808)
[Train] Epoch= 84  BatchID= 70 Loss: 0.135 | Acc: 96.006% (8725/9088)
[Test] Epoch= 84  BatchID= 0 Loss: 0.115 | Acc: 97.656% (125/128)
[Test] Epoch= 84  BatchID= 10 Loss: 0.326 | Acc: 89.986% (1267/1408)
[Test] Epoch= 84  BatchID= 20 Loss: 0.312 | Acc: 90.662% (2437/2688)
[Test] Epoch= 84  BatchID= 30 Loss: 0.344 | Acc: 89.962% (3531/3925)

Epoch: 85
[Train] Epoch= 85  BatchID= 0 Loss: 0.195 | Acc: 95.312% (122/128)
[Train] Epoch= 85  BatchID= 10 Loss: 0.149 | Acc: 95.384% (1343/1408)
[Train] Epoch= 85  BatchID= 20 Loss: 0.141 | Acc: 95.499% (2567/2688)
[Train] Epoch= 85  BatchID= 30 Loss: 0.138 | Acc: 95.413% (3786/3968)
[Train] Epoch= 85  BatchID= 40 Loss: 0.138 | Acc: 95.484% (5011/5248)
[Train] Epoch= 85  BatchID= 50 Loss: 0.134 | Acc: 95.757% (6251/6528)
[Train] Epoch= 85  BatchID= 60 Loss: 0.134 | Acc: 95.786% (7479/7808)
[Train] Epoch= 85  BatchID= 70 Loss: 0.132 | Acc: 95.885% (8714/9088)
[Test] Epoch= 85  BatchID= 0 Loss: 0.113 | Acc: 96.875% (124/128)
[Test] Epoch= 85  BatchID= 10 Loss: 0.322 | Acc: 90.341% (1272/1408)
[Test] Epoch= 85  BatchID= 20 Loss: 0.310 | Acc: 90.997% (2446/2688)
[Test] Epoch= 85  BatchID= 30 Loss: 0.344 | Acc: 90.140% (3538/3925)

Epoch: 86
[Train] Epoch= 86  BatchID= 0 Loss: 0.191 | Acc: 93.750% (120/128)
[Train] Epoch= 86  BatchID= 10 Loss: 0.142 | Acc: 95.810% (1349/1408)
[Train] Epoch= 86  BatchID= 20 Loss: 0.137 | Acc: 95.759% (2574/2688)
[Train] Epoch= 86  BatchID= 30 Loss: 0.129 | Acc: 95.943% (3807/3968)
[Train] Epoch= 86  BatchID= 40 Loss: 0.129 | Acc: 96.018% (5039/5248)
[Train] Epoch= 86  BatchID= 50 Loss: 0.133 | Acc: 95.879% (6259/6528)
[Train] Epoch= 86  BatchID= 60 Loss: 0.130 | Acc: 95.966% (7493/7808)
[Train] Epoch= 86  BatchID= 70 Loss: 0.130 | Acc: 95.951% (8720/9088)
[Test] Epoch= 86  BatchID= 0 Loss: 0.133 | Acc: 96.094% (123/128)
[Test] Epoch= 86  BatchID= 10 Loss: 0.323 | Acc: 90.128% (1269/1408)
[Test] Epoch= 86  BatchID= 20 Loss: 0.314 | Acc: 90.513% (2433/2688)
[Test] Epoch= 86  BatchID= 30 Loss: 0.340 | Acc: 89.911% (3529/3925)

Epoch: 87
[Train] Epoch= 87  BatchID= 0 Loss: 0.115 | Acc: 96.875% (124/128)
[Train] Epoch= 87  BatchID= 10 Loss: 0.110 | Acc: 96.449% (1358/1408)
[Train] Epoch= 87  BatchID= 20 Loss: 0.120 | Acc: 96.317% (2589/2688)
[Train] Epoch= 87  BatchID= 30 Loss: 0.125 | Acc: 96.270% (3820/3968)
[Train] Epoch= 87  BatchID= 40 Loss: 0.128 | Acc: 96.113% (5044/5248)
[Train] Epoch= 87  BatchID= 50 Loss: 0.127 | Acc: 96.170% (6278/6528)
[Train] Epoch= 87  BatchID= 60 Loss: 0.129 | Acc: 96.043% (7499/7808)
[Train] Epoch= 87  BatchID= 70 Loss: 0.132 | Acc: 95.951% (8720/9088)
[Test] Epoch= 87  BatchID= 0 Loss: 0.110 | Acc: 97.656% (125/128)
[Test] Epoch= 87  BatchID= 10 Loss: 0.315 | Acc: 89.773% (1264/1408)
[Test] Epoch= 87  BatchID= 20 Loss: 0.306 | Acc: 90.365% (2429/2688)
[Test] Epoch= 87  BatchID= 30 Loss: 0.335 | Acc: 89.860% (3527/3925)

Epoch: 88
[Train] Epoch= 88  BatchID= 0 Loss: 0.080 | Acc: 97.656% (125/128)
[Train] Epoch= 88  BatchID= 10 Loss: 0.124 | Acc: 95.952% (1351/1408)
[Train] Epoch= 88  BatchID= 20 Loss: 0.137 | Acc: 95.536% (2568/2688)
[Train] Epoch= 88  BatchID= 30 Loss: 0.137 | Acc: 95.665% (3796/3968)
[Train] Epoch= 88  BatchID= 40 Loss: 0.133 | Acc: 95.713% (5023/5248)
[Train] Epoch= 88  BatchID= 50 Loss: 0.139 | Acc: 95.573% (6239/6528)
[Train] Epoch= 88  BatchID= 60 Loss: 0.139 | Acc: 95.530% (7459/7808)
[Train] Epoch= 88  BatchID= 70 Loss: 0.136 | Acc: 95.643% (8692/9088)
[Test] Epoch= 88  BatchID= 0 Loss: 0.126 | Acc: 96.094% (123/128)
[Test] Epoch= 88  BatchID= 10 Loss: 0.317 | Acc: 90.270% (1271/1408)
[Test] Epoch= 88  BatchID= 20 Loss: 0.303 | Acc: 90.737% (2439/2688)
[Test] Epoch= 88  BatchID= 30 Loss: 0.334 | Acc: 89.860% (3527/3925)

Epoch: 89
[Train] Epoch= 89  BatchID= 0 Loss: 0.134 | Acc: 96.875% (124/128)
[Train] Epoch= 89  BatchID= 10 Loss: 0.135 | Acc: 95.526% (1345/1408)
[Train] Epoch= 89  BatchID= 20 Loss: 0.131 | Acc: 95.871% (2577/2688)
[Train] Epoch= 89  BatchID= 30 Loss: 0.133 | Acc: 95.741% (3799/3968)
[Train] Epoch= 89  BatchID= 40 Loss: 0.129 | Acc: 95.827% (5029/5248)
[Train] Epoch= 89  BatchID= 50 Loss: 0.132 | Acc: 95.818% (6255/6528)
[Train] Epoch= 89  BatchID= 60 Loss: 0.132 | Acc: 95.850% (7484/7808)
[Train] Epoch= 89  BatchID= 70 Loss: 0.133 | Acc: 95.808% (8707/9088)
[Test] Epoch= 89  BatchID= 0 Loss: 0.113 | Acc: 96.094% (123/128)
[Test] Epoch= 89  BatchID= 10 Loss: 0.320 | Acc: 90.057% (1268/1408)
[Test] Epoch= 89  BatchID= 20 Loss: 0.309 | Acc: 90.439% (2431/2688)
[Test] Epoch= 89  BatchID= 30 Loss: 0.337 | Acc: 89.962% (3531/3925)
