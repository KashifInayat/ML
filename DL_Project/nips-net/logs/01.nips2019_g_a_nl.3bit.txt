==> Preparing data..
==> Building model..
DataParallel(
  (module): ResNet(
    (conv1): InputConv2dLSQ(
      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): LinearLSQ(
      in_features=512, out_features=10, bias=True
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=True)
    )
  )
)
==> Initializing from checkpoint..

Epoch: 0
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
[Train] Epoch= 0  BatchID= 0 Loss: 0.164 | Acc: 94.531% (121/128)
[Train] Epoch= 0  BatchID= 10 Loss: 0.776 | Acc: 78.764% (1109/1408)
[Train] Epoch= 0  BatchID= 20 Loss: 0.752 | Acc: 77.641% (2087/2688)
[Train] Epoch= 0  BatchID= 30 Loss: 0.703 | Acc: 78.528% (3116/3968)
[Train] Epoch= 0  BatchID= 40 Loss: 0.685 | Acc: 78.659% (4128/5248)
[Train] Epoch= 0  BatchID= 50 Loss: 0.672 | Acc: 78.906% (5151/6528)
[Train] Epoch= 0  BatchID= 60 Loss: 0.665 | Acc: 79.098% (6176/7808)
[Train] Epoch= 0  BatchID= 70 Loss: 0.649 | Acc: 79.544% (7229/9088)
[Test] Epoch= 0  BatchID= 0 Loss: 0.196 | Acc: 92.969% (119/128)
[Test] Epoch= 0  BatchID= 10 Loss: 0.642 | Acc: 81.534% (1148/1408)
[Test] Epoch= 0  BatchID= 20 Loss: 0.820 | Acc: 77.865% (2093/2688)
[Test] Epoch= 0  BatchID= 30 Loss: 0.780 | Acc: 78.650% (3087/3925)
Saving..
Best accuracy:  78.64968152866243

Epoch: 1
[Train] Epoch= 1  BatchID= 0 Loss: 0.490 | Acc: 83.594% (107/128)
[Train] Epoch= 1  BatchID= 10 Loss: 0.581 | Acc: 80.469% (1133/1408)
[Train] Epoch= 1  BatchID= 20 Loss: 0.596 | Acc: 80.022% (2151/2688)
[Train] Epoch= 1  BatchID= 30 Loss: 0.589 | Acc: 80.645% (3200/3968)
[Train] Epoch= 1  BatchID= 40 Loss: 0.571 | Acc: 81.402% (4272/5248)
[Train] Epoch= 1  BatchID= 50 Loss: 0.562 | Acc: 81.863% (5344/6528)
[Train] Epoch= 1  BatchID= 60 Loss: 0.554 | Acc: 82.082% (6409/7808)
[Train] Epoch= 1  BatchID= 70 Loss: 0.561 | Acc: 81.811% (7435/9088)
[Test] Epoch= 1  BatchID= 0 Loss: 0.174 | Acc: 95.312% (122/128)
[Test] Epoch= 1  BatchID= 10 Loss: 0.449 | Acc: 84.446% (1189/1408)
[Test] Epoch= 1  BatchID= 20 Loss: 0.516 | Acc: 82.812% (2226/2688)
[Test] Epoch= 1  BatchID= 30 Loss: 0.611 | Acc: 80.000% (3140/3925)
Saving..
Best accuracy:  80.0

Epoch: 2
[Train] Epoch= 2  BatchID= 0 Loss: 0.601 | Acc: 81.250% (104/128)
[Train] Epoch= 2  BatchID= 10 Loss: 0.619 | Acc: 79.332% (1117/1408)
[Train] Epoch= 2  BatchID= 20 Loss: 0.606 | Acc: 79.613% (2140/2688)
[Train] Epoch= 2  BatchID= 30 Loss: 0.606 | Acc: 79.914% (3171/3968)
[Train] Epoch= 2  BatchID= 40 Loss: 0.597 | Acc: 80.354% (4217/5248)
[Train] Epoch= 2  BatchID= 50 Loss: 0.597 | Acc: 80.423% (5250/6528)
[Train] Epoch= 2  BatchID= 60 Loss: 0.593 | Acc: 80.725% (6303/7808)
[Train] Epoch= 2  BatchID= 70 Loss: 0.586 | Acc: 80.964% (7358/9088)
[Test] Epoch= 2  BatchID= 0 Loss: 0.486 | Acc: 84.375% (108/128)
[Test] Epoch= 2  BatchID= 10 Loss: 0.719 | Acc: 76.207% (1073/1408)
[Test] Epoch= 2  BatchID= 20 Loss: 0.811 | Acc: 74.665% (2007/2688)
[Test] Epoch= 2  BatchID= 30 Loss: 0.714 | Acc: 77.885% (3057/3925)

Epoch: 3
[Train] Epoch= 3  BatchID= 0 Loss: 0.573 | Acc: 82.031% (105/128)
[Train] Epoch= 3  BatchID= 10 Loss: 0.547 | Acc: 82.315% (1159/1408)
[Train] Epoch= 3  BatchID= 20 Loss: 0.563 | Acc: 82.143% (2208/2688)
[Train] Epoch= 3  BatchID= 30 Loss: 0.572 | Acc: 81.754% (3244/3968)
[Train] Epoch= 3  BatchID= 40 Loss: 0.568 | Acc: 81.955% (4301/5248)
[Train] Epoch= 3  BatchID= 50 Loss: 0.578 | Acc: 81.771% (5338/6528)
[Train] Epoch= 3  BatchID= 60 Loss: 0.588 | Acc: 81.327% (6350/7808)
[Train] Epoch= 3  BatchID= 70 Loss: 0.583 | Acc: 81.547% (7411/9088)
[Test] Epoch= 3  BatchID= 0 Loss: 0.496 | Acc: 87.500% (112/128)
[Test] Epoch= 3  BatchID= 10 Loss: 0.523 | Acc: 83.807% (1180/1408)
[Test] Epoch= 3  BatchID= 20 Loss: 0.602 | Acc: 80.729% (2170/2688)
[Test] Epoch= 3  BatchID= 30 Loss: 0.579 | Acc: 82.166% (3225/3925)
Saving..
Best accuracy:  82.1656050955414

Epoch: 4
[Train] Epoch= 4  BatchID= 0 Loss: 0.442 | Acc: 85.938% (110/128)
[Train] Epoch= 4  BatchID= 10 Loss: 0.528 | Acc: 83.594% (1177/1408)
[Train] Epoch= 4  BatchID= 20 Loss: 0.550 | Acc: 82.775% (2225/2688)
[Train] Epoch= 4  BatchID= 30 Loss: 0.578 | Acc: 81.754% (3244/3968)
[Train] Epoch= 4  BatchID= 40 Loss: 0.584 | Acc: 81.460% (4275/5248)
[Train] Epoch= 4  BatchID= 50 Loss: 0.606 | Acc: 80.561% (5259/6528)
[Train] Epoch= 4  BatchID= 60 Loss: 0.607 | Acc: 80.571% (6291/7808)
[Train] Epoch= 4  BatchID= 70 Loss: 0.605 | Acc: 80.337% (7301/9088)
[Test] Epoch= 4  BatchID= 0 Loss: 0.595 | Acc: 84.375% (108/128)
[Test] Epoch= 4  BatchID= 10 Loss: 0.749 | Acc: 77.557% (1092/1408)
[Test] Epoch= 4  BatchID= 20 Loss: 0.739 | Acc: 77.827% (2092/2688)
[Test] Epoch= 4  BatchID= 30 Loss: 0.761 | Acc: 77.809% (3054/3925)

Epoch: 5
[Train] Epoch= 5  BatchID= 0 Loss: 0.601 | Acc: 79.688% (102/128)
[Train] Epoch= 5  BatchID= 10 Loss: 0.578 | Acc: 81.463% (1147/1408)
[Train] Epoch= 5  BatchID= 20 Loss: 0.555 | Acc: 82.515% (2218/2688)
[Train] Epoch= 5  BatchID= 30 Loss: 0.572 | Acc: 81.956% (3252/3968)
[Train] Epoch= 5  BatchID= 40 Loss: 0.573 | Acc: 81.593% (4282/5248)
[Train] Epoch= 5  BatchID= 50 Loss: 0.580 | Acc: 81.204% (5301/6528)
[Train] Epoch= 5  BatchID= 60 Loss: 0.584 | Acc: 81.019% (6326/7808)
[Train] Epoch= 5  BatchID= 70 Loss: 0.588 | Acc: 80.997% (7361/9088)
[Test] Epoch= 5  BatchID= 0 Loss: 0.388 | Acc: 87.500% (112/128)
[Test] Epoch= 5  BatchID= 10 Loss: 0.464 | Acc: 84.659% (1192/1408)
[Test] Epoch= 5  BatchID= 20 Loss: 0.478 | Acc: 84.003% (2258/2688)
[Test] Epoch= 5  BatchID= 30 Loss: 0.656 | Acc: 79.185% (3108/3925)

Epoch: 6
[Train] Epoch= 6  BatchID= 0 Loss: 0.560 | Acc: 82.812% (106/128)
[Train] Epoch= 6  BatchID= 10 Loss: 0.616 | Acc: 79.901% (1125/1408)
[Train] Epoch= 6  BatchID= 20 Loss: 0.617 | Acc: 80.208% (2156/2688)
[Train] Epoch= 6  BatchID= 30 Loss: 0.618 | Acc: 80.166% (3181/3968)
[Train] Epoch= 6  BatchID= 40 Loss: 0.606 | Acc: 80.602% (4230/5248)
[Train] Epoch= 6  BatchID= 50 Loss: 0.601 | Acc: 80.790% (5274/6528)
[Train] Epoch= 6  BatchID= 60 Loss: 0.605 | Acc: 80.776% (6307/7808)
[Train] Epoch= 6  BatchID= 70 Loss: 0.604 | Acc: 80.799% (7343/9088)
[Test] Epoch= 6  BatchID= 0 Loss: 0.122 | Acc: 95.312% (122/128)
[Test] Epoch= 6  BatchID= 10 Loss: 0.480 | Acc: 83.594% (1177/1408)
[Test] Epoch= 6  BatchID= 20 Loss: 0.545 | Acc: 82.515% (2218/2688)
[Test] Epoch= 6  BatchID= 30 Loss: 0.571 | Acc: 81.376% (3194/3925)

Epoch: 7
[Train] Epoch= 7  BatchID= 0 Loss: 0.502 | Acc: 81.250% (104/128)
[Train] Epoch= 7  BatchID= 10 Loss: 0.570 | Acc: 82.741% (1165/1408)
[Train] Epoch= 7  BatchID= 20 Loss: 0.582 | Acc: 81.436% (2189/2688)
[Train] Epoch= 7  BatchID= 30 Loss: 0.586 | Acc: 81.174% (3221/3968)
[Train] Epoch= 7  BatchID= 40 Loss: 0.584 | Acc: 81.079% (4255/5248)
[Train] Epoch= 7  BatchID= 50 Loss: 0.596 | Acc: 80.484% (5254/6528)
[Train] Epoch= 7  BatchID= 60 Loss: 0.607 | Acc: 80.059% (6251/7808)
[Train] Epoch= 7  BatchID= 70 Loss: 0.615 | Acc: 79.743% (7247/9088)
[Test] Epoch= 7  BatchID= 0 Loss: 1.446 | Acc: 69.531% (89/128)
[Test] Epoch= 7  BatchID= 10 Loss: 0.942 | Acc: 75.639% (1065/1408)
[Test] Epoch= 7  BatchID= 20 Loss: 1.070 | Acc: 71.280% (1916/2688)
[Test] Epoch= 7  BatchID= 30 Loss: 1.049 | Acc: 70.981% (2786/3925)

Epoch: 8
[Train] Epoch= 8  BatchID= 0 Loss: 0.632 | Acc: 77.344% (99/128)
[Train] Epoch= 8  BatchID= 10 Loss: 0.549 | Acc: 82.528% (1162/1408)
[Train] Epoch= 8  BatchID= 20 Loss: 0.566 | Acc: 81.882% (2201/2688)
[Train] Epoch= 8  BatchID= 30 Loss: 0.569 | Acc: 81.804% (3246/3968)
[Train] Epoch= 8  BatchID= 40 Loss: 0.571 | Acc: 81.745% (4290/5248)
[Train] Epoch= 8  BatchID= 50 Loss: 0.582 | Acc: 81.235% (5303/6528)
[Train] Epoch= 8  BatchID= 60 Loss: 0.585 | Acc: 81.019% (6326/7808)
[Train] Epoch= 8  BatchID= 70 Loss: 0.590 | Acc: 80.920% (7354/9088)
[Test] Epoch= 8  BatchID= 0 Loss: 0.536 | Acc: 86.719% (111/128)
[Test] Epoch= 8  BatchID= 10 Loss: 0.642 | Acc: 79.688% (1122/1408)
[Test] Epoch= 8  BatchID= 20 Loss: 0.567 | Acc: 82.254% (2211/2688)
[Test] Epoch= 8  BatchID= 30 Loss: 0.646 | Acc: 79.720% (3129/3925)

Epoch: 9
[Train] Epoch= 9  BatchID= 0 Loss: 0.502 | Acc: 86.719% (111/128)
[Train] Epoch= 9  BatchID= 10 Loss: 0.591 | Acc: 80.185% (1129/1408)
[Train] Epoch= 9  BatchID= 20 Loss: 0.587 | Acc: 80.804% (2172/2688)
[Train] Epoch= 9  BatchID= 30 Loss: 0.580 | Acc: 81.149% (3220/3968)
[Train] Epoch= 9  BatchID= 40 Loss: 0.594 | Acc: 80.621% (4231/5248)
[Train] Epoch= 9  BatchID= 50 Loss: 0.604 | Acc: 80.254% (5239/6528)
[Train] Epoch= 9  BatchID= 60 Loss: 0.612 | Acc: 80.008% (6247/7808)
[Train] Epoch= 9  BatchID= 70 Loss: 0.607 | Acc: 80.436% (7310/9088)
[Test] Epoch= 9  BatchID= 0 Loss: 0.333 | Acc: 89.844% (115/128)
[Test] Epoch= 9  BatchID= 10 Loss: 0.489 | Acc: 83.381% (1174/1408)
[Test] Epoch= 9  BatchID= 20 Loss: 0.476 | Acc: 84.561% (2273/2688)
[Test] Epoch= 9  BatchID= 30 Loss: 0.628 | Acc: 79.439% (3118/3925)

Epoch: 10
[Train] Epoch= 10  BatchID= 0 Loss: 0.630 | Acc: 76.562% (98/128)
[Train] Epoch= 10  BatchID= 10 Loss: 0.617 | Acc: 79.403% (1118/1408)
[Train] Epoch= 10  BatchID= 20 Loss: 0.610 | Acc: 79.650% (2141/2688)
[Train] Epoch= 10  BatchID= 30 Loss: 0.612 | Acc: 79.814% (3167/3968)
[Train] Epoch= 10  BatchID= 40 Loss: 0.612 | Acc: 79.783% (4187/5248)
[Train] Epoch= 10  BatchID= 50 Loss: 0.610 | Acc: 79.779% (5208/6528)
[Train] Epoch= 10  BatchID= 60 Loss: 0.608 | Acc: 80.033% (6249/7808)
[Train] Epoch= 10  BatchID= 70 Loss: 0.610 | Acc: 80.073% (7277/9088)
[Test] Epoch= 10  BatchID= 0 Loss: 0.311 | Acc: 90.625% (116/128)
[Test] Epoch= 10  BatchID= 10 Loss: 0.663 | Acc: 77.557% (1092/1408)
[Test] Epoch= 10  BatchID= 20 Loss: 0.647 | Acc: 78.423% (2108/2688)
[Test] Epoch= 10  BatchID= 30 Loss: 0.635 | Acc: 79.567% (3123/3925)

Epoch: 11
[Train] Epoch= 11  BatchID= 0 Loss: 0.596 | Acc: 84.375% (108/128)
[Train] Epoch= 11  BatchID= 10 Loss: 0.599 | Acc: 80.469% (1133/1408)
[Train] Epoch= 11  BatchID= 20 Loss: 0.597 | Acc: 80.543% (2165/2688)
[Train] Epoch= 11  BatchID= 30 Loss: 0.600 | Acc: 80.368% (3189/3968)
[Train] Epoch= 11  BatchID= 40 Loss: 0.606 | Acc: 80.107% (4204/5248)
[Train] Epoch= 11  BatchID= 50 Loss: 0.615 | Acc: 79.917% (5217/6528)
[Train] Epoch= 11  BatchID= 60 Loss: 0.612 | Acc: 80.033% (6249/7808)
[Train] Epoch= 11  BatchID= 70 Loss: 0.610 | Acc: 80.095% (7279/9088)
[Test] Epoch= 11  BatchID= 0 Loss: 0.102 | Acc: 96.875% (124/128)
[Test] Epoch= 11  BatchID= 10 Loss: 0.582 | Acc: 80.398% (1132/1408)
[Test] Epoch= 11  BatchID= 20 Loss: 0.505 | Acc: 83.296% (2239/2688)
[Test] Epoch= 11  BatchID= 30 Loss: 0.559 | Acc: 81.911% (3215/3925)

Epoch: 12
[Train] Epoch= 12  BatchID= 0 Loss: 0.632 | Acc: 78.125% (100/128)
[Train] Epoch= 12  BatchID= 10 Loss: 0.559 | Acc: 81.605% (1149/1408)
[Train] Epoch= 12  BatchID= 20 Loss: 0.588 | Acc: 81.362% (2187/2688)
[Train] Epoch= 12  BatchID= 30 Loss: 0.592 | Acc: 81.048% (3216/3968)
[Train] Epoch= 12  BatchID= 40 Loss: 0.599 | Acc: 80.945% (4248/5248)
[Train] Epoch= 12  BatchID= 50 Loss: 0.615 | Acc: 80.453% (5252/6528)
[Train] Epoch= 12  BatchID= 60 Loss: 0.602 | Acc: 80.725% (6303/7808)
[Train] Epoch= 12  BatchID= 70 Loss: 0.602 | Acc: 80.689% (7333/9088)
[Test] Epoch= 12  BatchID= 0 Loss: 0.284 | Acc: 91.406% (117/128)
[Test] Epoch= 12  BatchID= 10 Loss: 0.750 | Acc: 77.841% (1096/1408)
[Test] Epoch= 12  BatchID= 20 Loss: 0.799 | Acc: 76.600% (2059/2688)
[Test] Epoch= 12  BatchID= 30 Loss: 0.695 | Acc: 79.261% (3111/3925)

Epoch: 13
[Train] Epoch= 13  BatchID= 0 Loss: 0.469 | Acc: 84.375% (108/128)
[Train] Epoch= 13  BatchID= 10 Loss: 0.535 | Acc: 81.960% (1154/1408)
[Train] Epoch= 13  BatchID= 20 Loss: 0.555 | Acc: 82.031% (2205/2688)
[Train] Epoch= 13  BatchID= 30 Loss: 0.578 | Acc: 80.973% (3213/3968)
[Train] Epoch= 13  BatchID= 40 Loss: 0.581 | Acc: 80.926% (4247/5248)
[Train] Epoch= 13  BatchID= 50 Loss: 0.584 | Acc: 80.913% (5282/6528)
[Train] Epoch= 13  BatchID= 60 Loss: 0.593 | Acc: 80.584% (6292/7808)
[Train] Epoch= 13  BatchID= 70 Loss: 0.591 | Acc: 80.788% (7342/9088)
[Test] Epoch= 13  BatchID= 0 Loss: 0.122 | Acc: 96.875% (124/128)
[Test] Epoch= 13  BatchID= 10 Loss: 0.585 | Acc: 80.966% (1140/1408)
[Test] Epoch= 13  BatchID= 20 Loss: 0.620 | Acc: 80.134% (2154/2688)
[Test] Epoch= 13  BatchID= 30 Loss: 0.577 | Acc: 81.503% (3199/3925)

Epoch: 14
[Train] Epoch= 14  BatchID= 0 Loss: 0.525 | Acc: 84.375% (108/128)
[Train] Epoch= 14  BatchID= 10 Loss: 0.532 | Acc: 83.310% (1173/1408)
[Train] Epoch= 14  BatchID= 20 Loss: 0.529 | Acc: 83.408% (2242/2688)
[Train] Epoch= 14  BatchID= 30 Loss: 0.570 | Acc: 81.855% (3248/3968)
[Train] Epoch= 14  BatchID= 40 Loss: 0.572 | Acc: 81.707% (4288/5248)
[Train] Epoch= 14  BatchID= 50 Loss: 0.559 | Acc: 82.093% (5359/6528)
[Train] Epoch= 14  BatchID= 60 Loss: 0.568 | Acc: 81.698% (6379/7808)
[Train] Epoch= 14  BatchID= 70 Loss: 0.580 | Acc: 81.393% (7397/9088)
[Test] Epoch= 14  BatchID= 0 Loss: 0.256 | Acc: 92.969% (119/128)
[Test] Epoch= 14  BatchID= 10 Loss: 0.731 | Acc: 75.568% (1064/1408)
[Test] Epoch= 14  BatchID= 20 Loss: 0.715 | Acc: 76.786% (2064/2688)
[Test] Epoch= 14  BatchID= 30 Loss: 0.693 | Acc: 77.682% (3049/3925)

Epoch: 15
[Train] Epoch= 15  BatchID= 0 Loss: 0.608 | Acc: 78.906% (101/128)
[Train] Epoch= 15  BatchID= 10 Loss: 0.538 | Acc: 82.955% (1168/1408)
[Train] Epoch= 15  BatchID= 20 Loss: 0.542 | Acc: 82.924% (2229/2688)
[Train] Epoch= 15  BatchID= 30 Loss: 0.555 | Acc: 82.208% (3262/3968)
[Train] Epoch= 15  BatchID= 40 Loss: 0.564 | Acc: 82.146% (4311/5248)
[Train] Epoch= 15  BatchID= 50 Loss: 0.568 | Acc: 82.077% (5358/6528)
[Train] Epoch= 15  BatchID= 60 Loss: 0.565 | Acc: 82.134% (6413/7808)
[Train] Epoch= 15  BatchID= 70 Loss: 0.561 | Acc: 82.284% (7478/9088)
[Test] Epoch= 15  BatchID= 0 Loss: 0.114 | Acc: 96.094% (123/128)
[Test] Epoch= 15  BatchID= 10 Loss: 0.397 | Acc: 87.145% (1227/1408)
[Test] Epoch= 15  BatchID= 20 Loss: 0.535 | Acc: 83.185% (2236/2688)
[Test] Epoch= 15  BatchID= 30 Loss: 0.592 | Acc: 82.140% (3224/3925)

Epoch: 16
[Train] Epoch= 16  BatchID= 0 Loss: 0.543 | Acc: 83.594% (107/128)
[Train] Epoch= 16  BatchID= 10 Loss: 0.561 | Acc: 81.392% (1146/1408)
[Train] Epoch= 16  BatchID= 20 Loss: 0.553 | Acc: 82.254% (2211/2688)
[Train] Epoch= 16  BatchID= 30 Loss: 0.563 | Acc: 82.006% (3254/3968)
[Train] Epoch= 16  BatchID= 40 Loss: 0.577 | Acc: 81.421% (4273/5248)
[Train] Epoch= 16  BatchID= 50 Loss: 0.576 | Acc: 81.342% (5310/6528)
[Train] Epoch= 16  BatchID= 60 Loss: 0.581 | Acc: 81.160% (6337/7808)
[Train] Epoch= 16  BatchID= 70 Loss: 0.576 | Acc: 81.173% (7377/9088)
[Test] Epoch= 16  BatchID= 0 Loss: 0.439 | Acc: 87.500% (112/128)
[Test] Epoch= 16  BatchID= 10 Loss: 0.891 | Acc: 71.520% (1007/1408)
[Test] Epoch= 16  BatchID= 20 Loss: 0.663 | Acc: 78.571% (2112/2688)
[Test] Epoch= 16  BatchID= 30 Loss: 0.620 | Acc: 80.459% (3158/3925)

Epoch: 17
[Train] Epoch= 17  BatchID= 0 Loss: 0.501 | Acc: 82.031% (105/128)
[Train] Epoch= 17  BatchID= 10 Loss: 0.585 | Acc: 81.818% (1152/1408)
[Train] Epoch= 17  BatchID= 20 Loss: 0.575 | Acc: 81.734% (2197/2688)
[Train] Epoch= 17  BatchID= 30 Loss: 0.573 | Acc: 81.502% (3234/3968)
[Train] Epoch= 17  BatchID= 40 Loss: 0.581 | Acc: 81.402% (4272/5248)
[Train] Epoch= 17  BatchID= 50 Loss: 0.580 | Acc: 81.403% (5314/6528)
[Train] Epoch= 17  BatchID= 60 Loss: 0.587 | Acc: 81.263% (6345/7808)
[Train] Epoch= 17  BatchID= 70 Loss: 0.589 | Acc: 81.162% (7376/9088)
[Test] Epoch= 17  BatchID= 0 Loss: 0.204 | Acc: 92.969% (119/128)
[Test] Epoch= 17  BatchID= 10 Loss: 0.459 | Acc: 84.162% (1185/1408)
[Test] Epoch= 17  BatchID= 20 Loss: 0.482 | Acc: 84.375% (2268/2688)
[Test] Epoch= 17  BatchID= 30 Loss: 0.494 | Acc: 84.229% (3306/3925)
Saving..
Best accuracy:  84.22929936305732

Epoch: 18
[Train] Epoch= 18  BatchID= 0 Loss: 0.603 | Acc: 79.688% (102/128)
[Train] Epoch= 18  BatchID= 10 Loss: 0.552 | Acc: 80.895% (1139/1408)
[Train] Epoch= 18  BatchID= 20 Loss: 0.533 | Acc: 82.292% (2212/2688)
[Train] Epoch= 18  BatchID= 30 Loss: 0.552 | Acc: 81.502% (3234/3968)
[Train] Epoch= 18  BatchID= 40 Loss: 0.545 | Acc: 82.279% (4318/5248)
[Train] Epoch= 18  BatchID= 50 Loss: 0.543 | Acc: 82.399% (5379/6528)
[Train] Epoch= 18  BatchID= 60 Loss: 0.545 | Acc: 82.480% (6440/7808)
[Train] Epoch= 18  BatchID= 70 Loss: 0.547 | Acc: 82.449% (7493/9088)
[Test] Epoch= 18  BatchID= 0 Loss: 0.481 | Acc: 89.062% (114/128)
[Test] Epoch= 18  BatchID= 10 Loss: 0.565 | Acc: 81.889% (1153/1408)
[Test] Epoch= 18  BatchID= 20 Loss: 0.624 | Acc: 80.394% (2161/2688)
[Test] Epoch= 18  BatchID= 30 Loss: 0.624 | Acc: 80.790% (3171/3925)

Epoch: 19
[Train] Epoch= 19  BatchID= 0 Loss: 0.651 | Acc: 82.031% (105/128)
[Train] Epoch= 19  BatchID= 10 Loss: 0.579 | Acc: 81.534% (1148/1408)
[Train] Epoch= 19  BatchID= 20 Loss: 0.554 | Acc: 82.180% (2209/2688)
[Train] Epoch= 19  BatchID= 30 Loss: 0.558 | Acc: 81.855% (3248/3968)
[Train] Epoch= 19  BatchID= 40 Loss: 0.557 | Acc: 81.574% (4281/5248)
[Train] Epoch= 19  BatchID= 50 Loss: 0.554 | Acc: 81.587% (5326/6528)
[Train] Epoch= 19  BatchID= 60 Loss: 0.558 | Acc: 81.596% (6371/7808)
[Train] Epoch= 19  BatchID= 70 Loss: 0.562 | Acc: 81.657% (7421/9088)
[Test] Epoch= 19  BatchID= 0 Loss: 0.193 | Acc: 92.969% (119/128)
[Test] Epoch= 19  BatchID= 10 Loss: 0.448 | Acc: 85.085% (1198/1408)
[Test] Epoch= 19  BatchID= 20 Loss: 0.506 | Acc: 83.705% (2250/2688)
[Test] Epoch= 19  BatchID= 30 Loss: 0.544 | Acc: 82.624% (3243/3925)

Epoch: 20
[Train] Epoch= 20  BatchID= 0 Loss: 0.582 | Acc: 86.719% (111/128)
[Train] Epoch= 20  BatchID= 10 Loss: 0.523 | Acc: 83.594% (1177/1408)
[Train] Epoch= 20  BatchID= 20 Loss: 0.514 | Acc: 83.557% (2246/2688)
[Train] Epoch= 20  BatchID= 30 Loss: 0.525 | Acc: 83.518% (3314/3968)
[Train] Epoch= 20  BatchID= 40 Loss: 0.530 | Acc: 82.984% (4355/5248)
[Train] Epoch= 20  BatchID= 50 Loss: 0.537 | Acc: 82.721% (5400/6528)
[Train] Epoch= 20  BatchID= 60 Loss: 0.541 | Acc: 82.403% (6434/7808)
[Train] Epoch= 20  BatchID= 70 Loss: 0.543 | Acc: 82.438% (7492/9088)
[Test] Epoch= 20  BatchID= 0 Loss: 0.390 | Acc: 89.844% (115/128)
[Test] Epoch= 20  BatchID= 10 Loss: 0.482 | Acc: 83.736% (1179/1408)
[Test] Epoch= 20  BatchID= 20 Loss: 0.512 | Acc: 83.408% (2242/2688)
[Test] Epoch= 20  BatchID= 30 Loss: 0.597 | Acc: 80.510% (3160/3925)

Epoch: 21
[Train] Epoch= 21  BatchID= 0 Loss: 0.548 | Acc: 82.812% (106/128)
[Train] Epoch= 21  BatchID= 10 Loss: 0.498 | Acc: 84.162% (1185/1408)
[Train] Epoch= 21  BatchID= 20 Loss: 0.516 | Acc: 84.040% (2259/2688)
[Train] Epoch= 21  BatchID= 30 Loss: 0.522 | Acc: 83.468% (3312/3968)
[Train] Epoch= 21  BatchID= 40 Loss: 0.527 | Acc: 83.327% (4373/5248)
[Train] Epoch= 21  BatchID= 50 Loss: 0.543 | Acc: 82.675% (5397/6528)
[Train] Epoch= 21  BatchID= 60 Loss: 0.546 | Acc: 82.556% (6446/7808)
[Train] Epoch= 21  BatchID= 70 Loss: 0.540 | Acc: 82.812% (7526/9088)
[Test] Epoch= 21  BatchID= 0 Loss: 0.147 | Acc: 93.750% (120/128)
[Test] Epoch= 21  BatchID= 10 Loss: 0.913 | Acc: 69.531% (979/1408)
[Test] Epoch= 21  BatchID= 20 Loss: 0.644 | Acc: 78.646% (2114/2688)
[Test] Epoch= 21  BatchID= 30 Loss: 0.646 | Acc: 79.006% (3101/3925)

Epoch: 22
[Train] Epoch= 22  BatchID= 0 Loss: 0.525 | Acc: 81.250% (104/128)
[Train] Epoch= 22  BatchID= 10 Loss: 0.507 | Acc: 83.097% (1170/1408)
[Train] Epoch= 22  BatchID= 20 Loss: 0.514 | Acc: 83.110% (2234/2688)
[Train] Epoch= 22  BatchID= 30 Loss: 0.509 | Acc: 83.468% (3312/3968)
[Train] Epoch= 22  BatchID= 40 Loss: 0.514 | Acc: 83.308% (4372/5248)
[Train] Epoch= 22  BatchID= 50 Loss: 0.527 | Acc: 83.042% (5421/6528)
[Train] Epoch= 22  BatchID= 60 Loss: 0.532 | Acc: 82.928% (6475/7808)
[Train] Epoch= 22  BatchID= 70 Loss: 0.538 | Acc: 82.779% (7523/9088)
[Test] Epoch= 22  BatchID= 0 Loss: 0.074 | Acc: 97.656% (125/128)
[Test] Epoch= 22  BatchID= 10 Loss: 0.397 | Acc: 85.724% (1207/1408)
[Test] Epoch= 22  BatchID= 20 Loss: 0.412 | Acc: 86.272% (2319/2688)
[Test] Epoch= 22  BatchID= 30 Loss: 0.498 | Acc: 83.669% (3284/3925)

Epoch: 23
[Train] Epoch= 23  BatchID= 0 Loss: 0.456 | Acc: 86.719% (111/128)
[Train] Epoch= 23  BatchID= 10 Loss: 0.529 | Acc: 82.528% (1162/1408)
[Train] Epoch= 23  BatchID= 20 Loss: 0.561 | Acc: 81.287% (2185/2688)
[Train] Epoch= 23  BatchID= 30 Loss: 0.558 | Acc: 81.351% (3228/3968)
[Train] Epoch= 23  BatchID= 40 Loss: 0.555 | Acc: 81.326% (4268/5248)
[Train] Epoch= 23  BatchID= 50 Loss: 0.550 | Acc: 81.771% (5338/6528)
[Train] Epoch= 23  BatchID= 60 Loss: 0.548 | Acc: 81.749% (6383/7808)
[Train] Epoch= 23  BatchID= 70 Loss: 0.545 | Acc: 82.020% (7454/9088)
[Test] Epoch= 23  BatchID= 0 Loss: 0.439 | Acc: 84.375% (108/128)
[Test] Epoch= 23  BatchID= 10 Loss: 0.624 | Acc: 79.403% (1118/1408)
[Test] Epoch= 23  BatchID= 20 Loss: 0.627 | Acc: 79.129% (2127/2688)
[Test] Epoch= 23  BatchID= 30 Loss: 0.595 | Acc: 80.611% (3164/3925)

Epoch: 24
[Train] Epoch= 24  BatchID= 0 Loss: 0.457 | Acc: 86.719% (111/128)
[Train] Epoch= 24  BatchID= 10 Loss: 0.490 | Acc: 84.801% (1194/1408)
[Train] Epoch= 24  BatchID= 20 Loss: 0.496 | Acc: 84.635% (2275/2688)
[Train] Epoch= 24  BatchID= 30 Loss: 0.500 | Acc: 84.375% (3348/3968)
[Train] Epoch= 24  BatchID= 40 Loss: 0.514 | Acc: 83.899% (4403/5248)
[Train] Epoch= 24  BatchID= 50 Loss: 0.519 | Acc: 83.747% (5467/6528)
[Train] Epoch= 24  BatchID= 60 Loss: 0.534 | Acc: 83.363% (6509/7808)
[Train] Epoch= 24  BatchID= 70 Loss: 0.538 | Acc: 83.308% (7571/9088)
[Test] Epoch= 24  BatchID= 0 Loss: 0.217 | Acc: 94.531% (121/128)
[Test] Epoch= 24  BatchID= 10 Loss: 0.608 | Acc: 80.256% (1130/1408)
[Test] Epoch= 24  BatchID= 20 Loss: 0.652 | Acc: 79.018% (2124/2688)
[Test] Epoch= 24  BatchID= 30 Loss: 0.596 | Acc: 81.070% (3182/3925)

Epoch: 25
[Train] Epoch= 25  BatchID= 0 Loss: 0.481 | Acc: 84.375% (108/128)
[Train] Epoch= 25  BatchID= 10 Loss: 0.485 | Acc: 83.026% (1169/1408)
[Train] Epoch= 25  BatchID= 20 Loss: 0.483 | Acc: 83.185% (2236/2688)
[Train] Epoch= 25  BatchID= 30 Loss: 0.505 | Acc: 82.762% (3284/3968)
[Train] Epoch= 25  BatchID= 40 Loss: 0.527 | Acc: 82.393% (4324/5248)
[Train] Epoch= 25  BatchID= 50 Loss: 0.530 | Acc: 82.368% (5377/6528)
[Train] Epoch= 25  BatchID= 60 Loss: 0.538 | Acc: 82.223% (6420/7808)
[Train] Epoch= 25  BatchID= 70 Loss: 0.538 | Acc: 82.405% (7489/9088)
[Test] Epoch= 25  BatchID= 0 Loss: 0.232 | Acc: 92.969% (119/128)
[Test] Epoch= 25  BatchID= 10 Loss: 0.443 | Acc: 85.511% (1204/1408)
[Test] Epoch= 25  BatchID= 20 Loss: 0.483 | Acc: 84.301% (2266/2688)
[Test] Epoch= 25  BatchID= 30 Loss: 0.523 | Acc: 83.439% (3275/3925)

Epoch: 26
[Train] Epoch= 26  BatchID= 0 Loss: 0.482 | Acc: 82.031% (105/128)
[Train] Epoch= 26  BatchID= 10 Loss: 0.504 | Acc: 84.162% (1185/1408)
[Train] Epoch= 26  BatchID= 20 Loss: 0.497 | Acc: 84.449% (2270/2688)
[Train] Epoch= 26  BatchID= 30 Loss: 0.524 | Acc: 83.342% (3307/3968)
[Train] Epoch= 26  BatchID= 40 Loss: 0.522 | Acc: 83.403% (4377/5248)
[Train] Epoch= 26  BatchID= 50 Loss: 0.520 | Acc: 83.594% (5457/6528)
[Train] Epoch= 26  BatchID= 60 Loss: 0.521 | Acc: 83.517% (6521/7808)
[Train] Epoch= 26  BatchID= 70 Loss: 0.527 | Acc: 83.374% (7577/9088)
[Test] Epoch= 26  BatchID= 0 Loss: 0.158 | Acc: 95.312% (122/128)
[Test] Epoch= 26  BatchID= 10 Loss: 0.377 | Acc: 87.145% (1227/1408)
[Test] Epoch= 26  BatchID= 20 Loss: 0.471 | Acc: 84.710% (2277/2688)
[Test] Epoch= 26  BatchID= 30 Loss: 0.469 | Acc: 84.510% (3317/3925)
Saving..
Best accuracy:  84.50955414012739

Epoch: 27
[Train] Epoch= 27  BatchID= 0 Loss: 0.405 | Acc: 89.062% (114/128)
[Train] Epoch= 27  BatchID= 10 Loss: 0.507 | Acc: 83.310% (1173/1408)
[Train] Epoch= 27  BatchID= 20 Loss: 0.507 | Acc: 82.850% (2227/2688)
[Train] Epoch= 27  BatchID= 30 Loss: 0.506 | Acc: 83.115% (3298/3968)
[Train] Epoch= 27  BatchID= 40 Loss: 0.507 | Acc: 83.251% (4369/5248)
[Train] Epoch= 27  BatchID= 50 Loss: 0.515 | Acc: 83.058% (5422/6528)
[Train] Epoch= 27  BatchID= 60 Loss: 0.511 | Acc: 83.312% (6505/7808)
[Train] Epoch= 27  BatchID= 70 Loss: 0.510 | Acc: 83.176% (7559/9088)
[Test] Epoch= 27  BatchID= 0 Loss: 0.344 | Acc: 90.625% (116/128)
[Test] Epoch= 27  BatchID= 10 Loss: 0.483 | Acc: 83.807% (1180/1408)
[Test] Epoch= 27  BatchID= 20 Loss: 0.500 | Acc: 84.040% (2259/2688)
[Test] Epoch= 27  BatchID= 30 Loss: 0.484 | Acc: 84.306% (3309/3925)

Epoch: 28
[Train] Epoch= 28  BatchID= 0 Loss: 0.477 | Acc: 84.375% (108/128)
[Train] Epoch= 28  BatchID= 10 Loss: 0.495 | Acc: 84.517% (1190/1408)
[Train] Epoch= 28  BatchID= 20 Loss: 0.506 | Acc: 83.036% (2232/2688)
[Train] Epoch= 28  BatchID= 30 Loss: 0.498 | Acc: 83.795% (3325/3968)
[Train] Epoch= 28  BatchID= 40 Loss: 0.512 | Acc: 83.460% (4380/5248)
[Train] Epoch= 28  BatchID= 50 Loss: 0.514 | Acc: 83.318% (5439/6528)
[Train] Epoch= 28  BatchID= 60 Loss: 0.512 | Acc: 83.389% (6511/7808)
[Train] Epoch= 28  BatchID= 70 Loss: 0.512 | Acc: 83.242% (7565/9088)
[Test] Epoch= 28  BatchID= 0 Loss: 0.380 | Acc: 86.719% (111/128)
[Test] Epoch= 28  BatchID= 10 Loss: 0.809 | Acc: 75.142% (1058/1408)
[Test] Epoch= 28  BatchID= 20 Loss: 0.634 | Acc: 80.692% (2169/2688)
[Test] Epoch= 28  BatchID= 30 Loss: 0.615 | Acc: 81.096% (3183/3925)

Epoch: 29
[Train] Epoch= 29  BatchID= 0 Loss: 0.518 | Acc: 81.250% (104/128)
[Train] Epoch= 29  BatchID= 10 Loss: 0.514 | Acc: 82.386% (1160/1408)
[Train] Epoch= 29  BatchID= 20 Loss: 0.503 | Acc: 83.259% (2238/2688)
[Train] Epoch= 29  BatchID= 30 Loss: 0.519 | Acc: 83.039% (3295/3968)
[Train] Epoch= 29  BatchID= 40 Loss: 0.521 | Acc: 82.774% (4344/5248)
[Train] Epoch= 29  BatchID= 50 Loss: 0.522 | Acc: 82.659% (5396/6528)
[Train] Epoch= 29  BatchID= 60 Loss: 0.521 | Acc: 82.646% (6453/7808)
[Train] Epoch= 29  BatchID= 70 Loss: 0.527 | Acc: 82.548% (7502/9088)
[Test] Epoch= 29  BatchID= 0 Loss: 0.474 | Acc: 84.375% (108/128)
[Test] Epoch= 29  BatchID= 10 Loss: 0.510 | Acc: 82.955% (1168/1408)
[Test] Epoch= 29  BatchID= 20 Loss: 0.443 | Acc: 85.417% (2296/2688)
[Test] Epoch= 29  BatchID= 30 Loss: 0.490 | Acc: 83.771% (3288/3925)

Epoch: 30
[Train] Epoch= 30  BatchID= 0 Loss: 0.331 | Acc: 90.625% (116/128)
[Train] Epoch= 30  BatchID= 10 Loss: 0.473 | Acc: 84.304% (1187/1408)
[Train] Epoch= 30  BatchID= 20 Loss: 0.481 | Acc: 84.338% (2267/2688)
[Train] Epoch= 30  BatchID= 30 Loss: 0.503 | Acc: 83.392% (3309/3968)
[Train] Epoch= 30  BatchID= 40 Loss: 0.515 | Acc: 83.155% (4364/5248)
[Train] Epoch= 30  BatchID= 50 Loss: 0.515 | Acc: 82.996% (5418/6528)
[Train] Epoch= 30  BatchID= 60 Loss: 0.510 | Acc: 83.235% (6499/7808)
[Train] Epoch= 30  BatchID= 70 Loss: 0.512 | Acc: 83.231% (7564/9088)
[Test] Epoch= 30  BatchID= 0 Loss: 0.368 | Acc: 88.281% (113/128)
[Test] Epoch= 30  BatchID= 10 Loss: 0.554 | Acc: 82.315% (1159/1408)
[Test] Epoch= 30  BatchID= 20 Loss: 0.522 | Acc: 83.259% (2238/2688)
[Test] Epoch= 30  BatchID= 30 Loss: 0.542 | Acc: 82.854% (3252/3925)

Epoch: 31
[Train] Epoch= 31  BatchID= 0 Loss: 0.491 | Acc: 82.812% (106/128)
[Train] Epoch= 31  BatchID= 10 Loss: 0.511 | Acc: 83.026% (1169/1408)
[Train] Epoch= 31  BatchID= 20 Loss: 0.483 | Acc: 84.040% (2259/2688)
[Train] Epoch= 31  BatchID= 30 Loss: 0.484 | Acc: 83.896% (3329/3968)
[Train] Epoch= 31  BatchID= 40 Loss: 0.488 | Acc: 83.803% (4398/5248)
[Train] Epoch= 31  BatchID= 50 Loss: 0.493 | Acc: 83.747% (5467/6528)
[Train] Epoch= 31  BatchID= 60 Loss: 0.487 | Acc: 84.029% (6561/7808)
[Train] Epoch= 31  BatchID= 70 Loss: 0.490 | Acc: 83.968% (7631/9088)
[Test] Epoch= 31  BatchID= 0 Loss: 0.141 | Acc: 96.875% (124/128)
[Test] Epoch= 31  BatchID= 10 Loss: 0.474 | Acc: 84.446% (1189/1408)
[Test] Epoch= 31  BatchID= 20 Loss: 0.423 | Acc: 86.235% (2318/2688)
[Test] Epoch= 31  BatchID= 30 Loss: 0.455 | Acc: 85.401% (3352/3925)
Saving..
Best accuracy:  85.40127388535032

Epoch: 32
[Train] Epoch= 32  BatchID= 0 Loss: 0.472 | Acc: 82.812% (106/128)
[Train] Epoch= 32  BatchID= 10 Loss: 0.494 | Acc: 83.452% (1175/1408)
[Train] Epoch= 32  BatchID= 20 Loss: 0.467 | Acc: 85.082% (2287/2688)
[Train] Epoch= 32  BatchID= 30 Loss: 0.476 | Acc: 84.526% (3354/3968)
[Train] Epoch= 32  BatchID= 40 Loss: 0.492 | Acc: 83.765% (4396/5248)
[Train] Epoch= 32  BatchID= 50 Loss: 0.502 | Acc: 83.364% (5442/6528)
[Train] Epoch= 32  BatchID= 60 Loss: 0.501 | Acc: 83.530% (6522/7808)
[Train] Epoch= 32  BatchID= 70 Loss: 0.499 | Acc: 83.583% (7596/9088)
[Test] Epoch= 32  BatchID= 0 Loss: 0.411 | Acc: 88.281% (113/128)
[Test] Epoch= 32  BatchID= 10 Loss: 0.587 | Acc: 80.966% (1140/1408)
[Test] Epoch= 32  BatchID= 20 Loss: 0.460 | Acc: 84.784% (2279/2688)
[Test] Epoch= 32  BatchID= 30 Loss: 0.516 | Acc: 83.159% (3264/3925)

Epoch: 33
[Train] Epoch= 33  BatchID= 0 Loss: 0.404 | Acc: 92.188% (118/128)
[Train] Epoch= 33  BatchID= 10 Loss: 0.478 | Acc: 84.801% (1194/1408)
[Train] Epoch= 33  BatchID= 20 Loss: 0.452 | Acc: 85.677% (2303/2688)
[Train] Epoch= 33  BatchID= 30 Loss: 0.470 | Acc: 85.207% (3381/3968)
[Train] Epoch= 33  BatchID= 40 Loss: 0.472 | Acc: 84.985% (4460/5248)
[Train] Epoch= 33  BatchID= 50 Loss: 0.484 | Acc: 84.605% (5523/6528)
[Train] Epoch= 33  BatchID= 60 Loss: 0.485 | Acc: 84.413% (6591/7808)
[Train] Epoch= 33  BatchID= 70 Loss: 0.485 | Acc: 84.309% (7662/9088)
[Test] Epoch= 33  BatchID= 0 Loss: 0.158 | Acc: 95.312% (122/128)
[Test] Epoch= 33  BatchID= 10 Loss: 0.607 | Acc: 80.043% (1127/1408)
[Test] Epoch= 33  BatchID= 20 Loss: 0.653 | Acc: 79.613% (2140/2688)
[Test] Epoch= 33  BatchID= 30 Loss: 0.598 | Acc: 81.299% (3191/3925)

Epoch: 34
[Train] Epoch= 34  BatchID= 0 Loss: 0.270 | Acc: 93.750% (120/128)
[Train] Epoch= 34  BatchID= 10 Loss: 0.451 | Acc: 85.866% (1209/1408)
[Train] Epoch= 34  BatchID= 20 Loss: 0.444 | Acc: 85.491% (2298/2688)
[Train] Epoch= 34  BatchID= 30 Loss: 0.448 | Acc: 85.585% (3396/3968)
[Train] Epoch= 34  BatchID= 40 Loss: 0.457 | Acc: 85.118% (4467/5248)
[Train] Epoch= 34  BatchID= 50 Loss: 0.456 | Acc: 85.294% (5568/6528)
[Train] Epoch= 34  BatchID= 60 Loss: 0.468 | Acc: 84.836% (6624/7808)
[Train] Epoch= 34  BatchID= 70 Loss: 0.469 | Acc: 84.738% (7701/9088)
[Test] Epoch= 34  BatchID= 0 Loss: 0.241 | Acc: 93.750% (120/128)
[Test] Epoch= 34  BatchID= 10 Loss: 0.629 | Acc: 80.185% (1129/1408)
[Test] Epoch= 34  BatchID= 20 Loss: 0.518 | Acc: 83.147% (2235/2688)
[Test] Epoch= 34  BatchID= 30 Loss: 0.545 | Acc: 82.420% (3235/3925)

Epoch: 35
[Train] Epoch= 35  BatchID= 0 Loss: 0.540 | Acc: 85.156% (109/128)
[Train] Epoch= 35  BatchID= 10 Loss: 0.464 | Acc: 84.801% (1194/1408)
[Train] Epoch= 35  BatchID= 20 Loss: 0.466 | Acc: 84.338% (2267/2688)
[Train] Epoch= 35  BatchID= 30 Loss: 0.460 | Acc: 84.829% (3366/3968)
[Train] Epoch= 35  BatchID= 40 Loss: 0.474 | Acc: 84.261% (4422/5248)
[Train] Epoch= 35  BatchID= 50 Loss: 0.474 | Acc: 84.329% (5505/6528)
[Train] Epoch= 35  BatchID= 60 Loss: 0.474 | Acc: 84.337% (6585/7808)
[Train] Epoch= 35  BatchID= 70 Loss: 0.477 | Acc: 84.210% (7653/9088)
[Test] Epoch= 35  BatchID= 0 Loss: 0.296 | Acc: 92.188% (118/128)
[Test] Epoch= 35  BatchID= 10 Loss: 0.494 | Acc: 83.878% (1181/1408)
[Test] Epoch= 35  BatchID= 20 Loss: 0.521 | Acc: 83.557% (2246/2688)
[Test] Epoch= 35  BatchID= 30 Loss: 0.516 | Acc: 83.592% (3281/3925)

Epoch: 36
[Train] Epoch= 36  BatchID= 0 Loss: 0.415 | Acc: 85.938% (110/128)
[Train] Epoch= 36  BatchID= 10 Loss: 0.476 | Acc: 83.452% (1175/1408)
[Train] Epoch= 36  BatchID= 20 Loss: 0.463 | Acc: 84.375% (2268/2688)
[Train] Epoch= 36  BatchID= 30 Loss: 0.464 | Acc: 84.375% (3348/3968)
[Train] Epoch= 36  BatchID= 40 Loss: 0.471 | Acc: 84.585% (4439/5248)
[Train] Epoch= 36  BatchID= 50 Loss: 0.462 | Acc: 85.049% (5552/6528)
[Train] Epoch= 36  BatchID= 60 Loss: 0.459 | Acc: 85.067% (6642/7808)
[Train] Epoch= 36  BatchID= 70 Loss: 0.460 | Acc: 85.024% (7727/9088)
[Test] Epoch= 36  BatchID= 0 Loss: 0.221 | Acc: 92.969% (119/128)
[Test] Epoch= 36  BatchID= 10 Loss: 0.419 | Acc: 85.938% (1210/1408)
[Test] Epoch= 36  BatchID= 20 Loss: 0.427 | Acc: 86.272% (2319/2688)
[Test] Epoch= 36  BatchID= 30 Loss: 0.470 | Acc: 84.943% (3334/3925)

Epoch: 37
[Train] Epoch= 37  BatchID= 0 Loss: 0.442 | Acc: 84.375% (108/128)
[Train] Epoch= 37  BatchID= 10 Loss: 0.448 | Acc: 85.369% (1202/1408)
[Train] Epoch= 37  BatchID= 20 Loss: 0.445 | Acc: 85.417% (2296/2688)
[Train] Epoch= 37  BatchID= 30 Loss: 0.441 | Acc: 85.484% (3392/3968)
[Train] Epoch= 37  BatchID= 40 Loss: 0.447 | Acc: 85.480% (4486/5248)
[Train] Epoch= 37  BatchID= 50 Loss: 0.456 | Acc: 85.034% (5551/6528)
[Train] Epoch= 37  BatchID= 60 Loss: 0.456 | Acc: 85.015% (6638/7808)
[Train] Epoch= 37  BatchID= 70 Loss: 0.461 | Acc: 84.991% (7724/9088)
[Test] Epoch= 37  BatchID= 0 Loss: 0.153 | Acc: 96.875% (124/128)
[Test] Epoch= 37  BatchID= 10 Loss: 0.392 | Acc: 87.003% (1225/1408)
[Test] Epoch= 37  BatchID= 20 Loss: 0.393 | Acc: 87.388% (2349/2688)
[Test] Epoch= 37  BatchID= 30 Loss: 0.455 | Acc: 85.783% (3367/3925)
Saving..
Best accuracy:  85.78343949044586

Epoch: 38
[Train] Epoch= 38  BatchID= 0 Loss: 0.560 | Acc: 80.469% (103/128)
[Train] Epoch= 38  BatchID= 10 Loss: 0.453 | Acc: 84.304% (1187/1408)
[Train] Epoch= 38  BatchID= 20 Loss: 0.454 | Acc: 84.673% (2276/2688)
[Train] Epoch= 38  BatchID= 30 Loss: 0.443 | Acc: 85.383% (3388/3968)
[Train] Epoch= 38  BatchID= 40 Loss: 0.445 | Acc: 85.480% (4486/5248)
[Train] Epoch= 38  BatchID= 50 Loss: 0.447 | Acc: 85.555% (5585/6528)
[Train] Epoch= 38  BatchID= 60 Loss: 0.445 | Acc: 85.681% (6690/7808)
[Train] Epoch= 38  BatchID= 70 Loss: 0.448 | Acc: 85.563% (7776/9088)
[Test] Epoch= 38  BatchID= 0 Loss: 0.182 | Acc: 92.969% (119/128)
[Test] Epoch= 38  BatchID= 10 Loss: 0.403 | Acc: 85.653% (1206/1408)
[Test] Epoch= 38  BatchID= 20 Loss: 0.434 | Acc: 85.379% (2295/2688)
[Test] Epoch= 38  BatchID= 30 Loss: 0.448 | Acc: 84.815% (3329/3925)

Epoch: 39
[Train] Epoch= 39  BatchID= 0 Loss: 0.422 | Acc: 89.062% (114/128)
[Train] Epoch= 39  BatchID= 10 Loss: 0.406 | Acc: 86.577% (1219/1408)
[Train] Epoch= 39  BatchID= 20 Loss: 0.402 | Acc: 86.533% (2326/2688)
[Train] Epoch= 39  BatchID= 30 Loss: 0.415 | Acc: 86.542% (3434/3968)
[Train] Epoch= 39  BatchID= 40 Loss: 0.429 | Acc: 85.976% (4512/5248)
[Train] Epoch= 39  BatchID= 50 Loss: 0.438 | Acc: 85.646% (5591/6528)
[Train] Epoch= 39  BatchID= 60 Loss: 0.444 | Acc: 85.464% (6673/7808)
[Train] Epoch= 39  BatchID= 70 Loss: 0.446 | Acc: 85.453% (7766/9088)
[Test] Epoch= 39  BatchID= 0 Loss: 0.323 | Acc: 92.188% (118/128)
[Test] Epoch= 39  BatchID= 10 Loss: 0.482 | Acc: 85.227% (1200/1408)
[Test] Epoch= 39  BatchID= 20 Loss: 0.415 | Acc: 87.016% (2339/2688)
[Test] Epoch= 39  BatchID= 30 Loss: 0.475 | Acc: 84.459% (3315/3925)

Epoch: 40
[Train] Epoch= 40  BatchID= 0 Loss: 0.386 | Acc: 84.375% (108/128)
[Train] Epoch= 40  BatchID= 10 Loss: 0.417 | Acc: 86.009% (1211/1408)
[Train] Epoch= 40  BatchID= 20 Loss: 0.416 | Acc: 86.012% (2312/2688)
[Train] Epoch= 40  BatchID= 30 Loss: 0.431 | Acc: 85.433% (3390/3968)
[Train] Epoch= 40  BatchID= 40 Loss: 0.431 | Acc: 85.709% (4498/5248)
[Train] Epoch= 40  BatchID= 50 Loss: 0.437 | Acc: 85.539% (5584/6528)
[Train] Epoch= 40  BatchID= 60 Loss: 0.435 | Acc: 85.630% (6686/7808)
[Train] Epoch= 40  BatchID= 70 Loss: 0.437 | Acc: 85.574% (7777/9088)
[Test] Epoch= 40  BatchID= 0 Loss: 0.106 | Acc: 96.875% (124/128)
[Test] Epoch= 40  BatchID= 10 Loss: 0.317 | Acc: 90.057% (1268/1408)
[Test] Epoch= 40  BatchID= 20 Loss: 0.363 | Acc: 88.393% (2376/2688)
[Test] Epoch= 40  BatchID= 30 Loss: 0.393 | Acc: 87.338% (3428/3925)
Saving..
Best accuracy:  87.3375796178344

Epoch: 41
[Train] Epoch= 41  BatchID= 0 Loss: 0.459 | Acc: 89.062% (114/128)
[Train] Epoch= 41  BatchID= 10 Loss: 0.436 | Acc: 85.156% (1199/1408)
[Train] Epoch= 41  BatchID= 20 Loss: 0.415 | Acc: 85.975% (2311/2688)
[Train] Epoch= 41  BatchID= 30 Loss: 0.422 | Acc: 85.736% (3402/3968)
[Train] Epoch= 41  BatchID= 40 Loss: 0.427 | Acc: 85.595% (4492/5248)
[Train] Epoch= 41  BatchID= 50 Loss: 0.430 | Acc: 85.708% (5595/6528)
[Train] Epoch= 41  BatchID= 60 Loss: 0.426 | Acc: 85.938% (6710/7808)
[Train] Epoch= 41  BatchID= 70 Loss: 0.422 | Acc: 86.158% (7830/9088)
[Test] Epoch= 41  BatchID= 0 Loss: 0.249 | Acc: 91.406% (117/128)
[Test] Epoch= 41  BatchID= 10 Loss: 0.389 | Acc: 87.926% (1238/1408)
[Test] Epoch= 41  BatchID= 20 Loss: 0.393 | Acc: 87.723% (2358/2688)
[Test] Epoch= 41  BatchID= 30 Loss: 0.405 | Acc: 87.083% (3418/3925)

Epoch: 42
[Train] Epoch= 42  BatchID= 0 Loss: 0.404 | Acc: 83.594% (107/128)
[Train] Epoch= 42  BatchID= 10 Loss: 0.402 | Acc: 86.435% (1217/1408)
[Train] Epoch= 42  BatchID= 20 Loss: 0.394 | Acc: 86.979% (2338/2688)
[Train] Epoch= 42  BatchID= 30 Loss: 0.402 | Acc: 86.744% (3442/3968)
[Train] Epoch= 42  BatchID= 40 Loss: 0.403 | Acc: 86.852% (4558/5248)
[Train] Epoch= 42  BatchID= 50 Loss: 0.405 | Acc: 86.918% (5674/6528)
[Train] Epoch= 42  BatchID= 60 Loss: 0.408 | Acc: 86.693% (6769/7808)
[Train] Epoch= 42  BatchID= 70 Loss: 0.410 | Acc: 86.587% (7869/9088)
[Test] Epoch= 42  BatchID= 0 Loss: 0.128 | Acc: 96.094% (123/128)
[Test] Epoch= 42  BatchID= 10 Loss: 0.393 | Acc: 87.642% (1234/1408)
[Test] Epoch= 42  BatchID= 20 Loss: 0.409 | Acc: 87.128% (2342/2688)
[Test] Epoch= 42  BatchID= 30 Loss: 0.418 | Acc: 86.828% (3408/3925)

Epoch: 43
[Train] Epoch= 43  BatchID= 0 Loss: 0.407 | Acc: 87.500% (112/128)
[Train] Epoch= 43  BatchID= 10 Loss: 0.405 | Acc: 87.145% (1227/1408)
[Train] Epoch= 43  BatchID= 20 Loss: 0.416 | Acc: 87.314% (2347/2688)
[Train] Epoch= 43  BatchID= 30 Loss: 0.397 | Acc: 87.802% (3484/3968)
[Train] Epoch= 43  BatchID= 40 Loss: 0.397 | Acc: 87.538% (4594/5248)
[Train] Epoch= 43  BatchID= 50 Loss: 0.402 | Acc: 87.301% (5699/6528)
[Train] Epoch= 43  BatchID= 60 Loss: 0.410 | Acc: 86.962% (6790/7808)
[Train] Epoch= 43  BatchID= 70 Loss: 0.404 | Acc: 87.005% (7907/9088)
[Test] Epoch= 43  BatchID= 0 Loss: 0.323 | Acc: 91.406% (117/128)
[Test] Epoch= 43  BatchID= 10 Loss: 0.365 | Acc: 88.565% (1247/1408)
[Test] Epoch= 43  BatchID= 20 Loss: 0.356 | Acc: 88.467% (2378/2688)
[Test] Epoch= 43  BatchID= 30 Loss: 0.394 | Acc: 87.389% (3430/3925)
Saving..
Best accuracy:  87.38853503184713

Epoch: 44
[Train] Epoch= 44  BatchID= 0 Loss: 0.407 | Acc: 86.719% (111/128)
[Train] Epoch= 44  BatchID= 10 Loss: 0.379 | Acc: 88.707% (1249/1408)
[Train] Epoch= 44  BatchID= 20 Loss: 0.398 | Acc: 87.612% (2355/2688)
[Train] Epoch= 44  BatchID= 30 Loss: 0.391 | Acc: 87.676% (3479/3968)
[Train] Epoch= 44  BatchID= 40 Loss: 0.387 | Acc: 87.671% (4601/5248)
[Train] Epoch= 44  BatchID= 50 Loss: 0.401 | Acc: 87.316% (5700/6528)
[Train] Epoch= 44  BatchID= 60 Loss: 0.405 | Acc: 87.116% (6802/7808)
[Train] Epoch= 44  BatchID= 70 Loss: 0.409 | Acc: 87.027% (7909/9088)
[Test] Epoch= 44  BatchID= 0 Loss: 0.110 | Acc: 97.656% (125/128)
[Test] Epoch= 44  BatchID= 10 Loss: 0.343 | Acc: 89.205% (1256/1408)
[Test] Epoch= 44  BatchID= 20 Loss: 0.412 | Acc: 87.537% (2353/2688)
[Test] Epoch= 44  BatchID= 30 Loss: 0.425 | Acc: 86.624% (3400/3925)

Epoch: 45
[Train] Epoch= 45  BatchID= 0 Loss: 0.382 | Acc: 86.719% (111/128)
[Train] Epoch= 45  BatchID= 10 Loss: 0.327 | Acc: 89.631% (1262/1408)
[Train] Epoch= 45  BatchID= 20 Loss: 0.363 | Acc: 88.467% (2378/2688)
[Train] Epoch= 45  BatchID= 30 Loss: 0.365 | Acc: 88.332% (3505/3968)
[Train] Epoch= 45  BatchID= 40 Loss: 0.359 | Acc: 88.319% (4635/5248)
[Train] Epoch= 45  BatchID= 50 Loss: 0.358 | Acc: 88.281% (5763/6528)
[Train] Epoch= 45  BatchID= 60 Loss: 0.369 | Acc: 87.910% (6864/7808)
[Train] Epoch= 45  BatchID= 70 Loss: 0.376 | Acc: 87.742% (7974/9088)
[Test] Epoch= 45  BatchID= 0 Loss: 0.195 | Acc: 93.750% (120/128)
[Test] Epoch= 45  BatchID= 10 Loss: 0.312 | Acc: 89.915% (1266/1408)
[Test] Epoch= 45  BatchID= 20 Loss: 0.323 | Acc: 89.769% (2413/2688)
[Test] Epoch= 45  BatchID= 30 Loss: 0.389 | Acc: 88.076% (3457/3925)
Saving..
Best accuracy:  88.07643312101911

Epoch: 46
[Train] Epoch= 46  BatchID= 0 Loss: 0.308 | Acc: 87.500% (112/128)
[Train] Epoch= 46  BatchID= 10 Loss: 0.382 | Acc: 87.713% (1235/1408)
[Train] Epoch= 46  BatchID= 20 Loss: 0.386 | Acc: 87.351% (2348/2688)
[Train] Epoch= 46  BatchID= 30 Loss: 0.397 | Acc: 86.996% (3452/3968)
[Train] Epoch= 46  BatchID= 40 Loss: 0.402 | Acc: 86.909% (4561/5248)
[Train] Epoch= 46  BatchID= 50 Loss: 0.402 | Acc: 86.918% (5674/6528)
[Train] Epoch= 46  BatchID= 60 Loss: 0.398 | Acc: 86.898% (6785/7808)
[Train] Epoch= 46  BatchID= 70 Loss: 0.402 | Acc: 86.763% (7885/9088)
[Test] Epoch= 46  BatchID= 0 Loss: 0.409 | Acc: 86.719% (111/128)
[Test] Epoch= 46  BatchID= 10 Loss: 0.427 | Acc: 86.435% (1217/1408)
[Test] Epoch= 46  BatchID= 20 Loss: 0.405 | Acc: 87.277% (2346/2688)
[Test] Epoch= 46  BatchID= 30 Loss: 0.443 | Acc: 86.013% (3376/3925)

Epoch: 47
[Train] Epoch= 47  BatchID= 0 Loss: 0.479 | Acc: 85.156% (109/128)
[Train] Epoch= 47  BatchID= 10 Loss: 0.355 | Acc: 88.139% (1241/1408)
[Train] Epoch= 47  BatchID= 20 Loss: 0.359 | Acc: 88.430% (2377/2688)
[Train] Epoch= 47  BatchID= 30 Loss: 0.366 | Acc: 88.180% (3499/3968)
[Train] Epoch= 47  BatchID= 40 Loss: 0.370 | Acc: 88.205% (4629/5248)
[Train] Epoch= 47  BatchID= 50 Loss: 0.375 | Acc: 87.699% (5725/6528)
[Train] Epoch= 47  BatchID= 60 Loss: 0.378 | Acc: 87.346% (6820/7808)
[Train] Epoch= 47  BatchID= 70 Loss: 0.377 | Acc: 87.346% (7938/9088)
[Test] Epoch= 47  BatchID= 0 Loss: 0.231 | Acc: 92.188% (118/128)
[Test] Epoch= 47  BatchID= 10 Loss: 0.363 | Acc: 89.347% (1258/1408)
[Test] Epoch= 47  BatchID= 20 Loss: 0.348 | Acc: 89.472% (2405/2688)
[Test] Epoch= 47  BatchID= 30 Loss: 0.425 | Acc: 87.032% (3416/3925)

Epoch: 48
[Train] Epoch= 48  BatchID= 0 Loss: 0.280 | Acc: 89.844% (115/128)
[Train] Epoch= 48  BatchID= 10 Loss: 0.354 | Acc: 88.707% (1249/1408)
[Train] Epoch= 48  BatchID= 20 Loss: 0.365 | Acc: 88.244% (2372/2688)
[Train] Epoch= 48  BatchID= 30 Loss: 0.367 | Acc: 88.130% (3497/3968)
[Train] Epoch= 48  BatchID= 40 Loss: 0.363 | Acc: 88.072% (4622/5248)
[Train] Epoch= 48  BatchID= 50 Loss: 0.357 | Acc: 88.327% (5766/6528)
[Train] Epoch= 48  BatchID= 60 Loss: 0.360 | Acc: 88.358% (6899/7808)
[Train] Epoch= 48  BatchID= 70 Loss: 0.369 | Acc: 88.171% (8013/9088)
[Test] Epoch= 48  BatchID= 0 Loss: 0.267 | Acc: 91.406% (117/128)
[Test] Epoch= 48  BatchID= 10 Loss: 0.352 | Acc: 87.926% (1238/1408)
[Test] Epoch= 48  BatchID= 20 Loss: 0.338 | Acc: 89.062% (2394/2688)
[Test] Epoch= 48  BatchID= 30 Loss: 0.390 | Acc: 87.338% (3428/3925)

Epoch: 49
[Train] Epoch= 49  BatchID= 0 Loss: 0.447 | Acc: 85.938% (110/128)
[Train] Epoch= 49  BatchID= 10 Loss: 0.384 | Acc: 88.210% (1242/1408)
[Train] Epoch= 49  BatchID= 20 Loss: 0.368 | Acc: 88.430% (2377/2688)
[Train] Epoch= 49  BatchID= 30 Loss: 0.364 | Acc: 88.407% (3508/3968)
[Train] Epoch= 49  BatchID= 40 Loss: 0.368 | Acc: 88.319% (4635/5248)
[Train] Epoch= 49  BatchID= 50 Loss: 0.372 | Acc: 88.174% (5756/6528)
[Train] Epoch= 49  BatchID= 60 Loss: 0.372 | Acc: 88.204% (6887/7808)
[Train] Epoch= 49  BatchID= 70 Loss: 0.381 | Acc: 87.929% (7991/9088)
[Test] Epoch= 49  BatchID= 0 Loss: 0.184 | Acc: 96.094% (123/128)
[Test] Epoch= 49  BatchID= 10 Loss: 0.355 | Acc: 89.134% (1255/1408)
[Test] Epoch= 49  BatchID= 20 Loss: 0.368 | Acc: 88.616% (2382/2688)
[Test] Epoch= 49  BatchID= 30 Loss: 0.421 | Acc: 86.675% (3402/3925)

Epoch: 50
[Train] Epoch= 50  BatchID= 0 Loss: 0.431 | Acc: 85.938% (110/128)
[Train] Epoch= 50  BatchID= 10 Loss: 0.339 | Acc: 89.276% (1257/1408)
[Train] Epoch= 50  BatchID= 20 Loss: 0.366 | Acc: 88.579% (2381/2688)
[Train] Epoch= 50  BatchID= 30 Loss: 0.360 | Acc: 88.432% (3509/3968)
[Train] Epoch= 50  BatchID= 40 Loss: 0.357 | Acc: 88.567% (4648/5248)
[Train] Epoch= 50  BatchID= 50 Loss: 0.360 | Acc: 88.373% (5769/6528)
[Train] Epoch= 50  BatchID= 60 Loss: 0.364 | Acc: 88.012% (6872/7808)
[Train] Epoch= 50  BatchID= 70 Loss: 0.369 | Acc: 87.808% (7980/9088)
[Test] Epoch= 50  BatchID= 0 Loss: 0.141 | Acc: 97.656% (125/128)
[Test] Epoch= 50  BatchID= 10 Loss: 0.365 | Acc: 88.423% (1245/1408)
[Test] Epoch= 50  BatchID= 20 Loss: 0.348 | Acc: 88.802% (2387/2688)
[Test] Epoch= 50  BatchID= 30 Loss: 0.352 | Acc: 88.739% (3483/3925)
Saving..
Best accuracy:  88.73885350318471

Epoch: 51
[Train] Epoch= 51  BatchID= 0 Loss: 0.474 | Acc: 82.812% (106/128)
[Train] Epoch= 51  BatchID= 10 Loss: 0.374 | Acc: 88.068% (1240/1408)
[Train] Epoch= 51  BatchID= 20 Loss: 0.357 | Acc: 88.616% (2382/2688)
[Train] Epoch= 51  BatchID= 30 Loss: 0.353 | Acc: 88.710% (3520/3968)
[Train] Epoch= 51  BatchID= 40 Loss: 0.345 | Acc: 89.120% (4677/5248)
[Train] Epoch= 51  BatchID= 50 Loss: 0.345 | Acc: 89.047% (5813/6528)
[Train] Epoch= 51  BatchID= 60 Loss: 0.346 | Acc: 88.960% (6946/7808)
[Train] Epoch= 51  BatchID= 70 Loss: 0.348 | Acc: 88.765% (8067/9088)
[Test] Epoch= 51  BatchID= 0 Loss: 0.157 | Acc: 94.531% (121/128)
[Test] Epoch= 51  BatchID= 10 Loss: 0.343 | Acc: 88.778% (1250/1408)
[Test] Epoch= 51  BatchID= 20 Loss: 0.352 | Acc: 88.951% (2391/2688)
[Test] Epoch= 51  BatchID= 30 Loss: 0.367 | Acc: 88.280% (3465/3925)

Epoch: 52
[Train] Epoch= 52  BatchID= 0 Loss: 0.321 | Acc: 89.062% (114/128)
[Train] Epoch= 52  BatchID= 10 Loss: 0.345 | Acc: 88.210% (1242/1408)
[Train] Epoch= 52  BatchID= 20 Loss: 0.324 | Acc: 89.249% (2399/2688)
[Train] Epoch= 52  BatchID= 30 Loss: 0.340 | Acc: 88.836% (3525/3968)
[Train] Epoch= 52  BatchID= 40 Loss: 0.349 | Acc: 88.643% (4652/5248)
[Train] Epoch= 52  BatchID= 50 Loss: 0.351 | Acc: 88.664% (5788/6528)
[Train] Epoch= 52  BatchID= 60 Loss: 0.350 | Acc: 88.614% (6919/7808)
[Train] Epoch= 52  BatchID= 70 Loss: 0.354 | Acc: 88.512% (8044/9088)
[Test] Epoch= 52  BatchID= 0 Loss: 0.237 | Acc: 92.969% (119/128)
[Test] Epoch= 52  BatchID= 10 Loss: 0.452 | Acc: 86.009% (1211/1408)
[Test] Epoch= 52  BatchID= 20 Loss: 0.449 | Acc: 85.789% (2306/2688)
[Test] Epoch= 52  BatchID= 30 Loss: 0.414 | Acc: 86.828% (3408/3925)

Epoch: 53
[Train] Epoch= 53  BatchID= 0 Loss: 0.335 | Acc: 88.281% (113/128)
[Train] Epoch= 53  BatchID= 10 Loss: 0.324 | Acc: 89.560% (1261/1408)
[Train] Epoch= 53  BatchID= 20 Loss: 0.329 | Acc: 89.249% (2399/2688)
[Train] Epoch= 53  BatchID= 30 Loss: 0.332 | Acc: 89.390% (3547/3968)
[Train] Epoch= 53  BatchID= 40 Loss: 0.338 | Acc: 89.043% (4673/5248)
[Train] Epoch= 53  BatchID= 50 Loss: 0.337 | Acc: 89.047% (5813/6528)
[Train] Epoch= 53  BatchID= 60 Loss: 0.338 | Acc: 89.127% (6959/7808)
[Train] Epoch= 53  BatchID= 70 Loss: 0.337 | Acc: 89.140% (8101/9088)
[Test] Epoch= 53  BatchID= 0 Loss: 0.063 | Acc: 98.438% (126/128)
[Test] Epoch= 53  BatchID= 10 Loss: 0.375 | Acc: 88.068% (1240/1408)
[Test] Epoch= 53  BatchID= 20 Loss: 0.357 | Acc: 88.765% (2386/2688)
[Test] Epoch= 53  BatchID= 30 Loss: 0.364 | Acc: 88.382% (3469/3925)

Epoch: 54
[Train] Epoch= 54  BatchID= 0 Loss: 0.262 | Acc: 90.625% (116/128)
[Train] Epoch= 54  BatchID= 10 Loss: 0.347 | Acc: 88.849% (1251/1408)
[Train] Epoch= 54  BatchID= 20 Loss: 0.337 | Acc: 88.914% (2390/2688)
[Train] Epoch= 54  BatchID= 30 Loss: 0.334 | Acc: 88.810% (3524/3968)
[Train] Epoch= 54  BatchID= 40 Loss: 0.338 | Acc: 88.929% (4667/5248)
[Train] Epoch= 54  BatchID= 50 Loss: 0.334 | Acc: 89.062% (5814/6528)
[Train] Epoch= 54  BatchID= 60 Loss: 0.334 | Acc: 89.229% (6967/7808)
[Train] Epoch= 54  BatchID= 70 Loss: 0.340 | Acc: 89.062% (8094/9088)
[Test] Epoch= 54  BatchID= 0 Loss: 0.184 | Acc: 94.531% (121/128)
[Test] Epoch= 54  BatchID= 10 Loss: 0.357 | Acc: 89.418% (1259/1408)
[Test] Epoch= 54  BatchID= 20 Loss: 0.412 | Acc: 87.314% (2347/2688)
[Test] Epoch= 54  BatchID= 30 Loss: 0.404 | Acc: 87.108% (3419/3925)

Epoch: 55
[Train] Epoch= 55  BatchID= 0 Loss: 0.261 | Acc: 95.312% (122/128)
[Train] Epoch= 55  BatchID= 10 Loss: 0.295 | Acc: 89.986% (1267/1408)
[Train] Epoch= 55  BatchID= 20 Loss: 0.307 | Acc: 89.769% (2413/2688)
[Train] Epoch= 55  BatchID= 30 Loss: 0.303 | Acc: 90.247% (3581/3968)
[Train] Epoch= 55  BatchID= 40 Loss: 0.318 | Acc: 89.977% (4722/5248)
[Train] Epoch= 55  BatchID= 50 Loss: 0.322 | Acc: 89.966% (5873/6528)
[Train] Epoch= 55  BatchID= 60 Loss: 0.326 | Acc: 89.626% (6998/7808)
[Train] Epoch= 55  BatchID= 70 Loss: 0.330 | Acc: 89.536% (8137/9088)
[Test] Epoch= 55  BatchID= 0 Loss: 0.164 | Acc: 94.531% (121/128)
[Test] Epoch= 55  BatchID= 10 Loss: 0.313 | Acc: 90.199% (1270/1408)
[Test] Epoch= 55  BatchID= 20 Loss: 0.355 | Acc: 89.062% (2394/2688)
[Test] Epoch= 55  BatchID= 30 Loss: 0.361 | Acc: 88.841% (3487/3925)
Saving..
Best accuracy:  88.8407643312102

Epoch: 56
[Train] Epoch= 56  BatchID= 0 Loss: 0.331 | Acc: 89.844% (115/128)
[Train] Epoch= 56  BatchID= 10 Loss: 0.314 | Acc: 90.199% (1270/1408)
[Train] Epoch= 56  BatchID= 20 Loss: 0.302 | Acc: 90.365% (2429/2688)
[Train] Epoch= 56  BatchID= 30 Loss: 0.300 | Acc: 90.575% (3594/3968)
[Train] Epoch= 56  BatchID= 40 Loss: 0.311 | Acc: 90.034% (4725/5248)
[Train] Epoch= 56  BatchID= 50 Loss: 0.311 | Acc: 90.104% (5882/6528)
[Train] Epoch= 56  BatchID= 60 Loss: 0.312 | Acc: 90.228% (7045/7808)
[Train] Epoch= 56  BatchID= 70 Loss: 0.315 | Acc: 90.108% (8189/9088)
[Test] Epoch= 56  BatchID= 0 Loss: 0.174 | Acc: 93.750% (120/128)
[Test] Epoch= 56  BatchID= 10 Loss: 0.299 | Acc: 90.625% (1276/1408)
[Test] Epoch= 56  BatchID= 20 Loss: 0.373 | Acc: 88.021% (2366/2688)
[Test] Epoch= 56  BatchID= 30 Loss: 0.363 | Acc: 88.408% (3470/3925)

Epoch: 57
[Train] Epoch= 57  BatchID= 0 Loss: 0.303 | Acc: 91.406% (117/128)
[Train] Epoch= 57  BatchID= 10 Loss: 0.304 | Acc: 90.128% (1269/1408)
[Train] Epoch= 57  BatchID= 20 Loss: 0.298 | Acc: 90.476% (2432/2688)
[Train] Epoch= 57  BatchID= 30 Loss: 0.290 | Acc: 90.675% (3598/3968)
[Train] Epoch= 57  BatchID= 40 Loss: 0.287 | Acc: 90.682% (4759/5248)
[Train] Epoch= 57  BatchID= 50 Loss: 0.288 | Acc: 90.778% (5926/6528)
[Train] Epoch= 57  BatchID= 60 Loss: 0.294 | Acc: 90.433% (7061/7808)
[Train] Epoch= 57  BatchID= 70 Loss: 0.296 | Acc: 90.416% (8217/9088)
[Test] Epoch= 57  BatchID= 0 Loss: 0.248 | Acc: 92.188% (118/128)
[Test] Epoch= 57  BatchID= 10 Loss: 0.336 | Acc: 89.631% (1262/1408)
[Test] Epoch= 57  BatchID= 20 Loss: 0.318 | Acc: 90.216% (2425/2688)
[Test] Epoch= 57  BatchID= 30 Loss: 0.352 | Acc: 89.070% (3496/3925)
Saving..
Best accuracy:  89.07006369426752

Epoch: 58
[Train] Epoch= 58  BatchID= 0 Loss: 0.178 | Acc: 94.531% (121/128)
[Train] Epoch= 58  BatchID= 10 Loss: 0.287 | Acc: 90.767% (1278/1408)
[Train] Epoch= 58  BatchID= 20 Loss: 0.288 | Acc: 90.885% (2443/2688)
[Train] Epoch= 58  BatchID= 30 Loss: 0.299 | Acc: 90.348% (3585/3968)
[Train] Epoch= 58  BatchID= 40 Loss: 0.298 | Acc: 90.454% (4747/5248)
[Train] Epoch= 58  BatchID= 50 Loss: 0.298 | Acc: 90.411% (5902/6528)
[Train] Epoch= 58  BatchID= 60 Loss: 0.302 | Acc: 90.394% (7058/7808)
[Train] Epoch= 58  BatchID= 70 Loss: 0.300 | Acc: 90.537% (8228/9088)
[Test] Epoch= 58  BatchID= 0 Loss: 0.195 | Acc: 94.531% (121/128)
[Test] Epoch= 58  BatchID= 10 Loss: 0.320 | Acc: 89.844% (1265/1408)
[Test] Epoch= 58  BatchID= 20 Loss: 0.321 | Acc: 89.658% (2410/2688)
[Test] Epoch= 58  BatchID= 30 Loss: 0.339 | Acc: 89.096% (3497/3925)
Saving..
Best accuracy:  89.09554140127389

Epoch: 59
[Train] Epoch= 59  BatchID= 0 Loss: 0.422 | Acc: 86.719% (111/128)
[Train] Epoch= 59  BatchID= 10 Loss: 0.290 | Acc: 90.767% (1278/1408)
[Train] Epoch= 59  BatchID= 20 Loss: 0.283 | Acc: 91.295% (2454/2688)
[Train] Epoch= 59  BatchID= 30 Loss: 0.285 | Acc: 91.305% (3623/3968)
[Train] Epoch= 59  BatchID= 40 Loss: 0.287 | Acc: 91.139% (4783/5248)
[Train] Epoch= 59  BatchID= 50 Loss: 0.291 | Acc: 90.993% (5940/6528)
[Train] Epoch= 59  BatchID= 60 Loss: 0.299 | Acc: 90.676% (7080/7808)
[Train] Epoch= 59  BatchID= 70 Loss: 0.300 | Acc: 90.592% (8233/9088)
[Test] Epoch= 59  BatchID= 0 Loss: 0.320 | Acc: 92.969% (119/128)
[Test] Epoch= 59  BatchID= 10 Loss: 0.398 | Acc: 88.139% (1241/1408)
[Test] Epoch= 59  BatchID= 20 Loss: 0.390 | Acc: 88.356% (2375/2688)
[Test] Epoch= 59  BatchID= 30 Loss: 0.365 | Acc: 88.917% (3490/3925)

Epoch: 60
[Train] Epoch= 60  BatchID= 0 Loss: 0.359 | Acc: 88.281% (113/128)
[Train] Epoch= 60  BatchID= 10 Loss: 0.308 | Acc: 89.986% (1267/1408)
[Train] Epoch= 60  BatchID= 20 Loss: 0.294 | Acc: 90.476% (2432/2688)
[Train] Epoch= 60  BatchID= 30 Loss: 0.288 | Acc: 90.499% (3591/3968)
[Train] Epoch= 60  BatchID= 40 Loss: 0.290 | Acc: 90.434% (4746/5248)
[Train] Epoch= 60  BatchID= 50 Loss: 0.292 | Acc: 90.242% (5891/6528)
[Train] Epoch= 60  BatchID= 60 Loss: 0.292 | Acc: 90.202% (7043/7808)
[Train] Epoch= 60  BatchID= 70 Loss: 0.290 | Acc: 90.251% (8202/9088)
[Test] Epoch= 60  BatchID= 0 Loss: 0.108 | Acc: 95.312% (122/128)
[Test] Epoch= 60  BatchID= 10 Loss: 0.307 | Acc: 90.341% (1272/1408)
[Test] Epoch= 60  BatchID= 20 Loss: 0.286 | Acc: 90.885% (2443/2688)
[Test] Epoch= 60  BatchID= 30 Loss: 0.355 | Acc: 88.815% (3486/3925)

Epoch: 61
[Train] Epoch= 61  BatchID= 0 Loss: 0.321 | Acc: 90.625% (116/128)
[Train] Epoch= 61  BatchID= 10 Loss: 0.278 | Acc: 91.122% (1283/1408)
[Train] Epoch= 61  BatchID= 20 Loss: 0.283 | Acc: 91.034% (2447/2688)
[Train] Epoch= 61  BatchID= 30 Loss: 0.284 | Acc: 90.953% (3609/3968)
[Train] Epoch= 61  BatchID= 40 Loss: 0.286 | Acc: 90.777% (4764/5248)
[Train] Epoch= 61  BatchID= 50 Loss: 0.288 | Acc: 90.794% (5927/6528)
[Train] Epoch= 61  BatchID= 60 Loss: 0.281 | Acc: 90.996% (7105/7808)
[Train] Epoch= 61  BatchID= 70 Loss: 0.281 | Acc: 90.977% (8268/9088)
[Test] Epoch= 61  BatchID= 0 Loss: 0.167 | Acc: 95.312% (122/128)
[Test] Epoch= 61  BatchID= 10 Loss: 0.288 | Acc: 90.696% (1277/1408)
[Test] Epoch= 61  BatchID= 20 Loss: 0.329 | Acc: 89.881% (2416/2688)
[Test] Epoch= 61  BatchID= 30 Loss: 0.367 | Acc: 88.535% (3475/3925)

Epoch: 62
[Train] Epoch= 62  BatchID= 0 Loss: 0.245 | Acc: 94.531% (121/128)
[Train] Epoch= 62  BatchID= 10 Loss: 0.268 | Acc: 92.401% (1301/1408)
[Train] Epoch= 62  BatchID= 20 Loss: 0.283 | Acc: 91.183% (2451/2688)
[Train] Epoch= 62  BatchID= 30 Loss: 0.280 | Acc: 91.104% (3615/3968)
[Train] Epoch= 62  BatchID= 40 Loss: 0.279 | Acc: 91.235% (4788/5248)
[Train] Epoch= 62  BatchID= 50 Loss: 0.280 | Acc: 91.131% (5949/6528)
[Train] Epoch= 62  BatchID= 60 Loss: 0.275 | Acc: 91.253% (7125/7808)
[Train] Epoch= 62  BatchID= 70 Loss: 0.278 | Acc: 91.164% (8285/9088)
[Test] Epoch= 62  BatchID= 0 Loss: 0.157 | Acc: 96.875% (124/128)
[Test] Epoch= 62  BatchID= 10 Loss: 0.276 | Acc: 92.116% (1297/1408)
[Test] Epoch= 62  BatchID= 20 Loss: 0.319 | Acc: 90.551% (2434/2688)
[Test] Epoch= 62  BatchID= 30 Loss: 0.387 | Acc: 88.433% (3471/3925)

Epoch: 63
[Train] Epoch= 63  BatchID= 0 Loss: 0.237 | Acc: 91.406% (117/128)
[Train] Epoch= 63  BatchID= 10 Loss: 0.255 | Acc: 91.761% (1292/1408)
[Train] Epoch= 63  BatchID= 20 Loss: 0.245 | Acc: 92.299% (2481/2688)
[Train] Epoch= 63  BatchID= 30 Loss: 0.267 | Acc: 91.658% (3637/3968)
[Train] Epoch= 63  BatchID= 40 Loss: 0.270 | Acc: 91.482% (4801/5248)
[Train] Epoch= 63  BatchID= 50 Loss: 0.277 | Acc: 91.253% (5957/6528)
[Train] Epoch= 63  BatchID= 60 Loss: 0.276 | Acc: 91.240% (7124/7808)
[Train] Epoch= 63  BatchID= 70 Loss: 0.271 | Acc: 91.296% (8297/9088)
[Test] Epoch= 63  BatchID= 0 Loss: 0.125 | Acc: 97.656% (125/128)
[Test] Epoch= 63  BatchID= 10 Loss: 0.293 | Acc: 90.980% (1281/1408)
[Test] Epoch= 63  BatchID= 20 Loss: 0.303 | Acc: 91.071% (2448/2688)
[Test] Epoch= 63  BatchID= 30 Loss: 0.333 | Acc: 89.911% (3529/3925)
Saving..
Best accuracy:  89.91082802547771

Epoch: 64
[Train] Epoch= 64  BatchID= 0 Loss: 0.213 | Acc: 93.750% (120/128)
[Train] Epoch= 64  BatchID= 10 Loss: 0.255 | Acc: 91.761% (1292/1408)
[Train] Epoch= 64  BatchID= 20 Loss: 0.263 | Acc: 91.332% (2455/2688)
[Train] Epoch= 64  BatchID= 30 Loss: 0.266 | Acc: 91.633% (3636/3968)
[Train] Epoch= 64  BatchID= 40 Loss: 0.261 | Acc: 91.768% (4816/5248)
[Train] Epoch= 64  BatchID= 50 Loss: 0.262 | Acc: 91.835% (5995/6528)
[Train] Epoch= 64  BatchID= 60 Loss: 0.262 | Acc: 91.893% (7175/7808)
[Train] Epoch= 64  BatchID= 70 Loss: 0.263 | Acc: 91.846% (8347/9088)
[Test] Epoch= 64  BatchID= 0 Loss: 0.112 | Acc: 96.094% (123/128)
[Test] Epoch= 64  BatchID= 10 Loss: 0.307 | Acc: 90.057% (1268/1408)
[Test] Epoch= 64  BatchID= 20 Loss: 0.308 | Acc: 90.625% (2436/2688)
[Test] Epoch= 64  BatchID= 30 Loss: 0.343 | Acc: 89.529% (3514/3925)

Epoch: 65
[Train] Epoch= 65  BatchID= 0 Loss: 0.259 | Acc: 90.625% (116/128)
[Train] Epoch= 65  BatchID= 10 Loss: 0.262 | Acc: 91.122% (1283/1408)
[Train] Epoch= 65  BatchID= 20 Loss: 0.266 | Acc: 90.885% (2443/2688)
[Train] Epoch= 65  BatchID= 30 Loss: 0.263 | Acc: 91.179% (3618/3968)
[Train] Epoch= 65  BatchID= 40 Loss: 0.272 | Acc: 91.063% (4779/5248)
[Train] Epoch= 65  BatchID= 50 Loss: 0.264 | Acc: 91.391% (5966/6528)
[Train] Epoch= 65  BatchID= 60 Loss: 0.267 | Acc: 91.355% (7133/7808)
[Train] Epoch= 65  BatchID= 70 Loss: 0.270 | Acc: 91.296% (8297/9088)
[Test] Epoch= 65  BatchID= 0 Loss: 0.164 | Acc: 95.312% (122/128)
[Test] Epoch= 65  BatchID= 10 Loss: 0.300 | Acc: 90.554% (1275/1408)
[Test] Epoch= 65  BatchID= 20 Loss: 0.290 | Acc: 90.997% (2446/2688)
[Test] Epoch= 65  BatchID= 30 Loss: 0.316 | Acc: 90.166% (3539/3925)
Saving..
Best accuracy:  90.1656050955414

Epoch: 66
[Train] Epoch= 66  BatchID= 0 Loss: 0.320 | Acc: 87.500% (112/128)
[Train] Epoch= 66  BatchID= 10 Loss: 0.255 | Acc: 91.406% (1287/1408)
[Train] Epoch= 66  BatchID= 20 Loss: 0.250 | Acc: 91.741% (2466/2688)
[Train] Epoch= 66  BatchID= 30 Loss: 0.243 | Acc: 92.339% (3664/3968)
[Train] Epoch= 66  BatchID= 40 Loss: 0.242 | Acc: 92.359% (4847/5248)
[Train] Epoch= 66  BatchID= 50 Loss: 0.238 | Acc: 92.540% (6041/6528)
[Train] Epoch= 66  BatchID= 60 Loss: 0.238 | Acc: 92.431% (7217/7808)
[Train] Epoch= 66  BatchID= 70 Loss: 0.244 | Acc: 92.276% (8386/9088)
[Test] Epoch= 66  BatchID= 0 Loss: 0.138 | Acc: 96.094% (123/128)
[Test] Epoch= 66  BatchID= 10 Loss: 0.286 | Acc: 90.909% (1280/1408)
[Test] Epoch= 66  BatchID= 20 Loss: 0.280 | Acc: 91.146% (2450/2688)
[Test] Epoch= 66  BatchID= 30 Loss: 0.329 | Acc: 89.605% (3517/3925)

Epoch: 67
[Train] Epoch= 67  BatchID= 0 Loss: 0.341 | Acc: 87.500% (112/128)
[Train] Epoch= 67  BatchID= 10 Loss: 0.260 | Acc: 91.335% (1286/1408)
[Train] Epoch= 67  BatchID= 20 Loss: 0.256 | Acc: 91.853% (2469/2688)
[Train] Epoch= 67  BatchID= 30 Loss: 0.257 | Acc: 91.961% (3649/3968)
[Train] Epoch= 67  BatchID= 40 Loss: 0.259 | Acc: 91.959% (4826/5248)
[Train] Epoch= 67  BatchID= 50 Loss: 0.263 | Acc: 91.605% (5980/6528)
[Train] Epoch= 67  BatchID= 60 Loss: 0.259 | Acc: 91.829% (7170/7808)
[Train] Epoch= 67  BatchID= 70 Loss: 0.259 | Acc: 91.813% (8344/9088)
[Test] Epoch= 67  BatchID= 0 Loss: 0.168 | Acc: 96.094% (123/128)
[Test] Epoch= 67  BatchID= 10 Loss: 0.273 | Acc: 91.335% (1286/1408)
[Test] Epoch= 67  BatchID= 20 Loss: 0.281 | Acc: 91.220% (2452/2688)
[Test] Epoch= 67  BatchID= 30 Loss: 0.317 | Acc: 89.987% (3532/3925)

Epoch: 68
[Train] Epoch= 68  BatchID= 0 Loss: 0.271 | Acc: 92.188% (118/128)
[Train] Epoch= 68  BatchID= 10 Loss: 0.221 | Acc: 93.253% (1313/1408)
[Train] Epoch= 68  BatchID= 20 Loss: 0.222 | Acc: 92.969% (2499/2688)
[Train] Epoch= 68  BatchID= 30 Loss: 0.221 | Acc: 93.070% (3693/3968)
[Train] Epoch= 68  BatchID= 40 Loss: 0.223 | Acc: 92.931% (4877/5248)
[Train] Epoch= 68  BatchID= 50 Loss: 0.232 | Acc: 92.586% (6044/6528)
[Train] Epoch= 68  BatchID= 60 Loss: 0.233 | Acc: 92.533% (7225/7808)
[Train] Epoch= 68  BatchID= 70 Loss: 0.231 | Acc: 92.672% (8422/9088)
[Test] Epoch= 68  BatchID= 0 Loss: 0.204 | Acc: 93.750% (120/128)
[Test] Epoch= 68  BatchID= 10 Loss: 0.341 | Acc: 89.489% (1260/1408)
[Test] Epoch= 68  BatchID= 20 Loss: 0.320 | Acc: 90.365% (2429/2688)
[Test] Epoch= 68  BatchID= 30 Loss: 0.322 | Acc: 90.293% (3544/3925)
Saving..
Best accuracy:  90.29299363057325

Epoch: 69
[Train] Epoch= 69  BatchID= 0 Loss: 0.191 | Acc: 94.531% (121/128)
[Train] Epoch= 69  BatchID= 10 Loss: 0.230 | Acc: 93.253% (1313/1408)
[Train] Epoch= 69  BatchID= 20 Loss: 0.233 | Acc: 93.341% (2509/2688)
[Train] Epoch= 69  BatchID= 30 Loss: 0.245 | Acc: 92.944% (3688/3968)
[Train] Epoch= 69  BatchID= 40 Loss: 0.242 | Acc: 92.950% (4878/5248)
[Train] Epoch= 69  BatchID= 50 Loss: 0.244 | Acc: 92.907% (6065/6528)
[Train] Epoch= 69  BatchID= 60 Loss: 0.242 | Acc: 92.982% (7260/7808)
[Train] Epoch= 69  BatchID= 70 Loss: 0.237 | Acc: 93.123% (8463/9088)
[Test] Epoch= 69  BatchID= 0 Loss: 0.117 | Acc: 97.656% (125/128)
[Test] Epoch= 69  BatchID= 10 Loss: 0.291 | Acc: 90.767% (1278/1408)
[Test] Epoch= 69  BatchID= 20 Loss: 0.276 | Acc: 91.257% (2453/2688)
[Test] Epoch= 69  BatchID= 30 Loss: 0.301 | Acc: 90.446% (3550/3925)
Saving..
Best accuracy:  90.44585987261146

Epoch: 70
[Train] Epoch= 70  BatchID= 0 Loss: 0.249 | Acc: 92.969% (119/128)
[Train] Epoch= 70  BatchID= 10 Loss: 0.232 | Acc: 93.111% (1311/1408)
[Train] Epoch= 70  BatchID= 20 Loss: 0.220 | Acc: 93.490% (2513/2688)
[Train] Epoch= 70  BatchID= 30 Loss: 0.222 | Acc: 93.296% (3702/3968)
[Train] Epoch= 70  BatchID= 40 Loss: 0.225 | Acc: 93.121% (4887/5248)
[Train] Epoch= 70  BatchID= 50 Loss: 0.225 | Acc: 92.862% (6062/6528)
[Train] Epoch= 70  BatchID= 60 Loss: 0.227 | Acc: 92.764% (7243/7808)
[Train] Epoch= 70  BatchID= 70 Loss: 0.227 | Acc: 92.837% (8437/9088)
[Test] Epoch= 70  BatchID= 0 Loss: 0.119 | Acc: 96.875% (124/128)
[Test] Epoch= 70  BatchID= 10 Loss: 0.281 | Acc: 90.838% (1279/1408)
[Test] Epoch= 70  BatchID= 20 Loss: 0.269 | Acc: 91.295% (2454/2688)
[Test] Epoch= 70  BatchID= 30 Loss: 0.301 | Acc: 90.420% (3549/3925)

Epoch: 71
[Train] Epoch= 71  BatchID= 0 Loss: 0.269 | Acc: 91.406% (117/128)
[Train] Epoch= 71  BatchID= 10 Loss: 0.231 | Acc: 92.401% (1301/1408)
[Train] Epoch= 71  BatchID= 20 Loss: 0.234 | Acc: 92.374% (2483/2688)
[Train] Epoch= 71  BatchID= 30 Loss: 0.239 | Acc: 92.188% (3658/3968)
[Train] Epoch= 71  BatchID= 40 Loss: 0.227 | Acc: 92.740% (4867/5248)
[Train] Epoch= 71  BatchID= 50 Loss: 0.217 | Acc: 92.999% (6071/6528)
[Train] Epoch= 71  BatchID= 60 Loss: 0.218 | Acc: 93.007% (7262/7808)
[Train] Epoch= 71  BatchID= 70 Loss: 0.215 | Acc: 93.090% (8460/9088)
[Test] Epoch= 71  BatchID= 0 Loss: 0.100 | Acc: 96.875% (124/128)
[Test] Epoch= 71  BatchID= 10 Loss: 0.281 | Acc: 91.477% (1288/1408)
[Test] Epoch= 71  BatchID= 20 Loss: 0.269 | Acc: 91.704% (2465/2688)
[Test] Epoch= 71  BatchID= 30 Loss: 0.293 | Acc: 90.981% (3571/3925)
Saving..
Best accuracy:  90.98089171974522

Epoch: 72
[Train] Epoch= 72  BatchID= 0 Loss: 0.200 | Acc: 95.312% (122/128)
[Train] Epoch= 72  BatchID= 10 Loss: 0.204 | Acc: 93.679% (1319/1408)
[Train] Epoch= 72  BatchID= 20 Loss: 0.227 | Acc: 93.006% (2500/2688)
[Train] Epoch= 72  BatchID= 30 Loss: 0.230 | Acc: 92.792% (3682/3968)
[Train] Epoch= 72  BatchID= 40 Loss: 0.223 | Acc: 92.931% (4877/5248)
[Train] Epoch= 72  BatchID= 50 Loss: 0.227 | Acc: 92.862% (6062/6528)
[Train] Epoch= 72  BatchID= 60 Loss: 0.230 | Acc: 92.764% (7243/7808)
[Train] Epoch= 72  BatchID= 70 Loss: 0.229 | Acc: 92.771% (8431/9088)
[Test] Epoch= 72  BatchID= 0 Loss: 0.210 | Acc: 92.969% (119/128)
[Test] Epoch= 72  BatchID= 10 Loss: 0.283 | Acc: 91.193% (1284/1408)
[Test] Epoch= 72  BatchID= 20 Loss: 0.270 | Acc: 91.667% (2464/2688)
[Test] Epoch= 72  BatchID= 30 Loss: 0.302 | Acc: 90.522% (3553/3925)

Epoch: 73
[Train] Epoch= 73  BatchID= 0 Loss: 0.136 | Acc: 94.531% (121/128)
[Train] Epoch= 73  BatchID= 10 Loss: 0.206 | Acc: 93.395% (1315/1408)
[Train] Epoch= 73  BatchID= 20 Loss: 0.220 | Acc: 92.708% (2492/2688)
[Train] Epoch= 73  BatchID= 30 Loss: 0.218 | Acc: 93.019% (3691/3968)
[Train] Epoch= 73  BatchID= 40 Loss: 0.218 | Acc: 92.931% (4877/5248)
[Train] Epoch= 73  BatchID= 50 Loss: 0.218 | Acc: 92.862% (6062/6528)
[Train] Epoch= 73  BatchID= 60 Loss: 0.213 | Acc: 93.110% (7270/7808)
[Train] Epoch= 73  BatchID= 70 Loss: 0.215 | Acc: 93.057% (8457/9088)
[Test] Epoch= 73  BatchID= 0 Loss: 0.155 | Acc: 95.312% (122/128)
[Test] Epoch= 73  BatchID= 10 Loss: 0.310 | Acc: 90.625% (1276/1408)
[Test] Epoch= 73  BatchID= 20 Loss: 0.286 | Acc: 91.220% (2452/2688)
[Test] Epoch= 73  BatchID= 30 Loss: 0.297 | Acc: 90.752% (3562/3925)

Epoch: 74
[Train] Epoch= 74  BatchID= 0 Loss: 0.223 | Acc: 95.312% (122/128)
[Train] Epoch= 74  BatchID= 10 Loss: 0.226 | Acc: 93.253% (1313/1408)
[Train] Epoch= 74  BatchID= 20 Loss: 0.210 | Acc: 93.713% (2519/2688)
[Train] Epoch= 74  BatchID= 30 Loss: 0.214 | Acc: 93.372% (3705/3968)
[Train] Epoch= 74  BatchID= 40 Loss: 0.212 | Acc: 93.369% (4900/5248)
[Train] Epoch= 74  BatchID= 50 Loss: 0.217 | Acc: 93.199% (6084/6528)
[Train] Epoch= 74  BatchID= 60 Loss: 0.217 | Acc: 93.174% (7275/7808)
[Train] Epoch= 74  BatchID= 70 Loss: 0.218 | Acc: 93.123% (8463/9088)
[Test] Epoch= 74  BatchID= 0 Loss: 0.165 | Acc: 92.969% (119/128)
[Test] Epoch= 74  BatchID= 10 Loss: 0.325 | Acc: 90.270% (1271/1408)
[Test] Epoch= 74  BatchID= 20 Loss: 0.277 | Acc: 91.853% (2469/2688)
[Test] Epoch= 74  BatchID= 30 Loss: 0.306 | Acc: 90.854% (3566/3925)

Epoch: 75
[Train] Epoch= 75  BatchID= 0 Loss: 0.192 | Acc: 94.531% (121/128)
[Train] Epoch= 75  BatchID= 10 Loss: 0.188 | Acc: 93.821% (1321/1408)
[Train] Epoch= 75  BatchID= 20 Loss: 0.200 | Acc: 93.266% (2507/2688)
[Train] Epoch= 75  BatchID= 30 Loss: 0.198 | Acc: 93.372% (3705/3968)
[Train] Epoch= 75  BatchID= 40 Loss: 0.198 | Acc: 93.559% (4910/5248)
[Train] Epoch= 75  BatchID= 50 Loss: 0.198 | Acc: 93.612% (6111/6528)
[Train] Epoch= 75  BatchID= 60 Loss: 0.202 | Acc: 93.622% (7310/7808)
[Train] Epoch= 75  BatchID= 70 Loss: 0.201 | Acc: 93.673% (8513/9088)
[Test] Epoch= 75  BatchID= 0 Loss: 0.161 | Acc: 95.312% (122/128)
[Test] Epoch= 75  BatchID= 10 Loss: 0.307 | Acc: 90.980% (1281/1408)
[Test] Epoch= 75  BatchID= 20 Loss: 0.273 | Acc: 91.890% (2470/2688)
[Test] Epoch= 75  BatchID= 30 Loss: 0.285 | Acc: 91.363% (3586/3925)
Saving..
Best accuracy:  91.36305732484077

Epoch: 76
[Train] Epoch= 76  BatchID= 0 Loss: 0.179 | Acc: 96.094% (123/128)
[Train] Epoch= 76  BatchID= 10 Loss: 0.189 | Acc: 94.105% (1325/1408)
[Train] Epoch= 76  BatchID= 20 Loss: 0.195 | Acc: 94.010% (2527/2688)
[Train] Epoch= 76  BatchID= 30 Loss: 0.200 | Acc: 93.725% (3719/3968)
[Train] Epoch= 76  BatchID= 40 Loss: 0.201 | Acc: 93.674% (4916/5248)
[Train] Epoch= 76  BatchID= 50 Loss: 0.203 | Acc: 93.612% (6111/6528)
[Train] Epoch= 76  BatchID= 60 Loss: 0.198 | Acc: 93.878% (7330/7808)
[Train] Epoch= 76  BatchID= 70 Loss: 0.198 | Acc: 93.970% (8540/9088)
[Test] Epoch= 76  BatchID= 0 Loss: 0.184 | Acc: 92.969% (119/128)
[Test] Epoch= 76  BatchID= 10 Loss: 0.308 | Acc: 90.128% (1269/1408)
[Test] Epoch= 76  BatchID= 20 Loss: 0.270 | Acc: 91.518% (2460/2688)
[Test] Epoch= 76  BatchID= 30 Loss: 0.297 | Acc: 90.803% (3564/3925)

Epoch: 77
[Train] Epoch= 77  BatchID= 0 Loss: 0.112 | Acc: 96.094% (123/128)
[Train] Epoch= 77  BatchID= 10 Loss: 0.207 | Acc: 93.679% (1319/1408)
[Train] Epoch= 77  BatchID= 20 Loss: 0.216 | Acc: 93.118% (2503/2688)
[Train] Epoch= 77  BatchID= 30 Loss: 0.211 | Acc: 93.221% (3699/3968)
[Train] Epoch= 77  BatchID= 40 Loss: 0.209 | Acc: 93.331% (4898/5248)
[Train] Epoch= 77  BatchID= 50 Loss: 0.207 | Acc: 93.428% (6099/6528)
[Train] Epoch= 77  BatchID= 60 Loss: 0.208 | Acc: 93.430% (7295/7808)
[Train] Epoch= 77  BatchID= 70 Loss: 0.208 | Acc: 93.508% (8498/9088)
[Test] Epoch= 77  BatchID= 0 Loss: 0.134 | Acc: 96.875% (124/128)
[Test] Epoch= 77  BatchID= 10 Loss: 0.297 | Acc: 91.122% (1283/1408)
[Test] Epoch= 77  BatchID= 20 Loss: 0.281 | Acc: 91.592% (2462/2688)
[Test] Epoch= 77  BatchID= 30 Loss: 0.289 | Acc: 91.159% (3578/3925)

Epoch: 78
[Train] Epoch= 78  BatchID= 0 Loss: 0.186 | Acc: 93.750% (120/128)
[Train] Epoch= 78  BatchID= 10 Loss: 0.166 | Acc: 94.886% (1336/1408)
[Train] Epoch= 78  BatchID= 20 Loss: 0.174 | Acc: 94.792% (2548/2688)
[Train] Epoch= 78  BatchID= 30 Loss: 0.179 | Acc: 94.506% (3750/3968)
[Train] Epoch= 78  BatchID= 40 Loss: 0.186 | Acc: 94.264% (4947/5248)
[Train] Epoch= 78  BatchID= 50 Loss: 0.187 | Acc: 94.148% (6146/6528)
[Train] Epoch= 78  BatchID= 60 Loss: 0.186 | Acc: 94.249% (7359/7808)
[Train] Epoch= 78  BatchID= 70 Loss: 0.186 | Acc: 94.256% (8566/9088)
[Test] Epoch= 78  BatchID= 0 Loss: 0.116 | Acc: 96.875% (124/128)
[Test] Epoch= 78  BatchID= 10 Loss: 0.293 | Acc: 91.193% (1284/1408)
[Test] Epoch= 78  BatchID= 20 Loss: 0.270 | Acc: 91.890% (2470/2688)
[Test] Epoch= 78  BatchID= 30 Loss: 0.293 | Acc: 90.955% (3570/3925)

Epoch: 79
[Train] Epoch= 79  BatchID= 0 Loss: 0.196 | Acc: 95.312% (122/128)
[Train] Epoch= 79  BatchID= 10 Loss: 0.162 | Acc: 95.384% (1343/1408)
[Train] Epoch= 79  BatchID= 20 Loss: 0.174 | Acc: 94.866% (2550/2688)
[Train] Epoch= 79  BatchID= 30 Loss: 0.172 | Acc: 94.834% (3763/3968)
[Train] Epoch= 79  BatchID= 40 Loss: 0.176 | Acc: 94.665% (4968/5248)
[Train] Epoch= 79  BatchID= 50 Loss: 0.177 | Acc: 94.638% (6178/6528)
[Train] Epoch= 79  BatchID= 60 Loss: 0.179 | Acc: 94.416% (7372/7808)
[Train] Epoch= 79  BatchID= 70 Loss: 0.176 | Acc: 94.509% (8589/9088)
[Test] Epoch= 79  BatchID= 0 Loss: 0.134 | Acc: 95.312% (122/128)
[Test] Epoch= 79  BatchID= 10 Loss: 0.298 | Acc: 90.909% (1280/1408)
[Test] Epoch= 79  BatchID= 20 Loss: 0.268 | Acc: 91.667% (2464/2688)
[Test] Epoch= 79  BatchID= 30 Loss: 0.287 | Acc: 90.879% (3567/3925)

Epoch: 80
[Train] Epoch= 80  BatchID= 0 Loss: 0.265 | Acc: 92.188% (118/128)
[Train] Epoch= 80  BatchID= 10 Loss: 0.190 | Acc: 94.247% (1327/1408)
[Train] Epoch= 80  BatchID= 20 Loss: 0.192 | Acc: 94.159% (2531/2688)
[Train] Epoch= 80  BatchID= 30 Loss: 0.187 | Acc: 94.178% (3737/3968)
[Train] Epoch= 80  BatchID= 40 Loss: 0.186 | Acc: 94.303% (4949/5248)
[Train] Epoch= 80  BatchID= 50 Loss: 0.187 | Acc: 94.271% (6154/6528)
[Train] Epoch= 80  BatchID= 60 Loss: 0.185 | Acc: 94.326% (7365/7808)
[Train] Epoch= 80  BatchID= 70 Loss: 0.184 | Acc: 94.289% (8569/9088)
[Test] Epoch= 80  BatchID= 0 Loss: 0.099 | Acc: 98.438% (126/128)
[Test] Epoch= 80  BatchID= 10 Loss: 0.265 | Acc: 92.401% (1301/1408)
[Test] Epoch= 80  BatchID= 20 Loss: 0.259 | Acc: 92.336% (2482/2688)
[Test] Epoch= 80  BatchID= 30 Loss: 0.283 | Acc: 91.261% (3582/3925)

Epoch: 81
[Train] Epoch= 81  BatchID= 0 Loss: 0.129 | Acc: 97.656% (125/128)
[Train] Epoch= 81  BatchID= 10 Loss: 0.154 | Acc: 95.028% (1338/1408)
[Train] Epoch= 81  BatchID= 20 Loss: 0.179 | Acc: 94.085% (2529/2688)
[Train] Epoch= 81  BatchID= 30 Loss: 0.179 | Acc: 94.153% (3736/3968)
[Train] Epoch= 81  BatchID= 40 Loss: 0.184 | Acc: 93.883% (4927/5248)
[Train] Epoch= 81  BatchID= 50 Loss: 0.182 | Acc: 94.010% (6137/6528)
[Train] Epoch= 81  BatchID= 60 Loss: 0.184 | Acc: 94.006% (7340/7808)
[Train] Epoch= 81  BatchID= 70 Loss: 0.183 | Acc: 94.036% (8546/9088)
[Test] Epoch= 81  BatchID= 0 Loss: 0.124 | Acc: 96.094% (123/128)
[Test] Epoch= 81  BatchID= 10 Loss: 0.265 | Acc: 91.761% (1292/1408)
[Test] Epoch= 81  BatchID= 20 Loss: 0.253 | Acc: 92.299% (2481/2688)
[Test] Epoch= 81  BatchID= 30 Loss: 0.281 | Acc: 91.236% (3581/3925)

Epoch: 82
[Train] Epoch= 82  BatchID= 0 Loss: 0.169 | Acc: 94.531% (121/128)
[Train] Epoch= 82  BatchID= 10 Loss: 0.195 | Acc: 93.963% (1323/1408)
[Train] Epoch= 82  BatchID= 20 Loss: 0.182 | Acc: 94.085% (2529/2688)
[Train] Epoch= 82  BatchID= 30 Loss: 0.176 | Acc: 94.531% (3751/3968)
[Train] Epoch= 82  BatchID= 40 Loss: 0.177 | Acc: 94.646% (4967/5248)
[Train] Epoch= 82  BatchID= 50 Loss: 0.183 | Acc: 94.424% (6164/6528)
[Train] Epoch= 82  BatchID= 60 Loss: 0.184 | Acc: 94.288% (7362/7808)
[Train] Epoch= 82  BatchID= 70 Loss: 0.186 | Acc: 94.256% (8566/9088)
[Test] Epoch= 82  BatchID= 0 Loss: 0.102 | Acc: 96.875% (124/128)
[Test] Epoch= 82  BatchID= 10 Loss: 0.269 | Acc: 91.406% (1287/1408)
[Test] Epoch= 82  BatchID= 20 Loss: 0.255 | Acc: 91.815% (2468/2688)
[Test] Epoch= 82  BatchID= 30 Loss: 0.282 | Acc: 90.854% (3566/3925)

Epoch: 83
[Train] Epoch= 83  BatchID= 0 Loss: 0.166 | Acc: 95.312% (122/128)
[Train] Epoch= 83  BatchID= 10 Loss: 0.193 | Acc: 94.389% (1329/1408)
[Train] Epoch= 83  BatchID= 20 Loss: 0.181 | Acc: 94.680% (2545/2688)
[Train] Epoch= 83  BatchID= 30 Loss: 0.179 | Acc: 94.556% (3752/3968)
[Train] Epoch= 83  BatchID= 40 Loss: 0.184 | Acc: 94.417% (4955/5248)
[Train] Epoch= 83  BatchID= 50 Loss: 0.187 | Acc: 94.378% (6161/6528)
[Train] Epoch= 83  BatchID= 60 Loss: 0.185 | Acc: 94.518% (7380/7808)
[Train] Epoch= 83  BatchID= 70 Loss: 0.186 | Acc: 94.454% (8584/9088)
[Test] Epoch= 83  BatchID= 0 Loss: 0.105 | Acc: 97.656% (125/128)
[Test] Epoch= 83  BatchID= 10 Loss: 0.283 | Acc: 91.619% (1290/1408)
[Test] Epoch= 83  BatchID= 20 Loss: 0.267 | Acc: 92.150% (2477/2688)
[Test] Epoch= 83  BatchID= 30 Loss: 0.284 | Acc: 91.210% (3580/3925)

Epoch: 84
[Train] Epoch= 84  BatchID= 0 Loss: 0.164 | Acc: 95.312% (122/128)
[Train] Epoch= 84  BatchID= 10 Loss: 0.189 | Acc: 94.389% (1329/1408)
[Train] Epoch= 84  BatchID= 20 Loss: 0.189 | Acc: 94.122% (2530/2688)
[Train] Epoch= 84  BatchID= 30 Loss: 0.183 | Acc: 94.229% (3739/3968)
[Train] Epoch= 84  BatchID= 40 Loss: 0.187 | Acc: 94.207% (4944/5248)
[Train] Epoch= 84  BatchID= 50 Loss: 0.186 | Acc: 94.256% (6153/6528)
[Train] Epoch= 84  BatchID= 60 Loss: 0.183 | Acc: 94.301% (7363/7808)
[Train] Epoch= 84  BatchID= 70 Loss: 0.184 | Acc: 94.267% (8567/9088)
[Test] Epoch= 84  BatchID= 0 Loss: 0.096 | Acc: 96.875% (124/128)
[Test] Epoch= 84  BatchID= 10 Loss: 0.279 | Acc: 90.980% (1281/1408)
[Test] Epoch= 84  BatchID= 20 Loss: 0.262 | Acc: 91.815% (2468/2688)
[Test] Epoch= 84  BatchID= 30 Loss: 0.283 | Acc: 90.981% (3571/3925)

Epoch: 85
[Train] Epoch= 85  BatchID= 0 Loss: 0.178 | Acc: 94.531% (121/128)
[Train] Epoch= 85  BatchID= 10 Loss: 0.170 | Acc: 94.673% (1333/1408)
[Train] Epoch= 85  BatchID= 20 Loss: 0.181 | Acc: 94.382% (2537/2688)
[Train] Epoch= 85  BatchID= 30 Loss: 0.186 | Acc: 94.330% (3743/3968)
[Train] Epoch= 85  BatchID= 40 Loss: 0.179 | Acc: 94.684% (4969/5248)
[Train] Epoch= 85  BatchID= 50 Loss: 0.180 | Acc: 94.730% (6184/6528)
[Train] Epoch= 85  BatchID= 60 Loss: 0.183 | Acc: 94.672% (7392/7808)
[Train] Epoch= 85  BatchID= 70 Loss: 0.185 | Acc: 94.553% (8593/9088)
[Test] Epoch= 85  BatchID= 0 Loss: 0.110 | Acc: 96.875% (124/128)
[Test] Epoch= 85  BatchID= 10 Loss: 0.283 | Acc: 91.477% (1288/1408)
[Test] Epoch= 85  BatchID= 20 Loss: 0.257 | Acc: 92.225% (2479/2688)
[Test] Epoch= 85  BatchID= 30 Loss: 0.286 | Acc: 91.134% (3577/3925)

Epoch: 86
[Train] Epoch= 86  BatchID= 0 Loss: 0.180 | Acc: 95.312% (122/128)
[Train] Epoch= 86  BatchID= 10 Loss: 0.179 | Acc: 94.815% (1335/1408)
[Train] Epoch= 86  BatchID= 20 Loss: 0.188 | Acc: 94.531% (2541/2688)
[Train] Epoch= 86  BatchID= 30 Loss: 0.176 | Acc: 94.733% (3759/3968)
[Train] Epoch= 86  BatchID= 40 Loss: 0.173 | Acc: 94.855% (4978/5248)
[Train] Epoch= 86  BatchID= 50 Loss: 0.170 | Acc: 94.945% (6198/6528)
[Train] Epoch= 86  BatchID= 60 Loss: 0.178 | Acc: 94.544% (7382/7808)
[Train] Epoch= 86  BatchID= 70 Loss: 0.179 | Acc: 94.465% (8585/9088)
[Test] Epoch= 86  BatchID= 0 Loss: 0.111 | Acc: 95.312% (122/128)
[Test] Epoch= 86  BatchID= 10 Loss: 0.281 | Acc: 90.696% (1277/1408)
[Test] Epoch= 86  BatchID= 20 Loss: 0.263 | Acc: 91.555% (2461/2688)
[Test] Epoch= 86  BatchID= 30 Loss: 0.282 | Acc: 90.904% (3568/3925)

Epoch: 87
[Train] Epoch= 87  BatchID= 0 Loss: 0.171 | Acc: 93.750% (120/128)
[Train] Epoch= 87  BatchID= 10 Loss: 0.150 | Acc: 95.384% (1343/1408)
[Train] Epoch= 87  BatchID= 20 Loss: 0.174 | Acc: 94.792% (2548/2688)
[Train] Epoch= 87  BatchID= 30 Loss: 0.179 | Acc: 94.405% (3746/3968)
[Train] Epoch= 87  BatchID= 40 Loss: 0.177 | Acc: 94.417% (4955/5248)
[Train] Epoch= 87  BatchID= 50 Loss: 0.177 | Acc: 94.409% (6163/6528)
[Train] Epoch= 87  BatchID= 60 Loss: 0.177 | Acc: 94.531% (7381/7808)
[Train] Epoch= 87  BatchID= 70 Loss: 0.175 | Acc: 94.487% (8587/9088)
[Test] Epoch= 87  BatchID= 0 Loss: 0.119 | Acc: 96.875% (124/128)
[Test] Epoch= 87  BatchID= 10 Loss: 0.309 | Acc: 90.625% (1276/1408)
[Test] Epoch= 87  BatchID= 20 Loss: 0.265 | Acc: 91.853% (2469/2688)
[Test] Epoch= 87  BatchID= 30 Loss: 0.297 | Acc: 90.854% (3566/3925)

Epoch: 88
[Train] Epoch= 88  BatchID= 0 Loss: 0.147 | Acc: 94.531% (121/128)
[Train] Epoch= 88  BatchID= 10 Loss: 0.172 | Acc: 94.815% (1335/1408)
[Train] Epoch= 88  BatchID= 20 Loss: 0.163 | Acc: 95.238% (2560/2688)
[Train] Epoch= 88  BatchID= 30 Loss: 0.167 | Acc: 95.136% (3775/3968)
[Train] Epoch= 88  BatchID= 40 Loss: 0.168 | Acc: 94.931% (4982/5248)
[Train] Epoch= 88  BatchID= 50 Loss: 0.170 | Acc: 94.715% (6183/6528)
[Train] Epoch= 88  BatchID= 60 Loss: 0.172 | Acc: 94.659% (7391/7808)
[Train] Epoch= 88  BatchID= 70 Loss: 0.173 | Acc: 94.619% (8599/9088)
[Test] Epoch= 88  BatchID= 0 Loss: 0.103 | Acc: 97.656% (125/128)
[Test] Epoch= 88  BatchID= 10 Loss: 0.284 | Acc: 91.406% (1287/1408)
[Test] Epoch= 88  BatchID= 20 Loss: 0.264 | Acc: 92.150% (2477/2688)
[Test] Epoch= 88  BatchID= 30 Loss: 0.284 | Acc: 91.261% (3582/3925)

Epoch: 89
[Train] Epoch= 89  BatchID= 0 Loss: 0.233 | Acc: 92.188% (118/128)
[Train] Epoch= 89  BatchID= 10 Loss: 0.191 | Acc: 93.750% (1320/1408)
[Train] Epoch= 89  BatchID= 20 Loss: 0.179 | Acc: 94.457% (2539/2688)
[Train] Epoch= 89  BatchID= 30 Loss: 0.181 | Acc: 94.254% (3740/3968)
[Train] Epoch= 89  BatchID= 40 Loss: 0.176 | Acc: 94.436% (4956/5248)
[Train] Epoch= 89  BatchID= 50 Loss: 0.172 | Acc: 94.562% (6173/6528)
[Train] Epoch= 89  BatchID= 60 Loss: 0.171 | Acc: 94.608% (7387/7808)
[Train] Epoch= 89  BatchID= 70 Loss: 0.170 | Acc: 94.531% (8591/9088)
[Test] Epoch= 89  BatchID= 0 Loss: 0.111 | Acc: 98.438% (126/128)
[Test] Epoch= 89  BatchID= 10 Loss: 0.273 | Acc: 91.406% (1287/1408)
[Test] Epoch= 89  BatchID= 20 Loss: 0.255 | Acc: 92.113% (2476/2688)
[Test] Epoch= 89  BatchID= 30 Loss: 0.286 | Acc: 91.006% (3572/3925)
