==> Preparing data..
==> Building model..
DataParallel(
  (module): ResNet(
    (conv1): InputConv2dLSQ(
      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): LinearLSQ(
      in_features=512, out_features=10, bias=True
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=True)
    )
  )
)
==> Initializing from checkpoint..

Epoch: 0
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
[Train] Epoch= 0  BatchID= 0 Loss: 0.223 | Acc: 91.406% (117/128)
[Train] Epoch= 0  BatchID= 10 Loss: 0.675 | Acc: 82.386% (1160/1408)
[Train] Epoch= 0  BatchID= 20 Loss: 0.727 | Acc: 78.423% (2108/2688)
[Train] Epoch= 0  BatchID= 30 Loss: 0.726 | Acc: 77.873% (3090/3968)
[Train] Epoch= 0  BatchID= 40 Loss: 0.714 | Acc: 77.763% (4081/5248)
[Train] Epoch= 0  BatchID= 50 Loss: 0.689 | Acc: 78.339% (5114/6528)
[Train] Epoch= 0  BatchID= 60 Loss: 0.672 | Acc: 78.829% (6155/7808)
[Train] Epoch= 0  BatchID= 70 Loss: 0.660 | Acc: 79.104% (7189/9088)
[Test] Epoch= 0  BatchID= 0 Loss: 0.230 | Acc: 93.750% (120/128)
[Test] Epoch= 0  BatchID= 10 Loss: 0.661 | Acc: 81.037% (1141/1408)
[Test] Epoch= 0  BatchID= 20 Loss: 0.655 | Acc: 80.283% (2158/2688)
[Test] Epoch= 0  BatchID= 30 Loss: 0.614 | Acc: 80.994% (3179/3925)
Saving..
Best accuracy:  80.99363057324841

Epoch: 1
[Train] Epoch= 1  BatchID= 0 Loss: 0.526 | Acc: 82.031% (105/128)
[Train] Epoch= 1  BatchID= 10 Loss: 0.513 | Acc: 82.741% (1165/1408)
[Train] Epoch= 1  BatchID= 20 Loss: 0.534 | Acc: 82.031% (2205/2688)
[Train] Epoch= 1  BatchID= 30 Loss: 0.533 | Acc: 82.208% (3262/3968)
[Train] Epoch= 1  BatchID= 40 Loss: 0.541 | Acc: 82.050% (4306/5248)
[Train] Epoch= 1  BatchID= 50 Loss: 0.546 | Acc: 82.292% (5372/6528)
[Train] Epoch= 1  BatchID= 60 Loss: 0.540 | Acc: 82.531% (6444/7808)
[Train] Epoch= 1  BatchID= 70 Loss: 0.544 | Acc: 82.394% (7488/9088)
[Test] Epoch= 1  BatchID= 0 Loss: 0.278 | Acc: 89.062% (114/128)
[Test] Epoch= 1  BatchID= 10 Loss: 0.709 | Acc: 77.770% (1095/1408)
[Test] Epoch= 1  BatchID= 20 Loss: 0.720 | Acc: 77.121% (2073/2688)
[Test] Epoch= 1  BatchID= 30 Loss: 0.831 | Acc: 73.197% (2873/3925)

Epoch: 2
[Train] Epoch= 2  BatchID= 0 Loss: 0.630 | Acc: 75.000% (96/128)
[Train] Epoch= 2  BatchID= 10 Loss: 0.495 | Acc: 84.233% (1186/1408)
[Train] Epoch= 2  BatchID= 20 Loss: 0.516 | Acc: 83.371% (2241/2688)
[Train] Epoch= 2  BatchID= 30 Loss: 0.542 | Acc: 82.712% (3282/3968)
[Train] Epoch= 2  BatchID= 40 Loss: 0.544 | Acc: 82.489% (4329/5248)
[Train] Epoch= 2  BatchID= 50 Loss: 0.551 | Acc: 82.200% (5366/6528)
[Train] Epoch= 2  BatchID= 60 Loss: 0.555 | Acc: 82.070% (6408/7808)
[Train] Epoch= 2  BatchID= 70 Loss: 0.565 | Acc: 81.635% (7419/9088)
[Test] Epoch= 2  BatchID= 0 Loss: 0.325 | Acc: 89.062% (114/128)
[Test] Epoch= 2  BatchID= 10 Loss: 0.802 | Acc: 75.071% (1057/1408)
[Test] Epoch= 2  BatchID= 20 Loss: 0.884 | Acc: 73.251% (1969/2688)
[Test] Epoch= 2  BatchID= 30 Loss: 0.883 | Acc: 74.038% (2906/3925)

Epoch: 3
[Train] Epoch= 3  BatchID= 0 Loss: 0.627 | Acc: 77.344% (99/128)
[Train] Epoch= 3  BatchID= 10 Loss: 0.596 | Acc: 80.611% (1135/1408)
[Train] Epoch= 3  BatchID= 20 Loss: 0.584 | Acc: 81.138% (2181/2688)
[Train] Epoch= 3  BatchID= 30 Loss: 0.588 | Acc: 80.922% (3211/3968)
[Train] Epoch= 3  BatchID= 40 Loss: 0.587 | Acc: 81.117% (4257/5248)
[Train] Epoch= 3  BatchID= 50 Loss: 0.594 | Acc: 81.127% (5296/6528)
[Train] Epoch= 3  BatchID= 60 Loss: 0.599 | Acc: 80.943% (6320/7808)
[Train] Epoch= 3  BatchID= 70 Loss: 0.603 | Acc: 80.766% (7340/9088)
[Test] Epoch= 3  BatchID= 0 Loss: 0.199 | Acc: 91.406% (117/128)
[Test] Epoch= 3  BatchID= 10 Loss: 0.433 | Acc: 86.648% (1220/1408)
[Test] Epoch= 3  BatchID= 20 Loss: 0.573 | Acc: 81.920% (2202/2688)
[Test] Epoch= 3  BatchID= 30 Loss: 0.671 | Acc: 78.624% (3086/3925)

Epoch: 4
[Train] Epoch= 4  BatchID= 0 Loss: 0.530 | Acc: 80.469% (103/128)
[Train] Epoch= 4  BatchID= 10 Loss: 0.564 | Acc: 81.676% (1150/1408)
[Train] Epoch= 4  BatchID= 20 Loss: 0.554 | Acc: 81.920% (2202/2688)
[Train] Epoch= 4  BatchID= 30 Loss: 0.580 | Acc: 81.124% (3219/3968)
[Train] Epoch= 4  BatchID= 40 Loss: 0.589 | Acc: 80.755% (4238/5248)
[Train] Epoch= 4  BatchID= 50 Loss: 0.591 | Acc: 80.668% (5266/6528)
[Train] Epoch= 4  BatchID= 60 Loss: 0.595 | Acc: 80.507% (6286/7808)
[Train] Epoch= 4  BatchID= 70 Loss: 0.593 | Acc: 80.502% (7316/9088)
[Test] Epoch= 4  BatchID= 0 Loss: 0.353 | Acc: 89.844% (115/128)
[Test] Epoch= 4  BatchID= 10 Loss: 0.538 | Acc: 82.244% (1158/1408)
[Test] Epoch= 4  BatchID= 20 Loss: 0.568 | Acc: 82.068% (2206/2688)
[Test] Epoch= 4  BatchID= 30 Loss: 0.595 | Acc: 81.834% (3212/3925)
Saving..
Best accuracy:  81.8343949044586

Epoch: 5
[Train] Epoch= 5  BatchID= 0 Loss: 0.709 | Acc: 78.125% (100/128)
[Train] Epoch= 5  BatchID= 10 Loss: 0.602 | Acc: 79.972% (1126/1408)
[Train] Epoch= 5  BatchID= 20 Loss: 0.594 | Acc: 80.766% (2171/2688)
[Train] Epoch= 5  BatchID= 30 Loss: 0.589 | Acc: 80.544% (3196/3968)
[Train] Epoch= 5  BatchID= 40 Loss: 0.593 | Acc: 80.545% (4227/5248)
[Train] Epoch= 5  BatchID= 50 Loss: 0.593 | Acc: 80.699% (5268/6528)
[Train] Epoch= 5  BatchID= 60 Loss: 0.599 | Acc: 80.674% (6299/7808)
[Train] Epoch= 5  BatchID= 70 Loss: 0.602 | Acc: 80.480% (7314/9088)
[Test] Epoch= 5  BatchID= 0 Loss: 0.183 | Acc: 94.531% (121/128)
[Test] Epoch= 5  BatchID= 10 Loss: 0.460 | Acc: 84.446% (1189/1408)
[Test] Epoch= 5  BatchID= 20 Loss: 0.507 | Acc: 83.891% (2255/2688)
[Test] Epoch= 5  BatchID= 30 Loss: 0.525 | Acc: 83.439% (3275/3925)
Saving..
Best accuracy:  83.43949044585987

Epoch: 6
[Train] Epoch= 6  BatchID= 0 Loss: 0.595 | Acc: 81.250% (104/128)
[Train] Epoch= 6  BatchID= 10 Loss: 0.553 | Acc: 81.250% (1144/1408)
[Train] Epoch= 6  BatchID= 20 Loss: 0.567 | Acc: 81.176% (2182/2688)
[Train] Epoch= 6  BatchID= 30 Loss: 0.593 | Acc: 80.192% (3182/3968)
[Train] Epoch= 6  BatchID= 40 Loss: 0.590 | Acc: 80.545% (4227/5248)
[Train] Epoch= 6  BatchID= 50 Loss: 0.596 | Acc: 80.653% (5265/6528)
[Train] Epoch= 6  BatchID= 60 Loss: 0.597 | Acc: 80.789% (6308/7808)
[Train] Epoch= 6  BatchID= 70 Loss: 0.606 | Acc: 80.370% (7304/9088)
[Test] Epoch= 6  BatchID= 0 Loss: 0.253 | Acc: 88.281% (113/128)
[Test] Epoch= 6  BatchID= 10 Loss: 0.434 | Acc: 85.227% (1200/1408)
[Test] Epoch= 6  BatchID= 20 Loss: 0.492 | Acc: 84.003% (2258/2688)
[Test] Epoch= 6  BatchID= 30 Loss: 0.628 | Acc: 79.847% (3134/3925)

Epoch: 7
[Train] Epoch= 7  BatchID= 0 Loss: 0.671 | Acc: 77.344% (99/128)
[Train] Epoch= 7  BatchID= 10 Loss: 0.574 | Acc: 82.102% (1156/1408)
[Train] Epoch= 7  BatchID= 20 Loss: 0.582 | Acc: 81.585% (2193/2688)
[Train] Epoch= 7  BatchID= 30 Loss: 0.581 | Acc: 81.376% (3229/3968)
[Train] Epoch= 7  BatchID= 40 Loss: 0.586 | Acc: 81.288% (4266/5248)
[Train] Epoch= 7  BatchID= 50 Loss: 0.592 | Acc: 81.173% (5299/6528)
[Train] Epoch= 7  BatchID= 60 Loss: 0.598 | Acc: 80.943% (6320/7808)
[Train] Epoch= 7  BatchID= 70 Loss: 0.592 | Acc: 81.173% (7377/9088)
[Test] Epoch= 7  BatchID= 0 Loss: 0.274 | Acc: 94.531% (121/128)
[Test] Epoch= 7  BatchID= 10 Loss: 0.601 | Acc: 81.534% (1148/1408)
[Test] Epoch= 7  BatchID= 20 Loss: 0.686 | Acc: 79.613% (2140/2688)
[Test] Epoch= 7  BatchID= 30 Loss: 0.718 | Acc: 78.828% (3094/3925)

Epoch: 8
[Train] Epoch= 8  BatchID= 0 Loss: 0.528 | Acc: 84.375% (108/128)
[Train] Epoch= 8  BatchID= 10 Loss: 0.586 | Acc: 81.534% (1148/1408)
[Train] Epoch= 8  BatchID= 20 Loss: 0.590 | Acc: 80.841% (2173/2688)
[Train] Epoch= 8  BatchID= 30 Loss: 0.599 | Acc: 80.444% (3192/3968)
[Train] Epoch= 8  BatchID= 40 Loss: 0.608 | Acc: 80.450% (4222/5248)
[Train] Epoch= 8  BatchID= 50 Loss: 0.605 | Acc: 80.637% (5264/6528)
[Train] Epoch= 8  BatchID= 60 Loss: 0.604 | Acc: 80.571% (6291/7808)
[Train] Epoch= 8  BatchID= 70 Loss: 0.606 | Acc: 80.447% (7311/9088)
[Test] Epoch= 8  BatchID= 0 Loss: 0.345 | Acc: 92.969% (119/128)
[Test] Epoch= 8  BatchID= 10 Loss: 0.689 | Acc: 78.906% (1111/1408)
[Test] Epoch= 8  BatchID= 20 Loss: 0.659 | Acc: 80.022% (2151/2688)
[Test] Epoch= 8  BatchID= 30 Loss: 0.638 | Acc: 80.459% (3158/3925)

Epoch: 9
[Train] Epoch= 9  BatchID= 0 Loss: 0.541 | Acc: 80.469% (103/128)
[Train] Epoch= 9  BatchID= 10 Loss: 0.576 | Acc: 80.895% (1139/1408)
[Train] Epoch= 9  BatchID= 20 Loss: 0.595 | Acc: 80.432% (2162/2688)
[Train] Epoch= 9  BatchID= 30 Loss: 0.598 | Acc: 80.166% (3181/3968)
[Train] Epoch= 9  BatchID= 40 Loss: 0.610 | Acc: 79.973% (4197/5248)
[Train] Epoch= 9  BatchID= 50 Loss: 0.607 | Acc: 80.239% (5238/6528)
[Train] Epoch= 9  BatchID= 60 Loss: 0.611 | Acc: 80.059% (6251/7808)
[Train] Epoch= 9  BatchID= 70 Loss: 0.616 | Acc: 79.897% (7261/9088)
[Test] Epoch= 9  BatchID= 0 Loss: 0.194 | Acc: 92.969% (119/128)
[Test] Epoch= 9  BatchID= 10 Loss: 0.409 | Acc: 87.500% (1232/1408)
[Test] Epoch= 9  BatchID= 20 Loss: 0.505 | Acc: 84.747% (2278/2688)
[Test] Epoch= 9  BatchID= 30 Loss: 0.561 | Acc: 82.446% (3236/3925)

Epoch: 10
[Train] Epoch= 10  BatchID= 0 Loss: 0.491 | Acc: 84.375% (108/128)
[Train] Epoch= 10  BatchID= 10 Loss: 0.584 | Acc: 81.818% (1152/1408)
[Train] Epoch= 10  BatchID= 20 Loss: 0.596 | Acc: 81.510% (2191/2688)
[Train] Epoch= 10  BatchID= 30 Loss: 0.599 | Acc: 81.452% (3232/3968)
[Train] Epoch= 10  BatchID= 40 Loss: 0.588 | Acc: 81.860% (4296/5248)
[Train] Epoch= 10  BatchID= 50 Loss: 0.591 | Acc: 81.541% (5323/6528)
[Train] Epoch= 10  BatchID= 60 Loss: 0.603 | Acc: 81.032% (6327/7808)
[Train] Epoch= 10  BatchID= 70 Loss: 0.604 | Acc: 80.744% (7338/9088)
[Test] Epoch= 10  BatchID= 0 Loss: 0.084 | Acc: 97.656% (125/128)
[Test] Epoch= 10  BatchID= 10 Loss: 0.423 | Acc: 86.648% (1220/1408)
[Test] Epoch= 10  BatchID= 20 Loss: 0.515 | Acc: 83.743% (2251/2688)
[Test] Epoch= 10  BatchID= 30 Loss: 0.558 | Acc: 81.987% (3218/3925)

Epoch: 11
[Train] Epoch= 11  BatchID= 0 Loss: 0.540 | Acc: 80.469% (103/128)
[Train] Epoch= 11  BatchID= 10 Loss: 0.578 | Acc: 80.327% (1131/1408)
[Train] Epoch= 11  BatchID= 20 Loss: 0.562 | Acc: 81.585% (2193/2688)
[Train] Epoch= 11  BatchID= 30 Loss: 0.558 | Acc: 81.401% (3230/3968)
[Train] Epoch= 11  BatchID= 40 Loss: 0.553 | Acc: 81.631% (4284/5248)
[Train] Epoch= 11  BatchID= 50 Loss: 0.564 | Acc: 81.403% (5314/6528)
[Train] Epoch= 11  BatchID= 60 Loss: 0.578 | Acc: 81.084% (6331/7808)
[Train] Epoch= 11  BatchID= 70 Loss: 0.595 | Acc: 80.502% (7316/9088)
[Test] Epoch= 11  BatchID= 0 Loss: 0.467 | Acc: 85.156% (109/128)
[Test] Epoch= 11  BatchID= 10 Loss: 0.736 | Acc: 75.710% (1066/1408)
[Test] Epoch= 11  BatchID= 20 Loss: 0.671 | Acc: 78.051% (2098/2688)
[Test] Epoch= 11  BatchID= 30 Loss: 0.612 | Acc: 80.025% (3141/3925)

Epoch: 12
[Train] Epoch= 12  BatchID= 0 Loss: 0.541 | Acc: 82.812% (106/128)
[Train] Epoch= 12  BatchID= 10 Loss: 0.603 | Acc: 80.043% (1127/1408)
[Train] Epoch= 12  BatchID= 20 Loss: 0.617 | Acc: 80.022% (2151/2688)
[Train] Epoch= 12  BatchID= 30 Loss: 0.603 | Acc: 80.595% (3198/3968)
[Train] Epoch= 12  BatchID= 40 Loss: 0.615 | Acc: 80.126% (4205/5248)
[Train] Epoch= 12  BatchID= 50 Loss: 0.618 | Acc: 79.825% (5211/6528)
[Train] Epoch= 12  BatchID= 60 Loss: 0.609 | Acc: 80.187% (6261/7808)
[Train] Epoch= 12  BatchID= 70 Loss: 0.610 | Acc: 80.095% (7279/9088)
[Test] Epoch= 12  BatchID= 0 Loss: 0.180 | Acc: 93.750% (120/128)
[Test] Epoch= 12  BatchID= 10 Loss: 0.377 | Acc: 87.784% (1236/1408)
[Test] Epoch= 12  BatchID= 20 Loss: 0.552 | Acc: 83.371% (2241/2688)
[Test] Epoch= 12  BatchID= 30 Loss: 0.569 | Acc: 82.981% (3257/3925)

Epoch: 13
[Train] Epoch= 13  BatchID= 0 Loss: 0.444 | Acc: 85.156% (109/128)
[Train] Epoch= 13  BatchID= 10 Loss: 0.553 | Acc: 80.753% (1137/1408)
[Train] Epoch= 13  BatchID= 20 Loss: 0.564 | Acc: 80.469% (2163/2688)
[Train] Epoch= 13  BatchID= 30 Loss: 0.568 | Acc: 80.847% (3208/3968)
[Train] Epoch= 13  BatchID= 40 Loss: 0.564 | Acc: 81.326% (4268/5248)
[Train] Epoch= 13  BatchID= 50 Loss: 0.566 | Acc: 81.602% (5327/6528)
[Train] Epoch= 13  BatchID= 60 Loss: 0.570 | Acc: 81.314% (6349/7808)
[Train] Epoch= 13  BatchID= 70 Loss: 0.569 | Acc: 81.360% (7394/9088)
[Test] Epoch= 13  BatchID= 0 Loss: 0.326 | Acc: 92.188% (118/128)
[Test] Epoch= 13  BatchID= 10 Loss: 0.477 | Acc: 84.659% (1192/1408)
[Test] Epoch= 13  BatchID= 20 Loss: 0.554 | Acc: 82.924% (2229/2688)
[Test] Epoch= 13  BatchID= 30 Loss: 0.508 | Acc: 84.153% (3303/3925)
Saving..
Best accuracy:  84.15286624203821

Epoch: 14
[Train] Epoch= 14  BatchID= 0 Loss: 0.468 | Acc: 85.156% (109/128)
[Train] Epoch= 14  BatchID= 10 Loss: 0.523 | Acc: 83.381% (1174/1408)
[Train] Epoch= 14  BatchID= 20 Loss: 0.535 | Acc: 82.738% (2224/2688)
[Train] Epoch= 14  BatchID= 30 Loss: 0.557 | Acc: 81.880% (3249/3968)
[Train] Epoch= 14  BatchID= 40 Loss: 0.571 | Acc: 81.479% (4276/5248)
[Train] Epoch= 14  BatchID= 50 Loss: 0.582 | Acc: 81.204% (5301/6528)
[Train] Epoch= 14  BatchID= 60 Loss: 0.589 | Acc: 80.917% (6318/7808)
[Train] Epoch= 14  BatchID= 70 Loss: 0.589 | Acc: 80.854% (7348/9088)
[Test] Epoch= 14  BatchID= 0 Loss: 0.433 | Acc: 87.500% (112/128)
[Test] Epoch= 14  BatchID= 10 Loss: 0.684 | Acc: 79.332% (1117/1408)
[Test] Epoch= 14  BatchID= 20 Loss: 0.826 | Acc: 76.451% (2055/2688)
[Test] Epoch= 14  BatchID= 30 Loss: 0.756 | Acc: 78.522% (3082/3925)

Epoch: 15
[Train] Epoch= 15  BatchID= 0 Loss: 0.575 | Acc: 79.688% (102/128)
[Train] Epoch= 15  BatchID= 10 Loss: 0.555 | Acc: 82.457% (1161/1408)
[Train] Epoch= 15  BatchID= 20 Loss: 0.560 | Acc: 81.882% (2201/2688)
[Train] Epoch= 15  BatchID= 30 Loss: 0.576 | Acc: 81.628% (3239/3968)
[Train] Epoch= 15  BatchID= 40 Loss: 0.578 | Acc: 81.479% (4276/5248)
[Train] Epoch= 15  BatchID= 50 Loss: 0.583 | Acc: 81.173% (5299/6528)
[Train] Epoch= 15  BatchID= 60 Loss: 0.584 | Acc: 81.224% (6342/7808)
[Train] Epoch= 15  BatchID= 70 Loss: 0.585 | Acc: 81.085% (7369/9088)
[Test] Epoch= 15  BatchID= 0 Loss: 0.256 | Acc: 92.969% (119/128)
[Test] Epoch= 15  BatchID= 10 Loss: 0.641 | Acc: 79.332% (1117/1408)
[Test] Epoch= 15  BatchID= 20 Loss: 0.588 | Acc: 80.841% (2173/2688)
[Test] Epoch= 15  BatchID= 30 Loss: 0.546 | Acc: 82.471% (3237/3925)

Epoch: 16
[Train] Epoch= 16  BatchID= 0 Loss: 0.504 | Acc: 84.375% (108/128)
[Train] Epoch= 16  BatchID= 10 Loss: 0.531 | Acc: 82.528% (1162/1408)
[Train] Epoch= 16  BatchID= 20 Loss: 0.557 | Acc: 82.031% (2205/2688)
[Train] Epoch= 16  BatchID= 30 Loss: 0.573 | Acc: 81.426% (3231/3968)
[Train] Epoch= 16  BatchID= 40 Loss: 0.572 | Acc: 81.517% (4278/5248)
[Train] Epoch= 16  BatchID= 50 Loss: 0.568 | Acc: 81.602% (5327/6528)
[Train] Epoch= 16  BatchID= 60 Loss: 0.572 | Acc: 81.583% (6370/7808)
[Train] Epoch= 16  BatchID= 70 Loss: 0.581 | Acc: 81.283% (7387/9088)
[Test] Epoch= 16  BatchID= 0 Loss: 0.108 | Acc: 96.094% (123/128)
[Test] Epoch= 16  BatchID= 10 Loss: 0.528 | Acc: 82.315% (1159/1408)
[Test] Epoch= 16  BatchID= 20 Loss: 0.535 | Acc: 82.961% (2230/2688)
[Test] Epoch= 16  BatchID= 30 Loss: 0.558 | Acc: 82.369% (3233/3925)

Epoch: 17
[Train] Epoch= 17  BatchID= 0 Loss: 0.629 | Acc: 78.125% (100/128)
[Train] Epoch= 17  BatchID= 10 Loss: 0.562 | Acc: 81.108% (1142/1408)
[Train] Epoch= 17  BatchID= 20 Loss: 0.594 | Acc: 80.283% (2158/2688)
[Train] Epoch= 17  BatchID= 30 Loss: 0.586 | Acc: 80.922% (3211/3968)
[Train] Epoch= 17  BatchID= 40 Loss: 0.582 | Acc: 81.002% (4251/5248)
[Train] Epoch= 17  BatchID= 50 Loss: 0.582 | Acc: 80.990% (5287/6528)
[Train] Epoch= 17  BatchID= 60 Loss: 0.578 | Acc: 80.981% (6323/7808)
[Train] Epoch= 17  BatchID= 70 Loss: 0.572 | Acc: 81.228% (7382/9088)
[Test] Epoch= 17  BatchID= 0 Loss: 0.207 | Acc: 96.094% (123/128)
[Test] Epoch= 17  BatchID= 10 Loss: 0.603 | Acc: 80.682% (1136/1408)
[Test] Epoch= 17  BatchID= 20 Loss: 0.690 | Acc: 78.832% (2119/2688)
[Test] Epoch= 17  BatchID= 30 Loss: 0.648 | Acc: 79.745% (3130/3925)

Epoch: 18
[Train] Epoch= 18  BatchID= 0 Loss: 0.738 | Acc: 75.000% (96/128)
[Train] Epoch= 18  BatchID= 10 Loss: 0.585 | Acc: 80.256% (1130/1408)
[Train] Epoch= 18  BatchID= 20 Loss: 0.575 | Acc: 81.287% (2185/2688)
[Train] Epoch= 18  BatchID= 30 Loss: 0.564 | Acc: 81.804% (3246/3968)
[Train] Epoch= 18  BatchID= 40 Loss: 0.563 | Acc: 81.441% (4274/5248)
[Train] Epoch= 18  BatchID= 50 Loss: 0.572 | Acc: 81.419% (5315/6528)
[Train] Epoch= 18  BatchID= 60 Loss: 0.576 | Acc: 81.378% (6354/7808)
[Train] Epoch= 18  BatchID= 70 Loss: 0.570 | Acc: 81.569% (7413/9088)
[Test] Epoch= 18  BatchID= 0 Loss: 0.126 | Acc: 96.094% (123/128)
[Test] Epoch= 18  BatchID= 10 Loss: 0.444 | Acc: 84.588% (1191/1408)
[Test] Epoch= 18  BatchID= 20 Loss: 0.473 | Acc: 84.487% (2271/2688)
[Test] Epoch= 18  BatchID= 30 Loss: 0.511 | Acc: 83.567% (3280/3925)

Epoch: 19
[Train] Epoch= 19  BatchID= 0 Loss: 0.565 | Acc: 83.594% (107/128)
[Train] Epoch= 19  BatchID= 10 Loss: 0.530 | Acc: 83.239% (1172/1408)
[Train] Epoch= 19  BatchID= 20 Loss: 0.536 | Acc: 83.371% (2241/2688)
[Train] Epoch= 19  BatchID= 30 Loss: 0.523 | Acc: 83.821% (3326/3968)
[Train] Epoch= 19  BatchID= 40 Loss: 0.542 | Acc: 83.194% (4366/5248)
[Train] Epoch= 19  BatchID= 50 Loss: 0.556 | Acc: 82.583% (5391/6528)
[Train] Epoch= 19  BatchID= 60 Loss: 0.560 | Acc: 82.236% (6421/7808)
[Train] Epoch= 19  BatchID= 70 Loss: 0.557 | Acc: 82.273% (7477/9088)
[Test] Epoch= 19  BatchID= 0 Loss: 0.441 | Acc: 85.156% (109/128)
[Test] Epoch= 19  BatchID= 10 Loss: 0.454 | Acc: 84.020% (1183/1408)
[Test] Epoch= 19  BatchID= 20 Loss: 0.436 | Acc: 85.454% (2297/2688)
[Test] Epoch= 19  BatchID= 30 Loss: 0.487 | Acc: 84.000% (3297/3925)

Epoch: 20
[Train] Epoch= 20  BatchID= 0 Loss: 0.452 | Acc: 86.719% (111/128)
[Train] Epoch= 20  BatchID= 10 Loss: 0.517 | Acc: 83.807% (1180/1408)
[Train] Epoch= 20  BatchID= 20 Loss: 0.555 | Acc: 82.701% (2223/2688)
[Train] Epoch= 20  BatchID= 30 Loss: 0.552 | Acc: 82.560% (3276/3968)
[Train] Epoch= 20  BatchID= 40 Loss: 0.553 | Acc: 82.203% (4314/5248)
[Train] Epoch= 20  BatchID= 50 Loss: 0.551 | Acc: 81.924% (5348/6528)
[Train] Epoch= 20  BatchID= 60 Loss: 0.552 | Acc: 81.826% (6389/7808)
[Train] Epoch= 20  BatchID= 70 Loss: 0.550 | Acc: 82.053% (7457/9088)
[Test] Epoch= 20  BatchID= 0 Loss: 0.513 | Acc: 86.719% (111/128)
[Test] Epoch= 20  BatchID= 10 Loss: 0.577 | Acc: 81.250% (1144/1408)
[Test] Epoch= 20  BatchID= 20 Loss: 0.588 | Acc: 81.250% (2184/2688)
[Test] Epoch= 20  BatchID= 30 Loss: 0.605 | Acc: 80.968% (3178/3925)

Epoch: 21
[Train] Epoch= 21  BatchID= 0 Loss: 0.538 | Acc: 80.469% (103/128)
[Train] Epoch= 21  BatchID= 10 Loss: 0.542 | Acc: 81.960% (1154/1408)
[Train] Epoch= 21  BatchID= 20 Loss: 0.528 | Acc: 82.961% (2230/2688)
[Train] Epoch= 21  BatchID= 30 Loss: 0.532 | Acc: 82.888% (3289/3968)
[Train] Epoch= 21  BatchID= 40 Loss: 0.539 | Acc: 82.603% (4335/5248)
[Train] Epoch= 21  BatchID= 50 Loss: 0.542 | Acc: 82.475% (5384/6528)
[Train] Epoch= 21  BatchID= 60 Loss: 0.547 | Acc: 82.262% (6423/7808)
[Train] Epoch= 21  BatchID= 70 Loss: 0.546 | Acc: 82.328% (7482/9088)
[Test] Epoch= 21  BatchID= 0 Loss: 0.309 | Acc: 90.625% (116/128)
[Test] Epoch= 21  BatchID= 10 Loss: 0.520 | Acc: 82.812% (1166/1408)
[Test] Epoch= 21  BatchID= 20 Loss: 0.548 | Acc: 81.771% (2198/2688)
[Test] Epoch= 21  BatchID= 30 Loss: 0.536 | Acc: 82.166% (3225/3925)

Epoch: 22
[Train] Epoch= 22  BatchID= 0 Loss: 0.652 | Acc: 74.219% (95/128)
[Train] Epoch= 22  BatchID= 10 Loss: 0.468 | Acc: 84.162% (1185/1408)
[Train] Epoch= 22  BatchID= 20 Loss: 0.500 | Acc: 83.036% (2232/2688)
[Train] Epoch= 22  BatchID= 30 Loss: 0.494 | Acc: 83.165% (3300/3968)
[Train] Epoch= 22  BatchID= 40 Loss: 0.501 | Acc: 83.003% (4356/5248)
[Train] Epoch= 22  BatchID= 50 Loss: 0.523 | Acc: 82.384% (5378/6528)
[Train] Epoch= 22  BatchID= 60 Loss: 0.538 | Acc: 81.980% (6401/7808)
[Train] Epoch= 22  BatchID= 70 Loss: 0.543 | Acc: 81.998% (7452/9088)
[Test] Epoch= 22  BatchID= 0 Loss: 0.059 | Acc: 98.438% (126/128)
[Test] Epoch= 22  BatchID= 10 Loss: 0.422 | Acc: 86.222% (1214/1408)
[Test] Epoch= 22  BatchID= 20 Loss: 0.462 | Acc: 85.305% (2293/2688)
[Test] Epoch= 22  BatchID= 30 Loss: 0.523 | Acc: 83.541% (3279/3925)

Epoch: 23
[Train] Epoch= 23  BatchID= 0 Loss: 0.428 | Acc: 86.719% (111/128)
[Train] Epoch= 23  BatchID= 10 Loss: 0.527 | Acc: 82.884% (1167/1408)
[Train] Epoch= 23  BatchID= 20 Loss: 0.526 | Acc: 82.515% (2218/2688)
[Train] Epoch= 23  BatchID= 30 Loss: 0.527 | Acc: 82.535% (3275/3968)
[Train] Epoch= 23  BatchID= 40 Loss: 0.526 | Acc: 82.717% (4341/5248)
[Train] Epoch= 23  BatchID= 50 Loss: 0.535 | Acc: 82.552% (5389/6528)
[Train] Epoch= 23  BatchID= 60 Loss: 0.535 | Acc: 82.480% (6440/7808)
[Train] Epoch= 23  BatchID= 70 Loss: 0.535 | Acc: 82.493% (7497/9088)
[Test] Epoch= 23  BatchID= 0 Loss: 0.106 | Acc: 96.094% (123/128)
[Test] Epoch= 23  BatchID= 10 Loss: 0.333 | Acc: 89.276% (1257/1408)
[Test] Epoch= 23  BatchID= 20 Loss: 0.479 | Acc: 85.193% (2290/2688)
[Test] Epoch= 23  BatchID= 30 Loss: 0.555 | Acc: 82.904% (3254/3925)

Epoch: 24
[Train] Epoch= 24  BatchID= 0 Loss: 0.654 | Acc: 81.250% (104/128)
[Train] Epoch= 24  BatchID= 10 Loss: 0.535 | Acc: 82.741% (1165/1408)
[Train] Epoch= 24  BatchID= 20 Loss: 0.545 | Acc: 82.068% (2206/2688)
[Train] Epoch= 24  BatchID= 30 Loss: 0.526 | Acc: 82.434% (3271/3968)
[Train] Epoch= 24  BatchID= 40 Loss: 0.522 | Acc: 82.641% (4337/5248)
[Train] Epoch= 24  BatchID= 50 Loss: 0.533 | Acc: 82.276% (5371/6528)
[Train] Epoch= 24  BatchID= 60 Loss: 0.537 | Acc: 82.313% (6427/7808)
[Train] Epoch= 24  BatchID= 70 Loss: 0.531 | Acc: 82.581% (7505/9088)
[Test] Epoch= 24  BatchID= 0 Loss: 0.263 | Acc: 92.188% (118/128)
[Test] Epoch= 24  BatchID= 10 Loss: 0.505 | Acc: 83.523% (1176/1408)
[Test] Epoch= 24  BatchID= 20 Loss: 0.499 | Acc: 83.631% (2248/2688)
[Test] Epoch= 24  BatchID= 30 Loss: 0.479 | Acc: 84.382% (3312/3925)
Saving..
Best accuracy:  84.38216560509554

Epoch: 25
[Train] Epoch= 25  BatchID= 0 Loss: 0.494 | Acc: 81.250% (104/128)
[Train] Epoch= 25  BatchID= 10 Loss: 0.477 | Acc: 84.233% (1186/1408)
[Train] Epoch= 25  BatchID= 20 Loss: 0.506 | Acc: 83.371% (2241/2688)
[Train] Epoch= 25  BatchID= 30 Loss: 0.500 | Acc: 83.770% (3324/3968)
[Train] Epoch= 25  BatchID= 40 Loss: 0.510 | Acc: 83.518% (4383/5248)
[Train] Epoch= 25  BatchID= 50 Loss: 0.519 | Acc: 83.226% (5433/6528)
[Train] Epoch= 25  BatchID= 60 Loss: 0.523 | Acc: 83.107% (6489/7808)
[Train] Epoch= 25  BatchID= 70 Loss: 0.523 | Acc: 83.132% (7555/9088)
[Test] Epoch= 25  BatchID= 0 Loss: 0.190 | Acc: 94.531% (121/128)
[Test] Epoch= 25  BatchID= 10 Loss: 0.448 | Acc: 85.369% (1202/1408)
[Test] Epoch= 25  BatchID= 20 Loss: 0.481 | Acc: 84.226% (2264/2688)
[Test] Epoch= 25  BatchID= 30 Loss: 0.497 | Acc: 83.771% (3288/3925)

Epoch: 26
[Train] Epoch= 26  BatchID= 0 Loss: 0.499 | Acc: 85.938% (110/128)
[Train] Epoch= 26  BatchID= 10 Loss: 0.525 | Acc: 83.452% (1175/1408)
[Train] Epoch= 26  BatchID= 20 Loss: 0.506 | Acc: 83.557% (2246/2688)
[Train] Epoch= 26  BatchID= 30 Loss: 0.509 | Acc: 83.191% (3301/3968)
[Train] Epoch= 26  BatchID= 40 Loss: 0.512 | Acc: 83.251% (4369/5248)
[Train] Epoch= 26  BatchID= 50 Loss: 0.511 | Acc: 83.318% (5439/6528)
[Train] Epoch= 26  BatchID= 60 Loss: 0.511 | Acc: 83.478% (6518/7808)
[Train] Epoch= 26  BatchID= 70 Loss: 0.514 | Acc: 83.330% (7573/9088)
[Test] Epoch= 26  BatchID= 0 Loss: 0.203 | Acc: 92.969% (119/128)
[Test] Epoch= 26  BatchID= 10 Loss: 0.386 | Acc: 87.216% (1228/1408)
[Test] Epoch= 26  BatchID= 20 Loss: 0.460 | Acc: 84.263% (2265/2688)
[Test] Epoch= 26  BatchID= 30 Loss: 0.507 | Acc: 83.287% (3269/3925)

Epoch: 27
[Train] Epoch= 27  BatchID= 0 Loss: 0.458 | Acc: 83.594% (107/128)
[Train] Epoch= 27  BatchID= 10 Loss: 0.536 | Acc: 82.457% (1161/1408)
[Train] Epoch= 27  BatchID= 20 Loss: 0.528 | Acc: 83.185% (2236/2688)
[Train] Epoch= 27  BatchID= 30 Loss: 0.515 | Acc: 83.594% (3317/3968)
[Train] Epoch= 27  BatchID= 40 Loss: 0.518 | Acc: 83.346% (4374/5248)
[Train] Epoch= 27  BatchID= 50 Loss: 0.523 | Acc: 83.195% (5431/6528)
[Train] Epoch= 27  BatchID= 60 Loss: 0.525 | Acc: 83.248% (6500/7808)
[Train] Epoch= 27  BatchID= 70 Loss: 0.529 | Acc: 83.055% (7548/9088)
[Test] Epoch= 27  BatchID= 0 Loss: 0.263 | Acc: 91.406% (117/128)
[Test] Epoch= 27  BatchID= 10 Loss: 0.431 | Acc: 85.866% (1209/1408)
[Test] Epoch= 27  BatchID= 20 Loss: 0.466 | Acc: 85.342% (2294/2688)
[Test] Epoch= 27  BatchID= 30 Loss: 0.510 | Acc: 84.331% (3310/3925)

Epoch: 28
[Train] Epoch= 28  BatchID= 0 Loss: 0.384 | Acc: 85.938% (110/128)
[Train] Epoch= 28  BatchID= 10 Loss: 0.472 | Acc: 84.091% (1184/1408)
[Train] Epoch= 28  BatchID= 20 Loss: 0.478 | Acc: 84.189% (2263/2688)
[Train] Epoch= 28  BatchID= 30 Loss: 0.487 | Acc: 83.947% (3331/3968)
[Train] Epoch= 28  BatchID= 40 Loss: 0.502 | Acc: 83.232% (4368/5248)
[Train] Epoch= 28  BatchID= 50 Loss: 0.504 | Acc: 83.150% (5428/6528)
[Train] Epoch= 28  BatchID= 60 Loss: 0.502 | Acc: 83.261% (6501/7808)
[Train] Epoch= 28  BatchID= 70 Loss: 0.496 | Acc: 83.528% (7591/9088)
[Test] Epoch= 28  BatchID= 0 Loss: 0.108 | Acc: 96.094% (123/128)
[Test] Epoch= 28  BatchID= 10 Loss: 0.426 | Acc: 85.653% (1206/1408)
[Test] Epoch= 28  BatchID= 20 Loss: 0.411 | Acc: 86.272% (2319/2688)
[Test] Epoch= 28  BatchID= 30 Loss: 0.460 | Acc: 84.790% (3328/3925)
Saving..
Best accuracy:  84.78980891719745

Epoch: 29
[Train] Epoch= 29  BatchID= 0 Loss: 0.485 | Acc: 83.594% (107/128)
[Train] Epoch= 29  BatchID= 10 Loss: 0.486 | Acc: 83.949% (1182/1408)
[Train] Epoch= 29  BatchID= 20 Loss: 0.500 | Acc: 83.966% (2257/2688)
[Train] Epoch= 29  BatchID= 30 Loss: 0.509 | Acc: 83.745% (3323/3968)
[Train] Epoch= 29  BatchID= 40 Loss: 0.495 | Acc: 84.165% (4417/5248)
[Train] Epoch= 29  BatchID= 50 Loss: 0.492 | Acc: 84.130% (5492/6528)
[Train] Epoch= 29  BatchID= 60 Loss: 0.490 | Acc: 84.285% (6581/7808)
[Train] Epoch= 29  BatchID= 70 Loss: 0.494 | Acc: 84.276% (7659/9088)
[Test] Epoch= 29  BatchID= 0 Loss: 0.192 | Acc: 92.188% (118/128)
[Test] Epoch= 29  BatchID= 10 Loss: 0.378 | Acc: 88.281% (1243/1408)
[Test] Epoch= 29  BatchID= 20 Loss: 0.505 | Acc: 84.338% (2267/2688)
[Test] Epoch= 29  BatchID= 30 Loss: 0.564 | Acc: 82.166% (3225/3925)

Epoch: 30
[Train] Epoch= 30  BatchID= 0 Loss: 0.493 | Acc: 82.031% (105/128)
[Train] Epoch= 30  BatchID= 10 Loss: 0.513 | Acc: 83.026% (1169/1408)
[Train] Epoch= 30  BatchID= 20 Loss: 0.514 | Acc: 83.222% (2237/2688)
[Train] Epoch= 30  BatchID= 30 Loss: 0.504 | Acc: 83.669% (3320/3968)
[Train] Epoch= 30  BatchID= 40 Loss: 0.499 | Acc: 84.051% (4411/5248)
[Train] Epoch= 30  BatchID= 50 Loss: 0.497 | Acc: 83.900% (5477/6528)
[Train] Epoch= 30  BatchID= 60 Loss: 0.493 | Acc: 83.952% (6555/7808)
[Train] Epoch= 30  BatchID= 70 Loss: 0.491 | Acc: 84.056% (7639/9088)
[Test] Epoch= 30  BatchID= 0 Loss: 0.062 | Acc: 99.219% (127/128)
[Test] Epoch= 30  BatchID= 10 Loss: 0.383 | Acc: 88.423% (1245/1408)
[Test] Epoch= 30  BatchID= 20 Loss: 0.467 | Acc: 86.310% (2320/2688)
[Test] Epoch= 30  BatchID= 30 Loss: 0.519 | Acc: 84.484% (3316/3925)

Epoch: 31
[Train] Epoch= 31  BatchID= 0 Loss: 0.441 | Acc: 87.500% (112/128)
[Train] Epoch= 31  BatchID= 10 Loss: 0.499 | Acc: 83.878% (1181/1408)
[Train] Epoch= 31  BatchID= 20 Loss: 0.493 | Acc: 84.115% (2261/2688)
[Train] Epoch= 31  BatchID= 30 Loss: 0.480 | Acc: 84.476% (3352/3968)
[Train] Epoch= 31  BatchID= 40 Loss: 0.485 | Acc: 84.223% (4420/5248)
[Train] Epoch= 31  BatchID= 50 Loss: 0.482 | Acc: 84.191% (5496/6528)
[Train] Epoch= 31  BatchID= 60 Loss: 0.479 | Acc: 84.132% (6569/7808)
[Train] Epoch= 31  BatchID= 70 Loss: 0.479 | Acc: 84.188% (7651/9088)
[Test] Epoch= 31  BatchID= 0 Loss: 0.309 | Acc: 91.406% (117/128)
[Test] Epoch= 31  BatchID= 10 Loss: 0.400 | Acc: 87.287% (1229/1408)
[Test] Epoch= 31  BatchID= 20 Loss: 0.441 | Acc: 85.751% (2305/2688)
[Test] Epoch= 31  BatchID= 30 Loss: 0.500 | Acc: 84.000% (3297/3925)

Epoch: 32
[Train] Epoch= 32  BatchID= 0 Loss: 0.362 | Acc: 89.844% (115/128)
[Train] Epoch= 32  BatchID= 10 Loss: 0.489 | Acc: 83.452% (1175/1408)
[Train] Epoch= 32  BatchID= 20 Loss: 0.513 | Acc: 83.073% (2233/2688)
[Train] Epoch= 32  BatchID= 30 Loss: 0.509 | Acc: 83.695% (3321/3968)
[Train] Epoch= 32  BatchID= 40 Loss: 0.498 | Acc: 84.032% (4410/5248)
[Train] Epoch= 32  BatchID= 50 Loss: 0.502 | Acc: 83.716% (5465/6528)
[Train] Epoch= 32  BatchID= 60 Loss: 0.493 | Acc: 84.004% (6559/7808)
[Train] Epoch= 32  BatchID= 70 Loss: 0.490 | Acc: 84.210% (7653/9088)
[Test] Epoch= 32  BatchID= 0 Loss: 0.102 | Acc: 97.656% (125/128)
[Test] Epoch= 32  BatchID= 10 Loss: 0.505 | Acc: 83.381% (1174/1408)
[Test] Epoch= 32  BatchID= 20 Loss: 0.489 | Acc: 83.966% (2257/2688)
[Test] Epoch= 32  BatchID= 30 Loss: 0.559 | Acc: 82.293% (3230/3925)

Epoch: 33
[Train] Epoch= 33  BatchID= 0 Loss: 0.380 | Acc: 89.062% (114/128)
[Train] Epoch= 33  BatchID= 10 Loss: 0.452 | Acc: 85.653% (1206/1408)
[Train] Epoch= 33  BatchID= 20 Loss: 0.463 | Acc: 85.491% (2298/2688)
[Train] Epoch= 33  BatchID= 30 Loss: 0.480 | Acc: 85.106% (3377/3968)
[Train] Epoch= 33  BatchID= 40 Loss: 0.480 | Acc: 85.042% (4463/5248)
[Train] Epoch= 33  BatchID= 50 Loss: 0.480 | Acc: 84.804% (5536/6528)
[Train] Epoch= 33  BatchID= 60 Loss: 0.479 | Acc: 84.836% (6624/7808)
[Train] Epoch= 33  BatchID= 70 Loss: 0.479 | Acc: 84.815% (7708/9088)
[Test] Epoch= 33  BatchID= 0 Loss: 0.183 | Acc: 96.094% (123/128)
[Test] Epoch= 33  BatchID= 10 Loss: 0.389 | Acc: 87.571% (1233/1408)
[Test] Epoch= 33  BatchID= 20 Loss: 0.443 | Acc: 86.049% (2313/2688)
[Test] Epoch= 33  BatchID= 30 Loss: 0.457 | Acc: 85.656% (3362/3925)
Saving..
Best accuracy:  85.65605095541402

Epoch: 34
[Train] Epoch= 34  BatchID= 0 Loss: 0.438 | Acc: 85.156% (109/128)
[Train] Epoch= 34  BatchID= 10 Loss: 0.447 | Acc: 86.364% (1216/1408)
[Train] Epoch= 34  BatchID= 20 Loss: 0.485 | Acc: 84.710% (2277/2688)
[Train] Epoch= 34  BatchID= 30 Loss: 0.487 | Acc: 84.551% (3355/3968)
[Train] Epoch= 34  BatchID= 40 Loss: 0.485 | Acc: 84.546% (4437/5248)
[Train] Epoch= 34  BatchID= 50 Loss: 0.482 | Acc: 84.819% (5537/6528)
[Train] Epoch= 34  BatchID= 60 Loss: 0.477 | Acc: 84.913% (6630/7808)
[Train] Epoch= 34  BatchID= 70 Loss: 0.477 | Acc: 84.815% (7708/9088)
[Test] Epoch= 34  BatchID= 0 Loss: 0.271 | Acc: 90.625% (116/128)
[Test] Epoch= 34  BatchID= 10 Loss: 0.450 | Acc: 85.298% (1201/1408)
[Test] Epoch= 34  BatchID= 20 Loss: 0.530 | Acc: 83.333% (2240/2688)
[Test] Epoch= 34  BatchID= 30 Loss: 0.547 | Acc: 83.261% (3268/3925)

Epoch: 35
[Train] Epoch= 35  BatchID= 0 Loss: 0.320 | Acc: 91.406% (117/128)
[Train] Epoch= 35  BatchID= 10 Loss: 0.395 | Acc: 87.216% (1228/1408)
[Train] Epoch= 35  BatchID= 20 Loss: 0.427 | Acc: 86.086% (2314/2688)
[Train] Epoch= 35  BatchID= 30 Loss: 0.419 | Acc: 86.542% (3434/3968)
[Train] Epoch= 35  BatchID= 40 Loss: 0.426 | Acc: 86.490% (4539/5248)
[Train] Epoch= 35  BatchID= 50 Loss: 0.438 | Acc: 86.075% (5619/6528)
[Train] Epoch= 35  BatchID= 60 Loss: 0.445 | Acc: 85.835% (6702/7808)
[Train] Epoch= 35  BatchID= 70 Loss: 0.450 | Acc: 85.585% (7778/9088)
[Test] Epoch= 35  BatchID= 0 Loss: 0.146 | Acc: 96.094% (123/128)
[Test] Epoch= 35  BatchID= 10 Loss: 0.558 | Acc: 83.168% (1171/1408)
[Test] Epoch= 35  BatchID= 20 Loss: 0.459 | Acc: 85.826% (2307/2688)
[Test] Epoch= 35  BatchID= 30 Loss: 0.487 | Acc: 84.892% (3332/3925)

Epoch: 36
[Train] Epoch= 36  BatchID= 0 Loss: 0.394 | Acc: 89.062% (114/128)
[Train] Epoch= 36  BatchID= 10 Loss: 0.432 | Acc: 87.074% (1226/1408)
[Train] Epoch= 36  BatchID= 20 Loss: 0.455 | Acc: 85.751% (2305/2688)
[Train] Epoch= 36  BatchID= 30 Loss: 0.445 | Acc: 85.988% (3412/3968)
[Train] Epoch= 36  BatchID= 40 Loss: 0.446 | Acc: 85.918% (4509/5248)
[Train] Epoch= 36  BatchID= 50 Loss: 0.448 | Acc: 85.723% (5596/6528)
[Train] Epoch= 36  BatchID= 60 Loss: 0.448 | Acc: 85.630% (6686/7808)
[Train] Epoch= 36  BatchID= 70 Loss: 0.453 | Acc: 85.442% (7765/9088)
[Test] Epoch= 36  BatchID= 0 Loss: 0.382 | Acc: 89.062% (114/128)
[Test] Epoch= 36  BatchID= 10 Loss: 0.424 | Acc: 86.790% (1222/1408)
[Test] Epoch= 36  BatchID= 20 Loss: 0.519 | Acc: 83.966% (2257/2688)
[Test] Epoch= 36  BatchID= 30 Loss: 0.520 | Acc: 84.000% (3297/3925)

Epoch: 37
[Train] Epoch= 37  BatchID= 0 Loss: 0.350 | Acc: 88.281% (113/128)
[Train] Epoch= 37  BatchID= 10 Loss: 0.416 | Acc: 87.500% (1232/1408)
[Train] Epoch= 37  BatchID= 20 Loss: 0.441 | Acc: 86.458% (2324/2688)
[Train] Epoch= 37  BatchID= 30 Loss: 0.454 | Acc: 85.635% (3398/3968)
[Train] Epoch= 37  BatchID= 40 Loss: 0.457 | Acc: 85.404% (4482/5248)
[Train] Epoch= 37  BatchID= 50 Loss: 0.454 | Acc: 85.478% (5580/6528)
[Train] Epoch= 37  BatchID= 60 Loss: 0.447 | Acc: 85.707% (6692/7808)
[Train] Epoch= 37  BatchID= 70 Loss: 0.451 | Acc: 85.475% (7768/9088)
[Test] Epoch= 37  BatchID= 0 Loss: 0.152 | Acc: 95.312% (122/128)
[Test] Epoch= 37  BatchID= 10 Loss: 0.319 | Acc: 89.418% (1259/1408)
[Test] Epoch= 37  BatchID= 20 Loss: 0.360 | Acc: 88.839% (2388/2688)
[Test] Epoch= 37  BatchID= 30 Loss: 0.467 | Acc: 85.197% (3344/3925)

Epoch: 38
[Train] Epoch= 38  BatchID= 0 Loss: 0.301 | Acc: 91.406% (117/128)
[Train] Epoch= 38  BatchID= 10 Loss: 0.428 | Acc: 86.435% (1217/1408)
[Train] Epoch= 38  BatchID= 20 Loss: 0.437 | Acc: 85.900% (2309/2688)
[Train] Epoch= 38  BatchID= 30 Loss: 0.431 | Acc: 86.290% (3424/3968)
[Train] Epoch= 38  BatchID= 40 Loss: 0.443 | Acc: 85.938% (4510/5248)
[Train] Epoch= 38  BatchID= 50 Loss: 0.448 | Acc: 85.830% (5603/6528)
[Train] Epoch= 38  BatchID= 60 Loss: 0.451 | Acc: 85.745% (6695/7808)
[Train] Epoch= 38  BatchID= 70 Loss: 0.450 | Acc: 85.728% (7791/9088)
[Test] Epoch= 38  BatchID= 0 Loss: 0.192 | Acc: 96.094% (123/128)
[Test] Epoch= 38  BatchID= 10 Loss: 0.413 | Acc: 87.287% (1229/1408)
[Test] Epoch= 38  BatchID= 20 Loss: 0.412 | Acc: 87.314% (2347/2688)
[Test] Epoch= 38  BatchID= 30 Loss: 0.424 | Acc: 86.854% (3409/3925)
Saving..
Best accuracy:  86.85350318471338

Epoch: 39
[Train] Epoch= 39  BatchID= 0 Loss: 0.447 | Acc: 85.156% (109/128)
[Train] Epoch= 39  BatchID= 10 Loss: 0.411 | Acc: 86.080% (1212/1408)
[Train] Epoch= 39  BatchID= 20 Loss: 0.399 | Acc: 87.277% (2346/2688)
[Train] Epoch= 39  BatchID= 30 Loss: 0.411 | Acc: 87.349% (3466/3968)
[Train] Epoch= 39  BatchID= 40 Loss: 0.414 | Acc: 86.966% (4564/5248)
[Train] Epoch= 39  BatchID= 50 Loss: 0.421 | Acc: 86.734% (5662/6528)
[Train] Epoch= 39  BatchID= 60 Loss: 0.427 | Acc: 86.424% (6748/7808)
[Train] Epoch= 39  BatchID= 70 Loss: 0.431 | Acc: 86.158% (7830/9088)
[Test] Epoch= 39  BatchID= 0 Loss: 0.182 | Acc: 94.531% (121/128)
[Test] Epoch= 39  BatchID= 10 Loss: 0.427 | Acc: 86.080% (1212/1408)
[Test] Epoch= 39  BatchID= 20 Loss: 0.395 | Acc: 87.091% (2341/2688)
[Test] Epoch= 39  BatchID= 30 Loss: 0.401 | Acc: 86.803% (3407/3925)

Epoch: 40
[Train] Epoch= 40  BatchID= 0 Loss: 0.538 | Acc: 82.031% (105/128)
[Train] Epoch= 40  BatchID= 10 Loss: 0.459 | Acc: 86.080% (1212/1408)
[Train] Epoch= 40  BatchID= 20 Loss: 0.431 | Acc: 86.830% (2334/2688)
[Train] Epoch= 40  BatchID= 30 Loss: 0.415 | Acc: 87.147% (3458/3968)
[Train] Epoch= 40  BatchID= 40 Loss: 0.421 | Acc: 86.986% (4565/5248)
[Train] Epoch= 40  BatchID= 50 Loss: 0.422 | Acc: 86.918% (5674/6528)
[Train] Epoch= 40  BatchID= 60 Loss: 0.418 | Acc: 86.924% (6787/7808)
[Train] Epoch= 40  BatchID= 70 Loss: 0.420 | Acc: 86.807% (7889/9088)
[Test] Epoch= 40  BatchID= 0 Loss: 0.187 | Acc: 93.750% (120/128)
[Test] Epoch= 40  BatchID= 10 Loss: 0.328 | Acc: 89.347% (1258/1408)
[Test] Epoch= 40  BatchID= 20 Loss: 0.372 | Acc: 88.467% (2378/2688)
[Test] Epoch= 40  BatchID= 30 Loss: 0.422 | Acc: 86.701% (3403/3925)

Epoch: 41
[Train] Epoch= 41  BatchID= 0 Loss: 0.438 | Acc: 86.719% (111/128)
[Train] Epoch= 41  BatchID= 10 Loss: 0.415 | Acc: 87.429% (1231/1408)
[Train] Epoch= 41  BatchID= 20 Loss: 0.401 | Acc: 87.463% (2351/2688)
[Train] Epoch= 41  BatchID= 30 Loss: 0.404 | Acc: 87.399% (3468/3968)
[Train] Epoch= 41  BatchID= 40 Loss: 0.406 | Acc: 87.443% (4589/5248)
[Train] Epoch= 41  BatchID= 50 Loss: 0.415 | Acc: 86.811% (5667/6528)
[Train] Epoch= 41  BatchID= 60 Loss: 0.418 | Acc: 86.847% (6781/7808)
[Train] Epoch= 41  BatchID= 70 Loss: 0.424 | Acc: 86.532% (7864/9088)
[Test] Epoch= 41  BatchID= 0 Loss: 0.145 | Acc: 95.312% (122/128)
[Test] Epoch= 41  BatchID= 10 Loss: 0.391 | Acc: 86.364% (1216/1408)
[Test] Epoch= 41  BatchID= 20 Loss: 0.419 | Acc: 85.751% (2305/2688)
[Test] Epoch= 41  BatchID= 30 Loss: 0.433 | Acc: 85.682% (3363/3925)

Epoch: 42
[Train] Epoch= 42  BatchID= 0 Loss: 0.491 | Acc: 84.375% (108/128)
[Train] Epoch= 42  BatchID= 10 Loss: 0.406 | Acc: 87.571% (1233/1408)
[Train] Epoch= 42  BatchID= 20 Loss: 0.418 | Acc: 86.496% (2325/2688)
[Train] Epoch= 42  BatchID= 30 Loss: 0.418 | Acc: 86.442% (3430/3968)
[Train] Epoch= 42  BatchID= 40 Loss: 0.429 | Acc: 86.204% (4524/5248)
[Train] Epoch= 42  BatchID= 50 Loss: 0.437 | Acc: 85.815% (5602/6528)
[Train] Epoch= 42  BatchID= 60 Loss: 0.434 | Acc: 85.938% (6710/7808)
[Train] Epoch= 42  BatchID= 70 Loss: 0.429 | Acc: 86.191% (7833/9088)
[Test] Epoch= 42  BatchID= 0 Loss: 0.212 | Acc: 93.750% (120/128)
[Test] Epoch= 42  BatchID= 10 Loss: 0.397 | Acc: 86.648% (1220/1408)
[Test] Epoch= 42  BatchID= 20 Loss: 0.373 | Acc: 87.686% (2357/2688)
[Test] Epoch= 42  BatchID= 30 Loss: 0.422 | Acc: 86.446% (3393/3925)

Epoch: 43
[Train] Epoch= 43  BatchID= 0 Loss: 0.388 | Acc: 86.719% (111/128)
[Train] Epoch= 43  BatchID= 10 Loss: 0.410 | Acc: 86.222% (1214/1408)
[Train] Epoch= 43  BatchID= 20 Loss: 0.387 | Acc: 87.277% (2346/2688)
[Train] Epoch= 43  BatchID= 30 Loss: 0.384 | Acc: 87.576% (3475/3968)
[Train] Epoch= 43  BatchID= 40 Loss: 0.388 | Acc: 87.576% (4596/5248)
[Train] Epoch= 43  BatchID= 50 Loss: 0.393 | Acc: 87.393% (5705/6528)
[Train] Epoch= 43  BatchID= 60 Loss: 0.393 | Acc: 87.385% (6823/7808)
[Train] Epoch= 43  BatchID= 70 Loss: 0.401 | Acc: 87.104% (7916/9088)
[Test] Epoch= 43  BatchID= 0 Loss: 0.174 | Acc: 94.531% (121/128)
[Test] Epoch= 43  BatchID= 10 Loss: 0.313 | Acc: 89.631% (1262/1408)
[Test] Epoch= 43  BatchID= 20 Loss: 0.319 | Acc: 89.695% (2411/2688)
[Test] Epoch= 43  BatchID= 30 Loss: 0.366 | Acc: 88.051% (3456/3925)
Saving..
Best accuracy:  88.05095541401273

Epoch: 44
[Train] Epoch= 44  BatchID= 0 Loss: 0.403 | Acc: 89.844% (115/128)
[Train] Epoch= 44  BatchID= 10 Loss: 0.364 | Acc: 88.707% (1249/1408)
[Train] Epoch= 44  BatchID= 20 Loss: 0.367 | Acc: 88.616% (2382/2688)
[Train] Epoch= 44  BatchID= 30 Loss: 0.378 | Acc: 88.206% (3500/3968)
[Train] Epoch= 44  BatchID= 40 Loss: 0.387 | Acc: 87.824% (4609/5248)
[Train] Epoch= 44  BatchID= 50 Loss: 0.397 | Acc: 87.224% (5694/6528)
[Train] Epoch= 44  BatchID= 60 Loss: 0.402 | Acc: 87.052% (6797/7808)
[Train] Epoch= 44  BatchID= 70 Loss: 0.402 | Acc: 87.104% (7916/9088)
[Test] Epoch= 44  BatchID= 0 Loss: 0.209 | Acc: 94.531% (121/128)
[Test] Epoch= 44  BatchID= 10 Loss: 0.339 | Acc: 88.707% (1249/1408)
[Test] Epoch= 44  BatchID= 20 Loss: 0.364 | Acc: 88.504% (2379/2688)
[Test] Epoch= 44  BatchID= 30 Loss: 0.409 | Acc: 87.006% (3415/3925)

Epoch: 45
[Train] Epoch= 45  BatchID= 0 Loss: 0.330 | Acc: 91.406% (117/128)
[Train] Epoch= 45  BatchID= 10 Loss: 0.362 | Acc: 87.997% (1239/1408)
[Train] Epoch= 45  BatchID= 20 Loss: 0.391 | Acc: 86.644% (2329/2688)
[Train] Epoch= 45  BatchID= 30 Loss: 0.413 | Acc: 86.139% (3418/3968)
[Train] Epoch= 45  BatchID= 40 Loss: 0.419 | Acc: 86.014% (4514/5248)
[Train] Epoch= 45  BatchID= 50 Loss: 0.424 | Acc: 85.938% (5610/6528)
[Train] Epoch= 45  BatchID= 60 Loss: 0.421 | Acc: 86.168% (6728/7808)
[Train] Epoch= 45  BatchID= 70 Loss: 0.417 | Acc: 86.356% (7848/9088)
[Test] Epoch= 45  BatchID= 0 Loss: 0.200 | Acc: 95.312% (122/128)
[Test] Epoch= 45  BatchID= 10 Loss: 0.291 | Acc: 90.767% (1278/1408)
[Test] Epoch= 45  BatchID= 20 Loss: 0.350 | Acc: 89.025% (2393/2688)
[Test] Epoch= 45  BatchID= 30 Loss: 0.384 | Acc: 87.898% (3450/3925)

Epoch: 46
[Train] Epoch= 46  BatchID= 0 Loss: 0.381 | Acc: 89.844% (115/128)
[Train] Epoch= 46  BatchID= 10 Loss: 0.377 | Acc: 87.926% (1238/1408)
[Train] Epoch= 46  BatchID= 20 Loss: 0.374 | Acc: 88.281% (2373/2688)
[Train] Epoch= 46  BatchID= 30 Loss: 0.375 | Acc: 87.903% (3488/3968)
[Train] Epoch= 46  BatchID= 40 Loss: 0.372 | Acc: 88.034% (4620/5248)
[Train] Epoch= 46  BatchID= 50 Loss: 0.371 | Acc: 88.097% (5751/6528)
[Train] Epoch= 46  BatchID= 60 Loss: 0.375 | Acc: 88.025% (6873/7808)
[Train] Epoch= 46  BatchID= 70 Loss: 0.382 | Acc: 87.764% (7976/9088)
[Test] Epoch= 46  BatchID= 0 Loss: 0.209 | Acc: 93.750% (120/128)
[Test] Epoch= 46  BatchID= 10 Loss: 0.383 | Acc: 88.565% (1247/1408)
[Test] Epoch= 46  BatchID= 20 Loss: 0.367 | Acc: 89.062% (2394/2688)
[Test] Epoch= 46  BatchID= 30 Loss: 0.385 | Acc: 88.255% (3464/3925)
Saving..
Best accuracy:  88.2547770700637

Epoch: 47
[Train] Epoch= 47  BatchID= 0 Loss: 0.258 | Acc: 92.188% (118/128)
[Train] Epoch= 47  BatchID= 10 Loss: 0.357 | Acc: 88.494% (1246/1408)
[Train] Epoch= 47  BatchID= 20 Loss: 0.379 | Acc: 87.872% (2362/2688)
[Train] Epoch= 47  BatchID= 30 Loss: 0.383 | Acc: 87.828% (3485/3968)
[Train] Epoch= 47  BatchID= 40 Loss: 0.385 | Acc: 87.538% (4594/5248)
[Train] Epoch= 47  BatchID= 50 Loss: 0.391 | Acc: 87.393% (5705/6528)
[Train] Epoch= 47  BatchID= 60 Loss: 0.389 | Acc: 87.487% (6831/7808)
[Train] Epoch= 47  BatchID= 70 Loss: 0.385 | Acc: 87.511% (7953/9088)
[Test] Epoch= 47  BatchID= 0 Loss: 0.192 | Acc: 96.094% (123/128)
[Test] Epoch= 47  BatchID= 10 Loss: 0.367 | Acc: 88.636% (1248/1408)
[Test] Epoch= 47  BatchID= 20 Loss: 0.363 | Acc: 88.244% (2372/2688)
[Test] Epoch= 47  BatchID= 30 Loss: 0.360 | Acc: 88.229% (3463/3925)

Epoch: 48
[Train] Epoch= 48  BatchID= 0 Loss: 0.409 | Acc: 88.281% (113/128)
[Train] Epoch= 48  BatchID= 10 Loss: 0.374 | Acc: 87.926% (1238/1408)
[Train] Epoch= 48  BatchID= 20 Loss: 0.387 | Acc: 87.277% (2346/2688)
[Train] Epoch= 48  BatchID= 30 Loss: 0.381 | Acc: 87.324% (3465/3968)
[Train] Epoch= 48  BatchID= 40 Loss: 0.381 | Acc: 87.443% (4589/5248)
[Train] Epoch= 48  BatchID= 50 Loss: 0.378 | Acc: 87.699% (5725/6528)
[Train] Epoch= 48  BatchID= 60 Loss: 0.374 | Acc: 87.679% (6846/7808)
[Train] Epoch= 48  BatchID= 70 Loss: 0.369 | Acc: 87.962% (7994/9088)
[Test] Epoch= 48  BatchID= 0 Loss: 0.190 | Acc: 96.094% (123/128)
[Test] Epoch= 48  BatchID= 10 Loss: 0.429 | Acc: 86.293% (1215/1408)
[Test] Epoch= 48  BatchID= 20 Loss: 0.372 | Acc: 87.946% (2364/2688)
[Test] Epoch= 48  BatchID= 30 Loss: 0.399 | Acc: 87.159% (3421/3925)

Epoch: 49
[Train] Epoch= 49  BatchID= 0 Loss: 0.474 | Acc: 86.719% (111/128)
[Train] Epoch= 49  BatchID= 10 Loss: 0.341 | Acc: 88.494% (1246/1408)
[Train] Epoch= 49  BatchID= 20 Loss: 0.350 | Acc: 88.504% (2379/2688)
[Train] Epoch= 49  BatchID= 30 Loss: 0.343 | Acc: 88.810% (3524/3968)
[Train] Epoch= 49  BatchID= 40 Loss: 0.347 | Acc: 88.662% (4653/5248)
[Train] Epoch= 49  BatchID= 50 Loss: 0.344 | Acc: 88.787% (5796/6528)
[Train] Epoch= 49  BatchID= 60 Loss: 0.355 | Acc: 88.448% (6906/7808)
[Train] Epoch= 49  BatchID= 70 Loss: 0.353 | Acc: 88.534% (8046/9088)
[Test] Epoch= 49  BatchID= 0 Loss: 0.093 | Acc: 97.656% (125/128)
[Test] Epoch= 49  BatchID= 10 Loss: 0.351 | Acc: 88.778% (1250/1408)
[Test] Epoch= 49  BatchID= 20 Loss: 0.354 | Acc: 88.690% (2384/2688)
[Test] Epoch= 49  BatchID= 30 Loss: 0.359 | Acc: 88.561% (3476/3925)
Saving..
Best accuracy:  88.56050955414013

Epoch: 50
[Train] Epoch= 50  BatchID= 0 Loss: 0.412 | Acc: 89.844% (115/128)
[Train] Epoch= 50  BatchID= 10 Loss: 0.357 | Acc: 88.991% (1253/1408)
[Train] Epoch= 50  BatchID= 20 Loss: 0.345 | Acc: 89.435% (2404/2688)
[Train] Epoch= 50  BatchID= 30 Loss: 0.353 | Acc: 89.012% (3532/3968)
[Train] Epoch= 50  BatchID= 40 Loss: 0.358 | Acc: 88.777% (4659/5248)
[Train] Epoch= 50  BatchID= 50 Loss: 0.361 | Acc: 88.787% (5796/6528)
[Train] Epoch= 50  BatchID= 60 Loss: 0.369 | Acc: 88.409% (6903/7808)
[Train] Epoch= 50  BatchID= 70 Loss: 0.363 | Acc: 88.589% (8051/9088)
[Test] Epoch= 50  BatchID= 0 Loss: 0.180 | Acc: 96.094% (123/128)
[Test] Epoch= 50  BatchID= 10 Loss: 0.319 | Acc: 89.915% (1266/1408)
[Test] Epoch= 50  BatchID= 20 Loss: 0.310 | Acc: 90.141% (2423/2688)
[Test] Epoch= 50  BatchID= 30 Loss: 0.347 | Acc: 88.892% (3489/3925)
Saving..
Best accuracy:  88.89171974522293

Epoch: 51
[Train] Epoch= 51  BatchID= 0 Loss: 0.395 | Acc: 89.062% (114/128)
[Train] Epoch= 51  BatchID= 10 Loss: 0.357 | Acc: 89.915% (1266/1408)
[Train] Epoch= 51  BatchID= 20 Loss: 0.361 | Acc: 88.914% (2390/2688)
[Train] Epoch= 51  BatchID= 30 Loss: 0.368 | Acc: 88.684% (3519/3968)
[Train] Epoch= 51  BatchID= 40 Loss: 0.370 | Acc: 88.510% (4645/5248)
[Train] Epoch= 51  BatchID= 50 Loss: 0.366 | Acc: 88.557% (5781/6528)
[Train] Epoch= 51  BatchID= 60 Loss: 0.368 | Acc: 88.512% (6911/7808)
[Train] Epoch= 51  BatchID= 70 Loss: 0.363 | Acc: 88.600% (8052/9088)
[Test] Epoch= 51  BatchID= 0 Loss: 0.172 | Acc: 93.750% (120/128)
[Test] Epoch= 51  BatchID= 10 Loss: 0.298 | Acc: 90.128% (1269/1408)
[Test] Epoch= 51  BatchID= 20 Loss: 0.299 | Acc: 90.327% (2428/2688)
[Test] Epoch= 51  BatchID= 30 Loss: 0.362 | Acc: 88.535% (3475/3925)

Epoch: 52
[Train] Epoch= 52  BatchID= 0 Loss: 0.360 | Acc: 89.062% (114/128)
[Train] Epoch= 52  BatchID= 10 Loss: 0.352 | Acc: 88.991% (1253/1408)
[Train] Epoch= 52  BatchID= 20 Loss: 0.350 | Acc: 88.988% (2392/2688)
[Train] Epoch= 52  BatchID= 30 Loss: 0.351 | Acc: 88.962% (3530/3968)
[Train] Epoch= 52  BatchID= 40 Loss: 0.347 | Acc: 88.967% (4669/5248)
[Train] Epoch= 52  BatchID= 50 Loss: 0.338 | Acc: 89.185% (5822/6528)
[Train] Epoch= 52  BatchID= 60 Loss: 0.340 | Acc: 89.024% (6951/7808)
[Train] Epoch= 52  BatchID= 70 Loss: 0.345 | Acc: 88.919% (8081/9088)
[Test] Epoch= 52  BatchID= 0 Loss: 0.261 | Acc: 91.406% (117/128)
[Test] Epoch= 52  BatchID= 10 Loss: 0.441 | Acc: 84.943% (1196/1408)
[Test] Epoch= 52  BatchID= 20 Loss: 0.395 | Acc: 86.905% (2336/2688)
[Test] Epoch= 52  BatchID= 30 Loss: 0.359 | Acc: 88.051% (3456/3925)

Epoch: 53
[Train] Epoch= 53  BatchID= 0 Loss: 0.311 | Acc: 90.625% (116/128)
[Train] Epoch= 53  BatchID= 10 Loss: 0.360 | Acc: 88.778% (1250/1408)
[Train] Epoch= 53  BatchID= 20 Loss: 0.353 | Acc: 88.467% (2378/2688)
[Train] Epoch= 53  BatchID= 30 Loss: 0.359 | Acc: 88.432% (3509/3968)
[Train] Epoch= 53  BatchID= 40 Loss: 0.352 | Acc: 88.624% (4651/5248)
[Train] Epoch= 53  BatchID= 50 Loss: 0.350 | Acc: 88.710% (5791/6528)
[Train] Epoch= 53  BatchID= 60 Loss: 0.350 | Acc: 88.768% (6931/7808)
[Train] Epoch= 53  BatchID= 70 Loss: 0.346 | Acc: 88.908% (8080/9088)
[Test] Epoch= 53  BatchID= 0 Loss: 0.157 | Acc: 95.312% (122/128)
[Test] Epoch= 53  BatchID= 10 Loss: 0.360 | Acc: 88.210% (1242/1408)
[Test] Epoch= 53  BatchID= 20 Loss: 0.356 | Acc: 88.616% (2382/2688)
[Test] Epoch= 53  BatchID= 30 Loss: 0.353 | Acc: 88.790% (3485/3925)

Epoch: 54
[Train] Epoch= 54  BatchID= 0 Loss: 0.220 | Acc: 92.969% (119/128)
[Train] Epoch= 54  BatchID= 10 Loss: 0.322 | Acc: 89.844% (1265/1408)
[Train] Epoch= 54  BatchID= 20 Loss: 0.328 | Acc: 89.881% (2416/2688)
[Train] Epoch= 54  BatchID= 30 Loss: 0.336 | Acc: 89.567% (3554/3968)
[Train] Epoch= 54  BatchID= 40 Loss: 0.345 | Acc: 88.986% (4670/5248)
[Train] Epoch= 54  BatchID= 50 Loss: 0.342 | Acc: 89.093% (5816/6528)
[Train] Epoch= 54  BatchID= 60 Loss: 0.349 | Acc: 88.806% (6934/7808)
[Train] Epoch= 54  BatchID= 70 Loss: 0.346 | Acc: 88.820% (8072/9088)
[Test] Epoch= 54  BatchID= 0 Loss: 0.336 | Acc: 90.625% (116/128)
[Test] Epoch= 54  BatchID= 10 Loss: 0.422 | Acc: 85.866% (1209/1408)
[Test] Epoch= 54  BatchID= 20 Loss: 0.361 | Acc: 88.132% (2369/2688)
[Test] Epoch= 54  BatchID= 30 Loss: 0.385 | Acc: 87.592% (3438/3925)

Epoch: 55
[Train] Epoch= 55  BatchID= 0 Loss: 0.369 | Acc: 88.281% (113/128)
[Train] Epoch= 55  BatchID= 10 Loss: 0.320 | Acc: 89.560% (1261/1408)
[Train] Epoch= 55  BatchID= 20 Loss: 0.332 | Acc: 88.951% (2391/2688)
[Train] Epoch= 55  BatchID= 30 Loss: 0.324 | Acc: 89.214% (3540/3968)
[Train] Epoch= 55  BatchID= 40 Loss: 0.329 | Acc: 89.082% (4675/5248)
[Train] Epoch= 55  BatchID= 50 Loss: 0.329 | Acc: 88.925% (5805/6528)
[Train] Epoch= 55  BatchID= 60 Loss: 0.333 | Acc: 88.806% (6934/7808)
[Train] Epoch= 55  BatchID= 70 Loss: 0.333 | Acc: 88.820% (8072/9088)
[Test] Epoch= 55  BatchID= 0 Loss: 0.122 | Acc: 96.094% (123/128)
[Test] Epoch= 55  BatchID= 10 Loss: 0.360 | Acc: 88.565% (1247/1408)
[Test] Epoch= 55  BatchID= 20 Loss: 0.370 | Acc: 88.430% (2377/2688)
[Test] Epoch= 55  BatchID= 30 Loss: 0.400 | Acc: 87.210% (3423/3925)

Epoch: 56
[Train] Epoch= 56  BatchID= 0 Loss: 0.359 | Acc: 86.719% (111/128)
[Train] Epoch= 56  BatchID= 10 Loss: 0.297 | Acc: 90.057% (1268/1408)
[Train] Epoch= 56  BatchID= 20 Loss: 0.300 | Acc: 90.253% (2426/2688)
[Train] Epoch= 56  BatchID= 30 Loss: 0.314 | Acc: 89.869% (3566/3968)
[Train] Epoch= 56  BatchID= 40 Loss: 0.313 | Acc: 89.825% (4714/5248)
[Train] Epoch= 56  BatchID= 50 Loss: 0.313 | Acc: 90.012% (5876/6528)
[Train] Epoch= 56  BatchID= 60 Loss: 0.315 | Acc: 89.985% (7026/7808)
[Train] Epoch= 56  BatchID= 70 Loss: 0.312 | Acc: 89.987% (8178/9088)
[Test] Epoch= 56  BatchID= 0 Loss: 0.170 | Acc: 95.312% (122/128)
[Test] Epoch= 56  BatchID= 10 Loss: 0.332 | Acc: 89.986% (1267/1408)
[Test] Epoch= 56  BatchID= 20 Loss: 0.346 | Acc: 89.658% (2410/2688)
[Test] Epoch= 56  BatchID= 30 Loss: 0.394 | Acc: 88.331% (3467/3925)

Epoch: 57
[Train] Epoch= 57  BatchID= 0 Loss: 0.376 | Acc: 85.156% (109/128)
[Train] Epoch= 57  BatchID= 10 Loss: 0.309 | Acc: 90.199% (1270/1408)
[Train] Epoch= 57  BatchID= 20 Loss: 0.311 | Acc: 90.030% (2420/2688)
[Train] Epoch= 57  BatchID= 30 Loss: 0.309 | Acc: 90.247% (3581/3968)
[Train] Epoch= 57  BatchID= 40 Loss: 0.308 | Acc: 90.454% (4747/5248)
[Train] Epoch= 57  BatchID= 50 Loss: 0.307 | Acc: 90.472% (5906/6528)
[Train] Epoch= 57  BatchID= 60 Loss: 0.307 | Acc: 90.382% (7057/7808)
[Train] Epoch= 57  BatchID= 70 Loss: 0.312 | Acc: 90.141% (8192/9088)
[Test] Epoch= 57  BatchID= 0 Loss: 0.123 | Acc: 95.312% (122/128)
[Test] Epoch= 57  BatchID= 10 Loss: 0.229 | Acc: 93.750% (1320/1408)
[Test] Epoch= 57  BatchID= 20 Loss: 0.325 | Acc: 90.476% (2432/2688)
[Test] Epoch= 57  BatchID= 30 Loss: 0.395 | Acc: 88.255% (3464/3925)

Epoch: 58
[Train] Epoch= 58  BatchID= 0 Loss: 0.299 | Acc: 89.062% (114/128)
[Train] Epoch= 58  BatchID= 10 Loss: 0.303 | Acc: 89.773% (1264/1408)
[Train] Epoch= 58  BatchID= 20 Loss: 0.299 | Acc: 90.588% (2435/2688)
[Train] Epoch= 58  BatchID= 30 Loss: 0.305 | Acc: 90.323% (3584/3968)
[Train] Epoch= 58  BatchID= 40 Loss: 0.304 | Acc: 90.434% (4746/5248)
[Train] Epoch= 58  BatchID= 50 Loss: 0.298 | Acc: 90.625% (5916/6528)
[Train] Epoch= 58  BatchID= 60 Loss: 0.302 | Acc: 90.561% (7071/7808)
[Train] Epoch= 58  BatchID= 70 Loss: 0.298 | Acc: 90.702% (8243/9088)
[Test] Epoch= 58  BatchID= 0 Loss: 0.221 | Acc: 92.188% (118/128)
[Test] Epoch= 58  BatchID= 10 Loss: 0.364 | Acc: 88.849% (1251/1408)
[Test] Epoch= 58  BatchID= 20 Loss: 0.346 | Acc: 89.100% (2395/2688)
[Test] Epoch= 58  BatchID= 30 Loss: 0.362 | Acc: 88.637% (3479/3925)

Epoch: 59
[Train] Epoch= 59  BatchID= 0 Loss: 0.252 | Acc: 91.406% (117/128)
[Train] Epoch= 59  BatchID= 10 Loss: 0.240 | Acc: 92.116% (1297/1408)
[Train] Epoch= 59  BatchID= 20 Loss: 0.261 | Acc: 91.778% (2467/2688)
[Train] Epoch= 59  BatchID= 30 Loss: 0.266 | Acc: 91.557% (3633/3968)
[Train] Epoch= 59  BatchID= 40 Loss: 0.274 | Acc: 91.311% (4792/5248)
[Train] Epoch= 59  BatchID= 50 Loss: 0.274 | Acc: 91.376% (5965/6528)
[Train] Epoch= 59  BatchID= 60 Loss: 0.273 | Acc: 91.445% (7140/7808)
[Train] Epoch= 59  BatchID= 70 Loss: 0.275 | Acc: 91.318% (8299/9088)
[Test] Epoch= 59  BatchID= 0 Loss: 0.193 | Acc: 95.312% (122/128)
[Test] Epoch= 59  BatchID= 10 Loss: 0.277 | Acc: 91.335% (1286/1408)
[Test] Epoch= 59  BatchID= 20 Loss: 0.276 | Acc: 91.257% (2453/2688)
[Test] Epoch= 59  BatchID= 30 Loss: 0.341 | Acc: 89.376% (3508/3925)
Saving..
Best accuracy:  89.37579617834395

Epoch: 60
[Train] Epoch= 60  BatchID= 0 Loss: 0.194 | Acc: 93.750% (120/128)
[Train] Epoch= 60  BatchID= 10 Loss: 0.291 | Acc: 90.412% (1273/1408)
[Train] Epoch= 60  BatchID= 20 Loss: 0.288 | Acc: 90.662% (2437/2688)
[Train] Epoch= 60  BatchID= 30 Loss: 0.291 | Acc: 90.827% (3604/3968)
[Train] Epoch= 60  BatchID= 40 Loss: 0.295 | Acc: 90.568% (4753/5248)
[Train] Epoch= 60  BatchID= 50 Loss: 0.295 | Acc: 90.579% (5913/6528)
[Train] Epoch= 60  BatchID= 60 Loss: 0.293 | Acc: 90.574% (7072/7808)
[Train] Epoch= 60  BatchID= 70 Loss: 0.294 | Acc: 90.548% (8229/9088)
[Test] Epoch= 60  BatchID= 0 Loss: 0.168 | Acc: 95.312% (122/128)
[Test] Epoch= 60  BatchID= 10 Loss: 0.367 | Acc: 88.920% (1252/1408)
[Test] Epoch= 60  BatchID= 20 Loss: 0.341 | Acc: 89.658% (2410/2688)
[Test] Epoch= 60  BatchID= 30 Loss: 0.336 | Acc: 89.631% (3518/3925)
Saving..
Best accuracy:  89.63057324840764

Epoch: 61
[Train] Epoch= 61  BatchID= 0 Loss: 0.216 | Acc: 92.969% (119/128)
[Train] Epoch= 61  BatchID= 10 Loss: 0.266 | Acc: 91.903% (1294/1408)
[Train] Epoch= 61  BatchID= 20 Loss: 0.253 | Acc: 92.039% (2474/2688)
[Train] Epoch= 61  BatchID= 30 Loss: 0.265 | Acc: 91.431% (3628/3968)
[Train] Epoch= 61  BatchID= 40 Loss: 0.271 | Acc: 91.292% (4791/5248)
[Train] Epoch= 61  BatchID= 50 Loss: 0.267 | Acc: 91.483% (5972/6528)
[Train] Epoch= 61  BatchID= 60 Loss: 0.270 | Acc: 91.355% (7133/7808)
[Train] Epoch= 61  BatchID= 70 Loss: 0.272 | Acc: 91.142% (8283/9088)
[Test] Epoch= 61  BatchID= 0 Loss: 0.213 | Acc: 92.969% (119/128)
[Test] Epoch= 61  BatchID= 10 Loss: 0.385 | Acc: 87.429% (1231/1408)
[Test] Epoch= 61  BatchID= 20 Loss: 0.327 | Acc: 89.621% (2409/2688)
[Test] Epoch= 61  BatchID= 30 Loss: 0.366 | Acc: 88.561% (3476/3925)

Epoch: 62
[Train] Epoch= 62  BatchID= 0 Loss: 0.247 | Acc: 91.406% (117/128)
[Train] Epoch= 62  BatchID= 10 Loss: 0.284 | Acc: 90.128% (1269/1408)
[Train] Epoch= 62  BatchID= 20 Loss: 0.270 | Acc: 91.071% (2448/2688)
[Train] Epoch= 62  BatchID= 30 Loss: 0.272 | Acc: 91.129% (3616/3968)
[Train] Epoch= 62  BatchID= 40 Loss: 0.278 | Acc: 90.739% (4762/5248)
[Train] Epoch= 62  BatchID= 50 Loss: 0.284 | Acc: 90.717% (5922/6528)
[Train] Epoch= 62  BatchID= 60 Loss: 0.283 | Acc: 90.843% (7093/7808)
[Train] Epoch= 62  BatchID= 70 Loss: 0.284 | Acc: 90.922% (8263/9088)
[Test] Epoch= 62  BatchID= 0 Loss: 0.113 | Acc: 96.875% (124/128)
[Test] Epoch= 62  BatchID= 10 Loss: 0.288 | Acc: 90.909% (1280/1408)
[Test] Epoch= 62  BatchID= 20 Loss: 0.295 | Acc: 90.513% (2433/2688)
[Test] Epoch= 62  BatchID= 30 Loss: 0.343 | Acc: 89.350% (3507/3925)

Epoch: 63
[Train] Epoch= 63  BatchID= 0 Loss: 0.179 | Acc: 92.188% (118/128)
[Train] Epoch= 63  BatchID= 10 Loss: 0.284 | Acc: 90.625% (1276/1408)
[Train] Epoch= 63  BatchID= 20 Loss: 0.281 | Acc: 90.625% (2436/2688)
[Train] Epoch= 63  BatchID= 30 Loss: 0.276 | Acc: 91.028% (3612/3968)
[Train] Epoch= 63  BatchID= 40 Loss: 0.282 | Acc: 90.987% (4775/5248)
[Train] Epoch= 63  BatchID= 50 Loss: 0.279 | Acc: 91.284% (5959/6528)
[Train] Epoch= 63  BatchID= 60 Loss: 0.276 | Acc: 91.406% (7137/7808)
[Train] Epoch= 63  BatchID= 70 Loss: 0.277 | Acc: 91.318% (8299/9088)
[Test] Epoch= 63  BatchID= 0 Loss: 0.140 | Acc: 95.312% (122/128)
[Test] Epoch= 63  BatchID= 10 Loss: 0.304 | Acc: 90.199% (1270/1408)
[Test] Epoch= 63  BatchID= 20 Loss: 0.299 | Acc: 90.327% (2428/2688)
[Test] Epoch= 63  BatchID= 30 Loss: 0.317 | Acc: 89.885% (3528/3925)
Saving..
Best accuracy:  89.88535031847134

Epoch: 64
[Train] Epoch= 64  BatchID= 0 Loss: 0.366 | Acc: 87.500% (112/128)
[Train] Epoch= 64  BatchID= 10 Loss: 0.278 | Acc: 90.980% (1281/1408)
[Train] Epoch= 64  BatchID= 20 Loss: 0.263 | Acc: 91.443% (2458/2688)
[Train] Epoch= 64  BatchID= 30 Loss: 0.263 | Acc: 91.482% (3630/3968)
[Train] Epoch= 64  BatchID= 40 Loss: 0.264 | Acc: 91.502% (4802/5248)
[Train] Epoch= 64  BatchID= 50 Loss: 0.258 | Acc: 91.697% (5986/6528)
[Train] Epoch= 64  BatchID= 60 Loss: 0.256 | Acc: 91.637% (7155/7808)
[Train] Epoch= 64  BatchID= 70 Loss: 0.255 | Acc: 91.780% (8341/9088)
[Test] Epoch= 64  BatchID= 0 Loss: 0.137 | Acc: 96.094% (123/128)
[Test] Epoch= 64  BatchID= 10 Loss: 0.322 | Acc: 89.489% (1260/1408)
[Test] Epoch= 64  BatchID= 20 Loss: 0.305 | Acc: 90.327% (2428/2688)
[Test] Epoch= 64  BatchID= 30 Loss: 0.329 | Acc: 89.605% (3517/3925)

Epoch: 65
[Train] Epoch= 65  BatchID= 0 Loss: 0.209 | Acc: 92.969% (119/128)
[Train] Epoch= 65  BatchID= 10 Loss: 0.255 | Acc: 91.761% (1292/1408)
[Train] Epoch= 65  BatchID= 20 Loss: 0.254 | Acc: 91.778% (2467/2688)
[Train] Epoch= 65  BatchID= 30 Loss: 0.245 | Acc: 92.112% (3655/3968)
[Train] Epoch= 65  BatchID= 40 Loss: 0.247 | Acc: 92.226% (4840/5248)
[Train] Epoch= 65  BatchID= 50 Loss: 0.250 | Acc: 92.142% (6015/6528)
[Train] Epoch= 65  BatchID= 60 Loss: 0.248 | Acc: 92.213% (7200/7808)
[Train] Epoch= 65  BatchID= 70 Loss: 0.248 | Acc: 92.044% (8365/9088)
[Test] Epoch= 65  BatchID= 0 Loss: 0.137 | Acc: 94.531% (121/128)
[Test] Epoch= 65  BatchID= 10 Loss: 0.290 | Acc: 91.193% (1284/1408)
[Test] Epoch= 65  BatchID= 20 Loss: 0.269 | Acc: 91.481% (2459/2688)
[Test] Epoch= 65  BatchID= 30 Loss: 0.312 | Acc: 90.293% (3544/3925)
Saving..
Best accuracy:  90.29299363057325

Epoch: 66
[Train] Epoch= 66  BatchID= 0 Loss: 0.264 | Acc: 92.188% (118/128)
[Train] Epoch= 66  BatchID= 10 Loss: 0.243 | Acc: 92.259% (1299/1408)
[Train] Epoch= 66  BatchID= 20 Loss: 0.261 | Acc: 91.704% (2465/2688)
[Train] Epoch= 66  BatchID= 30 Loss: 0.261 | Acc: 91.658% (3637/3968)
[Train] Epoch= 66  BatchID= 40 Loss: 0.257 | Acc: 91.692% (4812/5248)
[Train] Epoch= 66  BatchID= 50 Loss: 0.255 | Acc: 91.774% (5991/6528)
[Train] Epoch= 66  BatchID= 60 Loss: 0.254 | Acc: 91.829% (7170/7808)
[Train] Epoch= 66  BatchID= 70 Loss: 0.254 | Acc: 91.824% (8345/9088)
[Test] Epoch= 66  BatchID= 0 Loss: 0.142 | Acc: 95.312% (122/128)
[Test] Epoch= 66  BatchID= 10 Loss: 0.279 | Acc: 91.335% (1286/1408)
[Test] Epoch= 66  BatchID= 20 Loss: 0.293 | Acc: 90.551% (2434/2688)
[Test] Epoch= 66  BatchID= 30 Loss: 0.309 | Acc: 90.089% (3536/3925)

Epoch: 67
[Train] Epoch= 67  BatchID= 0 Loss: 0.201 | Acc: 92.969% (119/128)
[Train] Epoch= 67  BatchID= 10 Loss: 0.218 | Acc: 92.969% (1309/1408)
[Train] Epoch= 67  BatchID= 20 Loss: 0.219 | Acc: 93.304% (2508/2688)
[Train] Epoch= 67  BatchID= 30 Loss: 0.228 | Acc: 92.969% (3689/3968)
[Train] Epoch= 67  BatchID= 40 Loss: 0.238 | Acc: 92.683% (4864/5248)
[Train] Epoch= 67  BatchID= 50 Loss: 0.238 | Acc: 92.647% (6048/6528)
[Train] Epoch= 67  BatchID= 60 Loss: 0.241 | Acc: 92.572% (7228/7808)
[Train] Epoch= 67  BatchID= 70 Loss: 0.242 | Acc: 92.485% (8405/9088)
[Test] Epoch= 67  BatchID= 0 Loss: 0.127 | Acc: 96.875% (124/128)
[Test] Epoch= 67  BatchID= 10 Loss: 0.286 | Acc: 91.122% (1283/1408)
[Test] Epoch= 67  BatchID= 20 Loss: 0.295 | Acc: 90.737% (2439/2688)
[Test] Epoch= 67  BatchID= 30 Loss: 0.316 | Acc: 90.038% (3534/3925)

Epoch: 68
[Train] Epoch= 68  BatchID= 0 Loss: 0.150 | Acc: 95.312% (122/128)
[Train] Epoch= 68  BatchID= 10 Loss: 0.235 | Acc: 92.472% (1302/1408)
[Train] Epoch= 68  BatchID= 20 Loss: 0.245 | Acc: 91.815% (2468/2688)
[Train] Epoch= 68  BatchID= 30 Loss: 0.248 | Acc: 92.011% (3651/3968)
[Train] Epoch= 68  BatchID= 40 Loss: 0.254 | Acc: 91.711% (4813/5248)
[Train] Epoch= 68  BatchID= 50 Loss: 0.246 | Acc: 91.927% (6001/6528)
[Train] Epoch= 68  BatchID= 60 Loss: 0.245 | Acc: 92.123% (7193/7808)
[Train] Epoch= 68  BatchID= 70 Loss: 0.245 | Acc: 92.221% (8381/9088)
[Test] Epoch= 68  BatchID= 0 Loss: 0.139 | Acc: 96.094% (123/128)
[Test] Epoch= 68  BatchID= 10 Loss: 0.323 | Acc: 89.702% (1263/1408)
[Test] Epoch= 68  BatchID= 20 Loss: 0.309 | Acc: 90.290% (2427/2688)
[Test] Epoch= 68  BatchID= 30 Loss: 0.316 | Acc: 90.115% (3537/3925)

Epoch: 69
[Train] Epoch= 69  BatchID= 0 Loss: 0.260 | Acc: 90.625% (116/128)
[Train] Epoch= 69  BatchID= 10 Loss: 0.211 | Acc: 93.182% (1312/1408)
[Train] Epoch= 69  BatchID= 20 Loss: 0.222 | Acc: 92.746% (2493/2688)
[Train] Epoch= 69  BatchID= 30 Loss: 0.226 | Acc: 92.792% (3682/3968)
[Train] Epoch= 69  BatchID= 40 Loss: 0.217 | Acc: 93.083% (4885/5248)
[Train] Epoch= 69  BatchID= 50 Loss: 0.216 | Acc: 93.076% (6076/6528)
[Train] Epoch= 69  BatchID= 60 Loss: 0.216 | Acc: 93.071% (7267/7808)
[Train] Epoch= 69  BatchID= 70 Loss: 0.220 | Acc: 92.969% (8449/9088)
[Test] Epoch= 69  BatchID= 0 Loss: 0.130 | Acc: 96.094% (123/128)
[Test] Epoch= 69  BatchID= 10 Loss: 0.296 | Acc: 91.264% (1285/1408)
[Test] Epoch= 69  BatchID= 20 Loss: 0.277 | Acc: 91.741% (2466/2688)
[Test] Epoch= 69  BatchID= 30 Loss: 0.296 | Acc: 90.955% (3570/3925)
Saving..
Best accuracy:  90.95541401273886

Epoch: 70
[Train] Epoch= 70  BatchID= 0 Loss: 0.187 | Acc: 94.531% (121/128)
[Train] Epoch= 70  BatchID= 10 Loss: 0.223 | Acc: 93.395% (1315/1408)
[Train] Epoch= 70  BatchID= 20 Loss: 0.213 | Acc: 93.266% (2507/2688)
[Train] Epoch= 70  BatchID= 30 Loss: 0.208 | Acc: 93.700% (3718/3968)
[Train] Epoch= 70  BatchID= 40 Loss: 0.212 | Acc: 93.369% (4900/5248)
[Train] Epoch= 70  BatchID= 50 Loss: 0.216 | Acc: 93.199% (6084/6528)
[Train] Epoch= 70  BatchID= 60 Loss: 0.217 | Acc: 93.097% (7269/7808)
[Train] Epoch= 70  BatchID= 70 Loss: 0.221 | Acc: 92.947% (8447/9088)
[Test] Epoch= 70  BatchID= 0 Loss: 0.130 | Acc: 96.094% (123/128)
[Test] Epoch= 70  BatchID= 10 Loss: 0.261 | Acc: 91.264% (1285/1408)
[Test] Epoch= 70  BatchID= 20 Loss: 0.256 | Acc: 91.592% (2462/2688)
[Test] Epoch= 70  BatchID= 30 Loss: 0.317 | Acc: 90.013% (3533/3925)

Epoch: 71
[Train] Epoch= 71  BatchID= 0 Loss: 0.146 | Acc: 94.531% (121/128)
[Train] Epoch= 71  BatchID= 10 Loss: 0.236 | Acc: 92.756% (1306/1408)
[Train] Epoch= 71  BatchID= 20 Loss: 0.222 | Acc: 93.229% (2506/2688)
[Train] Epoch= 71  BatchID= 30 Loss: 0.225 | Acc: 92.969% (3689/3968)
[Train] Epoch= 71  BatchID= 40 Loss: 0.229 | Acc: 92.873% (4874/5248)
[Train] Epoch= 71  BatchID= 50 Loss: 0.228 | Acc: 92.907% (6065/6528)
[Train] Epoch= 71  BatchID= 60 Loss: 0.230 | Acc: 92.777% (7244/7808)
[Train] Epoch= 71  BatchID= 70 Loss: 0.229 | Acc: 92.804% (8434/9088)
[Test] Epoch= 71  BatchID= 0 Loss: 0.128 | Acc: 96.094% (123/128)
[Test] Epoch= 71  BatchID= 10 Loss: 0.292 | Acc: 90.270% (1271/1408)
[Test] Epoch= 71  BatchID= 20 Loss: 0.288 | Acc: 90.885% (2443/2688)
[Test] Epoch= 71  BatchID= 30 Loss: 0.303 | Acc: 90.395% (3548/3925)

Epoch: 72
[Train] Epoch= 72  BatchID= 0 Loss: 0.221 | Acc: 93.750% (120/128)
[Train] Epoch= 72  BatchID= 10 Loss: 0.239 | Acc: 92.045% (1296/1408)
[Train] Epoch= 72  BatchID= 20 Loss: 0.218 | Acc: 92.857% (2496/2688)
[Train] Epoch= 72  BatchID= 30 Loss: 0.223 | Acc: 92.818% (3683/3968)
[Train] Epoch= 72  BatchID= 40 Loss: 0.227 | Acc: 92.740% (4867/5248)
[Train] Epoch= 72  BatchID= 50 Loss: 0.231 | Acc: 92.739% (6054/6528)
[Train] Epoch= 72  BatchID= 60 Loss: 0.226 | Acc: 93.058% (7266/7808)
[Train] Epoch= 72  BatchID= 70 Loss: 0.233 | Acc: 92.848% (8438/9088)
[Test] Epoch= 72  BatchID= 0 Loss: 0.147 | Acc: 95.312% (122/128)
[Test] Epoch= 72  BatchID= 10 Loss: 0.296 | Acc: 90.696% (1277/1408)
[Test] Epoch= 72  BatchID= 20 Loss: 0.281 | Acc: 91.034% (2447/2688)
[Test] Epoch= 72  BatchID= 30 Loss: 0.310 | Acc: 90.395% (3548/3925)

Epoch: 73
[Train] Epoch= 73  BatchID= 0 Loss: 0.194 | Acc: 91.406% (117/128)
[Train] Epoch= 73  BatchID= 10 Loss: 0.246 | Acc: 92.330% (1300/1408)
[Train] Epoch= 73  BatchID= 20 Loss: 0.239 | Acc: 92.485% (2486/2688)
[Train] Epoch= 73  BatchID= 30 Loss: 0.231 | Acc: 92.843% (3684/3968)
[Train] Epoch= 73  BatchID= 40 Loss: 0.231 | Acc: 92.873% (4874/5248)
[Train] Epoch= 73  BatchID= 50 Loss: 0.231 | Acc: 92.831% (6060/6528)
[Train] Epoch= 73  BatchID= 60 Loss: 0.228 | Acc: 92.905% (7254/7808)
[Train] Epoch= 73  BatchID= 70 Loss: 0.228 | Acc: 92.969% (8449/9088)
[Test] Epoch= 73  BatchID= 0 Loss: 0.141 | Acc: 95.312% (122/128)
[Test] Epoch= 73  BatchID= 10 Loss: 0.285 | Acc: 91.122% (1283/1408)
[Test] Epoch= 73  BatchID= 20 Loss: 0.267 | Acc: 91.481% (2459/2688)
[Test] Epoch= 73  BatchID= 30 Loss: 0.290 | Acc: 91.134% (3577/3925)
Saving..
Best accuracy:  91.13375796178345

Epoch: 74
[Train] Epoch= 74  BatchID= 0 Loss: 0.236 | Acc: 93.750% (120/128)
[Train] Epoch= 74  BatchID= 10 Loss: 0.211 | Acc: 93.608% (1318/1408)
[Train] Epoch= 74  BatchID= 20 Loss: 0.197 | Acc: 94.048% (2528/2688)
[Train] Epoch= 74  BatchID= 30 Loss: 0.200 | Acc: 93.977% (3729/3968)
[Train] Epoch= 74  BatchID= 40 Loss: 0.193 | Acc: 94.379% (4953/5248)
[Train] Epoch= 74  BatchID= 50 Loss: 0.194 | Acc: 94.271% (6154/6528)
[Train] Epoch= 74  BatchID= 60 Loss: 0.200 | Acc: 93.942% (7335/7808)
[Train] Epoch= 74  BatchID= 70 Loss: 0.200 | Acc: 93.915% (8535/9088)
[Test] Epoch= 74  BatchID= 0 Loss: 0.168 | Acc: 95.312% (122/128)
[Test] Epoch= 74  BatchID= 10 Loss: 0.293 | Acc: 90.128% (1269/1408)
[Test] Epoch= 74  BatchID= 20 Loss: 0.273 | Acc: 90.923% (2444/2688)
[Test] Epoch= 74  BatchID= 30 Loss: 0.291 | Acc: 90.420% (3549/3925)

Epoch: 75
[Train] Epoch= 75  BatchID= 0 Loss: 0.275 | Acc: 89.062% (114/128)
[Train] Epoch= 75  BatchID= 10 Loss: 0.227 | Acc: 92.969% (1309/1408)
[Train] Epoch= 75  BatchID= 20 Loss: 0.208 | Acc: 93.304% (2508/2688)
[Train] Epoch= 75  BatchID= 30 Loss: 0.207 | Acc: 93.523% (3711/3968)
[Train] Epoch= 75  BatchID= 40 Loss: 0.207 | Acc: 93.540% (4909/5248)
[Train] Epoch= 75  BatchID= 50 Loss: 0.202 | Acc: 93.689% (6116/6528)
[Train] Epoch= 75  BatchID= 60 Loss: 0.202 | Acc: 93.724% (7318/7808)
[Train] Epoch= 75  BatchID= 70 Loss: 0.203 | Acc: 93.794% (8524/9088)
[Test] Epoch= 75  BatchID= 0 Loss: 0.141 | Acc: 94.531% (121/128)
[Test] Epoch= 75  BatchID= 10 Loss: 0.280 | Acc: 91.122% (1283/1408)
[Test] Epoch= 75  BatchID= 20 Loss: 0.266 | Acc: 91.853% (2469/2688)
[Test] Epoch= 75  BatchID= 30 Loss: 0.293 | Acc: 90.930% (3569/3925)

Epoch: 76
[Train] Epoch= 76  BatchID= 0 Loss: 0.211 | Acc: 92.188% (118/128)
[Train] Epoch= 76  BatchID= 10 Loss: 0.194 | Acc: 93.750% (1320/1408)
[Train] Epoch= 76  BatchID= 20 Loss: 0.190 | Acc: 94.048% (2528/2688)
[Train] Epoch= 76  BatchID= 30 Loss: 0.195 | Acc: 93.876% (3725/3968)
[Train] Epoch= 76  BatchID= 40 Loss: 0.197 | Acc: 93.845% (4925/5248)
[Train] Epoch= 76  BatchID= 50 Loss: 0.196 | Acc: 93.995% (6136/6528)
[Train] Epoch= 76  BatchID= 60 Loss: 0.196 | Acc: 93.955% (7336/7808)
[Train] Epoch= 76  BatchID= 70 Loss: 0.197 | Acc: 93.948% (8538/9088)
[Test] Epoch= 76  BatchID= 0 Loss: 0.125 | Acc: 95.312% (122/128)
[Test] Epoch= 76  BatchID= 10 Loss: 0.289 | Acc: 90.696% (1277/1408)
[Test] Epoch= 76  BatchID= 20 Loss: 0.277 | Acc: 91.257% (2453/2688)
[Test] Epoch= 76  BatchID= 30 Loss: 0.285 | Acc: 90.930% (3569/3925)

Epoch: 77
[Train] Epoch= 77  BatchID= 0 Loss: 0.212 | Acc: 91.406% (117/128)
[Train] Epoch= 77  BatchID= 10 Loss: 0.184 | Acc: 94.176% (1326/1408)
[Train] Epoch= 77  BatchID= 20 Loss: 0.179 | Acc: 94.345% (2536/2688)
[Train] Epoch= 77  BatchID= 30 Loss: 0.194 | Acc: 93.649% (3716/3968)
[Train] Epoch= 77  BatchID= 40 Loss: 0.199 | Acc: 93.502% (4907/5248)
[Train] Epoch= 77  BatchID= 50 Loss: 0.198 | Acc: 93.536% (6106/6528)
[Train] Epoch= 77  BatchID= 60 Loss: 0.199 | Acc: 93.558% (7305/7808)
[Train] Epoch= 77  BatchID= 70 Loss: 0.199 | Acc: 93.585% (8505/9088)
[Test] Epoch= 77  BatchID= 0 Loss: 0.132 | Acc: 95.312% (122/128)
[Test] Epoch= 77  BatchID= 10 Loss: 0.269 | Acc: 91.335% (1286/1408)
[Test] Epoch= 77  BatchID= 20 Loss: 0.256 | Acc: 91.964% (2472/2688)
[Test] Epoch= 77  BatchID= 30 Loss: 0.283 | Acc: 91.057% (3574/3925)

Epoch: 78
[Train] Epoch= 78  BatchID= 0 Loss: 0.276 | Acc: 93.750% (120/128)
[Train] Epoch= 78  BatchID= 10 Loss: 0.206 | Acc: 94.105% (1325/1408)
[Train] Epoch= 78  BatchID= 20 Loss: 0.196 | Acc: 94.196% (2532/2688)
[Train] Epoch= 78  BatchID= 30 Loss: 0.198 | Acc: 94.052% (3732/3968)
[Train] Epoch= 78  BatchID= 40 Loss: 0.197 | Acc: 94.055% (4936/5248)
[Train] Epoch= 78  BatchID= 50 Loss: 0.199 | Acc: 93.888% (6129/6528)
[Train] Epoch= 78  BatchID= 60 Loss: 0.198 | Acc: 93.891% (7331/7808)
[Train] Epoch= 78  BatchID= 70 Loss: 0.199 | Acc: 93.849% (8529/9088)
[Test] Epoch= 78  BatchID= 0 Loss: 0.128 | Acc: 96.094% (123/128)
[Test] Epoch= 78  BatchID= 10 Loss: 0.275 | Acc: 91.122% (1283/1408)
[Test] Epoch= 78  BatchID= 20 Loss: 0.264 | Acc: 91.927% (2471/2688)
[Test] Epoch= 78  BatchID= 30 Loss: 0.281 | Acc: 91.287% (3583/3925)
Saving..
Best accuracy:  91.28662420382166

Epoch: 79
[Train] Epoch= 79  BatchID= 0 Loss: 0.190 | Acc: 94.531% (121/128)
[Train] Epoch= 79  BatchID= 10 Loss: 0.191 | Acc: 94.318% (1328/1408)
[Train] Epoch= 79  BatchID= 20 Loss: 0.191 | Acc: 94.345% (2536/2688)
[Train] Epoch= 79  BatchID= 30 Loss: 0.190 | Acc: 94.178% (3737/3968)
[Train] Epoch= 79  BatchID= 40 Loss: 0.187 | Acc: 94.303% (4949/5248)
[Train] Epoch= 79  BatchID= 50 Loss: 0.194 | Acc: 94.056% (6140/6528)
[Train] Epoch= 79  BatchID= 60 Loss: 0.195 | Acc: 93.993% (7339/7808)
[Train] Epoch= 79  BatchID= 70 Loss: 0.195 | Acc: 93.860% (8530/9088)
[Test] Epoch= 79  BatchID= 0 Loss: 0.123 | Acc: 95.312% (122/128)
[Test] Epoch= 79  BatchID= 10 Loss: 0.273 | Acc: 91.761% (1292/1408)
[Test] Epoch= 79  BatchID= 20 Loss: 0.260 | Acc: 92.150% (2477/2688)
[Test] Epoch= 79  BatchID= 30 Loss: 0.285 | Acc: 91.287% (3583/3925)

Epoch: 80
[Train] Epoch= 80  BatchID= 0 Loss: 0.135 | Acc: 96.875% (124/128)
[Train] Epoch= 80  BatchID= 10 Loss: 0.149 | Acc: 95.526% (1345/1408)
[Train] Epoch= 80  BatchID= 20 Loss: 0.161 | Acc: 94.978% (2553/2688)
[Train] Epoch= 80  BatchID= 30 Loss: 0.174 | Acc: 94.481% (3749/3968)
[Train] Epoch= 80  BatchID= 40 Loss: 0.177 | Acc: 94.341% (4951/5248)
[Train] Epoch= 80  BatchID= 50 Loss: 0.182 | Acc: 94.133% (6145/6528)
[Train] Epoch= 80  BatchID= 60 Loss: 0.189 | Acc: 93.942% (7335/7808)
[Train] Epoch= 80  BatchID= 70 Loss: 0.191 | Acc: 93.937% (8537/9088)
[Test] Epoch= 80  BatchID= 0 Loss: 0.109 | Acc: 96.094% (123/128)
[Test] Epoch= 80  BatchID= 10 Loss: 0.259 | Acc: 92.188% (1298/1408)
[Test] Epoch= 80  BatchID= 20 Loss: 0.257 | Acc: 92.076% (2475/2688)
[Test] Epoch= 80  BatchID= 30 Loss: 0.281 | Acc: 91.287% (3583/3925)

Epoch: 81
[Train] Epoch= 81  BatchID= 0 Loss: 0.182 | Acc: 93.750% (120/128)
[Train] Epoch= 81  BatchID= 10 Loss: 0.196 | Acc: 93.466% (1316/1408)
[Train] Epoch= 81  BatchID= 20 Loss: 0.188 | Acc: 93.899% (2524/2688)
[Train] Epoch= 81  BatchID= 30 Loss: 0.188 | Acc: 93.952% (3728/3968)
[Train] Epoch= 81  BatchID= 40 Loss: 0.184 | Acc: 94.112% (4939/5248)
[Train] Epoch= 81  BatchID= 50 Loss: 0.185 | Acc: 94.164% (6147/6528)
[Train] Epoch= 81  BatchID= 60 Loss: 0.183 | Acc: 94.275% (7361/7808)
[Train] Epoch= 81  BatchID= 70 Loss: 0.185 | Acc: 94.157% (8557/9088)
[Test] Epoch= 81  BatchID= 0 Loss: 0.151 | Acc: 95.312% (122/128)
[Test] Epoch= 81  BatchID= 10 Loss: 0.265 | Acc: 91.903% (1294/1408)
[Test] Epoch= 81  BatchID= 20 Loss: 0.253 | Acc: 92.448% (2485/2688)
[Test] Epoch= 81  BatchID= 30 Loss: 0.280 | Acc: 91.516% (3592/3925)
Saving..
Best accuracy:  91.51592356687898

Epoch: 82
[Train] Epoch= 82  BatchID= 0 Loss: 0.156 | Acc: 93.750% (120/128)
[Train] Epoch= 82  BatchID= 10 Loss: 0.180 | Acc: 94.460% (1330/1408)
[Train] Epoch= 82  BatchID= 20 Loss: 0.175 | Acc: 94.829% (2549/2688)
[Train] Epoch= 82  BatchID= 30 Loss: 0.183 | Acc: 94.405% (3746/3968)
[Train] Epoch= 82  BatchID= 40 Loss: 0.189 | Acc: 94.131% (4940/5248)
[Train] Epoch= 82  BatchID= 50 Loss: 0.188 | Acc: 94.056% (6140/6528)
[Train] Epoch= 82  BatchID= 60 Loss: 0.188 | Acc: 94.185% (7354/7808)
[Train] Epoch= 82  BatchID= 70 Loss: 0.185 | Acc: 94.256% (8566/9088)
[Test] Epoch= 82  BatchID= 0 Loss: 0.128 | Acc: 95.312% (122/128)
[Test] Epoch= 82  BatchID= 10 Loss: 0.266 | Acc: 91.548% (1289/1408)
[Test] Epoch= 82  BatchID= 20 Loss: 0.257 | Acc: 91.927% (2471/2688)
[Test] Epoch= 82  BatchID= 30 Loss: 0.279 | Acc: 91.236% (3581/3925)

Epoch: 83
[Train] Epoch= 83  BatchID= 0 Loss: 0.191 | Acc: 93.750% (120/128)
[Train] Epoch= 83  BatchID= 10 Loss: 0.203 | Acc: 93.608% (1318/1408)
[Train] Epoch= 83  BatchID= 20 Loss: 0.193 | Acc: 93.936% (2525/2688)
[Train] Epoch= 83  BatchID= 30 Loss: 0.192 | Acc: 93.851% (3724/3968)
[Train] Epoch= 83  BatchID= 40 Loss: 0.189 | Acc: 94.036% (4935/5248)
[Train] Epoch= 83  BatchID= 50 Loss: 0.185 | Acc: 94.179% (6148/6528)
[Train] Epoch= 83  BatchID= 60 Loss: 0.183 | Acc: 94.198% (7355/7808)
[Train] Epoch= 83  BatchID= 70 Loss: 0.182 | Acc: 94.245% (8565/9088)
[Test] Epoch= 83  BatchID= 0 Loss: 0.127 | Acc: 95.312% (122/128)
[Test] Epoch= 83  BatchID= 10 Loss: 0.253 | Acc: 91.761% (1292/1408)
[Test] Epoch= 83  BatchID= 20 Loss: 0.253 | Acc: 91.890% (2470/2688)
[Test] Epoch= 83  BatchID= 30 Loss: 0.275 | Acc: 91.236% (3581/3925)

Epoch: 84
[Train] Epoch= 84  BatchID= 0 Loss: 0.226 | Acc: 93.750% (120/128)
[Train] Epoch= 84  BatchID= 10 Loss: 0.202 | Acc: 93.821% (1321/1408)
[Train] Epoch= 84  BatchID= 20 Loss: 0.202 | Acc: 93.601% (2516/2688)
[Train] Epoch= 84  BatchID= 30 Loss: 0.200 | Acc: 93.750% (3720/3968)
[Train] Epoch= 84  BatchID= 40 Loss: 0.201 | Acc: 93.731% (4919/5248)
[Train] Epoch= 84  BatchID= 50 Loss: 0.199 | Acc: 93.704% (6117/6528)
[Train] Epoch= 84  BatchID= 60 Loss: 0.193 | Acc: 94.006% (7340/7808)
[Train] Epoch= 84  BatchID= 70 Loss: 0.195 | Acc: 93.981% (8541/9088)
[Test] Epoch= 84  BatchID= 0 Loss: 0.136 | Acc: 96.094% (123/128)
[Test] Epoch= 84  BatchID= 10 Loss: 0.255 | Acc: 91.974% (1295/1408)
[Test] Epoch= 84  BatchID= 20 Loss: 0.249 | Acc: 92.411% (2484/2688)
[Test] Epoch= 84  BatchID= 30 Loss: 0.275 | Acc: 91.363% (3586/3925)

Epoch: 85
[Train] Epoch= 85  BatchID= 0 Loss: 0.253 | Acc: 90.625% (116/128)
[Train] Epoch= 85  BatchID= 10 Loss: 0.191 | Acc: 93.963% (1323/1408)
[Train] Epoch= 85  BatchID= 20 Loss: 0.187 | Acc: 94.308% (2535/2688)
[Train] Epoch= 85  BatchID= 30 Loss: 0.184 | Acc: 94.229% (3739/3968)
[Train] Epoch= 85  BatchID= 40 Loss: 0.183 | Acc: 94.322% (4950/5248)
[Train] Epoch= 85  BatchID= 50 Loss: 0.190 | Acc: 94.148% (6146/6528)
[Train] Epoch= 85  BatchID= 60 Loss: 0.188 | Acc: 94.185% (7354/7808)
[Train] Epoch= 85  BatchID= 70 Loss: 0.186 | Acc: 94.245% (8565/9088)
[Test] Epoch= 85  BatchID= 0 Loss: 0.114 | Acc: 96.094% (123/128)
[Test] Epoch= 85  BatchID= 10 Loss: 0.265 | Acc: 92.045% (1296/1408)
[Test] Epoch= 85  BatchID= 20 Loss: 0.259 | Acc: 92.150% (2477/2688)
[Test] Epoch= 85  BatchID= 30 Loss: 0.275 | Acc: 91.439% (3589/3925)

Epoch: 86
[Train] Epoch= 86  BatchID= 0 Loss: 0.115 | Acc: 96.875% (124/128)
[Train] Epoch= 86  BatchID= 10 Loss: 0.181 | Acc: 94.460% (1330/1408)
[Train] Epoch= 86  BatchID= 20 Loss: 0.180 | Acc: 94.829% (2549/2688)
[Train] Epoch= 86  BatchID= 30 Loss: 0.182 | Acc: 94.657% (3756/3968)
[Train] Epoch= 86  BatchID= 40 Loss: 0.183 | Acc: 94.798% (4975/5248)
[Train] Epoch= 86  BatchID= 50 Loss: 0.190 | Acc: 94.439% (6165/6528)
[Train] Epoch= 86  BatchID= 60 Loss: 0.189 | Acc: 94.429% (7373/7808)
[Train] Epoch= 86  BatchID= 70 Loss: 0.187 | Acc: 94.443% (8583/9088)
[Test] Epoch= 86  BatchID= 0 Loss: 0.090 | Acc: 96.094% (123/128)
[Test] Epoch= 86  BatchID= 10 Loss: 0.263 | Acc: 91.903% (1294/1408)
[Test] Epoch= 86  BatchID= 20 Loss: 0.257 | Acc: 92.076% (2475/2688)
[Test] Epoch= 86  BatchID= 30 Loss: 0.278 | Acc: 91.261% (3582/3925)

Epoch: 87
[Train] Epoch= 87  BatchID= 0 Loss: 0.143 | Acc: 96.094% (123/128)
[Train] Epoch= 87  BatchID= 10 Loss: 0.166 | Acc: 95.526% (1345/1408)
[Train] Epoch= 87  BatchID= 20 Loss: 0.170 | Acc: 95.052% (2555/2688)
[Train] Epoch= 87  BatchID= 30 Loss: 0.178 | Acc: 94.355% (3744/3968)
[Train] Epoch= 87  BatchID= 40 Loss: 0.176 | Acc: 94.284% (4948/5248)
[Train] Epoch= 87  BatchID= 50 Loss: 0.175 | Acc: 94.347% (6159/6528)
[Train] Epoch= 87  BatchID= 60 Loss: 0.173 | Acc: 94.493% (7378/7808)
[Train] Epoch= 87  BatchID= 70 Loss: 0.174 | Acc: 94.531% (8591/9088)
[Test] Epoch= 87  BatchID= 0 Loss: 0.126 | Acc: 95.312% (122/128)
[Test] Epoch= 87  BatchID= 10 Loss: 0.261 | Acc: 91.903% (1294/1408)
[Test] Epoch= 87  BatchID= 20 Loss: 0.254 | Acc: 92.336% (2482/2688)
[Test] Epoch= 87  BatchID= 30 Loss: 0.273 | Acc: 91.541% (3593/3925)
Saving..
Best accuracy:  91.54140127388536

Epoch: 88
[Train] Epoch= 88  BatchID= 0 Loss: 0.211 | Acc: 92.188% (118/128)
[Train] Epoch= 88  BatchID= 10 Loss: 0.184 | Acc: 94.531% (1331/1408)
[Train] Epoch= 88  BatchID= 20 Loss: 0.175 | Acc: 94.754% (2547/2688)
[Train] Epoch= 88  BatchID= 30 Loss: 0.169 | Acc: 94.859% (3764/3968)
[Train] Epoch= 88  BatchID= 40 Loss: 0.170 | Acc: 94.779% (4974/5248)
[Train] Epoch= 88  BatchID= 50 Loss: 0.170 | Acc: 94.853% (6192/6528)
[Train] Epoch= 88  BatchID= 60 Loss: 0.171 | Acc: 94.877% (7408/7808)
[Train] Epoch= 88  BatchID= 70 Loss: 0.172 | Acc: 94.806% (8616/9088)
[Test] Epoch= 88  BatchID= 0 Loss: 0.107 | Acc: 96.875% (124/128)
[Test] Epoch= 88  BatchID= 10 Loss: 0.255 | Acc: 92.330% (1300/1408)
[Test] Epoch= 88  BatchID= 20 Loss: 0.248 | Acc: 92.671% (2491/2688)
[Test] Epoch= 88  BatchID= 30 Loss: 0.271 | Acc: 91.669% (3598/3925)
Saving..
Best accuracy:  91.6687898089172

Epoch: 89
[Train] Epoch= 89  BatchID= 0 Loss: 0.184 | Acc: 94.531% (121/128)
[Train] Epoch= 89  BatchID= 10 Loss: 0.180 | Acc: 94.247% (1327/1408)
[Train] Epoch= 89  BatchID= 20 Loss: 0.185 | Acc: 94.048% (2528/2688)
[Train] Epoch= 89  BatchID= 30 Loss: 0.174 | Acc: 94.506% (3750/3968)
[Train] Epoch= 89  BatchID= 40 Loss: 0.180 | Acc: 94.188% (4943/5248)
[Train] Epoch= 89  BatchID= 50 Loss: 0.179 | Acc: 94.225% (6151/6528)
[Train] Epoch= 89  BatchID= 60 Loss: 0.179 | Acc: 94.262% (7360/7808)
[Train] Epoch= 89  BatchID= 70 Loss: 0.175 | Acc: 94.355% (8575/9088)
[Test] Epoch= 89  BatchID= 0 Loss: 0.119 | Acc: 96.875% (124/128)
[Test] Epoch= 89  BatchID= 10 Loss: 0.278 | Acc: 91.335% (1286/1408)
[Test] Epoch= 89  BatchID= 20 Loss: 0.258 | Acc: 92.039% (2474/2688)
[Test] Epoch= 89  BatchID= 30 Loss: 0.272 | Acc: 91.490% (3591/3925)
