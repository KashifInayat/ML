==> Preparing data..
==> Building model..
DataParallel(
  (module): ResNet(
    (conv1): InputConv2dLSQ(
      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): LinearLSQ(
      in_features=512, out_features=10, bias=True
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=True)
    )
  )
)
==> Initializing from checkpoint..

Epoch: 0
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
[Train] Epoch= 0  BatchID= 0 Loss: 0.163 | Acc: 96.094% (123/128)
[Train] Epoch= 0  BatchID= 10 Loss: 0.701 | Acc: 80.185% (1129/1408)
[Train] Epoch= 0  BatchID= 20 Loss: 0.703 | Acc: 79.427% (2135/2688)
[Train] Epoch= 0  BatchID= 30 Loss: 0.702 | Acc: 78.730% (3124/3968)
[Train] Epoch= 0  BatchID= 40 Loss: 0.685 | Acc: 79.116% (4152/5248)
[Train] Epoch= 0  BatchID= 50 Loss: 0.673 | Acc: 79.259% (5174/6528)
[Train] Epoch= 0  BatchID= 60 Loss: 0.657 | Acc: 79.649% (6219/7808)
[Train] Epoch= 0  BatchID= 70 Loss: 0.643 | Acc: 80.062% (7276/9088)
[Test] Epoch= 0  BatchID= 0 Loss: 0.309 | Acc: 91.406% (117/128)
[Test] Epoch= 0  BatchID= 10 Loss: 0.476 | Acc: 84.730% (1193/1408)
[Test] Epoch= 0  BatchID= 20 Loss: 0.520 | Acc: 84.375% (2268/2688)
[Test] Epoch= 0  BatchID= 30 Loss: 0.580 | Acc: 82.548% (3240/3925)
Saving..
Best accuracy:  82.54777070063695

Epoch: 1
[Train] Epoch= 1  BatchID= 0 Loss: 0.631 | Acc: 77.344% (99/128)
[Train] Epoch= 1  BatchID= 10 Loss: 0.592 | Acc: 80.895% (1139/1408)
[Train] Epoch= 1  BatchID= 20 Loss: 0.557 | Acc: 81.473% (2190/2688)
[Train] Epoch= 1  BatchID= 30 Loss: 0.555 | Acc: 81.326% (3227/3968)
[Train] Epoch= 1  BatchID= 40 Loss: 0.556 | Acc: 81.383% (4271/5248)
[Train] Epoch= 1  BatchID= 50 Loss: 0.558 | Acc: 81.526% (5322/6528)
[Train] Epoch= 1  BatchID= 60 Loss: 0.563 | Acc: 81.429% (6358/7808)
[Train] Epoch= 1  BatchID= 70 Loss: 0.554 | Acc: 81.932% (7446/9088)
[Test] Epoch= 1  BatchID= 0 Loss: 0.357 | Acc: 90.625% (116/128)
[Test] Epoch= 1  BatchID= 10 Loss: 0.588 | Acc: 81.037% (1141/1408)
[Test] Epoch= 1  BatchID= 20 Loss: 0.636 | Acc: 79.948% (2149/2688)
[Test] Epoch= 1  BatchID= 30 Loss: 0.580 | Acc: 81.580% (3202/3925)

Epoch: 2
[Train] Epoch= 2  BatchID= 0 Loss: 0.542 | Acc: 83.594% (107/128)
[Train] Epoch= 2  BatchID= 10 Loss: 0.503 | Acc: 83.168% (1171/1408)
[Train] Epoch= 2  BatchID= 20 Loss: 0.515 | Acc: 83.631% (2248/2688)
[Train] Epoch= 2  BatchID= 30 Loss: 0.539 | Acc: 83.039% (3295/3968)
[Train] Epoch= 2  BatchID= 40 Loss: 0.552 | Acc: 82.584% (4334/5248)
[Train] Epoch= 2  BatchID= 50 Loss: 0.552 | Acc: 82.567% (5390/6528)
[Train] Epoch= 2  BatchID= 60 Loss: 0.568 | Acc: 82.108% (6411/7808)
[Train] Epoch= 2  BatchID= 70 Loss: 0.570 | Acc: 81.965% (7449/9088)
[Test] Epoch= 2  BatchID= 0 Loss: 0.213 | Acc: 92.188% (118/128)
[Test] Epoch= 2  BatchID= 10 Loss: 0.659 | Acc: 78.764% (1109/1408)
[Test] Epoch= 2  BatchID= 20 Loss: 0.667 | Acc: 78.534% (2111/2688)
[Test] Epoch= 2  BatchID= 30 Loss: 0.561 | Acc: 82.089% (3222/3925)

Epoch: 3
[Train] Epoch= 3  BatchID= 0 Loss: 0.670 | Acc: 79.688% (102/128)
[Train] Epoch= 3  BatchID= 10 Loss: 0.548 | Acc: 82.173% (1157/1408)
[Train] Epoch= 3  BatchID= 20 Loss: 0.584 | Acc: 80.952% (2176/2688)
[Train] Epoch= 3  BatchID= 30 Loss: 0.585 | Acc: 80.696% (3202/3968)
[Train] Epoch= 3  BatchID= 40 Loss: 0.595 | Acc: 80.488% (4224/5248)
[Train] Epoch= 3  BatchID= 50 Loss: 0.597 | Acc: 80.530% (5257/6528)
[Train] Epoch= 3  BatchID= 60 Loss: 0.592 | Acc: 80.751% (6305/7808)
[Train] Epoch= 3  BatchID= 70 Loss: 0.591 | Acc: 80.733% (7337/9088)
[Test] Epoch= 3  BatchID= 0 Loss: 0.761 | Acc: 80.469% (103/128)
[Test] Epoch= 3  BatchID= 10 Loss: 0.615 | Acc: 81.250% (1144/1408)
[Test] Epoch= 3  BatchID= 20 Loss: 0.692 | Acc: 79.948% (2149/2688)
[Test] Epoch= 3  BatchID= 30 Loss: 0.718 | Acc: 79.414% (3117/3925)

Epoch: 4
[Train] Epoch= 4  BatchID= 0 Loss: 0.660 | Acc: 78.906% (101/128)
[Train] Epoch= 4  BatchID= 10 Loss: 0.568 | Acc: 82.315% (1159/1408)
[Train] Epoch= 4  BatchID= 20 Loss: 0.567 | Acc: 81.845% (2200/2688)
[Train] Epoch= 4  BatchID= 30 Loss: 0.580 | Acc: 81.250% (3224/3968)
[Train] Epoch= 4  BatchID= 40 Loss: 0.588 | Acc: 80.907% (4246/5248)
[Train] Epoch= 4  BatchID= 50 Loss: 0.594 | Acc: 80.653% (5265/6528)
[Train] Epoch= 4  BatchID= 60 Loss: 0.595 | Acc: 80.763% (6306/7808)
[Train] Epoch= 4  BatchID= 70 Loss: 0.590 | Acc: 80.865% (7349/9088)
[Test] Epoch= 4  BatchID= 0 Loss: 0.235 | Acc: 92.969% (119/128)
[Test] Epoch= 4  BatchID= 10 Loss: 0.747 | Acc: 76.207% (1073/1408)
[Test] Epoch= 4  BatchID= 20 Loss: 0.862 | Acc: 74.107% (1992/2688)
[Test] Epoch= 4  BatchID= 30 Loss: 0.717 | Acc: 78.318% (3074/3925)

Epoch: 5
[Train] Epoch= 5  BatchID= 0 Loss: 0.612 | Acc: 82.031% (105/128)
[Train] Epoch= 5  BatchID= 10 Loss: 0.588 | Acc: 79.830% (1124/1408)
[Train] Epoch= 5  BatchID= 20 Loss: 0.609 | Acc: 79.948% (2149/2688)
[Train] Epoch= 5  BatchID= 30 Loss: 0.617 | Acc: 79.587% (3158/3968)
[Train] Epoch= 5  BatchID= 40 Loss: 0.607 | Acc: 80.107% (4204/5248)
[Train] Epoch= 5  BatchID= 50 Loss: 0.602 | Acc: 80.162% (5233/6528)
[Train] Epoch= 5  BatchID= 60 Loss: 0.596 | Acc: 80.328% (6272/7808)
[Train] Epoch= 5  BatchID= 70 Loss: 0.602 | Acc: 80.227% (7291/9088)
[Test] Epoch= 5  BatchID= 0 Loss: 0.159 | Acc: 94.531% (121/128)
[Test] Epoch= 5  BatchID= 10 Loss: 0.653 | Acc: 79.048% (1113/1408)
[Test] Epoch= 5  BatchID= 20 Loss: 0.709 | Acc: 77.753% (2090/2688)
[Test] Epoch= 5  BatchID= 30 Loss: 0.662 | Acc: 79.439% (3118/3925)

Epoch: 6
[Train] Epoch= 6  BatchID= 0 Loss: 0.672 | Acc: 78.906% (101/128)
[Train] Epoch= 6  BatchID= 10 Loss: 0.577 | Acc: 81.179% (1143/1408)
[Train] Epoch= 6  BatchID= 20 Loss: 0.592 | Acc: 80.060% (2152/2688)
[Train] Epoch= 6  BatchID= 30 Loss: 0.590 | Acc: 80.368% (3189/3968)
[Train] Epoch= 6  BatchID= 40 Loss: 0.602 | Acc: 80.088% (4203/5248)
[Train] Epoch= 6  BatchID= 50 Loss: 0.606 | Acc: 79.917% (5217/6528)
[Train] Epoch= 6  BatchID= 60 Loss: 0.600 | Acc: 80.020% (6248/7808)
[Train] Epoch= 6  BatchID= 70 Loss: 0.599 | Acc: 80.150% (7284/9088)
[Test] Epoch= 6  BatchID= 0 Loss: 0.126 | Acc: 96.875% (124/128)
[Test] Epoch= 6  BatchID= 10 Loss: 0.543 | Acc: 82.031% (1155/1408)
[Test] Epoch= 6  BatchID= 20 Loss: 0.615 | Acc: 80.171% (2155/2688)
[Test] Epoch= 6  BatchID= 30 Loss: 0.693 | Acc: 78.064% (3064/3925)

Epoch: 7
[Train] Epoch= 7  BatchID= 0 Loss: 0.593 | Acc: 82.812% (106/128)
[Train] Epoch= 7  BatchID= 10 Loss: 0.558 | Acc: 83.168% (1171/1408)
[Train] Epoch= 7  BatchID= 20 Loss: 0.571 | Acc: 82.292% (2212/2688)
[Train] Epoch= 7  BatchID= 30 Loss: 0.577 | Acc: 82.031% (3255/3968)
[Train] Epoch= 7  BatchID= 40 Loss: 0.580 | Acc: 82.031% (4305/5248)
[Train] Epoch= 7  BatchID= 50 Loss: 0.588 | Acc: 81.510% (5321/6528)
[Train] Epoch= 7  BatchID= 60 Loss: 0.587 | Acc: 81.352% (6352/7808)
[Train] Epoch= 7  BatchID= 70 Loss: 0.593 | Acc: 81.195% (7379/9088)
[Test] Epoch= 7  BatchID= 0 Loss: 0.388 | Acc: 87.500% (112/128)
[Test] Epoch= 7  BatchID= 10 Loss: 0.493 | Acc: 83.239% (1172/1408)
[Test] Epoch= 7  BatchID= 20 Loss: 0.626 | Acc: 78.832% (2119/2688)
[Test] Epoch= 7  BatchID= 30 Loss: 0.634 | Acc: 79.541% (3122/3925)

Epoch: 8
[Train] Epoch= 8  BatchID= 0 Loss: 0.597 | Acc: 78.906% (101/128)
[Train] Epoch= 8  BatchID= 10 Loss: 0.615 | Acc: 79.688% (1122/1408)
[Train] Epoch= 8  BatchID= 20 Loss: 0.597 | Acc: 80.394% (2161/2688)
[Train] Epoch= 8  BatchID= 30 Loss: 0.593 | Acc: 80.620% (3199/3968)
[Train] Epoch= 8  BatchID= 40 Loss: 0.591 | Acc: 80.507% (4225/5248)
[Train] Epoch= 8  BatchID= 50 Loss: 0.595 | Acc: 80.407% (5249/6528)
[Train] Epoch= 8  BatchID= 60 Loss: 0.600 | Acc: 80.251% (6266/7808)
[Train] Epoch= 8  BatchID= 70 Loss: 0.605 | Acc: 80.062% (7276/9088)
[Test] Epoch= 8  BatchID= 0 Loss: 0.179 | Acc: 93.750% (120/128)
[Test] Epoch= 8  BatchID= 10 Loss: 0.450 | Acc: 84.233% (1186/1408)
[Test] Epoch= 8  BatchID= 20 Loss: 0.509 | Acc: 83.259% (2238/2688)
[Test] Epoch= 8  BatchID= 30 Loss: 0.602 | Acc: 80.790% (3171/3925)

Epoch: 9
[Train] Epoch= 9  BatchID= 0 Loss: 0.526 | Acc: 83.594% (107/128)
[Train] Epoch= 9  BatchID= 10 Loss: 0.618 | Acc: 80.256% (1130/1408)
[Train] Epoch= 9  BatchID= 20 Loss: 0.597 | Acc: 80.841% (2173/2688)
[Train] Epoch= 9  BatchID= 30 Loss: 0.594 | Acc: 80.847% (3208/3968)
[Train] Epoch= 9  BatchID= 40 Loss: 0.597 | Acc: 80.888% (4245/5248)
[Train] Epoch= 9  BatchID= 50 Loss: 0.600 | Acc: 80.699% (5268/6528)
[Train] Epoch= 9  BatchID= 60 Loss: 0.602 | Acc: 80.699% (6301/7808)
[Train] Epoch= 9  BatchID= 70 Loss: 0.605 | Acc: 80.546% (7320/9088)
[Test] Epoch= 9  BatchID= 0 Loss: 0.769 | Acc: 77.344% (99/128)
[Test] Epoch= 9  BatchID= 10 Loss: 1.020 | Acc: 69.886% (984/1408)
[Test] Epoch= 9  BatchID= 20 Loss: 0.902 | Acc: 73.065% (1964/2688)
[Test] Epoch= 9  BatchID= 30 Loss: 0.831 | Acc: 75.108% (2948/3925)

Epoch: 10
[Train] Epoch= 10  BatchID= 0 Loss: 0.604 | Acc: 82.031% (105/128)
[Train] Epoch= 10  BatchID= 10 Loss: 0.621 | Acc: 79.332% (1117/1408)
[Train] Epoch= 10  BatchID= 20 Loss: 0.598 | Acc: 80.804% (2172/2688)
[Train] Epoch= 10  BatchID= 30 Loss: 0.604 | Acc: 80.519% (3195/3968)
[Train] Epoch= 10  BatchID= 40 Loss: 0.611 | Acc: 80.297% (4214/5248)
[Train] Epoch= 10  BatchID= 50 Loss: 0.610 | Acc: 80.423% (5250/6528)
[Train] Epoch= 10  BatchID= 60 Loss: 0.611 | Acc: 80.456% (6282/7808)
[Train] Epoch= 10  BatchID= 70 Loss: 0.610 | Acc: 80.557% (7321/9088)
[Test] Epoch= 10  BatchID= 0 Loss: 0.253 | Acc: 91.406% (117/128)
[Test] Epoch= 10  BatchID= 10 Loss: 0.522 | Acc: 84.233% (1186/1408)
[Test] Epoch= 10  BatchID= 20 Loss: 0.538 | Acc: 83.110% (2234/2688)
[Test] Epoch= 10  BatchID= 30 Loss: 0.607 | Acc: 81.554% (3201/3925)

Epoch: 11
[Train] Epoch= 11  BatchID= 0 Loss: 0.440 | Acc: 85.156% (109/128)
[Train] Epoch= 11  BatchID= 10 Loss: 0.601 | Acc: 80.966% (1140/1408)
[Train] Epoch= 11  BatchID= 20 Loss: 0.593 | Acc: 81.324% (2186/2688)
[Train] Epoch= 11  BatchID= 30 Loss: 0.597 | Acc: 80.721% (3203/3968)
[Train] Epoch= 11  BatchID= 40 Loss: 0.582 | Acc: 81.002% (4251/5248)
[Train] Epoch= 11  BatchID= 50 Loss: 0.586 | Acc: 80.867% (5279/6528)
[Train] Epoch= 11  BatchID= 60 Loss: 0.583 | Acc: 81.224% (6342/7808)
[Train] Epoch= 11  BatchID= 70 Loss: 0.587 | Acc: 81.063% (7367/9088)
[Test] Epoch= 11  BatchID= 0 Loss: 0.324 | Acc: 89.844% (115/128)
[Test] Epoch= 11  BatchID= 10 Loss: 0.580 | Acc: 81.037% (1141/1408)
[Test] Epoch= 11  BatchID= 20 Loss: 0.552 | Acc: 81.771% (2198/2688)
[Test] Epoch= 11  BatchID= 30 Loss: 0.614 | Acc: 80.025% (3141/3925)

Epoch: 12
[Train] Epoch= 12  BatchID= 0 Loss: 0.578 | Acc: 80.469% (103/128)
[Train] Epoch= 12  BatchID= 10 Loss: 0.546 | Acc: 82.173% (1157/1408)
[Train] Epoch= 12  BatchID= 20 Loss: 0.543 | Acc: 82.403% (2215/2688)
[Train] Epoch= 12  BatchID= 30 Loss: 0.562 | Acc: 81.804% (3246/3968)
[Train] Epoch= 12  BatchID= 40 Loss: 0.576 | Acc: 81.231% (4263/5248)
[Train] Epoch= 12  BatchID= 50 Loss: 0.589 | Acc: 80.990% (5287/6528)
[Train] Epoch= 12  BatchID= 60 Loss: 0.590 | Acc: 80.968% (6322/7808)
[Train] Epoch= 12  BatchID= 70 Loss: 0.589 | Acc: 80.975% (7359/9088)
[Test] Epoch= 12  BatchID= 0 Loss: 0.157 | Acc: 96.094% (123/128)
[Test] Epoch= 12  BatchID= 10 Loss: 0.426 | Acc: 86.861% (1223/1408)
[Test] Epoch= 12  BatchID= 20 Loss: 0.421 | Acc: 86.793% (2333/2688)
[Test] Epoch= 12  BatchID= 30 Loss: 0.560 | Acc: 82.217% (3227/3925)

Epoch: 13
[Train] Epoch= 13  BatchID= 0 Loss: 0.695 | Acc: 78.125% (100/128)
[Train] Epoch= 13  BatchID= 10 Loss: 0.577 | Acc: 81.676% (1150/1408)
[Train] Epoch= 13  BatchID= 20 Loss: 0.594 | Acc: 80.990% (2177/2688)
[Train] Epoch= 13  BatchID= 30 Loss: 0.575 | Acc: 81.376% (3229/3968)
[Train] Epoch= 13  BatchID= 40 Loss: 0.575 | Acc: 81.498% (4277/5248)
[Train] Epoch= 13  BatchID= 50 Loss: 0.573 | Acc: 81.740% (5336/6528)
[Train] Epoch= 13  BatchID= 60 Loss: 0.580 | Acc: 81.442% (6359/7808)
[Train] Epoch= 13  BatchID= 70 Loss: 0.587 | Acc: 81.063% (7367/9088)
[Test] Epoch= 13  BatchID= 0 Loss: 0.219 | Acc: 95.312% (122/128)
[Test] Epoch= 13  BatchID= 10 Loss: 0.584 | Acc: 80.540% (1134/1408)
[Test] Epoch= 13  BatchID= 20 Loss: 0.534 | Acc: 83.110% (2234/2688)
[Test] Epoch= 13  BatchID= 30 Loss: 0.592 | Acc: 81.401% (3195/3925)

Epoch: 14
[Train] Epoch= 14  BatchID= 0 Loss: 0.569 | Acc: 81.250% (104/128)
[Train] Epoch= 14  BatchID= 10 Loss: 0.589 | Acc: 80.327% (1131/1408)
[Train] Epoch= 14  BatchID= 20 Loss: 0.579 | Acc: 80.804% (2172/2688)
[Train] Epoch= 14  BatchID= 30 Loss: 0.592 | Acc: 80.696% (3202/3968)
[Train] Epoch= 14  BatchID= 40 Loss: 0.595 | Acc: 80.850% (4243/5248)
[Train] Epoch= 14  BatchID= 50 Loss: 0.582 | Acc: 81.311% (5308/6528)
[Train] Epoch= 14  BatchID= 60 Loss: 0.585 | Acc: 81.122% (6334/7808)
[Train] Epoch= 14  BatchID= 70 Loss: 0.591 | Acc: 81.019% (7363/9088)
[Test] Epoch= 14  BatchID= 0 Loss: 0.138 | Acc: 96.094% (123/128)
[Test] Epoch= 14  BatchID= 10 Loss: 0.465 | Acc: 84.659% (1192/1408)
[Test] Epoch= 14  BatchID= 20 Loss: 0.447 | Acc: 85.379% (2295/2688)
[Test] Epoch= 14  BatchID= 30 Loss: 0.477 | Acc: 84.790% (3328/3925)
Saving..
Best accuracy:  84.78980891719745

Epoch: 15
[Train] Epoch= 15  BatchID= 0 Loss: 0.601 | Acc: 78.906% (101/128)
[Train] Epoch= 15  BatchID= 10 Loss: 0.520 | Acc: 82.102% (1156/1408)
[Train] Epoch= 15  BatchID= 20 Loss: 0.558 | Acc: 81.808% (2199/2688)
[Train] Epoch= 15  BatchID= 30 Loss: 0.573 | Acc: 81.678% (3241/3968)
[Train] Epoch= 15  BatchID= 40 Loss: 0.570 | Acc: 81.860% (4296/5248)
[Train] Epoch= 15  BatchID= 50 Loss: 0.568 | Acc: 81.817% (5341/6528)
[Train] Epoch= 15  BatchID= 60 Loss: 0.572 | Acc: 81.506% (6364/7808)
[Train] Epoch= 15  BatchID= 70 Loss: 0.573 | Acc: 81.547% (7411/9088)
[Test] Epoch= 15  BatchID= 0 Loss: 0.306 | Acc: 90.625% (116/128)
[Test] Epoch= 15  BatchID= 10 Loss: 0.628 | Acc: 78.480% (1105/1408)
[Test] Epoch= 15  BatchID= 20 Loss: 0.633 | Acc: 78.609% (2113/2688)
[Test] Epoch= 15  BatchID= 30 Loss: 0.591 | Acc: 80.076% (3143/3925)

Epoch: 16
[Train] Epoch= 16  BatchID= 0 Loss: 0.468 | Acc: 85.938% (110/128)
[Train] Epoch= 16  BatchID= 10 Loss: 0.544 | Acc: 82.315% (1159/1408)
[Train] Epoch= 16  BatchID= 20 Loss: 0.544 | Acc: 81.808% (2199/2688)
[Train] Epoch= 16  BatchID= 30 Loss: 0.556 | Acc: 81.552% (3236/3968)
[Train] Epoch= 16  BatchID= 40 Loss: 0.564 | Acc: 81.441% (4274/5248)
[Train] Epoch= 16  BatchID= 50 Loss: 0.567 | Acc: 81.419% (5315/6528)
[Train] Epoch= 16  BatchID= 60 Loss: 0.568 | Acc: 81.416% (6357/7808)
[Train] Epoch= 16  BatchID= 70 Loss: 0.569 | Acc: 81.327% (7391/9088)
[Test] Epoch= 16  BatchID= 0 Loss: 0.332 | Acc: 90.625% (116/128)
[Test] Epoch= 16  BatchID= 10 Loss: 0.548 | Acc: 82.244% (1158/1408)
[Test] Epoch= 16  BatchID= 20 Loss: 0.523 | Acc: 83.482% (2244/2688)
[Test] Epoch= 16  BatchID= 30 Loss: 0.502 | Acc: 83.796% (3289/3925)

Epoch: 17
[Train] Epoch= 17  BatchID= 0 Loss: 0.590 | Acc: 81.250% (104/128)
[Train] Epoch= 17  BatchID= 10 Loss: 0.528 | Acc: 81.960% (1154/1408)
[Train] Epoch= 17  BatchID= 20 Loss: 0.530 | Acc: 82.329% (2213/2688)
[Train] Epoch= 17  BatchID= 30 Loss: 0.541 | Acc: 81.981% (3253/3968)
[Train] Epoch= 17  BatchID= 40 Loss: 0.548 | Acc: 81.688% (4287/5248)
[Train] Epoch= 17  BatchID= 50 Loss: 0.557 | Acc: 81.587% (5326/6528)
[Train] Epoch= 17  BatchID= 60 Loss: 0.569 | Acc: 81.263% (6345/7808)
[Train] Epoch= 17  BatchID= 70 Loss: 0.575 | Acc: 81.239% (7383/9088)
[Test] Epoch= 17  BatchID= 0 Loss: 0.126 | Acc: 97.656% (125/128)
[Test] Epoch= 17  BatchID= 10 Loss: 0.571 | Acc: 81.392% (1146/1408)
[Test] Epoch= 17  BatchID= 20 Loss: 0.564 | Acc: 82.366% (2214/2688)
[Test] Epoch= 17  BatchID= 30 Loss: 0.543 | Acc: 82.803% (3250/3925)

Epoch: 18
[Train] Epoch= 18  BatchID= 0 Loss: 0.696 | Acc: 78.906% (101/128)
[Train] Epoch= 18  BatchID= 10 Loss: 0.565 | Acc: 81.392% (1146/1408)
[Train] Epoch= 18  BatchID= 20 Loss: 0.539 | Acc: 82.664% (2222/2688)
[Train] Epoch= 18  BatchID= 30 Loss: 0.534 | Acc: 82.611% (3278/3968)
[Train] Epoch= 18  BatchID= 40 Loss: 0.531 | Acc: 82.812% (4346/5248)
[Train] Epoch= 18  BatchID= 50 Loss: 0.539 | Acc: 82.675% (5397/6528)
[Train] Epoch= 18  BatchID= 60 Loss: 0.556 | Acc: 82.236% (6421/7808)
[Train] Epoch= 18  BatchID= 70 Loss: 0.564 | Acc: 81.987% (7451/9088)
[Test] Epoch= 18  BatchID= 0 Loss: 0.229 | Acc: 92.969% (119/128)
[Test] Epoch= 18  BatchID= 10 Loss: 0.406 | Acc: 86.719% (1221/1408)
[Test] Epoch= 18  BatchID= 20 Loss: 0.431 | Acc: 85.565% (2300/2688)
[Test] Epoch= 18  BatchID= 30 Loss: 0.508 | Acc: 83.057% (3260/3925)

Epoch: 19
[Train] Epoch= 19  BatchID= 0 Loss: 0.588 | Acc: 81.250% (104/128)
[Train] Epoch= 19  BatchID= 10 Loss: 0.542 | Acc: 82.457% (1161/1408)
[Train] Epoch= 19  BatchID= 20 Loss: 0.552 | Acc: 81.957% (2203/2688)
[Train] Epoch= 19  BatchID= 30 Loss: 0.547 | Acc: 82.359% (3268/3968)
[Train] Epoch= 19  BatchID= 40 Loss: 0.548 | Acc: 82.203% (4314/5248)
[Train] Epoch= 19  BatchID= 50 Loss: 0.543 | Acc: 82.414% (5380/6528)
[Train] Epoch= 19  BatchID= 60 Loss: 0.548 | Acc: 82.441% (6437/7808)
[Train] Epoch= 19  BatchID= 70 Loss: 0.552 | Acc: 82.427% (7491/9088)
[Test] Epoch= 19  BatchID= 0 Loss: 0.409 | Acc: 87.500% (112/128)
[Test] Epoch= 19  BatchID= 10 Loss: 0.572 | Acc: 80.611% (1135/1408)
[Test] Epoch= 19  BatchID= 20 Loss: 0.621 | Acc: 79.501% (2137/2688)
[Test] Epoch= 19  BatchID= 30 Loss: 0.587 | Acc: 80.764% (3170/3925)

Epoch: 20
[Train] Epoch= 20  BatchID= 0 Loss: 0.621 | Acc: 77.344% (99/128)
[Train] Epoch= 20  BatchID= 10 Loss: 0.498 | Acc: 83.665% (1178/1408)
[Train] Epoch= 20  BatchID= 20 Loss: 0.510 | Acc: 82.999% (2231/2688)
[Train] Epoch= 20  BatchID= 30 Loss: 0.506 | Acc: 83.317% (3306/3968)
[Train] Epoch= 20  BatchID= 40 Loss: 0.518 | Acc: 82.793% (4345/5248)
[Train] Epoch= 20  BatchID= 50 Loss: 0.540 | Acc: 82.215% (5367/6528)
[Train] Epoch= 20  BatchID= 60 Loss: 0.550 | Acc: 81.839% (6390/7808)
[Train] Epoch= 20  BatchID= 70 Loss: 0.555 | Acc: 81.723% (7427/9088)
[Test] Epoch= 20  BatchID= 0 Loss: 0.105 | Acc: 96.875% (124/128)
[Test] Epoch= 20  BatchID= 10 Loss: 0.386 | Acc: 86.790% (1222/1408)
[Test] Epoch= 20  BatchID= 20 Loss: 0.420 | Acc: 85.714% (2304/2688)
[Test] Epoch= 20  BatchID= 30 Loss: 0.491 | Acc: 84.025% (3298/3925)

Epoch: 21
[Train] Epoch= 21  BatchID= 0 Loss: 0.531 | Acc: 82.031% (105/128)
[Train] Epoch= 21  BatchID= 10 Loss: 0.509 | Acc: 83.452% (1175/1408)
[Train] Epoch= 21  BatchID= 20 Loss: 0.527 | Acc: 83.594% (2247/2688)
[Train] Epoch= 21  BatchID= 30 Loss: 0.528 | Acc: 83.594% (3317/3968)
[Train] Epoch= 21  BatchID= 40 Loss: 0.544 | Acc: 82.870% (4349/5248)
[Train] Epoch= 21  BatchID= 50 Loss: 0.548 | Acc: 82.675% (5397/6528)
[Train] Epoch= 21  BatchID= 60 Loss: 0.549 | Acc: 82.595% (6449/7808)
[Train] Epoch= 21  BatchID= 70 Loss: 0.554 | Acc: 82.548% (7502/9088)
[Test] Epoch= 21  BatchID= 0 Loss: 0.175 | Acc: 92.969% (119/128)
[Test] Epoch= 21  BatchID= 10 Loss: 0.571 | Acc: 81.392% (1146/1408)
[Test] Epoch= 21  BatchID= 20 Loss: 0.599 | Acc: 81.064% (2179/2688)
[Test] Epoch= 21  BatchID= 30 Loss: 0.577 | Acc: 81.580% (3202/3925)

Epoch: 22
[Train] Epoch= 22  BatchID= 0 Loss: 0.570 | Acc: 82.031% (105/128)
[Train] Epoch= 22  BatchID= 10 Loss: 0.497 | Acc: 83.310% (1173/1408)
[Train] Epoch= 22  BatchID= 20 Loss: 0.510 | Acc: 82.850% (2227/2688)
[Train] Epoch= 22  BatchID= 30 Loss: 0.520 | Acc: 82.863% (3288/3968)
[Train] Epoch= 22  BatchID= 40 Loss: 0.521 | Acc: 82.851% (4348/5248)
[Train] Epoch= 22  BatchID= 50 Loss: 0.525 | Acc: 82.858% (5409/6528)
[Train] Epoch= 22  BatchID= 60 Loss: 0.528 | Acc: 82.889% (6472/7808)
[Train] Epoch= 22  BatchID= 70 Loss: 0.537 | Acc: 82.614% (7508/9088)
[Test] Epoch= 22  BatchID= 0 Loss: 0.917 | Acc: 77.344% (99/128)
[Test] Epoch= 22  BatchID= 10 Loss: 0.706 | Acc: 78.764% (1109/1408)
[Test] Epoch= 22  BatchID= 20 Loss: 0.659 | Acc: 79.427% (2135/2688)
[Test] Epoch= 22  BatchID= 30 Loss: 0.643 | Acc: 80.076% (3143/3925)

Epoch: 23
[Train] Epoch= 23  BatchID= 0 Loss: 0.509 | Acc: 80.469% (103/128)
[Train] Epoch= 23  BatchID= 10 Loss: 0.565 | Acc: 81.605% (1149/1408)
[Train] Epoch= 23  BatchID= 20 Loss: 0.545 | Acc: 82.106% (2207/2688)
[Train] Epoch= 23  BatchID= 30 Loss: 0.548 | Acc: 82.031% (3255/3968)
[Train] Epoch= 23  BatchID= 40 Loss: 0.541 | Acc: 82.431% (4326/5248)
[Train] Epoch= 23  BatchID= 50 Loss: 0.541 | Acc: 82.399% (5379/6528)
[Train] Epoch= 23  BatchID= 60 Loss: 0.542 | Acc: 82.582% (6448/7808)
[Train] Epoch= 23  BatchID= 70 Loss: 0.537 | Acc: 82.735% (7519/9088)
[Test] Epoch= 23  BatchID= 0 Loss: 0.108 | Acc: 96.875% (124/128)
[Test] Epoch= 23  BatchID= 10 Loss: 0.399 | Acc: 86.364% (1216/1408)
[Test] Epoch= 23  BatchID= 20 Loss: 0.465 | Acc: 85.045% (2286/2688)
[Test] Epoch= 23  BatchID= 30 Loss: 0.478 | Acc: 84.586% (3320/3925)

Epoch: 24
[Train] Epoch= 24  BatchID= 0 Loss: 0.376 | Acc: 87.500% (112/128)
[Train] Epoch= 24  BatchID= 10 Loss: 0.536 | Acc: 82.457% (1161/1408)
[Train] Epoch= 24  BatchID= 20 Loss: 0.528 | Acc: 83.519% (2245/2688)
[Train] Epoch= 24  BatchID= 30 Loss: 0.527 | Acc: 83.090% (3297/3968)
[Train] Epoch= 24  BatchID= 40 Loss: 0.532 | Acc: 82.641% (4337/5248)
[Train] Epoch= 24  BatchID= 50 Loss: 0.542 | Acc: 82.414% (5380/6528)
[Train] Epoch= 24  BatchID= 60 Loss: 0.545 | Acc: 82.262% (6423/7808)
[Train] Epoch= 24  BatchID= 70 Loss: 0.542 | Acc: 82.427% (7491/9088)
[Test] Epoch= 24  BatchID= 0 Loss: 0.249 | Acc: 92.188% (118/128)
[Test] Epoch= 24  BatchID= 10 Loss: 0.511 | Acc: 84.517% (1190/1408)
[Test] Epoch= 24  BatchID= 20 Loss: 0.448 | Acc: 86.310% (2320/2688)
[Test] Epoch= 24  BatchID= 30 Loss: 0.518 | Acc: 84.153% (3303/3925)

Epoch: 25
[Train] Epoch= 25  BatchID= 0 Loss: 0.555 | Acc: 79.688% (102/128)
[Train] Epoch= 25  BatchID= 10 Loss: 0.508 | Acc: 83.310% (1173/1408)
[Train] Epoch= 25  BatchID= 20 Loss: 0.498 | Acc: 84.003% (2258/2688)
[Train] Epoch= 25  BatchID= 30 Loss: 0.498 | Acc: 83.745% (3323/3968)
[Train] Epoch= 25  BatchID= 40 Loss: 0.520 | Acc: 83.232% (4368/5248)
[Train] Epoch= 25  BatchID= 50 Loss: 0.531 | Acc: 82.981% (5417/6528)
[Train] Epoch= 25  BatchID= 60 Loss: 0.530 | Acc: 82.966% (6478/7808)
[Train] Epoch= 25  BatchID= 70 Loss: 0.528 | Acc: 82.967% (7540/9088)
[Test] Epoch= 25  BatchID= 0 Loss: 0.144 | Acc: 94.531% (121/128)
[Test] Epoch= 25  BatchID= 10 Loss: 0.539 | Acc: 82.528% (1162/1408)
[Test] Epoch= 25  BatchID= 20 Loss: 0.462 | Acc: 84.821% (2280/2688)
[Test] Epoch= 25  BatchID= 30 Loss: 0.484 | Acc: 84.586% (3320/3925)

Epoch: 26
[Train] Epoch= 26  BatchID= 0 Loss: 0.394 | Acc: 88.281% (113/128)
[Train] Epoch= 26  BatchID= 10 Loss: 0.494 | Acc: 83.736% (1179/1408)
[Train] Epoch= 26  BatchID= 20 Loss: 0.476 | Acc: 84.077% (2260/2688)
[Train] Epoch= 26  BatchID= 30 Loss: 0.485 | Acc: 83.972% (3332/3968)
[Train] Epoch= 26  BatchID= 40 Loss: 0.495 | Acc: 83.575% (4386/5248)
[Train] Epoch= 26  BatchID= 50 Loss: 0.503 | Acc: 83.287% (5437/6528)
[Train] Epoch= 26  BatchID= 60 Loss: 0.502 | Acc: 83.389% (6511/7808)
[Train] Epoch= 26  BatchID= 70 Loss: 0.508 | Acc: 83.407% (7580/9088)
[Test] Epoch= 26  BatchID= 0 Loss: 0.232 | Acc: 93.750% (120/128)
[Test] Epoch= 26  BatchID= 10 Loss: 0.653 | Acc: 78.977% (1112/1408)
[Test] Epoch= 26  BatchID= 20 Loss: 0.586 | Acc: 80.729% (2170/2688)
[Test] Epoch= 26  BatchID= 30 Loss: 0.536 | Acc: 82.522% (3239/3925)

Epoch: 27
[Train] Epoch= 27  BatchID= 0 Loss: 0.477 | Acc: 84.375% (108/128)
[Train] Epoch= 27  BatchID= 10 Loss: 0.525 | Acc: 82.315% (1159/1408)
[Train] Epoch= 27  BatchID= 20 Loss: 0.526 | Acc: 82.626% (2221/2688)
[Train] Epoch= 27  BatchID= 30 Loss: 0.524 | Acc: 82.762% (3284/3968)
[Train] Epoch= 27  BatchID= 40 Loss: 0.514 | Acc: 83.175% (4365/5248)
[Train] Epoch= 27  BatchID= 50 Loss: 0.505 | Acc: 83.502% (5451/6528)
[Train] Epoch= 27  BatchID= 60 Loss: 0.509 | Acc: 83.543% (6523/7808)
[Train] Epoch= 27  BatchID= 70 Loss: 0.508 | Acc: 83.506% (7589/9088)
[Test] Epoch= 27  BatchID= 0 Loss: 0.298 | Acc: 89.844% (115/128)
[Test] Epoch= 27  BatchID= 10 Loss: 0.403 | Acc: 85.938% (1210/1408)
[Test] Epoch= 27  BatchID= 20 Loss: 0.436 | Acc: 85.231% (2291/2688)
[Test] Epoch= 27  BatchID= 30 Loss: 0.460 | Acc: 84.815% (3329/3925)
Saving..
Best accuracy:  84.81528662420382

Epoch: 28
[Train] Epoch= 28  BatchID= 0 Loss: 0.534 | Acc: 80.469% (103/128)
[Train] Epoch= 28  BatchID= 10 Loss: 0.438 | Acc: 85.440% (1203/1408)
[Train] Epoch= 28  BatchID= 20 Loss: 0.458 | Acc: 85.751% (2305/2688)
[Train] Epoch= 28  BatchID= 30 Loss: 0.474 | Acc: 84.929% (3370/3968)
[Train] Epoch= 28  BatchID= 40 Loss: 0.474 | Acc: 84.832% (4452/5248)
[Train] Epoch= 28  BatchID= 50 Loss: 0.481 | Acc: 84.727% (5531/6528)
[Train] Epoch= 28  BatchID= 60 Loss: 0.486 | Acc: 84.606% (6606/7808)
[Train] Epoch= 28  BatchID= 70 Loss: 0.484 | Acc: 84.496% (7679/9088)
[Test] Epoch= 28  BatchID= 0 Loss: 0.342 | Acc: 90.625% (116/128)
[Test] Epoch= 28  BatchID= 10 Loss: 0.422 | Acc: 87.145% (1227/1408)
[Test] Epoch= 28  BatchID= 20 Loss: 0.397 | Acc: 87.500% (2352/2688)
[Test] Epoch= 28  BatchID= 30 Loss: 0.447 | Acc: 85.631% (3361/3925)
Saving..
Best accuracy:  85.63057324840764

Epoch: 29
[Train] Epoch= 29  BatchID= 0 Loss: 0.413 | Acc: 85.156% (109/128)
[Train] Epoch= 29  BatchID= 10 Loss: 0.491 | Acc: 83.452% (1175/1408)
[Train] Epoch= 29  BatchID= 20 Loss: 0.480 | Acc: 83.817% (2253/2688)
[Train] Epoch= 29  BatchID= 30 Loss: 0.501 | Acc: 83.720% (3322/3968)
[Train] Epoch= 29  BatchID= 40 Loss: 0.492 | Acc: 84.013% (4409/5248)
[Train] Epoch= 29  BatchID= 50 Loss: 0.507 | Acc: 83.686% (5463/6528)
[Train] Epoch= 29  BatchID= 60 Loss: 0.500 | Acc: 83.901% (6551/7808)
[Train] Epoch= 29  BatchID= 70 Loss: 0.504 | Acc: 83.726% (7609/9088)
[Test] Epoch= 29  BatchID= 0 Loss: 0.169 | Acc: 93.750% (120/128)
[Test] Epoch= 29  BatchID= 10 Loss: 0.463 | Acc: 85.085% (1198/1408)
[Test] Epoch= 29  BatchID= 20 Loss: 0.466 | Acc: 85.231% (2291/2688)
[Test] Epoch= 29  BatchID= 30 Loss: 0.439 | Acc: 86.013% (3376/3925)
Saving..
Best accuracy:  86.01273885350318

Epoch: 30
[Train] Epoch= 30  BatchID= 0 Loss: 0.365 | Acc: 89.062% (114/128)
[Train] Epoch= 30  BatchID= 10 Loss: 0.456 | Acc: 86.080% (1212/1408)
[Train] Epoch= 30  BatchID= 20 Loss: 0.486 | Acc: 84.859% (2281/2688)
[Train] Epoch= 30  BatchID= 30 Loss: 0.499 | Acc: 84.375% (3348/3968)
[Train] Epoch= 30  BatchID= 40 Loss: 0.504 | Acc: 84.032% (4410/5248)
[Train] Epoch= 30  BatchID= 50 Loss: 0.509 | Acc: 83.701% (5464/6528)
[Train] Epoch= 30  BatchID= 60 Loss: 0.512 | Acc: 83.543% (6523/7808)
[Train] Epoch= 30  BatchID= 70 Loss: 0.513 | Acc: 83.396% (7579/9088)
[Test] Epoch= 30  BatchID= 0 Loss: 0.107 | Acc: 97.656% (125/128)
[Test] Epoch= 30  BatchID= 10 Loss: 0.390 | Acc: 87.500% (1232/1408)
[Test] Epoch= 30  BatchID= 20 Loss: 0.418 | Acc: 86.682% (2330/2688)
[Test] Epoch= 30  BatchID= 30 Loss: 0.504 | Acc: 84.051% (3299/3925)

Epoch: 31
[Train] Epoch= 31  BatchID= 0 Loss: 0.533 | Acc: 83.594% (107/128)
[Train] Epoch= 31  BatchID= 10 Loss: 0.459 | Acc: 85.866% (1209/1408)
[Train] Epoch= 31  BatchID= 20 Loss: 0.457 | Acc: 85.640% (2302/2688)
[Train] Epoch= 31  BatchID= 30 Loss: 0.475 | Acc: 85.081% (3376/3968)
[Train] Epoch= 31  BatchID= 40 Loss: 0.474 | Acc: 84.889% (4455/5248)
[Train] Epoch= 31  BatchID= 50 Loss: 0.469 | Acc: 84.988% (5548/6528)
[Train] Epoch= 31  BatchID= 60 Loss: 0.475 | Acc: 84.785% (6620/7808)
[Train] Epoch= 31  BatchID= 70 Loss: 0.481 | Acc: 84.496% (7679/9088)
[Test] Epoch= 31  BatchID= 0 Loss: 0.194 | Acc: 95.312% (122/128)
[Test] Epoch= 31  BatchID= 10 Loss: 0.459 | Acc: 85.511% (1204/1408)
[Test] Epoch= 31  BatchID= 20 Loss: 0.429 | Acc: 86.682% (2330/2688)
[Test] Epoch= 31  BatchID= 30 Loss: 0.427 | Acc: 86.522% (3396/3925)
Saving..
Best accuracy:  86.52229299363057

Epoch: 32
[Train] Epoch= 32  BatchID= 0 Loss: 0.428 | Acc: 89.062% (114/128)
[Train] Epoch= 32  BatchID= 10 Loss: 0.432 | Acc: 86.222% (1214/1408)
[Train] Epoch= 32  BatchID= 20 Loss: 0.426 | Acc: 86.496% (2325/2688)
[Train] Epoch= 32  BatchID= 30 Loss: 0.436 | Acc: 86.089% (3416/3968)
[Train] Epoch= 32  BatchID= 40 Loss: 0.451 | Acc: 85.633% (4494/5248)
[Train] Epoch= 32  BatchID= 50 Loss: 0.462 | Acc: 85.248% (5565/6528)
[Train] Epoch= 32  BatchID= 60 Loss: 0.459 | Acc: 85.323% (6662/7808)
[Train] Epoch= 32  BatchID= 70 Loss: 0.461 | Acc: 85.189% (7742/9088)
[Test] Epoch= 32  BatchID= 0 Loss: 0.167 | Acc: 93.750% (120/128)
[Test] Epoch= 32  BatchID= 10 Loss: 0.635 | Acc: 79.474% (1119/1408)
[Test] Epoch= 32  BatchID= 20 Loss: 0.541 | Acc: 83.222% (2237/2688)
[Test] Epoch= 32  BatchID= 30 Loss: 0.525 | Acc: 83.439% (3275/3925)

Epoch: 33
[Train] Epoch= 33  BatchID= 0 Loss: 0.269 | Acc: 94.531% (121/128)
[Train] Epoch= 33  BatchID= 10 Loss: 0.419 | Acc: 86.719% (1221/1408)
[Train] Epoch= 33  BatchID= 20 Loss: 0.423 | Acc: 86.124% (2315/2688)
[Train] Epoch= 33  BatchID= 30 Loss: 0.429 | Acc: 85.685% (3400/3968)
[Train] Epoch= 33  BatchID= 40 Loss: 0.436 | Acc: 85.556% (4490/5248)
[Train] Epoch= 33  BatchID= 50 Loss: 0.440 | Acc: 85.279% (5567/6528)
[Train] Epoch= 33  BatchID= 60 Loss: 0.450 | Acc: 85.195% (6652/7808)
[Train] Epoch= 33  BatchID= 70 Loss: 0.456 | Acc: 85.123% (7736/9088)
[Test] Epoch= 33  BatchID= 0 Loss: 0.083 | Acc: 97.656% (125/128)
[Test] Epoch= 33  BatchID= 10 Loss: 0.399 | Acc: 86.577% (1219/1408)
[Test] Epoch= 33  BatchID= 20 Loss: 0.409 | Acc: 86.644% (2329/2688)
[Test] Epoch= 33  BatchID= 30 Loss: 0.513 | Acc: 83.338% (3271/3925)

Epoch: 34
[Train] Epoch= 34  BatchID= 0 Loss: 0.421 | Acc: 83.594% (107/128)
[Train] Epoch= 34  BatchID= 10 Loss: 0.429 | Acc: 85.866% (1209/1408)
[Train] Epoch= 34  BatchID= 20 Loss: 0.459 | Acc: 84.710% (2277/2688)
[Train] Epoch= 34  BatchID= 30 Loss: 0.457 | Acc: 85.131% (3378/3968)
[Train] Epoch= 34  BatchID= 40 Loss: 0.447 | Acc: 85.747% (4500/5248)
[Train] Epoch= 34  BatchID= 50 Loss: 0.443 | Acc: 85.876% (5606/6528)
[Train] Epoch= 34  BatchID= 60 Loss: 0.453 | Acc: 85.592% (6683/7808)
[Train] Epoch= 34  BatchID= 70 Loss: 0.455 | Acc: 85.596% (7779/9088)
[Test] Epoch= 34  BatchID= 0 Loss: 0.395 | Acc: 89.062% (114/128)
[Test] Epoch= 34  BatchID= 10 Loss: 0.398 | Acc: 87.429% (1231/1408)
[Test] Epoch= 34  BatchID= 20 Loss: 0.480 | Acc: 84.524% (2272/2688)
[Test] Epoch= 34  BatchID= 30 Loss: 0.481 | Acc: 84.611% (3321/3925)

Epoch: 35
[Train] Epoch= 35  BatchID= 0 Loss: 0.473 | Acc: 83.594% (107/128)
[Train] Epoch= 35  BatchID= 10 Loss: 0.455 | Acc: 85.582% (1205/1408)
[Train] Epoch= 35  BatchID= 20 Loss: 0.433 | Acc: 86.049% (2313/2688)
[Train] Epoch= 35  BatchID= 30 Loss: 0.440 | Acc: 86.013% (3413/3968)
[Train] Epoch= 35  BatchID= 40 Loss: 0.460 | Acc: 85.518% (4488/5248)
[Train] Epoch= 35  BatchID= 50 Loss: 0.469 | Acc: 85.141% (5558/6528)
[Train] Epoch= 35  BatchID= 60 Loss: 0.472 | Acc: 84.939% (6632/7808)
[Train] Epoch= 35  BatchID= 70 Loss: 0.473 | Acc: 84.881% (7714/9088)
[Test] Epoch= 35  BatchID= 0 Loss: 0.127 | Acc: 96.875% (124/128)
[Test] Epoch= 35  BatchID= 10 Loss: 0.424 | Acc: 86.364% (1216/1408)
[Test] Epoch= 35  BatchID= 20 Loss: 0.390 | Acc: 87.686% (2357/2688)
[Test] Epoch= 35  BatchID= 30 Loss: 0.414 | Acc: 86.675% (3402/3925)
Saving..
Best accuracy:  86.67515923566879

Epoch: 36
[Train] Epoch= 36  BatchID= 0 Loss: 0.479 | Acc: 85.938% (110/128)
[Train] Epoch= 36  BatchID= 10 Loss: 0.446 | Acc: 85.369% (1202/1408)
[Train] Epoch= 36  BatchID= 20 Loss: 0.454 | Acc: 85.231% (2291/2688)
[Train] Epoch= 36  BatchID= 30 Loss: 0.444 | Acc: 85.837% (3406/3968)
[Train] Epoch= 36  BatchID= 40 Loss: 0.435 | Acc: 86.090% (4518/5248)
[Train] Epoch= 36  BatchID= 50 Loss: 0.446 | Acc: 85.692% (5594/6528)
[Train] Epoch= 36  BatchID= 60 Loss: 0.457 | Acc: 85.400% (6668/7808)
[Train] Epoch= 36  BatchID= 70 Loss: 0.458 | Acc: 85.321% (7754/9088)
[Test] Epoch= 36  BatchID= 0 Loss: 0.168 | Acc: 96.094% (123/128)
[Test] Epoch= 36  BatchID= 10 Loss: 0.391 | Acc: 86.719% (1221/1408)
[Test] Epoch= 36  BatchID= 20 Loss: 0.374 | Acc: 87.946% (2364/2688)
[Test] Epoch= 36  BatchID= 30 Loss: 0.426 | Acc: 86.268% (3386/3925)

Epoch: 37
[Train] Epoch= 37  BatchID= 0 Loss: 0.358 | Acc: 85.938% (110/128)
[Train] Epoch= 37  BatchID= 10 Loss: 0.424 | Acc: 85.582% (1205/1408)
[Train] Epoch= 37  BatchID= 20 Loss: 0.418 | Acc: 85.975% (2311/2688)
[Train] Epoch= 37  BatchID= 30 Loss: 0.439 | Acc: 85.736% (3402/3968)
[Train] Epoch= 37  BatchID= 40 Loss: 0.433 | Acc: 85.976% (4512/5248)
[Train] Epoch= 37  BatchID= 50 Loss: 0.429 | Acc: 86.275% (5632/6528)
[Train] Epoch= 37  BatchID= 60 Loss: 0.438 | Acc: 85.886% (6706/7808)
[Train] Epoch= 37  BatchID= 70 Loss: 0.443 | Acc: 85.706% (7789/9088)
[Test] Epoch= 37  BatchID= 0 Loss: 0.185 | Acc: 95.312% (122/128)
[Test] Epoch= 37  BatchID= 10 Loss: 0.418 | Acc: 86.435% (1217/1408)
[Test] Epoch= 37  BatchID= 20 Loss: 0.409 | Acc: 86.905% (2336/2688)
[Test] Epoch= 37  BatchID= 30 Loss: 0.478 | Acc: 84.815% (3329/3925)

Epoch: 38
[Train] Epoch= 38  BatchID= 0 Loss: 0.416 | Acc: 86.719% (111/128)
[Train] Epoch= 38  BatchID= 10 Loss: 0.449 | Acc: 86.506% (1218/1408)
[Train] Epoch= 38  BatchID= 20 Loss: 0.449 | Acc: 85.863% (2308/2688)
[Train] Epoch= 38  BatchID= 30 Loss: 0.453 | Acc: 85.509% (3393/3968)
[Train] Epoch= 38  BatchID= 40 Loss: 0.450 | Acc: 85.614% (4493/5248)
[Train] Epoch= 38  BatchID= 50 Loss: 0.450 | Acc: 85.539% (5584/6528)
[Train] Epoch= 38  BatchID= 60 Loss: 0.451 | Acc: 85.476% (6674/7808)
[Train] Epoch= 38  BatchID= 70 Loss: 0.445 | Acc: 85.794% (7797/9088)
[Test] Epoch= 38  BatchID= 0 Loss: 0.112 | Acc: 96.875% (124/128)
[Test] Epoch= 38  BatchID= 10 Loss: 0.388 | Acc: 87.997% (1239/1408)
[Test] Epoch= 38  BatchID= 20 Loss: 0.437 | Acc: 86.310% (2320/2688)
[Test] Epoch= 38  BatchID= 30 Loss: 0.449 | Acc: 85.885% (3371/3925)

Epoch: 39
[Train] Epoch= 39  BatchID= 0 Loss: 0.401 | Acc: 89.062% (114/128)
[Train] Epoch= 39  BatchID= 10 Loss: 0.447 | Acc: 85.156% (1199/1408)
[Train] Epoch= 39  BatchID= 20 Loss: 0.428 | Acc: 85.751% (2305/2688)
[Train] Epoch= 39  BatchID= 30 Loss: 0.433 | Acc: 85.837% (3406/3968)
[Train] Epoch= 39  BatchID= 40 Loss: 0.442 | Acc: 85.976% (4512/5248)
[Train] Epoch= 39  BatchID= 50 Loss: 0.441 | Acc: 86.075% (5619/6528)
[Train] Epoch= 39  BatchID= 60 Loss: 0.439 | Acc: 86.181% (6729/7808)
[Train] Epoch= 39  BatchID= 70 Loss: 0.441 | Acc: 86.037% (7819/9088)
[Test] Epoch= 39  BatchID= 0 Loss: 0.252 | Acc: 95.312% (122/128)
[Test] Epoch= 39  BatchID= 10 Loss: 0.428 | Acc: 85.582% (1205/1408)
[Test] Epoch= 39  BatchID= 20 Loss: 0.621 | Acc: 80.469% (2163/2688)
[Test] Epoch= 39  BatchID= 30 Loss: 0.510 | Acc: 83.873% (3292/3925)

Epoch: 40
[Train] Epoch= 40  BatchID= 0 Loss: 0.522 | Acc: 83.594% (107/128)
[Train] Epoch= 40  BatchID= 10 Loss: 0.448 | Acc: 85.653% (1206/1408)
[Train] Epoch= 40  BatchID= 20 Loss: 0.437 | Acc: 85.640% (2302/2688)
[Train] Epoch= 40  BatchID= 30 Loss: 0.439 | Acc: 85.585% (3396/3968)
[Train] Epoch= 40  BatchID= 40 Loss: 0.441 | Acc: 85.442% (4484/5248)
[Train] Epoch= 40  BatchID= 50 Loss: 0.439 | Acc: 85.417% (5576/6528)
[Train] Epoch= 40  BatchID= 60 Loss: 0.435 | Acc: 85.579% (6682/7808)
[Train] Epoch= 40  BatchID= 70 Loss: 0.430 | Acc: 85.871% (7804/9088)
[Test] Epoch= 40  BatchID= 0 Loss: 0.150 | Acc: 95.312% (122/128)
[Test] Epoch= 40  BatchID= 10 Loss: 0.331 | Acc: 88.778% (1250/1408)
[Test] Epoch= 40  BatchID= 20 Loss: 0.368 | Acc: 87.723% (2358/2688)
[Test] Epoch= 40  BatchID= 30 Loss: 0.422 | Acc: 86.217% (3384/3925)

Epoch: 41
[Train] Epoch= 41  BatchID= 0 Loss: 0.476 | Acc: 85.156% (109/128)
[Train] Epoch= 41  BatchID= 10 Loss: 0.415 | Acc: 85.938% (1210/1408)
[Train] Epoch= 41  BatchID= 20 Loss: 0.446 | Acc: 85.082% (2287/2688)
[Train] Epoch= 41  BatchID= 30 Loss: 0.441 | Acc: 84.929% (3370/3968)
[Train] Epoch= 41  BatchID= 40 Loss: 0.447 | Acc: 84.928% (4457/5248)
[Train] Epoch= 41  BatchID= 50 Loss: 0.441 | Acc: 85.447% (5578/6528)
[Train] Epoch= 41  BatchID= 60 Loss: 0.440 | Acc: 85.630% (6686/7808)
[Train] Epoch= 41  BatchID= 70 Loss: 0.437 | Acc: 85.750% (7793/9088)
[Test] Epoch= 41  BatchID= 0 Loss: 0.201 | Acc: 92.188% (118/128)
[Test] Epoch= 41  BatchID= 10 Loss: 0.459 | Acc: 85.014% (1197/1408)
[Test] Epoch= 41  BatchID= 20 Loss: 0.469 | Acc: 84.673% (2276/2688)
[Test] Epoch= 41  BatchID= 30 Loss: 0.540 | Acc: 82.573% (3241/3925)

Epoch: 42
[Train] Epoch= 42  BatchID= 0 Loss: 0.386 | Acc: 89.062% (114/128)
[Train] Epoch= 42  BatchID= 10 Loss: 0.425 | Acc: 86.364% (1216/1408)
[Train] Epoch= 42  BatchID= 20 Loss: 0.408 | Acc: 87.016% (2339/2688)
[Train] Epoch= 42  BatchID= 30 Loss: 0.406 | Acc: 87.021% (3453/3968)
[Train] Epoch= 42  BatchID= 40 Loss: 0.409 | Acc: 87.005% (4566/5248)
[Train] Epoch= 42  BatchID= 50 Loss: 0.399 | Acc: 87.163% (5690/6528)
[Train] Epoch= 42  BatchID= 60 Loss: 0.409 | Acc: 86.796% (6777/7808)
[Train] Epoch= 42  BatchID= 70 Loss: 0.412 | Acc: 86.686% (7878/9088)
[Test] Epoch= 42  BatchID= 0 Loss: 0.095 | Acc: 96.094% (123/128)
[Test] Epoch= 42  BatchID= 10 Loss: 0.479 | Acc: 84.588% (1191/1408)
[Test] Epoch= 42  BatchID= 20 Loss: 0.391 | Acc: 87.426% (2350/2688)
[Test] Epoch= 42  BatchID= 30 Loss: 0.435 | Acc: 85.936% (3373/3925)

Epoch: 43
[Train] Epoch= 43  BatchID= 0 Loss: 0.339 | Acc: 89.844% (115/128)
[Train] Epoch= 43  BatchID= 10 Loss: 0.401 | Acc: 86.719% (1221/1408)
[Train] Epoch= 43  BatchID= 20 Loss: 0.394 | Acc: 86.533% (2326/2688)
[Train] Epoch= 43  BatchID= 30 Loss: 0.395 | Acc: 86.895% (3448/3968)
[Train] Epoch= 43  BatchID= 40 Loss: 0.415 | Acc: 86.471% (4538/5248)
[Train] Epoch= 43  BatchID= 50 Loss: 0.411 | Acc: 86.627% (5655/6528)
[Train] Epoch= 43  BatchID= 60 Loss: 0.413 | Acc: 86.591% (6761/7808)
[Train] Epoch= 43  BatchID= 70 Loss: 0.418 | Acc: 86.356% (7848/9088)
[Test] Epoch= 43  BatchID= 0 Loss: 0.116 | Acc: 95.312% (122/128)
[Test] Epoch= 43  BatchID= 10 Loss: 0.337 | Acc: 88.565% (1247/1408)
[Test] Epoch= 43  BatchID= 20 Loss: 0.325 | Acc: 89.249% (2399/2688)
[Test] Epoch= 43  BatchID= 30 Loss: 0.395 | Acc: 87.108% (3419/3925)
Saving..
Best accuracy:  87.10828025477707

Epoch: 44
[Train] Epoch= 44  BatchID= 0 Loss: 0.360 | Acc: 86.719% (111/128)
[Train] Epoch= 44  BatchID= 10 Loss: 0.368 | Acc: 88.423% (1245/1408)
[Train] Epoch= 44  BatchID= 20 Loss: 0.353 | Acc: 89.137% (2396/2688)
[Train] Epoch= 44  BatchID= 30 Loss: 0.361 | Acc: 88.836% (3525/3968)
[Train] Epoch= 44  BatchID= 40 Loss: 0.372 | Acc: 88.396% (4639/5248)
[Train] Epoch= 44  BatchID= 50 Loss: 0.390 | Acc: 87.745% (5728/6528)
[Train] Epoch= 44  BatchID= 60 Loss: 0.396 | Acc: 87.577% (6838/7808)
[Train] Epoch= 44  BatchID= 70 Loss: 0.400 | Acc: 87.368% (7940/9088)
[Test] Epoch= 44  BatchID= 0 Loss: 0.245 | Acc: 92.969% (119/128)
[Test] Epoch= 44  BatchID= 10 Loss: 0.489 | Acc: 84.091% (1184/1408)
[Test] Epoch= 44  BatchID= 20 Loss: 0.405 | Acc: 87.128% (2342/2688)
[Test] Epoch= 44  BatchID= 30 Loss: 0.398 | Acc: 86.828% (3408/3925)

Epoch: 45
[Train] Epoch= 45  BatchID= 0 Loss: 0.379 | Acc: 89.062% (114/128)
[Train] Epoch= 45  BatchID= 10 Loss: 0.346 | Acc: 89.205% (1256/1408)
[Train] Epoch= 45  BatchID= 20 Loss: 0.367 | Acc: 88.542% (2380/2688)
[Train] Epoch= 45  BatchID= 30 Loss: 0.388 | Acc: 87.676% (3479/3968)
[Train] Epoch= 45  BatchID= 40 Loss: 0.384 | Acc: 87.767% (4606/5248)
[Train] Epoch= 45  BatchID= 50 Loss: 0.387 | Acc: 87.638% (5721/6528)
[Train] Epoch= 45  BatchID= 60 Loss: 0.389 | Acc: 87.449% (6828/7808)
[Train] Epoch= 45  BatchID= 70 Loss: 0.393 | Acc: 87.368% (7940/9088)
[Test] Epoch= 45  BatchID= 0 Loss: 0.230 | Acc: 93.750% (120/128)
[Test] Epoch= 45  BatchID= 10 Loss: 0.374 | Acc: 87.926% (1238/1408)
[Test] Epoch= 45  BatchID= 20 Loss: 0.350 | Acc: 88.653% (2383/2688)
[Test] Epoch= 45  BatchID= 30 Loss: 0.379 | Acc: 87.796% (3446/3925)
Saving..
Best accuracy:  87.79617834394904

Epoch: 46
[Train] Epoch= 46  BatchID= 0 Loss: 0.457 | Acc: 82.031% (105/128)
[Train] Epoch= 46  BatchID= 10 Loss: 0.386 | Acc: 86.932% (1224/1408)
[Train] Epoch= 46  BatchID= 20 Loss: 0.391 | Acc: 86.830% (2334/2688)
[Train] Epoch= 46  BatchID= 30 Loss: 0.385 | Acc: 87.500% (3472/3968)
[Train] Epoch= 46  BatchID= 40 Loss: 0.384 | Acc: 87.614% (4598/5248)
[Train] Epoch= 46  BatchID= 50 Loss: 0.390 | Acc: 87.515% (5713/6528)
[Train] Epoch= 46  BatchID= 60 Loss: 0.392 | Acc: 87.410% (6825/7808)
[Train] Epoch= 46  BatchID= 70 Loss: 0.390 | Acc: 87.555% (7957/9088)
[Test] Epoch= 46  BatchID= 0 Loss: 0.126 | Acc: 95.312% (122/128)
[Test] Epoch= 46  BatchID= 10 Loss: 0.363 | Acc: 87.571% (1233/1408)
[Test] Epoch= 46  BatchID= 20 Loss: 0.362 | Acc: 88.095% (2368/2688)
[Test] Epoch= 46  BatchID= 30 Loss: 0.460 | Acc: 85.070% (3339/3925)

Epoch: 47
[Train] Epoch= 47  BatchID= 0 Loss: 0.380 | Acc: 85.156% (109/128)
[Train] Epoch= 47  BatchID= 10 Loss: 0.424 | Acc: 86.435% (1217/1408)
[Train] Epoch= 47  BatchID= 20 Loss: 0.420 | Acc: 86.793% (2333/2688)
[Train] Epoch= 47  BatchID= 30 Loss: 0.398 | Acc: 87.802% (3484/3968)
[Train] Epoch= 47  BatchID= 40 Loss: 0.401 | Acc: 87.576% (4596/5248)
[Train] Epoch= 47  BatchID= 50 Loss: 0.397 | Acc: 87.531% (5714/6528)
[Train] Epoch= 47  BatchID= 60 Loss: 0.392 | Acc: 87.666% (6845/7808)
[Train] Epoch= 47  BatchID= 70 Loss: 0.395 | Acc: 87.555% (7957/9088)
[Test] Epoch= 47  BatchID= 0 Loss: 0.089 | Acc: 96.094% (123/128)
[Test] Epoch= 47  BatchID= 10 Loss: 0.330 | Acc: 88.920% (1252/1408)
[Test] Epoch= 47  BatchID= 20 Loss: 0.361 | Acc: 88.467% (2378/2688)
[Test] Epoch= 47  BatchID= 30 Loss: 0.391 | Acc: 87.592% (3438/3925)

Epoch: 48
[Train] Epoch= 48  BatchID= 0 Loss: 0.215 | Acc: 92.188% (118/128)
[Train] Epoch= 48  BatchID= 10 Loss: 0.378 | Acc: 87.997% (1239/1408)
[Train] Epoch= 48  BatchID= 20 Loss: 0.383 | Acc: 87.946% (2364/2688)
[Train] Epoch= 48  BatchID= 30 Loss: 0.377 | Acc: 88.256% (3502/3968)
[Train] Epoch= 48  BatchID= 40 Loss: 0.374 | Acc: 88.281% (4633/5248)
[Train] Epoch= 48  BatchID= 50 Loss: 0.378 | Acc: 87.837% (5734/6528)
[Train] Epoch= 48  BatchID= 60 Loss: 0.377 | Acc: 87.884% (6862/7808)
[Train] Epoch= 48  BatchID= 70 Loss: 0.375 | Acc: 87.918% (7990/9088)
[Test] Epoch= 48  BatchID= 0 Loss: 0.101 | Acc: 96.875% (124/128)
[Test] Epoch= 48  BatchID= 10 Loss: 0.325 | Acc: 89.205% (1256/1408)
[Test] Epoch= 48  BatchID= 20 Loss: 0.359 | Acc: 88.170% (2370/2688)
[Test] Epoch= 48  BatchID= 30 Loss: 0.383 | Acc: 87.439% (3432/3925)

Epoch: 49
[Train] Epoch= 49  BatchID= 0 Loss: 0.351 | Acc: 87.500% (112/128)
[Train] Epoch= 49  BatchID= 10 Loss: 0.378 | Acc: 87.784% (1236/1408)
[Train] Epoch= 49  BatchID= 20 Loss: 0.376 | Acc: 87.909% (2363/2688)
[Train] Epoch= 49  BatchID= 30 Loss: 0.376 | Acc: 87.752% (3482/3968)
[Train] Epoch= 49  BatchID= 40 Loss: 0.374 | Acc: 87.691% (4602/5248)
[Train] Epoch= 49  BatchID= 50 Loss: 0.369 | Acc: 88.006% (5745/6528)
[Train] Epoch= 49  BatchID= 60 Loss: 0.359 | Acc: 88.307% (6895/7808)
[Train] Epoch= 49  BatchID= 70 Loss: 0.359 | Acc: 88.358% (8030/9088)
[Test] Epoch= 49  BatchID= 0 Loss: 0.127 | Acc: 96.875% (124/128)
[Test] Epoch= 49  BatchID= 10 Loss: 0.307 | Acc: 89.773% (1264/1408)
[Test] Epoch= 49  BatchID= 20 Loss: 0.304 | Acc: 89.881% (2416/2688)
[Test] Epoch= 49  BatchID= 30 Loss: 0.357 | Acc: 88.306% (3466/3925)
Saving..
Best accuracy:  88.30573248407643

Epoch: 50
[Train] Epoch= 50  BatchID= 0 Loss: 0.500 | Acc: 87.500% (112/128)
[Train] Epoch= 50  BatchID= 10 Loss: 0.398 | Acc: 86.648% (1220/1408)
[Train] Epoch= 50  BatchID= 20 Loss: 0.367 | Acc: 87.872% (2362/2688)
[Train] Epoch= 50  BatchID= 30 Loss: 0.355 | Acc: 88.357% (3506/3968)
[Train] Epoch= 50  BatchID= 40 Loss: 0.359 | Acc: 88.357% (4637/5248)
[Train] Epoch= 50  BatchID= 50 Loss: 0.365 | Acc: 88.404% (5771/6528)
[Train] Epoch= 50  BatchID= 60 Loss: 0.364 | Acc: 88.204% (6887/7808)
[Train] Epoch= 50  BatchID= 70 Loss: 0.369 | Acc: 88.127% (8009/9088)
[Test] Epoch= 50  BatchID= 0 Loss: 0.133 | Acc: 96.875% (124/128)
[Test] Epoch= 50  BatchID= 10 Loss: 0.366 | Acc: 88.281% (1243/1408)
[Test] Epoch= 50  BatchID= 20 Loss: 0.342 | Acc: 89.100% (2395/2688)
[Test] Epoch= 50  BatchID= 30 Loss: 0.360 | Acc: 88.611% (3478/3925)
Saving..
Best accuracy:  88.61146496815287

Epoch: 51
[Train] Epoch= 51  BatchID= 0 Loss: 0.164 | Acc: 96.094% (123/128)
[Train] Epoch= 51  BatchID= 10 Loss: 0.364 | Acc: 88.210% (1242/1408)
[Train] Epoch= 51  BatchID= 20 Loss: 0.361 | Acc: 88.356% (2375/2688)
[Train] Epoch= 51  BatchID= 30 Loss: 0.358 | Acc: 88.760% (3522/3968)
[Train] Epoch= 51  BatchID= 40 Loss: 0.362 | Acc: 88.567% (4648/5248)
[Train] Epoch= 51  BatchID= 50 Loss: 0.358 | Acc: 88.725% (5792/6528)
[Train] Epoch= 51  BatchID= 60 Loss: 0.354 | Acc: 88.832% (6936/7808)
[Train] Epoch= 51  BatchID= 70 Loss: 0.353 | Acc: 88.941% (8083/9088)
[Test] Epoch= 51  BatchID= 0 Loss: 0.186 | Acc: 94.531% (121/128)
[Test] Epoch= 51  BatchID= 10 Loss: 0.344 | Acc: 88.707% (1249/1408)
[Test] Epoch= 51  BatchID= 20 Loss: 0.353 | Acc: 88.951% (2391/2688)
[Test] Epoch= 51  BatchID= 30 Loss: 0.380 | Acc: 88.102% (3458/3925)

Epoch: 52
[Train] Epoch= 52  BatchID= 0 Loss: 0.341 | Acc: 89.062% (114/128)
[Train] Epoch= 52  BatchID= 10 Loss: 0.308 | Acc: 90.128% (1269/1408)
[Train] Epoch= 52  BatchID= 20 Loss: 0.352 | Acc: 88.542% (2380/2688)
[Train] Epoch= 52  BatchID= 30 Loss: 0.339 | Acc: 89.088% (3535/3968)
[Train] Epoch= 52  BatchID= 40 Loss: 0.344 | Acc: 89.024% (4672/5248)
[Train] Epoch= 52  BatchID= 50 Loss: 0.345 | Acc: 89.093% (5816/6528)
[Train] Epoch= 52  BatchID= 60 Loss: 0.343 | Acc: 89.062% (6954/7808)
[Train] Epoch= 52  BatchID= 70 Loss: 0.343 | Acc: 89.096% (8097/9088)
[Test] Epoch= 52  BatchID= 0 Loss: 0.151 | Acc: 96.094% (123/128)
[Test] Epoch= 52  BatchID= 10 Loss: 0.319 | Acc: 89.560% (1261/1408)
[Test] Epoch= 52  BatchID= 20 Loss: 0.318 | Acc: 89.807% (2414/2688)
[Test] Epoch= 52  BatchID= 30 Loss: 0.367 | Acc: 88.102% (3458/3925)

Epoch: 53
[Train] Epoch= 53  BatchID= 0 Loss: 0.370 | Acc: 85.938% (110/128)
[Train] Epoch= 53  BatchID= 10 Loss: 0.320 | Acc: 88.991% (1253/1408)
[Train] Epoch= 53  BatchID= 20 Loss: 0.328 | Acc: 89.286% (2400/2688)
[Train] Epoch= 53  BatchID= 30 Loss: 0.335 | Acc: 89.415% (3548/3968)
[Train] Epoch= 53  BatchID= 40 Loss: 0.334 | Acc: 89.539% (4699/5248)
[Train] Epoch= 53  BatchID= 50 Loss: 0.336 | Acc: 89.614% (5850/6528)
[Train] Epoch= 53  BatchID= 60 Loss: 0.334 | Acc: 89.524% (6990/7808)
[Train] Epoch= 53  BatchID= 70 Loss: 0.335 | Acc: 89.470% (8131/9088)
[Test] Epoch= 53  BatchID= 0 Loss: 0.204 | Acc: 92.969% (119/128)
[Test] Epoch= 53  BatchID= 10 Loss: 0.509 | Acc: 83.807% (1180/1408)
[Test] Epoch= 53  BatchID= 20 Loss: 0.506 | Acc: 83.966% (2257/2688)
[Test] Epoch= 53  BatchID= 30 Loss: 0.457 | Acc: 85.554% (3358/3925)

Epoch: 54
[Train] Epoch= 54  BatchID= 0 Loss: 0.245 | Acc: 90.625% (116/128)
[Train] Epoch= 54  BatchID= 10 Loss: 0.306 | Acc: 89.702% (1263/1408)
[Train] Epoch= 54  BatchID= 20 Loss: 0.333 | Acc: 89.025% (2393/2688)
[Train] Epoch= 54  BatchID= 30 Loss: 0.338 | Acc: 88.936% (3529/3968)
[Train] Epoch= 54  BatchID= 40 Loss: 0.335 | Acc: 89.253% (4684/5248)
[Train] Epoch= 54  BatchID= 50 Loss: 0.339 | Acc: 89.017% (5811/6528)
[Train] Epoch= 54  BatchID= 60 Loss: 0.340 | Acc: 89.024% (6951/7808)
[Train] Epoch= 54  BatchID= 70 Loss: 0.342 | Acc: 88.985% (8087/9088)
[Test] Epoch= 54  BatchID= 0 Loss: 0.142 | Acc: 96.094% (123/128)
[Test] Epoch= 54  BatchID= 10 Loss: 0.310 | Acc: 89.844% (1265/1408)
[Test] Epoch= 54  BatchID= 20 Loss: 0.336 | Acc: 89.211% (2398/2688)
[Test] Epoch= 54  BatchID= 30 Loss: 0.341 | Acc: 89.070% (3496/3925)
Saving..
Best accuracy:  89.07006369426752

Epoch: 55
[Train] Epoch= 55  BatchID= 0 Loss: 0.254 | Acc: 91.406% (117/128)
[Train] Epoch= 55  BatchID= 10 Loss: 0.285 | Acc: 91.477% (1288/1408)
[Train] Epoch= 55  BatchID= 20 Loss: 0.294 | Acc: 90.960% (2445/2688)
[Train] Epoch= 55  BatchID= 30 Loss: 0.308 | Acc: 90.222% (3580/3968)
[Train] Epoch= 55  BatchID= 40 Loss: 0.315 | Acc: 89.825% (4714/5248)
[Train] Epoch= 55  BatchID= 50 Loss: 0.312 | Acc: 89.828% (5864/6528)
[Train] Epoch= 55  BatchID= 60 Loss: 0.315 | Acc: 89.600% (6996/7808)
[Train] Epoch= 55  BatchID= 70 Loss: 0.317 | Acc: 89.624% (8145/9088)
[Test] Epoch= 55  BatchID= 0 Loss: 0.150 | Acc: 95.312% (122/128)
[Test] Epoch= 55  BatchID= 10 Loss: 0.338 | Acc: 89.062% (1254/1408)
[Test] Epoch= 55  BatchID= 20 Loss: 0.374 | Acc: 88.616% (2382/2688)
[Test] Epoch= 55  BatchID= 30 Loss: 0.473 | Acc: 85.936% (3373/3925)

Epoch: 56
[Train] Epoch= 56  BatchID= 0 Loss: 0.361 | Acc: 87.500% (112/128)
[Train] Epoch= 56  BatchID= 10 Loss: 0.314 | Acc: 90.341% (1272/1408)
[Train] Epoch= 56  BatchID= 20 Loss: 0.320 | Acc: 89.881% (2416/2688)
[Train] Epoch= 56  BatchID= 30 Loss: 0.317 | Acc: 90.020% (3572/3968)
[Train] Epoch= 56  BatchID= 40 Loss: 0.318 | Acc: 90.111% (4729/5248)
[Train] Epoch= 56  BatchID= 50 Loss: 0.315 | Acc: 90.104% (5882/6528)
[Train] Epoch= 56  BatchID= 60 Loss: 0.315 | Acc: 90.190% (7042/7808)
[Train] Epoch= 56  BatchID= 70 Loss: 0.314 | Acc: 90.174% (8195/9088)
[Test] Epoch= 56  BatchID= 0 Loss: 0.097 | Acc: 96.875% (124/128)
[Test] Epoch= 56  BatchID= 10 Loss: 0.362 | Acc: 89.062% (1254/1408)
[Test] Epoch= 56  BatchID= 20 Loss: 0.378 | Acc: 88.207% (2371/2688)
[Test] Epoch= 56  BatchID= 30 Loss: 0.370 | Acc: 88.306% (3466/3925)

Epoch: 57
[Train] Epoch= 57  BatchID= 0 Loss: 0.320 | Acc: 89.062% (114/128)
[Train] Epoch= 57  BatchID= 10 Loss: 0.291 | Acc: 90.625% (1276/1408)
[Train] Epoch= 57  BatchID= 20 Loss: 0.299 | Acc: 90.476% (2432/2688)
[Train] Epoch= 57  BatchID= 30 Loss: 0.311 | Acc: 89.793% (3563/3968)
[Train] Epoch= 57  BatchID= 40 Loss: 0.318 | Acc: 89.748% (4710/5248)
[Train] Epoch= 57  BatchID= 50 Loss: 0.315 | Acc: 89.752% (5859/6528)
[Train] Epoch= 57  BatchID= 60 Loss: 0.313 | Acc: 89.946% (7023/7808)
[Train] Epoch= 57  BatchID= 70 Loss: 0.313 | Acc: 89.943% (8174/9088)
[Test] Epoch= 57  BatchID= 0 Loss: 0.098 | Acc: 96.094% (123/128)
[Test] Epoch= 57  BatchID= 10 Loss: 0.261 | Acc: 90.909% (1280/1408)
[Test] Epoch= 57  BatchID= 20 Loss: 0.285 | Acc: 90.513% (2433/2688)
[Test] Epoch= 57  BatchID= 30 Loss: 0.339 | Acc: 88.790% (3485/3925)

Epoch: 58
[Train] Epoch= 58  BatchID= 0 Loss: 0.300 | Acc: 90.625% (116/128)
[Train] Epoch= 58  BatchID= 10 Loss: 0.269 | Acc: 91.477% (1288/1408)
[Train] Epoch= 58  BatchID= 20 Loss: 0.273 | Acc: 91.629% (2463/2688)
[Train] Epoch= 58  BatchID= 30 Loss: 0.272 | Acc: 91.557% (3633/3968)
[Train] Epoch= 58  BatchID= 40 Loss: 0.281 | Acc: 91.292% (4791/5248)
[Train] Epoch= 58  BatchID= 50 Loss: 0.290 | Acc: 90.901% (5934/6528)
[Train] Epoch= 58  BatchID= 60 Loss: 0.296 | Acc: 90.651% (7078/7808)
[Train] Epoch= 58  BatchID= 70 Loss: 0.297 | Acc: 90.680% (8241/9088)
[Test] Epoch= 58  BatchID= 0 Loss: 0.107 | Acc: 96.875% (124/128)
[Test] Epoch= 58  BatchID= 10 Loss: 0.341 | Acc: 89.773% (1264/1408)
[Test] Epoch= 58  BatchID= 20 Loss: 0.322 | Acc: 90.439% (2431/2688)
[Test] Epoch= 58  BatchID= 30 Loss: 0.334 | Acc: 89.809% (3525/3925)
Saving..
Best accuracy:  89.80891719745223

Epoch: 59
[Train] Epoch= 59  BatchID= 0 Loss: 0.361 | Acc: 89.062% (114/128)
[Train] Epoch= 59  BatchID= 10 Loss: 0.279 | Acc: 91.335% (1286/1408)
[Train] Epoch= 59  BatchID= 20 Loss: 0.276 | Acc: 91.183% (2451/2688)
[Train] Epoch= 59  BatchID= 30 Loss: 0.282 | Acc: 90.902% (3607/3968)
[Train] Epoch= 59  BatchID= 40 Loss: 0.284 | Acc: 90.892% (4770/5248)
[Train] Epoch= 59  BatchID= 50 Loss: 0.288 | Acc: 90.778% (5926/6528)
[Train] Epoch= 59  BatchID= 60 Loss: 0.292 | Acc: 90.535% (7069/7808)
[Train] Epoch= 59  BatchID= 70 Loss: 0.294 | Acc: 90.559% (8230/9088)
[Test] Epoch= 59  BatchID= 0 Loss: 0.088 | Acc: 96.875% (124/128)
[Test] Epoch= 59  BatchID= 10 Loss: 0.288 | Acc: 90.625% (1276/1408)
[Test] Epoch= 59  BatchID= 20 Loss: 0.292 | Acc: 90.737% (2439/2688)
[Test] Epoch= 59  BatchID= 30 Loss: 0.330 | Acc: 89.401% (3509/3925)

Epoch: 60
[Train] Epoch= 60  BatchID= 0 Loss: 0.298 | Acc: 87.500% (112/128)
[Train] Epoch= 60  BatchID= 10 Loss: 0.271 | Acc: 90.909% (1280/1408)
[Train] Epoch= 60  BatchID= 20 Loss: 0.287 | Acc: 90.179% (2424/2688)
[Train] Epoch= 60  BatchID= 30 Loss: 0.281 | Acc: 90.423% (3588/3968)
[Train] Epoch= 60  BatchID= 40 Loss: 0.278 | Acc: 90.701% (4760/5248)
[Train] Epoch= 60  BatchID= 50 Loss: 0.284 | Acc: 90.732% (5923/6528)
[Train] Epoch= 60  BatchID= 60 Loss: 0.288 | Acc: 90.638% (7077/7808)
[Train] Epoch= 60  BatchID= 70 Loss: 0.292 | Acc: 90.482% (8223/9088)
[Test] Epoch= 60  BatchID= 0 Loss: 0.144 | Acc: 96.094% (123/128)
[Test] Epoch= 60  BatchID= 10 Loss: 0.337 | Acc: 89.773% (1264/1408)
[Test] Epoch= 60  BatchID= 20 Loss: 0.313 | Acc: 90.141% (2423/2688)
[Test] Epoch= 60  BatchID= 30 Loss: 0.349 | Acc: 89.019% (3494/3925)

Epoch: 61
[Train] Epoch= 61  BatchID= 0 Loss: 0.214 | Acc: 93.750% (120/128)
[Train] Epoch= 61  BatchID= 10 Loss: 0.285 | Acc: 91.477% (1288/1408)
[Train] Epoch= 61  BatchID= 20 Loss: 0.276 | Acc: 91.332% (2455/2688)
[Train] Epoch= 61  BatchID= 30 Loss: 0.280 | Acc: 91.179% (3618/3968)
[Train] Epoch= 61  BatchID= 40 Loss: 0.289 | Acc: 90.835% (4767/5248)
[Train] Epoch= 61  BatchID= 50 Loss: 0.295 | Acc: 90.671% (5919/6528)
[Train] Epoch= 61  BatchID= 60 Loss: 0.295 | Acc: 90.689% (7081/7808)
[Train] Epoch= 61  BatchID= 70 Loss: 0.292 | Acc: 90.768% (8249/9088)
[Test] Epoch= 61  BatchID= 0 Loss: 0.119 | Acc: 96.094% (123/128)
[Test] Epoch= 61  BatchID= 10 Loss: 0.292 | Acc: 90.412% (1273/1408)
[Test] Epoch= 61  BatchID= 20 Loss: 0.288 | Acc: 90.811% (2441/2688)
[Test] Epoch= 61  BatchID= 30 Loss: 0.324 | Acc: 89.732% (3522/3925)

Epoch: 62
[Train] Epoch= 62  BatchID= 0 Loss: 0.202 | Acc: 93.750% (120/128)
[Train] Epoch= 62  BatchID= 10 Loss: 0.269 | Acc: 91.690% (1291/1408)
[Train] Epoch= 62  BatchID= 20 Loss: 0.292 | Acc: 90.997% (2446/2688)
[Train] Epoch= 62  BatchID= 30 Loss: 0.283 | Acc: 91.079% (3614/3968)
[Train] Epoch= 62  BatchID= 40 Loss: 0.282 | Acc: 91.044% (4778/5248)
[Train] Epoch= 62  BatchID= 50 Loss: 0.284 | Acc: 90.916% (5935/6528)
[Train] Epoch= 62  BatchID= 60 Loss: 0.284 | Acc: 90.996% (7105/7808)
[Train] Epoch= 62  BatchID= 70 Loss: 0.279 | Acc: 91.197% (8288/9088)
[Test] Epoch= 62  BatchID= 0 Loss: 0.100 | Acc: 96.875% (124/128)
[Test] Epoch= 62  BatchID= 10 Loss: 0.281 | Acc: 91.548% (1289/1408)
[Test] Epoch= 62  BatchID= 20 Loss: 0.280 | Acc: 91.146% (2450/2688)
[Test] Epoch= 62  BatchID= 30 Loss: 0.319 | Acc: 90.089% (3536/3925)
Saving..
Best accuracy:  90.08917197452229

Epoch: 63
[Train] Epoch= 63  BatchID= 0 Loss: 0.248 | Acc: 92.188% (118/128)
[Train] Epoch= 63  BatchID= 10 Loss: 0.290 | Acc: 90.554% (1275/1408)
[Train] Epoch= 63  BatchID= 20 Loss: 0.285 | Acc: 90.848% (2442/2688)
[Train] Epoch= 63  BatchID= 30 Loss: 0.286 | Acc: 90.726% (3600/3968)
[Train] Epoch= 63  BatchID= 40 Loss: 0.284 | Acc: 90.701% (4760/5248)
[Train] Epoch= 63  BatchID= 50 Loss: 0.283 | Acc: 90.794% (5927/6528)
[Train] Epoch= 63  BatchID= 60 Loss: 0.278 | Acc: 90.945% (7101/7808)
[Train] Epoch= 63  BatchID= 70 Loss: 0.273 | Acc: 91.120% (8281/9088)
[Test] Epoch= 63  BatchID= 0 Loss: 0.072 | Acc: 97.656% (125/128)
[Test] Epoch= 63  BatchID= 10 Loss: 0.288 | Acc: 91.264% (1285/1408)
[Test] Epoch= 63  BatchID= 20 Loss: 0.280 | Acc: 91.295% (2454/2688)
[Test] Epoch= 63  BatchID= 30 Loss: 0.303 | Acc: 90.624% (3557/3925)
Saving..
Best accuracy:  90.62420382165605

Epoch: 64
[Train] Epoch= 64  BatchID= 0 Loss: 0.205 | Acc: 92.969% (119/128)
[Train] Epoch= 64  BatchID= 10 Loss: 0.233 | Acc: 92.614% (1304/1408)
[Train] Epoch= 64  BatchID= 20 Loss: 0.251 | Acc: 92.188% (2478/2688)
[Train] Epoch= 64  BatchID= 30 Loss: 0.242 | Acc: 92.591% (3674/3968)
[Train] Epoch= 64  BatchID= 40 Loss: 0.235 | Acc: 92.664% (4863/5248)
[Train] Epoch= 64  BatchID= 50 Loss: 0.243 | Acc: 92.233% (6021/6528)
[Train] Epoch= 64  BatchID= 60 Loss: 0.248 | Acc: 92.136% (7194/7808)
[Train] Epoch= 64  BatchID= 70 Loss: 0.252 | Acc: 92.077% (8368/9088)
[Test] Epoch= 64  BatchID= 0 Loss: 0.140 | Acc: 96.094% (123/128)
[Test] Epoch= 64  BatchID= 10 Loss: 0.307 | Acc: 90.057% (1268/1408)
[Test] Epoch= 64  BatchID= 20 Loss: 0.302 | Acc: 89.955% (2418/2688)
[Test] Epoch= 64  BatchID= 30 Loss: 0.325 | Acc: 89.427% (3510/3925)

Epoch: 65
[Train] Epoch= 65  BatchID= 0 Loss: 0.253 | Acc: 89.844% (115/128)
[Train] Epoch= 65  BatchID= 10 Loss: 0.249 | Acc: 92.116% (1297/1408)
[Train] Epoch= 65  BatchID= 20 Loss: 0.252 | Acc: 92.001% (2473/2688)
[Train] Epoch= 65  BatchID= 30 Loss: 0.261 | Acc: 91.784% (3642/3968)
[Train] Epoch= 65  BatchID= 40 Loss: 0.260 | Acc: 91.692% (4812/5248)
[Train] Epoch= 65  BatchID= 50 Loss: 0.249 | Acc: 92.157% (6016/6528)
[Train] Epoch= 65  BatchID= 60 Loss: 0.251 | Acc: 92.059% (7188/7808)
[Train] Epoch= 65  BatchID= 70 Loss: 0.257 | Acc: 91.945% (8356/9088)
[Test] Epoch= 65  BatchID= 0 Loss: 0.171 | Acc: 94.531% (121/128)
[Test] Epoch= 65  BatchID= 10 Loss: 0.312 | Acc: 89.062% (1254/1408)
[Test] Epoch= 65  BatchID= 20 Loss: 0.275 | Acc: 90.885% (2443/2688)
[Test] Epoch= 65  BatchID= 30 Loss: 0.303 | Acc: 90.038% (3534/3925)

Epoch: 66
[Train] Epoch= 66  BatchID= 0 Loss: 0.226 | Acc: 92.969% (119/128)
[Train] Epoch= 66  BatchID= 10 Loss: 0.239 | Acc: 92.188% (1298/1408)
[Train] Epoch= 66  BatchID= 20 Loss: 0.235 | Acc: 92.336% (2482/2688)
[Train] Epoch= 66  BatchID= 30 Loss: 0.236 | Acc: 92.616% (3675/3968)
[Train] Epoch= 66  BatchID= 40 Loss: 0.238 | Acc: 92.511% (4855/5248)
[Train] Epoch= 66  BatchID= 50 Loss: 0.236 | Acc: 92.463% (6036/6528)
[Train] Epoch= 66  BatchID= 60 Loss: 0.241 | Acc: 92.392% (7214/7808)
[Train] Epoch= 66  BatchID= 70 Loss: 0.243 | Acc: 92.364% (8394/9088)
[Test] Epoch= 66  BatchID= 0 Loss: 0.049 | Acc: 98.438% (126/128)
[Test] Epoch= 66  BatchID= 10 Loss: 0.287 | Acc: 91.193% (1284/1408)
[Test] Epoch= 66  BatchID= 20 Loss: 0.288 | Acc: 90.960% (2445/2688)
[Test] Epoch= 66  BatchID= 30 Loss: 0.315 | Acc: 90.013% (3533/3925)

Epoch: 67
[Train] Epoch= 67  BatchID= 0 Loss: 0.212 | Acc: 94.531% (121/128)
[Train] Epoch= 67  BatchID= 10 Loss: 0.216 | Acc: 93.395% (1315/1408)
[Train] Epoch= 67  BatchID= 20 Loss: 0.223 | Acc: 93.304% (2508/2688)
[Train] Epoch= 67  BatchID= 30 Loss: 0.228 | Acc: 92.994% (3690/3968)
[Train] Epoch= 67  BatchID= 40 Loss: 0.237 | Acc: 92.626% (4861/5248)
[Train] Epoch= 67  BatchID= 50 Loss: 0.241 | Acc: 92.494% (6038/6528)
[Train] Epoch= 67  BatchID= 60 Loss: 0.239 | Acc: 92.585% (7229/7808)
[Train] Epoch= 67  BatchID= 70 Loss: 0.239 | Acc: 92.529% (8409/9088)
[Test] Epoch= 67  BatchID= 0 Loss: 0.118 | Acc: 96.094% (123/128)
[Test] Epoch= 67  BatchID= 10 Loss: 0.298 | Acc: 90.412% (1273/1408)
[Test] Epoch= 67  BatchID= 20 Loss: 0.333 | Acc: 89.509% (2406/2688)
[Test] Epoch= 67  BatchID= 30 Loss: 0.348 | Acc: 88.917% (3490/3925)

Epoch: 68
[Train] Epoch= 68  BatchID= 0 Loss: 0.217 | Acc: 94.531% (121/128)
[Train] Epoch= 68  BatchID= 10 Loss: 0.236 | Acc: 93.182% (1312/1408)
[Train] Epoch= 68  BatchID= 20 Loss: 0.224 | Acc: 93.155% (2504/2688)
[Train] Epoch= 68  BatchID= 30 Loss: 0.216 | Acc: 93.221% (3699/3968)
[Train] Epoch= 68  BatchID= 40 Loss: 0.227 | Acc: 92.950% (4878/5248)
[Train] Epoch= 68  BatchID= 50 Loss: 0.228 | Acc: 92.831% (6060/6528)
[Train] Epoch= 68  BatchID= 60 Loss: 0.234 | Acc: 92.585% (7229/7808)
[Train] Epoch= 68  BatchID= 70 Loss: 0.238 | Acc: 92.408% (8398/9088)
[Test] Epoch= 68  BatchID= 0 Loss: 0.095 | Acc: 96.875% (124/128)
[Test] Epoch= 68  BatchID= 10 Loss: 0.317 | Acc: 89.986% (1267/1408)
[Test] Epoch= 68  BatchID= 20 Loss: 0.301 | Acc: 90.216% (2425/2688)
[Test] Epoch= 68  BatchID= 30 Loss: 0.315 | Acc: 89.962% (3531/3925)

Epoch: 69
[Train] Epoch= 69  BatchID= 0 Loss: 0.239 | Acc: 90.625% (116/128)
[Train] Epoch= 69  BatchID= 10 Loss: 0.242 | Acc: 92.543% (1303/1408)
[Train] Epoch= 69  BatchID= 20 Loss: 0.230 | Acc: 92.783% (2494/2688)
[Train] Epoch= 69  BatchID= 30 Loss: 0.234 | Acc: 92.490% (3670/3968)
[Train] Epoch= 69  BatchID= 40 Loss: 0.239 | Acc: 92.397% (4849/5248)
[Train] Epoch= 69  BatchID= 50 Loss: 0.232 | Acc: 92.632% (6047/6528)
[Train] Epoch= 69  BatchID= 60 Loss: 0.230 | Acc: 92.713% (7239/7808)
[Train] Epoch= 69  BatchID= 70 Loss: 0.230 | Acc: 92.804% (8434/9088)
[Test] Epoch= 69  BatchID= 0 Loss: 0.101 | Acc: 96.875% (124/128)
[Test] Epoch= 69  BatchID= 10 Loss: 0.285 | Acc: 90.625% (1276/1408)
[Test] Epoch= 69  BatchID= 20 Loss: 0.284 | Acc: 90.737% (2439/2688)
[Test] Epoch= 69  BatchID= 30 Loss: 0.300 | Acc: 90.318% (3545/3925)

Epoch: 70
[Train] Epoch= 70  BatchID= 0 Loss: 0.227 | Acc: 95.312% (122/128)
[Train] Epoch= 70  BatchID= 10 Loss: 0.201 | Acc: 93.892% (1322/1408)
[Train] Epoch= 70  BatchID= 20 Loss: 0.195 | Acc: 94.085% (2529/2688)
[Train] Epoch= 70  BatchID= 30 Loss: 0.214 | Acc: 93.397% (3706/3968)
[Train] Epoch= 70  BatchID= 40 Loss: 0.219 | Acc: 93.293% (4896/5248)
[Train] Epoch= 70  BatchID= 50 Loss: 0.222 | Acc: 93.244% (6087/6528)
[Train] Epoch= 70  BatchID= 60 Loss: 0.222 | Acc: 93.276% (7283/7808)
[Train] Epoch= 70  BatchID= 70 Loss: 0.220 | Acc: 93.310% (8480/9088)
[Test] Epoch= 70  BatchID= 0 Loss: 0.215 | Acc: 93.750% (120/128)
[Test] Epoch= 70  BatchID= 10 Loss: 0.295 | Acc: 90.838% (1279/1408)
[Test] Epoch= 70  BatchID= 20 Loss: 0.302 | Acc: 90.923% (2444/2688)
[Test] Epoch= 70  BatchID= 30 Loss: 0.321 | Acc: 90.446% (3550/3925)

Epoch: 71
[Train] Epoch= 71  BatchID= 0 Loss: 0.192 | Acc: 94.531% (121/128)
[Train] Epoch= 71  BatchID= 10 Loss: 0.225 | Acc: 93.324% (1314/1408)
[Train] Epoch= 71  BatchID= 20 Loss: 0.227 | Acc: 93.080% (2502/2688)
[Train] Epoch= 71  BatchID= 30 Loss: 0.229 | Acc: 92.843% (3684/3968)
[Train] Epoch= 71  BatchID= 40 Loss: 0.232 | Acc: 92.778% (4869/5248)
[Train] Epoch= 71  BatchID= 50 Loss: 0.237 | Acc: 92.555% (6042/6528)
[Train] Epoch= 71  BatchID= 60 Loss: 0.238 | Acc: 92.495% (7222/7808)
[Train] Epoch= 71  BatchID= 70 Loss: 0.238 | Acc: 92.331% (8391/9088)
[Test] Epoch= 71  BatchID= 0 Loss: 0.115 | Acc: 96.875% (124/128)
[Test] Epoch= 71  BatchID= 10 Loss: 0.300 | Acc: 90.696% (1277/1408)
[Test] Epoch= 71  BatchID= 20 Loss: 0.283 | Acc: 90.960% (2445/2688)
[Test] Epoch= 71  BatchID= 30 Loss: 0.320 | Acc: 89.783% (3524/3925)

Epoch: 72
[Train] Epoch= 72  BatchID= 0 Loss: 0.187 | Acc: 94.531% (121/128)
[Train] Epoch= 72  BatchID= 10 Loss: 0.171 | Acc: 95.028% (1338/1408)
[Train] Epoch= 72  BatchID= 20 Loss: 0.184 | Acc: 94.494% (2540/2688)
[Train] Epoch= 72  BatchID= 30 Loss: 0.195 | Acc: 94.178% (3737/3968)
[Train] Epoch= 72  BatchID= 40 Loss: 0.200 | Acc: 94.017% (4934/5248)
[Train] Epoch= 72  BatchID= 50 Loss: 0.206 | Acc: 93.781% (6122/6528)
[Train] Epoch= 72  BatchID= 60 Loss: 0.211 | Acc: 93.596% (7308/7808)
[Train] Epoch= 72  BatchID= 70 Loss: 0.208 | Acc: 93.662% (8512/9088)
[Test] Epoch= 72  BatchID= 0 Loss: 0.107 | Acc: 97.656% (125/128)
[Test] Epoch= 72  BatchID= 10 Loss: 0.296 | Acc: 90.696% (1277/1408)
[Test] Epoch= 72  BatchID= 20 Loss: 0.282 | Acc: 91.071% (2448/2688)
[Test] Epoch= 72  BatchID= 30 Loss: 0.303 | Acc: 90.446% (3550/3925)

Epoch: 73
[Train] Epoch= 73  BatchID= 0 Loss: 0.217 | Acc: 93.750% (120/128)
[Train] Epoch= 73  BatchID= 10 Loss: 0.224 | Acc: 93.253% (1313/1408)
[Train] Epoch= 73  BatchID= 20 Loss: 0.228 | Acc: 92.708% (2492/2688)
[Train] Epoch= 73  BatchID= 30 Loss: 0.217 | Acc: 93.221% (3699/3968)
[Train] Epoch= 73  BatchID= 40 Loss: 0.218 | Acc: 93.236% (4893/5248)
[Train] Epoch= 73  BatchID= 50 Loss: 0.220 | Acc: 93.076% (6076/6528)
[Train] Epoch= 73  BatchID= 60 Loss: 0.218 | Acc: 93.225% (7279/7808)
[Train] Epoch= 73  BatchID= 70 Loss: 0.217 | Acc: 93.244% (8474/9088)
[Test] Epoch= 73  BatchID= 0 Loss: 0.099 | Acc: 96.094% (123/128)
[Test] Epoch= 73  BatchID= 10 Loss: 0.296 | Acc: 90.625% (1276/1408)
[Test] Epoch= 73  BatchID= 20 Loss: 0.275 | Acc: 91.332% (2455/2688)
[Test] Epoch= 73  BatchID= 30 Loss: 0.296 | Acc: 90.650% (3558/3925)
Saving..
Best accuracy:  90.64968152866243

Epoch: 74
[Train] Epoch= 74  BatchID= 0 Loss: 0.204 | Acc: 96.094% (123/128)
[Train] Epoch= 74  BatchID= 10 Loss: 0.175 | Acc: 95.028% (1338/1408)
[Train] Epoch= 74  BatchID= 20 Loss: 0.185 | Acc: 94.196% (2532/2688)
[Train] Epoch= 74  BatchID= 30 Loss: 0.190 | Acc: 94.178% (3737/3968)
[Train] Epoch= 74  BatchID= 40 Loss: 0.197 | Acc: 93.921% (4929/5248)
[Train] Epoch= 74  BatchID= 50 Loss: 0.204 | Acc: 93.597% (6110/6528)
[Train] Epoch= 74  BatchID= 60 Loss: 0.202 | Acc: 93.673% (7314/7808)
[Train] Epoch= 74  BatchID= 70 Loss: 0.202 | Acc: 93.717% (8517/9088)
[Test] Epoch= 74  BatchID= 0 Loss: 0.076 | Acc: 97.656% (125/128)
[Test] Epoch= 74  BatchID= 10 Loss: 0.271 | Acc: 91.051% (1282/1408)
[Test] Epoch= 74  BatchID= 20 Loss: 0.267 | Acc: 91.257% (2453/2688)
[Test] Epoch= 74  BatchID= 30 Loss: 0.289 | Acc: 90.599% (3556/3925)

Epoch: 75
[Train] Epoch= 75  BatchID= 0 Loss: 0.233 | Acc: 92.188% (118/128)
[Train] Epoch= 75  BatchID= 10 Loss: 0.182 | Acc: 94.176% (1326/1408)
[Train] Epoch= 75  BatchID= 20 Loss: 0.190 | Acc: 94.122% (2530/2688)
[Train] Epoch= 75  BatchID= 30 Loss: 0.192 | Acc: 94.178% (3737/3968)
[Train] Epoch= 75  BatchID= 40 Loss: 0.193 | Acc: 94.169% (4942/5248)
[Train] Epoch= 75  BatchID= 50 Loss: 0.195 | Acc: 94.010% (6137/6528)
[Train] Epoch= 75  BatchID= 60 Loss: 0.194 | Acc: 94.057% (7344/7808)
[Train] Epoch= 75  BatchID= 70 Loss: 0.196 | Acc: 93.981% (8541/9088)
[Test] Epoch= 75  BatchID= 0 Loss: 0.127 | Acc: 95.312% (122/128)
[Test] Epoch= 75  BatchID= 10 Loss: 0.270 | Acc: 91.406% (1287/1408)
[Test] Epoch= 75  BatchID= 20 Loss: 0.269 | Acc: 91.667% (2464/2688)
[Test] Epoch= 75  BatchID= 30 Loss: 0.303 | Acc: 90.420% (3549/3925)

Epoch: 76
[Train] Epoch= 76  BatchID= 0 Loss: 0.212 | Acc: 91.406% (117/128)
[Train] Epoch= 76  BatchID= 10 Loss: 0.206 | Acc: 93.395% (1315/1408)
[Train] Epoch= 76  BatchID= 20 Loss: 0.213 | Acc: 93.490% (2513/2688)
[Train] Epoch= 76  BatchID= 30 Loss: 0.214 | Acc: 93.599% (3714/3968)
[Train] Epoch= 76  BatchID= 40 Loss: 0.212 | Acc: 93.750% (4920/5248)
[Train] Epoch= 76  BatchID= 50 Loss: 0.214 | Acc: 93.673% (6115/6528)
[Train] Epoch= 76  BatchID= 60 Loss: 0.210 | Acc: 93.712% (7317/7808)
[Train] Epoch= 76  BatchID= 70 Loss: 0.208 | Acc: 93.772% (8522/9088)
[Test] Epoch= 76  BatchID= 0 Loss: 0.105 | Acc: 96.094% (123/128)
[Test] Epoch= 76  BatchID= 10 Loss: 0.273 | Acc: 91.761% (1292/1408)
[Test] Epoch= 76  BatchID= 20 Loss: 0.283 | Acc: 91.146% (2450/2688)
[Test] Epoch= 76  BatchID= 30 Loss: 0.295 | Acc: 90.930% (3569/3925)
Saving..
Best accuracy:  90.92993630573248

Epoch: 77
[Train] Epoch= 77  BatchID= 0 Loss: 0.129 | Acc: 96.094% (123/128)
[Train] Epoch= 77  BatchID= 10 Loss: 0.185 | Acc: 94.247% (1327/1408)
[Train] Epoch= 77  BatchID= 20 Loss: 0.187 | Acc: 93.824% (2522/2688)
[Train] Epoch= 77  BatchID= 30 Loss: 0.194 | Acc: 93.800% (3722/3968)
[Train] Epoch= 77  BatchID= 40 Loss: 0.193 | Acc: 93.845% (4925/5248)
[Train] Epoch= 77  BatchID= 50 Loss: 0.194 | Acc: 93.857% (6127/6528)
[Train] Epoch= 77  BatchID= 60 Loss: 0.194 | Acc: 93.827% (7326/7808)
[Train] Epoch= 77  BatchID= 70 Loss: 0.196 | Acc: 93.794% (8524/9088)
[Test] Epoch= 77  BatchID= 0 Loss: 0.114 | Acc: 96.094% (123/128)
[Test] Epoch= 77  BatchID= 10 Loss: 0.262 | Acc: 91.477% (1288/1408)
[Test] Epoch= 77  BatchID= 20 Loss: 0.263 | Acc: 91.890% (2470/2688)
[Test] Epoch= 77  BatchID= 30 Loss: 0.289 | Acc: 90.904% (3568/3925)

Epoch: 78
[Train] Epoch= 78  BatchID= 0 Loss: 0.126 | Acc: 96.875% (124/128)
[Train] Epoch= 78  BatchID= 10 Loss: 0.190 | Acc: 94.034% (1324/1408)
[Train] Epoch= 78  BatchID= 20 Loss: 0.191 | Acc: 93.899% (2524/2688)
[Train] Epoch= 78  BatchID= 30 Loss: 0.192 | Acc: 93.926% (3727/3968)
[Train] Epoch= 78  BatchID= 40 Loss: 0.196 | Acc: 93.731% (4919/5248)
[Train] Epoch= 78  BatchID= 50 Loss: 0.197 | Acc: 93.857% (6127/6528)
[Train] Epoch= 78  BatchID= 60 Loss: 0.199 | Acc: 93.596% (7308/7808)
[Train] Epoch= 78  BatchID= 70 Loss: 0.196 | Acc: 93.706% (8516/9088)
[Test] Epoch= 78  BatchID= 0 Loss: 0.113 | Acc: 96.875% (124/128)
[Test] Epoch= 78  BatchID= 10 Loss: 0.298 | Acc: 90.625% (1276/1408)
[Test] Epoch= 78  BatchID= 20 Loss: 0.289 | Acc: 91.034% (2447/2688)
[Test] Epoch= 78  BatchID= 30 Loss: 0.294 | Acc: 90.650% (3558/3925)

Epoch: 79
[Train] Epoch= 79  BatchID= 0 Loss: 0.139 | Acc: 93.750% (120/128)
[Train] Epoch= 79  BatchID= 10 Loss: 0.169 | Acc: 94.460% (1330/1408)
[Train] Epoch= 79  BatchID= 20 Loss: 0.185 | Acc: 94.122% (2530/2688)
[Train] Epoch= 79  BatchID= 30 Loss: 0.204 | Acc: 93.548% (3712/3968)
[Train] Epoch= 79  BatchID= 40 Loss: 0.200 | Acc: 93.579% (4911/5248)
[Train] Epoch= 79  BatchID= 50 Loss: 0.201 | Acc: 93.689% (6116/6528)
[Train] Epoch= 79  BatchID= 60 Loss: 0.197 | Acc: 93.840% (7327/7808)
[Train] Epoch= 79  BatchID= 70 Loss: 0.191 | Acc: 94.080% (8550/9088)
[Test] Epoch= 79  BatchID= 0 Loss: 0.091 | Acc: 96.875% (124/128)
[Test] Epoch= 79  BatchID= 10 Loss: 0.240 | Acc: 92.756% (1306/1408)
[Test] Epoch= 79  BatchID= 20 Loss: 0.249 | Acc: 92.113% (2476/2688)
[Test] Epoch= 79  BatchID= 30 Loss: 0.281 | Acc: 91.057% (3574/3925)
Saving..
Best accuracy:  91.05732484076434

Epoch: 80
[Train] Epoch= 80  BatchID= 0 Loss: 0.156 | Acc: 96.875% (124/128)
[Train] Epoch= 80  BatchID= 10 Loss: 0.195 | Acc: 94.460% (1330/1408)
[Train] Epoch= 80  BatchID= 20 Loss: 0.202 | Acc: 94.010% (2527/2688)
[Train] Epoch= 80  BatchID= 30 Loss: 0.203 | Acc: 94.027% (3731/3968)
[Train] Epoch= 80  BatchID= 40 Loss: 0.192 | Acc: 94.436% (4956/5248)
[Train] Epoch= 80  BatchID= 50 Loss: 0.193 | Acc: 94.317% (6157/6528)
[Train] Epoch= 80  BatchID= 60 Loss: 0.189 | Acc: 94.493% (7378/7808)
[Train] Epoch= 80  BatchID= 70 Loss: 0.188 | Acc: 94.520% (8590/9088)
[Test] Epoch= 80  BatchID= 0 Loss: 0.116 | Acc: 96.094% (123/128)
[Test] Epoch= 80  BatchID= 10 Loss: 0.295 | Acc: 90.696% (1277/1408)
[Test] Epoch= 80  BatchID= 20 Loss: 0.269 | Acc: 91.778% (2467/2688)
[Test] Epoch= 80  BatchID= 30 Loss: 0.286 | Acc: 91.261% (3582/3925)
Saving..
Best accuracy:  91.26114649681529

Epoch: 81
[Train] Epoch= 81  BatchID= 0 Loss: 0.117 | Acc: 96.875% (124/128)
[Train] Epoch= 81  BatchID= 10 Loss: 0.205 | Acc: 93.608% (1318/1408)
[Train] Epoch= 81  BatchID= 20 Loss: 0.195 | Acc: 94.048% (2528/2688)
[Train] Epoch= 81  BatchID= 30 Loss: 0.191 | Acc: 94.153% (3736/3968)
[Train] Epoch= 81  BatchID= 40 Loss: 0.185 | Acc: 94.417% (4955/5248)
[Train] Epoch= 81  BatchID= 50 Loss: 0.188 | Acc: 94.317% (6157/6528)
[Train] Epoch= 81  BatchID= 60 Loss: 0.185 | Acc: 94.352% (7367/7808)
[Train] Epoch= 81  BatchID= 70 Loss: 0.185 | Acc: 94.421% (8581/9088)
[Test] Epoch= 81  BatchID= 0 Loss: 0.092 | Acc: 97.656% (125/128)
[Test] Epoch= 81  BatchID= 10 Loss: 0.271 | Acc: 91.406% (1287/1408)
[Test] Epoch= 81  BatchID= 20 Loss: 0.265 | Acc: 91.481% (2459/2688)
[Test] Epoch= 81  BatchID= 30 Loss: 0.287 | Acc: 90.726% (3561/3925)

Epoch: 82
[Train] Epoch= 82  BatchID= 0 Loss: 0.377 | Acc: 90.625% (116/128)
[Train] Epoch= 82  BatchID= 10 Loss: 0.194 | Acc: 93.963% (1323/1408)
[Train] Epoch= 82  BatchID= 20 Loss: 0.191 | Acc: 94.308% (2535/2688)
[Train] Epoch= 82  BatchID= 30 Loss: 0.179 | Acc: 94.582% (3753/3968)
[Train] Epoch= 82  BatchID= 40 Loss: 0.189 | Acc: 94.360% (4952/5248)
[Train] Epoch= 82  BatchID= 50 Loss: 0.188 | Acc: 94.378% (6161/6528)
[Train] Epoch= 82  BatchID= 60 Loss: 0.191 | Acc: 94.262% (7360/7808)
[Train] Epoch= 82  BatchID= 70 Loss: 0.187 | Acc: 94.311% (8571/9088)
[Test] Epoch= 82  BatchID= 0 Loss: 0.097 | Acc: 95.312% (122/128)
[Test] Epoch= 82  BatchID= 10 Loss: 0.267 | Acc: 91.690% (1291/1408)
[Test] Epoch= 82  BatchID= 20 Loss: 0.254 | Acc: 92.076% (2475/2688)
[Test] Epoch= 82  BatchID= 30 Loss: 0.282 | Acc: 91.057% (3574/3925)

Epoch: 83
[Train] Epoch= 83  BatchID= 0 Loss: 0.273 | Acc: 91.406% (117/128)
[Train] Epoch= 83  BatchID= 10 Loss: 0.188 | Acc: 94.318% (1328/1408)
[Train] Epoch= 83  BatchID= 20 Loss: 0.192 | Acc: 94.345% (2536/2688)
[Train] Epoch= 83  BatchID= 30 Loss: 0.187 | Acc: 94.279% (3741/3968)
[Train] Epoch= 83  BatchID= 40 Loss: 0.191 | Acc: 94.036% (4935/5248)
[Train] Epoch= 83  BatchID= 50 Loss: 0.185 | Acc: 94.256% (6153/6528)
[Train] Epoch= 83  BatchID= 60 Loss: 0.184 | Acc: 94.352% (7367/7808)
[Train] Epoch= 83  BatchID= 70 Loss: 0.183 | Acc: 94.256% (8566/9088)
[Test] Epoch= 83  BatchID= 0 Loss: 0.108 | Acc: 96.875% (124/128)
[Test] Epoch= 83  BatchID= 10 Loss: 0.267 | Acc: 91.761% (1292/1408)
[Test] Epoch= 83  BatchID= 20 Loss: 0.263 | Acc: 92.001% (2473/2688)
[Test] Epoch= 83  BatchID= 30 Loss: 0.281 | Acc: 91.261% (3582/3925)

Epoch: 84
[Train] Epoch= 84  BatchID= 0 Loss: 0.101 | Acc: 96.875% (124/128)
[Train] Epoch= 84  BatchID= 10 Loss: 0.165 | Acc: 94.957% (1337/1408)
[Train] Epoch= 84  BatchID= 20 Loss: 0.181 | Acc: 94.606% (2543/2688)
[Train] Epoch= 84  BatchID= 30 Loss: 0.183 | Acc: 94.582% (3753/3968)
[Train] Epoch= 84  BatchID= 40 Loss: 0.184 | Acc: 94.531% (4961/5248)
[Train] Epoch= 84  BatchID= 50 Loss: 0.184 | Acc: 94.455% (6166/6528)
[Train] Epoch= 84  BatchID= 60 Loss: 0.185 | Acc: 94.416% (7372/7808)
[Train] Epoch= 84  BatchID= 70 Loss: 0.183 | Acc: 94.476% (8586/9088)
[Test] Epoch= 84  BatchID= 0 Loss: 0.091 | Acc: 96.875% (124/128)
[Test] Epoch= 84  BatchID= 10 Loss: 0.276 | Acc: 91.761% (1292/1408)
[Test] Epoch= 84  BatchID= 20 Loss: 0.263 | Acc: 92.225% (2479/2688)
[Test] Epoch= 84  BatchID= 30 Loss: 0.284 | Acc: 91.490% (3591/3925)
Saving..
Best accuracy:  91.49044585987261

Epoch: 85
[Train] Epoch= 85  BatchID= 0 Loss: 0.118 | Acc: 96.875% (124/128)
[Train] Epoch= 85  BatchID= 10 Loss: 0.197 | Acc: 93.679% (1319/1408)
[Train] Epoch= 85  BatchID= 20 Loss: 0.183 | Acc: 94.234% (2533/2688)
[Train] Epoch= 85  BatchID= 30 Loss: 0.178 | Acc: 94.456% (3748/3968)
[Train] Epoch= 85  BatchID= 40 Loss: 0.176 | Acc: 94.493% (4959/5248)
[Train] Epoch= 85  BatchID= 50 Loss: 0.181 | Acc: 94.455% (6166/6528)
[Train] Epoch= 85  BatchID= 60 Loss: 0.180 | Acc: 94.442% (7374/7808)
[Train] Epoch= 85  BatchID= 70 Loss: 0.178 | Acc: 94.520% (8590/9088)
[Test] Epoch= 85  BatchID= 0 Loss: 0.104 | Acc: 96.875% (124/128)
[Test] Epoch= 85  BatchID= 10 Loss: 0.276 | Acc: 91.619% (1290/1408)
[Test] Epoch= 85  BatchID= 20 Loss: 0.260 | Acc: 92.001% (2473/2688)
[Test] Epoch= 85  BatchID= 30 Loss: 0.281 | Acc: 91.261% (3582/3925)

Epoch: 86
[Train] Epoch= 86  BatchID= 0 Loss: 0.141 | Acc: 96.875% (124/128)
[Train] Epoch= 86  BatchID= 10 Loss: 0.186 | Acc: 95.028% (1338/1408)
[Train] Epoch= 86  BatchID= 20 Loss: 0.176 | Acc: 94.792% (2548/2688)
[Train] Epoch= 86  BatchID= 30 Loss: 0.170 | Acc: 95.060% (3772/3968)
[Train] Epoch= 86  BatchID= 40 Loss: 0.175 | Acc: 94.836% (4977/5248)
[Train] Epoch= 86  BatchID= 50 Loss: 0.176 | Acc: 94.776% (6187/6528)
[Train] Epoch= 86  BatchID= 60 Loss: 0.174 | Acc: 94.813% (7403/7808)
[Train] Epoch= 86  BatchID= 70 Loss: 0.169 | Acc: 94.993% (8633/9088)
[Test] Epoch= 86  BatchID= 0 Loss: 0.093 | Acc: 96.875% (124/128)
[Test] Epoch= 86  BatchID= 10 Loss: 0.267 | Acc: 91.619% (1290/1408)
[Test] Epoch= 86  BatchID= 20 Loss: 0.256 | Acc: 91.890% (2470/2688)
[Test] Epoch= 86  BatchID= 30 Loss: 0.278 | Acc: 91.134% (3577/3925)

Epoch: 87
[Train] Epoch= 87  BatchID= 0 Loss: 0.246 | Acc: 93.750% (120/128)
[Train] Epoch= 87  BatchID= 10 Loss: 0.163 | Acc: 95.526% (1345/1408)
[Train] Epoch= 87  BatchID= 20 Loss: 0.165 | Acc: 95.015% (2554/2688)
[Train] Epoch= 87  BatchID= 30 Loss: 0.175 | Acc: 94.834% (3763/3968)
[Train] Epoch= 87  BatchID= 40 Loss: 0.176 | Acc: 94.684% (4969/5248)
[Train] Epoch= 87  BatchID= 50 Loss: 0.176 | Acc: 94.562% (6173/6528)
[Train] Epoch= 87  BatchID= 60 Loss: 0.178 | Acc: 94.544% (7382/7808)
[Train] Epoch= 87  BatchID= 70 Loss: 0.177 | Acc: 94.553% (8593/9088)
[Test] Epoch= 87  BatchID= 0 Loss: 0.103 | Acc: 96.875% (124/128)
[Test] Epoch= 87  BatchID= 10 Loss: 0.278 | Acc: 91.406% (1287/1408)
[Test] Epoch= 87  BatchID= 20 Loss: 0.266 | Acc: 91.741% (2466/2688)
[Test] Epoch= 87  BatchID= 30 Loss: 0.282 | Acc: 91.236% (3581/3925)

Epoch: 88
[Train] Epoch= 88  BatchID= 0 Loss: 0.081 | Acc: 96.875% (124/128)
[Train] Epoch= 88  BatchID= 10 Loss: 0.190 | Acc: 93.892% (1322/1408)
[Train] Epoch= 88  BatchID= 20 Loss: 0.192 | Acc: 93.899% (2524/2688)
[Train] Epoch= 88  BatchID= 30 Loss: 0.185 | Acc: 94.153% (3736/3968)
[Train] Epoch= 88  BatchID= 40 Loss: 0.178 | Acc: 94.455% (4957/5248)
[Train] Epoch= 88  BatchID= 50 Loss: 0.173 | Acc: 94.577% (6174/6528)
[Train] Epoch= 88  BatchID= 60 Loss: 0.173 | Acc: 94.493% (7378/7808)
[Train] Epoch= 88  BatchID= 70 Loss: 0.174 | Acc: 94.465% (8585/9088)
[Test] Epoch= 88  BatchID= 0 Loss: 0.093 | Acc: 96.875% (124/128)
[Test] Epoch= 88  BatchID= 10 Loss: 0.277 | Acc: 91.335% (1286/1408)
[Test] Epoch= 88  BatchID= 20 Loss: 0.261 | Acc: 91.704% (2465/2688)
[Test] Epoch= 88  BatchID= 30 Loss: 0.279 | Acc: 91.032% (3573/3925)

Epoch: 89
[Train] Epoch= 89  BatchID= 0 Loss: 0.218 | Acc: 92.969% (119/128)
[Train] Epoch= 89  BatchID= 10 Loss: 0.190 | Acc: 93.750% (1320/1408)
[Train] Epoch= 89  BatchID= 20 Loss: 0.175 | Acc: 94.754% (2547/2688)
[Train] Epoch= 89  BatchID= 30 Loss: 0.179 | Acc: 94.657% (3756/3968)
[Train] Epoch= 89  BatchID= 40 Loss: 0.176 | Acc: 94.703% (4970/5248)
[Train] Epoch= 89  BatchID= 50 Loss: 0.173 | Acc: 94.761% (6186/6528)
[Train] Epoch= 89  BatchID= 60 Loss: 0.175 | Acc: 94.736% (7397/7808)
[Train] Epoch= 89  BatchID= 70 Loss: 0.176 | Acc: 94.674% (8604/9088)
[Test] Epoch= 89  BatchID= 0 Loss: 0.122 | Acc: 96.875% (124/128)
[Test] Epoch= 89  BatchID= 10 Loss: 0.288 | Acc: 90.909% (1280/1408)
[Test] Epoch= 89  BatchID= 20 Loss: 0.268 | Acc: 91.555% (2461/2688)
[Test] Epoch= 89  BatchID= 30 Loss: 0.283 | Acc: 91.006% (3572/3925)
