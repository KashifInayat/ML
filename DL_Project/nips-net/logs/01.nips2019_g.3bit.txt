==> Preparing data..
==> Building model..
DataParallel(
  (module): ResNet(
    (conv1): InputConv2dLSQ(
      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): LinearLSQ(
      in_features=512, out_features=10, bias=True
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=True)
    )
  )
)
==> Initializing from checkpoint..

Epoch: 0
==> Preparing data..
==> Building model..
DataParallel(
  (module): ResNet(
    (conv1): InputConv2dLSQ(
      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): LinearLSQ(
      in_features=512, out_features=10, bias=True
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=True)
    )
  )
)
==> Initializing from checkpoint..

Epoch: 0
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
[Train] Epoch= 0  BatchID= 0 Loss: 0.121 | Acc: 96.875% (124/128)
[Train] Epoch= 0  BatchID= 10 Loss: 0.215 | Acc: 93.679% (1319/1408)
[Train] Epoch= 0  BatchID= 20 Loss: 0.189 | Acc: 94.420% (2538/2688)
[Train] Epoch= 0  BatchID= 30 Loss: 0.194 | Acc: 94.078% (3733/3968)
[Train] Epoch= 0  BatchID= 40 Loss: 0.182 | Acc: 94.436% (4956/5248)
[Train] Epoch= 0  BatchID= 50 Loss: 0.185 | Acc: 94.194% (6149/6528)
[Train] Epoch= 0  BatchID= 60 Loss: 0.188 | Acc: 94.045% (7343/7808)
[Train] Epoch= 0  BatchID= 70 Loss: 0.183 | Acc: 94.179% (8559/9088)
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
[Train] Epoch= 0  BatchID= 0 Loss: 0.210 | Acc: 94.531% (121/128)
[Train] Epoch= 0  BatchID= 10 Loss: 0.190 | Acc: 94.247% (1327/1408)
[Train] Epoch= 0  BatchID= 20 Loss: 0.175 | Acc: 94.717% (2546/2688)
[Train] Epoch= 0  BatchID= 30 Loss: 0.177 | Acc: 94.607% (3754/3968)
[Train] Epoch= 0  BatchID= 40 Loss: 0.171 | Acc: 94.741% (4972/5248)
[Train] Epoch= 0  BatchID= 50 Loss: 0.166 | Acc: 94.776% (6187/6528)
[Train] Epoch= 0  BatchID= 60 Loss: 0.172 | Acc: 94.595% (7386/7808)
[Train] Epoch= 0  BatchID= 70 Loss: 0.172 | Acc: 94.542% (8592/9088)
[Test] Epoch= 0  BatchID= 0 Loss: 0.082 | Acc: 97.656% (125/128)
[Test] Epoch= 0  BatchID= 10 Loss: 0.360 | Acc: 89.702% (1263/1408)
[Test] Epoch= 0  BatchID= 20 Loss: 0.318 | Acc: 90.885% (2443/2688)
[Test] Epoch= 0  BatchID= 30 Loss: 0.353 | Acc: 89.885% (3528/3925)
Saving..
Best accuracy:  89.88535031847134

Epoch: 1
[Test] Epoch= 0  BatchID= 0 Loss: 0.095 | Acc: 97.656% (125/128)
[Test] Epoch= 0  BatchID= 10 Loss: 0.349 | Acc: 89.844% (1265/1408)
[Test] Epoch= 0  BatchID= 20 Loss: 0.324 | Acc: 90.737% (2439/2688)
[Test] Epoch= 0  BatchID= 30 Loss: 0.344 | Acc: 90.242% (3542/3925)
Saving..
Best accuracy:  90.24203821656052

Epoch: 1
[Train] Epoch= 1  BatchID= 0 Loss: 0.193 | Acc: 93.750% (120/128)
[Train] Epoch= 1  BatchID= 10 Loss: 0.168 | Acc: 94.318% (1328/1408)
[Train] Epoch= 1  BatchID= 20 Loss: 0.172 | Acc: 94.531% (2541/2688)
[Train] Epoch= 1  BatchID= 30 Loss: 0.171 | Acc: 94.632% (3755/3968)
[Train] Epoch= 1  BatchID= 40 Loss: 0.173 | Acc: 94.569% (4963/5248)
[Train] Epoch= 1  BatchID= 50 Loss: 0.170 | Acc: 94.730% (6184/6528)
[Train] Epoch= 1  BatchID= 60 Loss: 0.166 | Acc: 94.864% (7407/7808)
[Train] Epoch= 1  BatchID= 70 Loss: 0.160 | Acc: 95.092% (8642/9088)
[Train] Epoch= 1  BatchID= 0 Loss: 0.214 | Acc: 92.188% (118/128)
[Train] Epoch= 1  BatchID= 10 Loss: 0.163 | Acc: 95.028% (1338/1408)
[Train] Epoch= 1  BatchID= 20 Loss: 0.171 | Acc: 94.866% (2550/2688)
[Train] Epoch= 1  BatchID= 30 Loss: 0.175 | Acc: 94.834% (3763/3968)
[Train] Epoch= 1  BatchID= 40 Loss: 0.177 | Acc: 94.703% (4970/5248)
[Train] Epoch= 1  BatchID= 50 Loss: 0.172 | Acc: 94.853% (6192/6528)
[Train] Epoch= 1  BatchID= 60 Loss: 0.171 | Acc: 94.877% (7408/7808)
[Train] Epoch= 1  BatchID= 70 Loss: 0.174 | Acc: 94.762% (8612/9088)
[Test] Epoch= 1  BatchID= 0 Loss: 0.089 | Acc: 97.656% (125/128)
[Test] Epoch= 1  BatchID= 10 Loss: 0.351 | Acc: 89.844% (1265/1408)
[Test] Epoch= 1  BatchID= 20 Loss: 0.308 | Acc: 90.960% (2445/2688)
[Test] Epoch= 1  BatchID= 30 Loss: 0.332 | Acc: 90.344% (3546/3925)
Saving..
Best accuracy:  90.34394904458598

Epoch: 2
[Test] Epoch= 1  BatchID= 0 Loss: 0.072 | Acc: 97.656% (125/128)
[Test] Epoch= 1  BatchID= 10 Loss: 0.342 | Acc: 89.702% (1263/1408)
[Test] Epoch= 1  BatchID= 20 Loss: 0.315 | Acc: 90.588% (2435/2688)
[Test] Epoch= 1  BatchID= 30 Loss: 0.357 | Acc: 89.809% (3525/3925)

Epoch: 2
[Train] Epoch= 2  BatchID= 0 Loss: 0.107 | Acc: 96.875% (124/128)
[Train] Epoch= 2  BatchID= 10 Loss: 0.160 | Acc: 95.028% (1338/1408)
[Train] Epoch= 2  BatchID= 20 Loss: 0.153 | Acc: 95.089% (2556/2688)
[Train] Epoch= 2  BatchID= 30 Loss: 0.153 | Acc: 95.186% (3777/3968)
[Train] Epoch= 2  BatchID= 40 Loss: 0.155 | Acc: 95.179% (4995/5248)
[Train] Epoch= 2  BatchID= 50 Loss: 0.153 | Acc: 95.297% (6221/6528)
[Train] Epoch= 2  BatchID= 60 Loss: 0.151 | Acc: 95.415% (7450/7808)
[Train] Epoch= 2  BatchID= 70 Loss: 0.153 | Acc: 95.368% (8667/9088)
[Train] Epoch= 2  BatchID= 0 Loss: 0.109 | Acc: 96.875% (124/128)
[Train] Epoch= 2  BatchID= 10 Loss: 0.157 | Acc: 95.028% (1338/1408)
[Train] Epoch= 2  BatchID= 20 Loss: 0.147 | Acc: 95.201% (2559/2688)
[Train] Epoch= 2  BatchID= 30 Loss: 0.156 | Acc: 94.859% (3764/3968)
[Train] Epoch= 2  BatchID= 40 Loss: 0.155 | Acc: 94.874% (4979/5248)
[Train] Epoch= 2  BatchID= 50 Loss: 0.161 | Acc: 94.776% (6187/6528)
[Train] Epoch= 2  BatchID= 60 Loss: 0.166 | Acc: 94.698% (7394/7808)
[Train] Epoch= 2  BatchID= 70 Loss: 0.170 | Acc: 94.509% (8589/9088)
[Test] Epoch= 2  BatchID= 0 Loss: 0.090 | Acc: 97.656% (125/128)
[Test] Epoch= 2  BatchID= 10 Loss: 0.335 | Acc: 90.199% (1270/1408)
[Test] Epoch= 2  BatchID= 20 Loss: 0.325 | Acc: 90.588% (2435/2688)
[Test] Epoch= 2  BatchID= 30 Loss: 0.333 | Acc: 90.191% (3540/3925)

Epoch: 3
[Test] Epoch= 2  BatchID= 0 Loss: 0.090 | Acc: 96.875% (124/128)
[Test] Epoch= 2  BatchID= 10 Loss: 0.350 | Acc: 89.986% (1267/1408)
[Test] Epoch= 2  BatchID= 20 Loss: 0.313 | Acc: 90.848% (2442/2688)
[Test] Epoch= 2  BatchID= 30 Loss: 0.342 | Acc: 90.013% (3533/3925)

Epoch: 3
[Train] Epoch= 3  BatchID= 0 Loss: 0.095 | Acc: 96.875% (124/128)
[Train] Epoch= 3  BatchID= 10 Loss: 0.135 | Acc: 95.455% (1344/1408)
[Train] Epoch= 3  BatchID= 20 Loss: 0.134 | Acc: 95.685% (2572/2688)
[Train] Epoch= 3  BatchID= 30 Loss: 0.134 | Acc: 95.766% (3800/3968)
[Train] Epoch= 3  BatchID= 40 Loss: 0.141 | Acc: 95.389% (5006/5248)
[Train] Epoch= 3  BatchID= 50 Loss: 0.147 | Acc: 95.312% (6222/6528)
[Train] Epoch= 3  BatchID= 60 Loss: 0.147 | Acc: 95.415% (7450/7808)
[Train] Epoch= 3  BatchID= 70 Loss: 0.148 | Acc: 95.401% (8670/9088)
[Train] Epoch= 3  BatchID= 0 Loss: 0.139 | Acc: 96.094% (123/128)
[Train] Epoch= 3  BatchID= 10 Loss: 0.145 | Acc: 95.170% (1340/1408)
[Train] Epoch= 3  BatchID= 20 Loss: 0.139 | Acc: 95.312% (2562/2688)
[Train] Epoch= 3  BatchID= 30 Loss: 0.135 | Acc: 95.514% (3790/3968)
[Train] Epoch= 3  BatchID= 40 Loss: 0.137 | Acc: 95.655% (5020/5248)
[Train] Epoch= 3  BatchID= 50 Loss: 0.135 | Acc: 95.711% (6248/6528)
[Train] Epoch= 3  BatchID= 60 Loss: 0.137 | Acc: 95.633% (7467/7808)
[Train] Epoch= 3  BatchID= 70 Loss: 0.142 | Acc: 95.566% (8685/9088)
[Test] Epoch= 3  BatchID= 0 Loss: 0.147 | Acc: 96.094% (123/128)
[Test] Epoch= 3  BatchID= 10 Loss: 0.310 | Acc: 91.548% (1289/1408)
[Test] Epoch= 3  BatchID= 20 Loss: 0.313 | Acc: 91.183% (2451/2688)
[Test] Epoch= 3  BatchID= 30 Loss: 0.344 | Acc: 90.471% (3551/3925)
Saving..
Best accuracy:  90.47133757961784

Epoch: 4
[Test] Epoch= 3  BatchID= 0 Loss: 0.127 | Acc: 95.312% (122/128)
[Test] Epoch= 3  BatchID= 10 Loss: 0.353 | Acc: 89.062% (1254/1408)
[Test] Epoch= 3  BatchID= 20 Loss: 0.320 | Acc: 90.439% (2431/2688)
[Test] Epoch= 3  BatchID= 30 Loss: 0.354 | Acc: 89.732% (3522/3925)

Epoch: 4
[Train] Epoch= 4  BatchID= 0 Loss: 0.151 | Acc: 93.750% (120/128)
[Train] Epoch= 4  BatchID= 10 Loss: 0.158 | Acc: 94.744% (1334/1408)
[Train] Epoch= 4  BatchID= 20 Loss: 0.162 | Acc: 94.754% (2547/2688)
[Train] Epoch= 4  BatchID= 30 Loss: 0.167 | Acc: 94.632% (3755/3968)
[Train] Epoch= 4  BatchID= 40 Loss: 0.166 | Acc: 94.760% (4973/5248)
[Train] Epoch= 4  BatchID= 50 Loss: 0.165 | Acc: 94.884% (6194/6528)
[Train] Epoch= 4  BatchID= 60 Loss: 0.165 | Acc: 94.915% (7411/7808)
[Train] Epoch= 4  BatchID= 70 Loss: 0.167 | Acc: 94.839% (8619/9088)
[Train] Epoch= 4  BatchID= 0 Loss: 0.143 | Acc: 96.875% (124/128)
[Train] Epoch= 4  BatchID= 10 Loss: 0.159 | Acc: 95.597% (1346/1408)
[Train] Epoch= 4  BatchID= 20 Loss: 0.142 | Acc: 96.019% (2581/2688)
[Train] Epoch= 4  BatchID= 30 Loss: 0.156 | Acc: 95.363% (3784/3968)
[Train] Epoch= 4  BatchID= 40 Loss: 0.158 | Acc: 95.370% (5005/5248)
[Train] Epoch= 4  BatchID= 50 Loss: 0.159 | Acc: 95.144% (6211/6528)
[Train] Epoch= 4  BatchID= 60 Loss: 0.160 | Acc: 95.069% (7423/7808)
[Train] Epoch= 4  BatchID= 70 Loss: 0.158 | Acc: 95.169% (8649/9088)
[Test] Epoch= 4  BatchID= 0 Loss: 0.126 | Acc: 95.312% (122/128)
[Test] Epoch= 4  BatchID= 10 Loss: 0.345 | Acc: 90.199% (1270/1408)
[Test] Epoch= 4  BatchID= 20 Loss: 0.309 | Acc: 91.034% (2447/2688)
[Test] Epoch= 4  BatchID= 30 Loss: 0.338 | Acc: 90.038% (3534/3925)

Epoch: 5
[Test] Epoch= 4  BatchID= 0 Loss: 0.165 | Acc: 94.531% (121/128)
[Test] Epoch= 4  BatchID= 10 Loss: 0.329 | Acc: 90.625% (1276/1408)
[Test] Epoch= 4  BatchID= 20 Loss: 0.299 | Acc: 91.257% (2453/2688)
[Test] Epoch= 4  BatchID= 30 Loss: 0.350 | Acc: 89.732% (3522/3925)

Epoch: 5
[Train] Epoch= 5  BatchID= 0 Loss: 0.230 | Acc: 93.750% (120/128)
[Train] Epoch= 5  BatchID= 10 Loss: 0.155 | Acc: 95.384% (1343/1408)
[Train] Epoch= 5  BatchID= 20 Loss: 0.150 | Acc: 94.829% (2549/2688)
[Train] Epoch= 5  BatchID= 30 Loss: 0.156 | Acc: 94.783% (3761/3968)
[Train] Epoch= 5  BatchID= 40 Loss: 0.151 | Acc: 95.160% (4994/5248)
[Train] Epoch= 5  BatchID= 50 Loss: 0.146 | Acc: 95.328% (6223/6528)
[Train] Epoch= 5  BatchID= 60 Loss: 0.149 | Acc: 95.261% (7438/7808)
[Train] Epoch= 5  BatchID= 70 Loss: 0.152 | Acc: 95.213% (8653/9088)
[Train] Epoch= 5  BatchID= 0 Loss: 0.088 | Acc: 96.875% (124/128)
[Train] Epoch= 5  BatchID= 10 Loss: 0.144 | Acc: 95.668% (1347/1408)
[Train] Epoch= 5  BatchID= 20 Loss: 0.139 | Acc: 95.871% (2577/2688)
[Train] Epoch= 5  BatchID= 30 Loss: 0.143 | Acc: 95.817% (3802/3968)
[Train] Epoch= 5  BatchID= 40 Loss: 0.149 | Acc: 95.636% (5019/5248)
[Train] Epoch= 5  BatchID= 50 Loss: 0.150 | Acc: 95.619% (6242/6528)
[Train] Epoch= 5  BatchID= 60 Loss: 0.146 | Acc: 95.735% (7475/7808)
[Train] Epoch= 5  BatchID= 70 Loss: 0.149 | Acc: 95.676% (8695/9088)
[Test] Epoch= 5  BatchID= 0 Loss: 0.122 | Acc: 97.656% (125/128)
[Test] Epoch= 5  BatchID= 10 Loss: 0.341 | Acc: 90.199% (1270/1408)
[Test] Epoch= 5  BatchID= 20 Loss: 0.324 | Acc: 90.253% (2426/2688)
[Test] Epoch= 5  BatchID= 30 Loss: 0.356 | Acc: 89.350% (3507/3925)

Epoch: 6
[Test] Epoch= 5  BatchID= 0 Loss: 0.159 | Acc: 94.531% (121/128)
[Test] Epoch= 5  BatchID= 10 Loss: 0.295 | Acc: 91.193% (1284/1408)
[Test] Epoch= 5  BatchID= 20 Loss: 0.290 | Acc: 91.257% (2453/2688)
[Test] Epoch= 5  BatchID= 30 Loss: 0.342 | Acc: 89.987% (3532/3925)

Epoch: 6
[Train] Epoch= 6  BatchID= 0 Loss: 0.220 | Acc: 94.531% (121/128)
[Train] Epoch= 6  BatchID= 10 Loss: 0.149 | Acc: 95.384% (1343/1408)
[Train] Epoch= 6  BatchID= 20 Loss: 0.144 | Acc: 95.461% (2566/2688)
[Train] Epoch= 6  BatchID= 30 Loss: 0.141 | Acc: 95.590% (3793/3968)
[Train] Epoch= 6  BatchID= 40 Loss: 0.140 | Acc: 95.694% (5022/5248)
[Train] Epoch= 6  BatchID= 50 Loss: 0.143 | Acc: 95.604% (6241/6528)
[Train] Epoch= 6  BatchID= 60 Loss: 0.148 | Acc: 95.428% (7451/7808)
[Train] Epoch= 6  BatchID= 70 Loss: 0.151 | Acc: 95.346% (8665/9088)
[Train] Epoch= 6  BatchID= 0 Loss: 0.118 | Acc: 96.875% (124/128)
[Train] Epoch= 6  BatchID= 10 Loss: 0.170 | Acc: 94.247% (1327/1408)
[Train] Epoch= 6  BatchID= 20 Loss: 0.161 | Acc: 94.978% (2553/2688)
[Train] Epoch= 6  BatchID= 30 Loss: 0.164 | Acc: 95.035% (3771/3968)
[Train] Epoch= 6  BatchID= 40 Loss: 0.159 | Acc: 95.274% (5000/5248)
[Train] Epoch= 6  BatchID= 50 Loss: 0.153 | Acc: 95.343% (6224/6528)
[Train] Epoch= 6  BatchID= 60 Loss: 0.158 | Acc: 95.146% (7429/7808)
[Train] Epoch= 6  BatchID= 70 Loss: 0.159 | Acc: 95.070% (8640/9088)
[Test] Epoch= 6  BatchID= 0 Loss: 0.169 | Acc: 93.750% (120/128)
[Test] Epoch= 6  BatchID= 10 Loss: 0.344 | Acc: 90.128% (1269/1408)
[Test] Epoch= 6  BatchID= 20 Loss: 0.325 | Acc: 90.513% (2433/2688)
[Test] Epoch= 6  BatchID= 30 Loss: 0.351 | Acc: 89.605% (3517/3925)

Epoch: 7
[Test] Epoch= 6  BatchID= 0 Loss: 0.117 | Acc: 96.875% (124/128)
[Test] Epoch= 6  BatchID= 10 Loss: 0.323 | Acc: 90.838% (1279/1408)
[Test] Epoch= 6  BatchID= 20 Loss: 0.295 | Acc: 91.257% (2453/2688)
[Test] Epoch= 6  BatchID= 30 Loss: 0.342 | Acc: 89.962% (3531/3925)

Epoch: 7
[Train] Epoch= 7  BatchID= 0 Loss: 0.070 | Acc: 98.438% (126/128)
[Train] Epoch= 7  BatchID= 10 Loss: 0.123 | Acc: 96.307% (1356/1408)
[Train] Epoch= 7  BatchID= 20 Loss: 0.140 | Acc: 95.945% (2579/2688)
[Train] Epoch= 7  BatchID= 30 Loss: 0.143 | Acc: 95.791% (3801/3968)
[Train] Epoch= 7  BatchID= 40 Loss: 0.157 | Acc: 95.389% (5006/5248)
[Train] Epoch= 7  BatchID= 50 Loss: 0.155 | Acc: 95.466% (6232/6528)
[Train] Epoch= 7  BatchID= 60 Loss: 0.153 | Acc: 95.530% (7459/7808)
[Train] Epoch= 7  BatchID= 70 Loss: 0.152 | Acc: 95.511% (8680/9088)
[Train] Epoch= 7  BatchID= 0 Loss: 0.101 | Acc: 96.875% (124/128)
[Train] Epoch= 7  BatchID= 10 Loss: 0.154 | Acc: 95.241% (1341/1408)
[Train] Epoch= 7  BatchID= 20 Loss: 0.144 | Acc: 95.387% (2564/2688)
[Train] Epoch= 7  BatchID= 30 Loss: 0.146 | Acc: 95.312% (3782/3968)
[Train] Epoch= 7  BatchID= 40 Loss: 0.155 | Acc: 95.179% (4995/5248)
[Train] Epoch= 7  BatchID= 50 Loss: 0.154 | Acc: 95.221% (6216/6528)
[Train] Epoch= 7  BatchID= 60 Loss: 0.155 | Acc: 95.159% (7430/7808)
[Train] Epoch= 7  BatchID= 70 Loss: 0.157 | Acc: 95.037% (8637/9088)
[Test] Epoch= 7  BatchID= 0 Loss: 0.150 | Acc: 95.312% (122/128)
[Test] Epoch= 7  BatchID= 10 Loss: 0.322 | Acc: 90.838% (1279/1408)
[Test] Epoch= 7  BatchID= 20 Loss: 0.306 | Acc: 91.071% (2448/2688)
[Test] Epoch= 7  BatchID= 30 Loss: 0.351 | Acc: 89.656% (3519/3925)

Epoch: 8
[Test] Epoch= 7  BatchID= 0 Loss: 0.102 | Acc: 96.875% (124/128)
[Test] Epoch= 7  BatchID= 10 Loss: 0.323 | Acc: 90.625% (1276/1408)
[Test] Epoch= 7  BatchID= 20 Loss: 0.293 | Acc: 91.332% (2455/2688)
[Test] Epoch= 7  BatchID= 30 Loss: 0.341 | Acc: 90.318% (3545/3925)
Saving..
Best accuracy:  90.31847133757962

Epoch: 8
[Train] Epoch= 8  BatchID= 0 Loss: 0.208 | Acc: 92.969% (119/128)
[Train] Epoch= 8  BatchID= 10 Loss: 0.151 | Acc: 95.312% (1342/1408)
[Train] Epoch= 8  BatchID= 20 Loss: 0.147 | Acc: 95.647% (2571/2688)
[Train] Epoch= 8  BatchID= 30 Loss: 0.148 | Acc: 95.615% (3794/3968)
[Train] Epoch= 8  BatchID= 40 Loss: 0.145 | Acc: 95.636% (5019/5248)
[Train] Epoch= 8  BatchID= 50 Loss: 0.142 | Acc: 95.726% (6249/6528)
[Train] Epoch= 8  BatchID= 60 Loss: 0.141 | Acc: 95.799% (7480/7808)
[Train] Epoch= 8  BatchID= 70 Loss: 0.141 | Acc: 95.775% (8704/9088)
[Train] Epoch= 8  BatchID= 0 Loss: 0.093 | Acc: 96.875% (124/128)
[Train] Epoch= 8  BatchID= 10 Loss: 0.149 | Acc: 95.526% (1345/1408)
[Train] Epoch= 8  BatchID= 20 Loss: 0.156 | Acc: 95.275% (2561/2688)
[Train] Epoch= 8  BatchID= 30 Loss: 0.155 | Acc: 95.212% (3778/3968)
[Train] Epoch= 8  BatchID= 40 Loss: 0.153 | Acc: 95.293% (5001/5248)
[Train] Epoch= 8  BatchID= 50 Loss: 0.146 | Acc: 95.481% (6233/6528)
[Train] Epoch= 8  BatchID= 60 Loss: 0.146 | Acc: 95.466% (7454/7808)
[Train] Epoch= 8  BatchID= 70 Loss: 0.148 | Acc: 95.434% (8673/9088)
[Test] Epoch= 8  BatchID= 0 Loss: 0.118 | Acc: 96.094% (123/128)
[Test] Epoch= 8  BatchID= 10 Loss: 0.345 | Acc: 90.412% (1273/1408)
[Test] Epoch= 8  BatchID= 20 Loss: 0.317 | Acc: 90.923% (2444/2688)
[Test] Epoch= 8  BatchID= 30 Loss: 0.354 | Acc: 90.013% (3533/3925)

Epoch: 9
[Test] Epoch= 8  BatchID= 0 Loss: 0.107 | Acc: 97.656% (125/128)
[Test] Epoch= 8  BatchID= 10 Loss: 0.322 | Acc: 91.051% (1282/1408)
[Test] Epoch= 8  BatchID= 20 Loss: 0.302 | Acc: 91.555% (2461/2688)
[Test] Epoch= 8  BatchID= 30 Loss: 0.336 | Acc: 90.497% (3552/3925)
Saving..
Best accuracy:  90.4968152866242

Epoch: 9
[Train] Epoch= 9  BatchID= 0 Loss: 0.218 | Acc: 94.531% (121/128)
[Train] Epoch= 9  BatchID= 10 Loss: 0.150 | Acc: 95.881% (1350/1408)
[Train] Epoch= 9  BatchID= 20 Loss: 0.141 | Acc: 95.871% (2577/2688)
[Train] Epoch= 9  BatchID= 30 Loss: 0.137 | Acc: 95.892% (3805/3968)
[Train] Epoch= 9  BatchID= 40 Loss: 0.145 | Acc: 95.751% (5025/5248)
[Train] Epoch= 9  BatchID= 50 Loss: 0.149 | Acc: 95.634% (6243/6528)
[Train] Epoch= 9  BatchID= 60 Loss: 0.150 | Acc: 95.620% (7466/7808)
[Train] Epoch= 9  BatchID= 70 Loss: 0.152 | Acc: 95.522% (8681/9088)
[Train] Epoch= 9  BatchID= 0 Loss: 0.038 | Acc: 99.219% (127/128)
[Train] Epoch= 9  BatchID= 10 Loss: 0.123 | Acc: 96.662% (1361/1408)
[Train] Epoch= 9  BatchID= 20 Loss: 0.136 | Acc: 96.131% (2584/2688)
[Train] Epoch= 9  BatchID= 30 Loss: 0.144 | Acc: 95.640% (3795/3968)
[Train] Epoch= 9  BatchID= 40 Loss: 0.143 | Acc: 95.655% (5020/5248)
[Train] Epoch= 9  BatchID= 50 Loss: 0.147 | Acc: 95.527% (6236/6528)
[Train] Epoch= 9  BatchID= 60 Loss: 0.148 | Acc: 95.402% (7449/7808)
[Train] Epoch= 9  BatchID= 70 Loss: 0.149 | Acc: 95.412% (8671/9088)
[Test] Epoch= 9  BatchID= 0 Loss: 0.105 | Acc: 96.875% (124/128)
[Test] Epoch= 9  BatchID= 10 Loss: 0.339 | Acc: 89.844% (1265/1408)
[Test] Epoch= 9  BatchID= 20 Loss: 0.314 | Acc: 90.551% (2434/2688)
[Test] Epoch= 9  BatchID= 30 Loss: 0.343 | Acc: 89.834% (3526/3925)

Epoch: 10
[Test] Epoch= 9  BatchID= 0 Loss: 0.150 | Acc: 96.875% (124/128)
[Test] Epoch= 9  BatchID= 10 Loss: 0.326 | Acc: 90.625% (1276/1408)
[Test] Epoch= 9  BatchID= 20 Loss: 0.306 | Acc: 90.848% (2442/2688)
[Test] Epoch= 9  BatchID= 30 Loss: 0.331 | Acc: 90.471% (3551/3925)

Epoch: 10
[Train] Epoch= 10  BatchID= 0 Loss: 0.090 | Acc: 97.656% (125/128)
[Train] Epoch= 10  BatchID= 10 Loss: 0.129 | Acc: 96.520% (1359/1408)
[Train] Epoch= 10  BatchID= 20 Loss: 0.140 | Acc: 95.871% (2577/2688)
[Train] Epoch= 10  BatchID= 30 Loss: 0.152 | Acc: 95.186% (3777/3968)
[Train] Epoch= 10  BatchID= 40 Loss: 0.144 | Acc: 95.541% (5014/5248)
[Train] Epoch= 10  BatchID= 50 Loss: 0.147 | Acc: 95.389% (6227/6528)
[Train] Epoch= 10  BatchID= 60 Loss: 0.148 | Acc: 95.351% (7445/7808)
[Train] Epoch= 10  BatchID= 70 Loss: 0.145 | Acc: 95.368% (8667/9088)
[Train] Epoch= 10  BatchID= 0 Loss: 0.141 | Acc: 93.750% (120/128)
[Train] Epoch= 10  BatchID= 10 Loss: 0.165 | Acc: 94.815% (1335/1408)
[Train] Epoch= 10  BatchID= 20 Loss: 0.146 | Acc: 95.424% (2565/2688)
[Train] Epoch= 10  BatchID= 30 Loss: 0.150 | Acc: 95.363% (3784/3968)
[Train] Epoch= 10  BatchID= 40 Loss: 0.147 | Acc: 95.332% (5003/5248)
[Train] Epoch= 10  BatchID= 50 Loss: 0.153 | Acc: 95.067% (6206/6528)
[Train] Epoch= 10  BatchID= 60 Loss: 0.153 | Acc: 95.031% (7420/7808)
[Train] Epoch= 10  BatchID= 70 Loss: 0.156 | Acc: 94.894% (8624/9088)
[Test] Epoch= 10  BatchID= 0 Loss: 0.150 | Acc: 96.875% (124/128)
[Test] Epoch= 10  BatchID= 10 Loss: 0.335 | Acc: 89.844% (1265/1408)
[Test] Epoch= 10  BatchID= 20 Loss: 0.313 | Acc: 90.290% (2427/2688)
[Test] Epoch= 10  BatchID= 30 Loss: 0.338 | Acc: 89.529% (3514/3925)

Epoch: 11
[Test] Epoch= 10  BatchID= 0 Loss: 0.139 | Acc: 96.094% (123/128)
[Test] Epoch= 10  BatchID= 10 Loss: 0.304 | Acc: 91.619% (1290/1408)
[Test] Epoch= 10  BatchID= 20 Loss: 0.287 | Acc: 91.592% (2462/2688)
[Test] Epoch= 10  BatchID= 30 Loss: 0.332 | Acc: 90.242% (3542/3925)

Epoch: 11
[Train] Epoch= 11  BatchID= 0 Loss: 0.274 | Acc: 92.969% (119/128)
[Train] Epoch= 11  BatchID= 10 Loss: 0.133 | Acc: 95.952% (1351/1408)
[Train] Epoch= 11  BatchID= 20 Loss: 0.140 | Acc: 95.461% (2566/2688)
[Train] Epoch= 11  BatchID= 30 Loss: 0.140 | Acc: 95.439% (3787/3968)
[Train] Epoch= 11  BatchID= 40 Loss: 0.139 | Acc: 95.503% (5012/5248)
[Train] Epoch= 11  BatchID= 50 Loss: 0.147 | Acc: 95.297% (6221/6528)
[Train] Epoch= 11  BatchID= 60 Loss: 0.147 | Acc: 95.364% (7446/7808)
[Train] Epoch= 11  BatchID= 70 Loss: 0.146 | Acc: 95.357% (8666/9088)
[Train] Epoch= 11  BatchID= 0 Loss: 0.109 | Acc: 96.875% (124/128)
[Train] Epoch= 11  BatchID= 10 Loss: 0.162 | Acc: 94.886% (1336/1408)
[Train] Epoch= 11  BatchID= 20 Loss: 0.156 | Acc: 95.052% (2555/2688)
[Train] Epoch= 11  BatchID= 30 Loss: 0.155 | Acc: 95.388% (3785/3968)
[Train] Epoch= 11  BatchID= 40 Loss: 0.153 | Acc: 95.370% (5005/5248)
[Train] Epoch= 11  BatchID= 50 Loss: 0.154 | Acc: 95.251% (6218/6528)
[Train] Epoch= 11  BatchID= 60 Loss: 0.153 | Acc: 95.248% (7437/7808)
[Train] Epoch= 11  BatchID= 70 Loss: 0.152 | Acc: 95.246% (8656/9088)
[Test] Epoch= 11  BatchID= 0 Loss: 0.130 | Acc: 96.875% (124/128)
[Test] Epoch= 11  BatchID= 10 Loss: 0.331 | Acc: 90.625% (1276/1408)
[Test] Epoch= 11  BatchID= 20 Loss: 0.320 | Acc: 90.625% (2436/2688)
[Test] Epoch= 11  BatchID= 30 Loss: 0.359 | Acc: 89.401% (3509/3925)

Epoch: 12
[Test] Epoch= 11  BatchID= 0 Loss: 0.119 | Acc: 96.094% (123/128)
[Test] Epoch= 11  BatchID= 10 Loss: 0.367 | Acc: 90.057% (1268/1408)
[Test] Epoch= 11  BatchID= 20 Loss: 0.320 | Acc: 90.662% (2437/2688)
[Test] Epoch= 11  BatchID= 30 Loss: 0.346 | Acc: 89.911% (3529/3925)

Epoch: 12
[Train] Epoch= 12  BatchID= 0 Loss: 0.207 | Acc: 92.188% (118/128)
[Train] Epoch= 12  BatchID= 10 Loss: 0.151 | Acc: 95.241% (1341/1408)
[Train] Epoch= 12  BatchID= 20 Loss: 0.151 | Acc: 95.089% (2556/2688)
[Train] Epoch= 12  BatchID= 30 Loss: 0.149 | Acc: 95.312% (3782/3968)
[Train] Epoch= 12  BatchID= 40 Loss: 0.149 | Acc: 95.217% (4997/5248)
[Train] Epoch= 12  BatchID= 50 Loss: 0.150 | Acc: 95.282% (6220/6528)
[Train] Epoch= 12  BatchID= 60 Loss: 0.145 | Acc: 95.428% (7451/7808)
[Train] Epoch= 12  BatchID= 70 Loss: 0.145 | Acc: 95.445% (8674/9088)
[Train] Epoch= 12  BatchID= 0 Loss: 0.086 | Acc: 96.875% (124/128)
[Train] Epoch= 12  BatchID= 10 Loss: 0.164 | Acc: 95.455% (1344/1408)
[Train] Epoch= 12  BatchID= 20 Loss: 0.154 | Acc: 95.871% (2577/2688)
[Train] Epoch= 12  BatchID= 30 Loss: 0.149 | Acc: 95.791% (3801/3968)
[Train] Epoch= 12  BatchID= 40 Loss: 0.147 | Acc: 95.694% (5022/5248)
[Train] Epoch= 12  BatchID= 50 Loss: 0.153 | Acc: 95.573% (6239/6528)
[Train] Epoch= 12  BatchID= 60 Loss: 0.151 | Acc: 95.556% (7461/7808)
[Train] Epoch= 12  BatchID= 70 Loss: 0.152 | Acc: 95.577% (8686/9088)
[Test] Epoch= 12  BatchID= 0 Loss: 0.100 | Acc: 97.656% (125/128)
[Test] Epoch= 12  BatchID= 10 Loss: 0.326 | Acc: 90.483% (1274/1408)
[Test] Epoch= 12  BatchID= 20 Loss: 0.308 | Acc: 90.848% (2442/2688)
[Test] Epoch= 12  BatchID= 30 Loss: 0.346 | Acc: 89.987% (3532/3925)

Epoch: 13
[Test] Epoch= 12  BatchID= 0 Loss: 0.093 | Acc: 98.438% (126/128)
[Test] Epoch= 12  BatchID= 10 Loss: 0.325 | Acc: 90.483% (1274/1408)
[Test] Epoch= 12  BatchID= 20 Loss: 0.306 | Acc: 90.960% (2445/2688)
[Test] Epoch= 12  BatchID= 30 Loss: 0.327 | Acc: 90.599% (3556/3925)
Saving..
Best accuracy:  90.59872611464968

Epoch: 13
[Train] Epoch= 13  BatchID= 0 Loss: 0.169 | Acc: 96.094% (123/128)
[Train] Epoch= 13  BatchID= 10 Loss: 0.148 | Acc: 95.597% (1346/1408)
[Train] Epoch= 13  BatchID= 20 Loss: 0.146 | Acc: 95.499% (2567/2688)
[Train] Epoch= 13  BatchID= 30 Loss: 0.151 | Acc: 95.338% (3783/3968)
[Train] Epoch= 13  BatchID= 40 Loss: 0.148 | Acc: 95.427% (5008/5248)
[Train] Epoch= 13  BatchID= 50 Loss: 0.150 | Acc: 95.512% (6235/6528)
[Train] Epoch= 13  BatchID= 60 Loss: 0.144 | Acc: 95.658% (7469/7808)
[Train] Epoch= 13  BatchID= 70 Loss: 0.144 | Acc: 95.588% (8687/9088)
[Train] Epoch= 13  BatchID= 0 Loss: 0.127 | Acc: 96.094% (123/128)
[Train] Epoch= 13  BatchID= 10 Loss: 0.141 | Acc: 95.881% (1350/1408)
[Train] Epoch= 13  BatchID= 20 Loss: 0.142 | Acc: 95.685% (2572/2688)
[Train] Epoch= 13  BatchID= 30 Loss: 0.147 | Acc: 95.539% (3791/3968)
[Train] Epoch= 13  BatchID= 40 Loss: 0.146 | Acc: 95.617% (5018/5248)
[Train] Epoch= 13  BatchID= 50 Loss: 0.148 | Acc: 95.527% (6236/6528)
[Train] Epoch= 13  BatchID= 60 Loss: 0.146 | Acc: 95.581% (7463/7808)
[Train] Epoch= 13  BatchID= 70 Loss: 0.146 | Acc: 95.654% (8693/9088)
[Test] Epoch= 13  BatchID= 0 Loss: 0.130 | Acc: 96.094% (123/128)
[Test] Epoch= 13  BatchID= 10 Loss: 0.339 | Acc: 89.773% (1264/1408)
[Test] Epoch= 13  BatchID= 20 Loss: 0.324 | Acc: 90.253% (2426/2688)
[Test] Epoch= 13  BatchID= 30 Loss: 0.345 | Acc: 89.809% (3525/3925)

Epoch: 14
[Test] Epoch= 13  BatchID= 0 Loss: 0.133 | Acc: 96.875% (124/128)
[Test] Epoch= 13  BatchID= 10 Loss: 0.330 | Acc: 90.341% (1272/1408)
[Test] Epoch= 13  BatchID= 20 Loss: 0.296 | Acc: 91.109% (2449/2688)
[Test] Epoch= 13  BatchID= 30 Loss: 0.346 | Acc: 90.038% (3534/3925)

Epoch: 14
[Train] Epoch= 14  BatchID= 0 Loss: 0.163 | Acc: 94.531% (121/128)
[Train] Epoch= 14  BatchID= 10 Loss: 0.149 | Acc: 95.668% (1347/1408)
[Train] Epoch= 14  BatchID= 20 Loss: 0.153 | Acc: 95.126% (2557/2688)
[Train] Epoch= 14  BatchID= 30 Loss: 0.145 | Acc: 95.665% (3796/3968)
[Train] Epoch= 14  BatchID= 40 Loss: 0.146 | Acc: 95.541% (5014/5248)
[Train] Epoch= 14  BatchID= 50 Loss: 0.150 | Acc: 95.512% (6235/6528)
[Train] Epoch= 14  BatchID= 60 Loss: 0.150 | Acc: 95.466% (7454/7808)
[Train] Epoch= 14  BatchID= 70 Loss: 0.151 | Acc: 95.401% (8670/9088)
[Train] Epoch= 14  BatchID= 0 Loss: 0.062 | Acc: 97.656% (125/128)
[Train] Epoch= 14  BatchID= 10 Loss: 0.111 | Acc: 96.804% (1363/1408)
[Train] Epoch= 14  BatchID= 20 Loss: 0.126 | Acc: 96.205% (2586/2688)
[Train] Epoch= 14  BatchID= 30 Loss: 0.136 | Acc: 95.665% (3796/3968)
[Train] Epoch= 14  BatchID= 40 Loss: 0.135 | Acc: 95.675% (5021/5248)
[Train] Epoch= 14  BatchID= 50 Loss: 0.136 | Acc: 95.711% (6248/6528)
[Train] Epoch= 14  BatchID= 60 Loss: 0.138 | Acc: 95.735% (7475/7808)
[Train] Epoch= 14  BatchID= 70 Loss: 0.135 | Acc: 95.808% (8707/9088)
[Test] Epoch= 14  BatchID= 0 Loss: 0.104 | Acc: 98.438% (126/128)
[Test] Epoch= 14  BatchID= 10 Loss: 0.341 | Acc: 90.270% (1271/1408)
[Test] Epoch= 14  BatchID= 20 Loss: 0.314 | Acc: 90.699% (2438/2688)
[Test] Epoch= 14  BatchID= 30 Loss: 0.345 | Acc: 89.936% (3530/3925)

Epoch: 15
[Test] Epoch= 14  BatchID= 0 Loss: 0.105 | Acc: 96.875% (124/128)
[Test] Epoch= 14  BatchID= 10 Loss: 0.323 | Acc: 90.554% (1275/1408)
[Test] Epoch= 14  BatchID= 20 Loss: 0.301 | Acc: 90.997% (2446/2688)
[Test] Epoch= 14  BatchID= 30 Loss: 0.341 | Acc: 90.140% (3538/3925)

Epoch: 15
[Train] Epoch= 15  BatchID= 0 Loss: 0.111 | Acc: 97.656% (125/128)
[Train] Epoch= 15  BatchID= 10 Loss: 0.117 | Acc: 96.662% (1361/1408)
[Train] Epoch= 15  BatchID= 20 Loss: 0.138 | Acc: 95.759% (2574/2688)
[Train] Epoch= 15  BatchID= 30 Loss: 0.144 | Acc: 95.615% (3794/3968)
[Train] Epoch= 15  BatchID= 40 Loss: 0.144 | Acc: 95.579% (5016/5248)
[Train] Epoch= 15  BatchID= 50 Loss: 0.146 | Acc: 95.558% (6238/6528)
[Train] Epoch= 15  BatchID= 60 Loss: 0.144 | Acc: 95.645% (7468/7808)
[Train] Epoch= 15  BatchID= 70 Loss: 0.141 | Acc: 95.720% (8699/9088)
[Train] Epoch= 15  BatchID= 0 Loss: 0.134 | Acc: 95.312% (122/128)
[Train] Epoch= 15  BatchID= 10 Loss: 0.133 | Acc: 95.881% (1350/1408)
[Train] Epoch= 15  BatchID= 20 Loss: 0.148 | Acc: 95.610% (2570/2688)
[Train] Epoch= 15  BatchID= 30 Loss: 0.138 | Acc: 95.867% (3804/3968)
[Train] Epoch= 15  BatchID= 40 Loss: 0.144 | Acc: 95.465% (5010/5248)
[Train] Epoch= 15  BatchID= 50 Loss: 0.141 | Acc: 95.573% (6239/6528)
[Train] Epoch= 15  BatchID= 60 Loss: 0.141 | Acc: 95.633% (7467/7808)
[Train] Epoch= 15  BatchID= 70 Loss: 0.143 | Acc: 95.643% (8692/9088)
[Test] Epoch= 15  BatchID= 0 Loss: 0.093 | Acc: 97.656% (125/128)
[Test] Epoch= 15  BatchID= 10 Loss: 0.282 | Acc: 92.259% (1299/1408)
[Test] Epoch= 15  BatchID= 20 Loss: 0.313 | Acc: 90.885% (2443/2688)
[Test] Epoch= 15  BatchID= 30 Loss: 0.349 | Acc: 90.038% (3534/3925)

Epoch: 16
[Test] Epoch= 15  BatchID= 0 Loss: 0.114 | Acc: 97.656% (125/128)
[Test] Epoch= 15  BatchID= 10 Loss: 0.317 | Acc: 90.838% (1279/1408)
[Test] Epoch= 15  BatchID= 20 Loss: 0.294 | Acc: 91.034% (2447/2688)
[Test] Epoch= 15  BatchID= 30 Loss: 0.335 | Acc: 90.217% (3541/3925)

Epoch: 16
[Train] Epoch= 16  BatchID= 0 Loss: 0.226 | Acc: 94.531% (121/128)
[Train] Epoch= 16  BatchID= 10 Loss: 0.152 | Acc: 95.739% (1348/1408)
[Train] Epoch= 16  BatchID= 20 Loss: 0.150 | Acc: 95.796% (2575/2688)
[Train] Epoch= 16  BatchID= 30 Loss: 0.148 | Acc: 95.968% (3808/3968)
[Train] Epoch= 16  BatchID= 40 Loss: 0.147 | Acc: 95.827% (5029/5248)
[Train] Epoch= 16  BatchID= 50 Loss: 0.144 | Acc: 95.803% (6254/6528)
[Train] Epoch= 16  BatchID= 60 Loss: 0.143 | Acc: 95.774% (7478/7808)
[Train] Epoch= 16  BatchID= 70 Loss: 0.142 | Acc: 95.775% (8704/9088)
[Train] Epoch= 16  BatchID= 0 Loss: 0.155 | Acc: 95.312% (122/128)
[Train] Epoch= 16  BatchID= 10 Loss: 0.144 | Acc: 95.668% (1347/1408)
[Train] Epoch= 16  BatchID= 20 Loss: 0.142 | Acc: 95.536% (2568/2688)
[Train] Epoch= 16  BatchID= 30 Loss: 0.138 | Acc: 95.615% (3794/3968)
[Train] Epoch= 16  BatchID= 40 Loss: 0.141 | Acc: 95.503% (5012/5248)
[Train] Epoch= 16  BatchID= 50 Loss: 0.140 | Acc: 95.496% (6234/6528)
[Train] Epoch= 16  BatchID= 60 Loss: 0.141 | Acc: 95.453% (7453/7808)
[Train] Epoch= 16  BatchID= 70 Loss: 0.141 | Acc: 95.467% (8676/9088)
[Test] Epoch= 16  BatchID= 0 Loss: 0.112 | Acc: 97.656% (125/128)
[Test] Epoch= 16  BatchID= 10 Loss: 0.330 | Acc: 90.270% (1271/1408)
[Test] Epoch= 16  BatchID= 20 Loss: 0.318 | Acc: 90.513% (2433/2688)
[Test] Epoch= 16  BatchID= 30 Loss: 0.333 | Acc: 90.115% (3537/3925)

Epoch: 17
[Test] Epoch= 16  BatchID= 0 Loss: 0.118 | Acc: 96.875% (124/128)
[Test] Epoch= 16  BatchID= 10 Loss: 0.318 | Acc: 90.838% (1279/1408)
[Test] Epoch= 16  BatchID= 20 Loss: 0.297 | Acc: 91.369% (2456/2688)
[Test] Epoch= 16  BatchID= 30 Loss: 0.357 | Acc: 89.911% (3529/3925)

Epoch: 17
[Train] Epoch= 17  BatchID= 0 Loss: 0.086 | Acc: 97.656% (125/128)
[Train] Epoch= 17  BatchID= 10 Loss: 0.119 | Acc: 96.520% (1359/1408)
[Train] Epoch= 17  BatchID= 20 Loss: 0.143 | Acc: 95.573% (2569/2688)
[Train] Epoch= 17  BatchID= 30 Loss: 0.138 | Acc: 95.766% (3800/3968)
[Train] Epoch= 17  BatchID= 40 Loss: 0.138 | Acc: 95.827% (5029/5248)
[Train] Epoch= 17  BatchID= 50 Loss: 0.138 | Acc: 95.910% (6261/6528)
[Train] Epoch= 17  BatchID= 60 Loss: 0.139 | Acc: 95.914% (7489/7808)
[Train] Epoch= 17  BatchID= 70 Loss: 0.144 | Acc: 95.698% (8697/9088)
[Train] Epoch= 17  BatchID= 0 Loss: 0.169 | Acc: 93.750% (120/128)
[Train] Epoch= 17  BatchID= 10 Loss: 0.146 | Acc: 95.312% (1342/1408)
[Train] Epoch= 17  BatchID= 20 Loss: 0.133 | Acc: 95.833% (2576/2688)
[Train] Epoch= 17  BatchID= 30 Loss: 0.146 | Acc: 95.413% (3786/3968)
[Train] Epoch= 17  BatchID= 40 Loss: 0.145 | Acc: 95.446% (5009/5248)
[Train] Epoch= 17  BatchID= 50 Loss: 0.147 | Acc: 95.450% (6231/6528)
[Train] Epoch= 17  BatchID= 60 Loss: 0.142 | Acc: 95.620% (7466/7808)
[Train] Epoch= 17  BatchID= 70 Loss: 0.140 | Acc: 95.621% (8690/9088)
[Test] Epoch= 17  BatchID= 0 Loss: 0.121 | Acc: 97.656% (125/128)
[Test] Epoch= 17  BatchID= 10 Loss: 0.333 | Acc: 90.412% (1273/1408)
[Test] Epoch= 17  BatchID= 20 Loss: 0.309 | Acc: 90.774% (2440/2688)
[Test] Epoch= 17  BatchID= 30 Loss: 0.345 | Acc: 89.758% (3523/3925)

Epoch: 18
[Test] Epoch= 17  BatchID= 0 Loss: 0.105 | Acc: 97.656% (125/128)
[Test] Epoch= 17  BatchID= 10 Loss: 0.312 | Acc: 91.051% (1282/1408)
[Test] Epoch= 17  BatchID= 20 Loss: 0.315 | Acc: 90.923% (2444/2688)
[Test] Epoch= 17  BatchID= 30 Loss: 0.356 | Acc: 90.140% (3538/3925)

Epoch: 18
[Train] Epoch= 18  BatchID= 0 Loss: 0.164 | Acc: 92.188% (118/128)
[Train] Epoch= 18  BatchID= 10 Loss: 0.130 | Acc: 95.312% (1342/1408)
[Train] Epoch= 18  BatchID= 20 Loss: 0.137 | Acc: 95.461% (2566/2688)
[Train] Epoch= 18  BatchID= 30 Loss: 0.146 | Acc: 95.262% (3780/3968)
[Train] Epoch= 18  BatchID= 40 Loss: 0.144 | Acc: 95.408% (5007/5248)
[Train] Epoch= 18  BatchID= 50 Loss: 0.142 | Acc: 95.481% (6233/6528)
[Train] Epoch= 18  BatchID= 60 Loss: 0.142 | Acc: 95.466% (7454/7808)
[Train] Epoch= 18  BatchID= 70 Loss: 0.140 | Acc: 95.533% (8682/9088)
[Train] Epoch= 18  BatchID= 0 Loss: 0.168 | Acc: 93.750% (120/128)
[Train] Epoch= 18  BatchID= 10 Loss: 0.147 | Acc: 95.312% (1342/1408)
[Train] Epoch= 18  BatchID= 20 Loss: 0.150 | Acc: 95.238% (2560/2688)
[Train] Epoch= 18  BatchID= 30 Loss: 0.145 | Acc: 95.514% (3790/3968)
[Train] Epoch= 18  BatchID= 40 Loss: 0.141 | Acc: 95.541% (5014/5248)
[Train] Epoch= 18  BatchID= 50 Loss: 0.145 | Acc: 95.573% (6239/6528)
[Train] Epoch= 18  BatchID= 60 Loss: 0.145 | Acc: 95.607% (7465/7808)
[Train] Epoch= 18  BatchID= 70 Loss: 0.148 | Acc: 95.489% (8678/9088)
[Test] Epoch= 18  BatchID= 0 Loss: 0.120 | Acc: 96.875% (124/128)
[Test] Epoch= 18  BatchID= 10 Loss: 0.342 | Acc: 89.844% (1265/1408)
[Test] Epoch= 18  BatchID= 20 Loss: 0.307 | Acc: 90.774% (2440/2688)
[Test] Epoch= 18  BatchID= 30 Loss: 0.338 | Acc: 90.038% (3534/3925)

Epoch: 19
[Test] Epoch= 18  BatchID= 0 Loss: 0.095 | Acc: 97.656% (125/128)
[Test] Epoch= 18  BatchID= 10 Loss: 0.330 | Acc: 91.122% (1283/1408)
[Test] Epoch= 18  BatchID= 20 Loss: 0.300 | Acc: 91.369% (2456/2688)
[Test] Epoch= 18  BatchID= 30 Loss: 0.330 | Acc: 90.701% (3560/3925)
Saving..
Best accuracy:  90.70063694267516

Epoch: 19
[Train] Epoch= 19  BatchID= 0 Loss: 0.156 | Acc: 95.312% (122/128)
[Train] Epoch= 19  BatchID= 10 Loss: 0.119 | Acc: 96.804% (1363/1408)
[Train] Epoch= 19  BatchID= 20 Loss: 0.124 | Acc: 96.391% (2591/2688)
[Train] Epoch= 19  BatchID= 30 Loss: 0.133 | Acc: 96.169% (3816/3968)
[Train] Epoch= 19  BatchID= 40 Loss: 0.140 | Acc: 95.789% (5027/5248)
[Train] Epoch= 19  BatchID= 50 Loss: 0.138 | Acc: 95.680% (6246/6528)
[Train] Epoch= 19  BatchID= 60 Loss: 0.139 | Acc: 95.671% (7470/7808)
[Train] Epoch= 19  BatchID= 70 Loss: 0.137 | Acc: 95.687% (8696/9088)
[Train] Epoch= 19  BatchID= 0 Loss: 0.119 | Acc: 95.312% (122/128)
[Train] Epoch= 19  BatchID= 10 Loss: 0.132 | Acc: 95.881% (1350/1408)
[Train] Epoch= 19  BatchID= 20 Loss: 0.138 | Acc: 95.908% (2578/2688)
[Train] Epoch= 19  BatchID= 30 Loss: 0.142 | Acc: 95.892% (3805/3968)
[Train] Epoch= 19  BatchID= 40 Loss: 0.146 | Acc: 95.713% (5023/5248)
[Train] Epoch= 19  BatchID= 50 Loss: 0.152 | Acc: 95.481% (6233/6528)
[Train] Epoch= 19  BatchID= 60 Loss: 0.150 | Acc: 95.492% (7456/7808)
[Train] Epoch= 19  BatchID= 70 Loss: 0.150 | Acc: 95.412% (8671/9088)
[Test] Epoch= 19  BatchID= 0 Loss: 0.179 | Acc: 94.531% (121/128)
[Test] Epoch= 19  BatchID= 10 Loss: 0.354 | Acc: 89.205% (1256/1408)
[Test] Epoch= 19  BatchID= 20 Loss: 0.309 | Acc: 90.327% (2428/2688)
[Test] Epoch= 19  BatchID= 30 Loss: 0.344 | Acc: 89.554% (3515/3925)

Epoch: 20
[Test] Epoch= 19  BatchID= 0 Loss: 0.089 | Acc: 98.438% (126/128)
[Test] Epoch= 19  BatchID= 10 Loss: 0.301 | Acc: 91.690% (1291/1408)
[Test] Epoch= 19  BatchID= 20 Loss: 0.291 | Acc: 91.481% (2459/2688)
[Test] Epoch= 19  BatchID= 30 Loss: 0.347 | Acc: 90.038% (3534/3925)

Epoch: 20
[Train] Epoch= 20  BatchID= 0 Loss: 0.058 | Acc: 98.438% (126/128)
[Train] Epoch= 20  BatchID= 10 Loss: 0.134 | Acc: 95.597% (1346/1408)
[Train] Epoch= 20  BatchID= 20 Loss: 0.138 | Acc: 95.499% (2567/2688)
[Train] Epoch= 20  BatchID= 30 Loss: 0.144 | Acc: 95.464% (3788/3968)
[Train] Epoch= 20  BatchID= 40 Loss: 0.145 | Acc: 95.332% (5003/5248)
[Train] Epoch= 20  BatchID= 50 Loss: 0.143 | Acc: 95.435% (6230/6528)
[Train] Epoch= 20  BatchID= 60 Loss: 0.146 | Acc: 95.364% (7446/7808)
[Train] Epoch= 20  BatchID= 70 Loss: 0.150 | Acc: 95.290% (8660/9088)
[Train] Epoch= 20  BatchID= 0 Loss: 0.131 | Acc: 92.969% (119/128)
[Train] Epoch= 20  BatchID= 10 Loss: 0.125 | Acc: 95.668% (1347/1408)
[Train] Epoch= 20  BatchID= 20 Loss: 0.123 | Acc: 95.796% (2575/2688)
[Train] Epoch= 20  BatchID= 30 Loss: 0.129 | Acc: 95.489% (3789/3968)
[Train] Epoch= 20  BatchID= 40 Loss: 0.126 | Acc: 95.732% (5024/5248)
[Train] Epoch= 20  BatchID= 50 Loss: 0.132 | Acc: 95.634% (6243/6528)
[Train] Epoch= 20  BatchID= 60 Loss: 0.137 | Acc: 95.543% (7460/7808)
[Train] Epoch= 20  BatchID= 70 Loss: 0.135 | Acc: 95.676% (8695/9088)
[Test] Epoch= 20  BatchID= 0 Loss: 0.125 | Acc: 96.875% (124/128)
[Test] Epoch= 20  BatchID= 10 Loss: 0.367 | Acc: 88.920% (1252/1408)
[Test] Epoch= 20  BatchID= 20 Loss: 0.330 | Acc: 89.881% (2416/2688)
[Test] Epoch= 20  BatchID= 30 Loss: 0.341 | Acc: 89.503% (3513/3925)

Epoch: 21
[Test] Epoch= 20  BatchID= 0 Loss: 0.130 | Acc: 96.875% (124/128)
[Test] Epoch= 20  BatchID= 10 Loss: 0.316 | Acc: 90.199% (1270/1408)
[Test] Epoch= 20  BatchID= 20 Loss: 0.291 | Acc: 91.071% (2448/2688)
[Test] Epoch= 20  BatchID= 30 Loss: 0.342 | Acc: 89.809% (3525/3925)

Epoch: 21
[Train] Epoch= 21  BatchID= 0 Loss: 0.141 | Acc: 95.312% (122/128)
[Train] Epoch= 21  BatchID= 10 Loss: 0.120 | Acc: 96.662% (1361/1408)
[Train] Epoch= 21  BatchID= 20 Loss: 0.128 | Acc: 96.243% (2587/2688)
[Train] Epoch= 21  BatchID= 30 Loss: 0.126 | Acc: 96.144% (3815/3968)
[Train] Epoch= 21  BatchID= 40 Loss: 0.129 | Acc: 96.132% (5045/5248)
[Train] Epoch= 21  BatchID= 50 Loss: 0.132 | Acc: 95.910% (6261/6528)
[Train] Epoch= 21  BatchID= 60 Loss: 0.130 | Acc: 95.953% (7492/7808)
[Train] Epoch= 21  BatchID= 70 Loss: 0.132 | Acc: 95.929% (8718/9088)
[Train] Epoch= 21  BatchID= 0 Loss: 0.168 | Acc: 92.969% (119/128)
[Train] Epoch= 21  BatchID= 10 Loss: 0.140 | Acc: 95.739% (1348/1408)
[Train] Epoch= 21  BatchID= 20 Loss: 0.144 | Acc: 95.982% (2580/2688)
[Train] Epoch= 21  BatchID= 30 Loss: 0.141 | Acc: 95.892% (3805/3968)
[Train] Epoch= 21  BatchID= 40 Loss: 0.143 | Acc: 95.941% (5035/5248)
[Train] Epoch= 21  BatchID= 50 Loss: 0.144 | Acc: 95.849% (6257/6528)
[Train] Epoch= 21  BatchID= 60 Loss: 0.146 | Acc: 95.748% (7476/7808)
[Train] Epoch= 21  BatchID= 70 Loss: 0.147 | Acc: 95.643% (8692/9088)
[Test] Epoch= 21  BatchID= 0 Loss: 0.109 | Acc: 97.656% (125/128)
[Test] Epoch= 21  BatchID= 10 Loss: 0.330 | Acc: 90.412% (1273/1408)
[Test] Epoch= 21  BatchID= 20 Loss: 0.302 | Acc: 90.662% (2437/2688)
[Test] Epoch= 21  BatchID= 30 Loss: 0.342 | Acc: 89.707% (3521/3925)

Epoch: 22
[Test] Epoch= 21  BatchID= 0 Loss: 0.104 | Acc: 98.438% (126/128)
[Test] Epoch= 21  BatchID= 10 Loss: 0.315 | Acc: 90.625% (1276/1408)
[Test] Epoch= 21  BatchID= 20 Loss: 0.303 | Acc: 90.848% (2442/2688)
[Test] Epoch= 21  BatchID= 30 Loss: 0.339 | Acc: 90.115% (3537/3925)

Epoch: 22
[Train] Epoch= 22  BatchID= 0 Loss: 0.255 | Acc: 94.531% (121/128)
[Train] Epoch= 22  BatchID= 10 Loss: 0.142 | Acc: 95.170% (1340/1408)
[Train] Epoch= 22  BatchID= 20 Loss: 0.147 | Acc: 95.164% (2558/2688)
[Train] Epoch= 22  BatchID= 30 Loss: 0.153 | Acc: 94.960% (3768/3968)
[Train] Epoch= 22  BatchID= 40 Loss: 0.145 | Acc: 95.236% (4998/5248)
[Train] Epoch= 22  BatchID= 50 Loss: 0.143 | Acc: 95.251% (6218/6528)
[Train] Epoch= 22  BatchID= 60 Loss: 0.142 | Acc: 95.351% (7445/7808)
[Train] Epoch= 22  BatchID= 70 Loss: 0.142 | Acc: 95.290% (8660/9088)
[Train] Epoch= 22  BatchID= 0 Loss: 0.165 | Acc: 96.094% (123/128)
[Train] Epoch= 22  BatchID= 10 Loss: 0.159 | Acc: 95.028% (1338/1408)
[Train] Epoch= 22  BatchID= 20 Loss: 0.159 | Acc: 94.978% (2553/2688)
[Train] Epoch= 22  BatchID= 30 Loss: 0.156 | Acc: 95.287% (3781/3968)
[Train] Epoch= 22  BatchID= 40 Loss: 0.148 | Acc: 95.598% (5017/5248)
[Train] Epoch= 22  BatchID= 50 Loss: 0.151 | Acc: 95.466% (6232/6528)
[Train] Epoch= 22  BatchID= 60 Loss: 0.152 | Acc: 95.453% (7453/7808)
[Train] Epoch= 22  BatchID= 70 Loss: 0.154 | Acc: 95.379% (8668/9088)
[Test] Epoch= 22  BatchID= 0 Loss: 0.111 | Acc: 98.438% (126/128)
[Test] Epoch= 22  BatchID= 10 Loss: 0.325 | Acc: 90.625% (1276/1408)
[Test] Epoch= 22  BatchID= 20 Loss: 0.311 | Acc: 90.662% (2437/2688)
[Test] Epoch= 22  BatchID= 30 Loss: 0.323 | Acc: 90.344% (3546/3925)

Epoch: 23
[Test] Epoch= 22  BatchID= 0 Loss: 0.132 | Acc: 96.094% (123/128)
[Test] Epoch= 22  BatchID= 10 Loss: 0.353 | Acc: 89.702% (1263/1408)
[Test] Epoch= 22  BatchID= 20 Loss: 0.319 | Acc: 90.513% (2433/2688)
[Test] Epoch= 22  BatchID= 30 Loss: 0.335 | Acc: 90.217% (3541/3925)

Epoch: 23
[Train] Epoch= 23  BatchID= 0 Loss: 0.181 | Acc: 93.750% (120/128)
[Train] Epoch= 23  BatchID= 10 Loss: 0.123 | Acc: 96.094% (1353/1408)
[Train] Epoch= 23  BatchID= 20 Loss: 0.131 | Acc: 95.573% (2569/2688)
[Train] Epoch= 23  BatchID= 30 Loss: 0.136 | Acc: 95.439% (3787/3968)
[Train] Epoch= 23  BatchID= 40 Loss: 0.134 | Acc: 95.541% (5014/5248)
[Train] Epoch= 23  BatchID= 50 Loss: 0.130 | Acc: 95.711% (6248/6528)
[Train] Epoch= 23  BatchID= 60 Loss: 0.131 | Acc: 95.671% (7470/7808)
[Train] Epoch= 23  BatchID= 70 Loss: 0.132 | Acc: 95.687% (8696/9088)
[Train] Epoch= 23  BatchID= 0 Loss: 0.103 | Acc: 96.875% (124/128)
[Train] Epoch= 23  BatchID= 10 Loss: 0.137 | Acc: 94.886% (1336/1408)
[Train] Epoch= 23  BatchID= 20 Loss: 0.136 | Acc: 95.499% (2567/2688)
[Train] Epoch= 23  BatchID= 30 Loss: 0.134 | Acc: 95.691% (3797/3968)
[Train] Epoch= 23  BatchID= 40 Loss: 0.136 | Acc: 95.751% (5025/5248)
[Train] Epoch= 23  BatchID= 50 Loss: 0.136 | Acc: 95.757% (6251/6528)
[Train] Epoch= 23  BatchID= 60 Loss: 0.140 | Acc: 95.569% (7462/7808)
[Train] Epoch= 23  BatchID= 70 Loss: 0.143 | Acc: 95.522% (8681/9088)
[Test] Epoch= 23  BatchID= 0 Loss: 0.097 | Acc: 98.438% (126/128)
[Test] Epoch= 23  BatchID= 10 Loss: 0.308 | Acc: 90.909% (1280/1408)
[Test] Epoch= 23  BatchID= 20 Loss: 0.286 | Acc: 91.369% (2456/2688)
[Test] Epoch= 23  BatchID= 30 Loss: 0.335 | Acc: 89.834% (3526/3925)

Epoch: 24
[Test] Epoch= 23  BatchID= 0 Loss: 0.141 | Acc: 96.875% (124/128)
[Test] Epoch= 23  BatchID= 10 Loss: 0.367 | Acc: 88.849% (1251/1408)
[Test] Epoch= 23  BatchID= 20 Loss: 0.329 | Acc: 89.881% (2416/2688)
[Test] Epoch= 23  BatchID= 30 Loss: 0.347 | Acc: 89.580% (3516/3925)

Epoch: 24
[Train] Epoch= 24  BatchID= 0 Loss: 0.082 | Acc: 97.656% (125/128)
[Train] Epoch= 24  BatchID= 10 Loss: 0.135 | Acc: 95.668% (1347/1408)
[Train] Epoch= 24  BatchID= 20 Loss: 0.143 | Acc: 95.238% (2560/2688)
[Train] Epoch= 24  BatchID= 30 Loss: 0.144 | Acc: 95.287% (3781/3968)
[Train] Epoch= 24  BatchID= 40 Loss: 0.148 | Acc: 95.103% (4991/5248)
[Train] Epoch= 24  BatchID= 50 Loss: 0.150 | Acc: 95.052% (6205/6528)
[Train] Epoch= 24  BatchID= 60 Loss: 0.146 | Acc: 95.210% (7434/7808)
[Train] Epoch= 24  BatchID= 70 Loss: 0.150 | Acc: 95.037% (8637/9088)
[Train] Epoch= 24  BatchID= 0 Loss: 0.098 | Acc: 97.656% (125/128)
[Train] Epoch= 24  BatchID= 10 Loss: 0.137 | Acc: 96.307% (1356/1408)
[Train] Epoch= 24  BatchID= 20 Loss: 0.133 | Acc: 96.057% (2582/2688)
[Train] Epoch= 24  BatchID= 30 Loss: 0.136 | Acc: 95.917% (3806/3968)
[Train] Epoch= 24  BatchID= 40 Loss: 0.140 | Acc: 95.789% (5027/5248)
[Train] Epoch= 24  BatchID= 50 Loss: 0.139 | Acc: 95.864% (6258/6528)
[Train] Epoch= 24  BatchID= 60 Loss: 0.141 | Acc: 95.812% (7481/7808)
[Train] Epoch= 24  BatchID= 70 Loss: 0.140 | Acc: 95.775% (8704/9088)
[Test] Epoch= 24  BatchID= 0 Loss: 0.114 | Acc: 97.656% (125/128)
[Test] Epoch= 24  BatchID= 10 Loss: 0.379 | Acc: 89.418% (1259/1408)
[Test] Epoch= 24  BatchID= 20 Loss: 0.333 | Acc: 90.290% (2427/2688)
[Test] Epoch= 24  BatchID= 30 Loss: 0.338 | Acc: 90.115% (3537/3925)

Epoch: 25
[Test] Epoch= 24  BatchID= 0 Loss: 0.130 | Acc: 96.875% (124/128)
[Test] Epoch= 24  BatchID= 10 Loss: 0.322 | Acc: 90.696% (1277/1408)
[Test] Epoch= 24  BatchID= 20 Loss: 0.289 | Acc: 91.109% (2449/2688)
[Test] Epoch= 24  BatchID= 30 Loss: 0.346 | Acc: 89.656% (3519/3925)

Epoch: 25
[Train] Epoch= 25  BatchID= 0 Loss: 0.131 | Acc: 96.094% (123/128)
[Train] Epoch= 25  BatchID= 10 Loss: 0.158 | Acc: 94.957% (1337/1408)
[Train] Epoch= 25  BatchID= 20 Loss: 0.147 | Acc: 95.536% (2568/2688)
[Train] Epoch= 25  BatchID= 30 Loss: 0.139 | Acc: 95.892% (3805/3968)
[Train] Epoch= 25  BatchID= 40 Loss: 0.136 | Acc: 95.865% (5031/5248)
[Train] Epoch= 25  BatchID= 50 Loss: 0.133 | Acc: 95.956% (6264/6528)
[Train] Epoch= 25  BatchID= 60 Loss: 0.135 | Acc: 95.876% (7486/7808)
[Train] Epoch= 25  BatchID= 70 Loss: 0.134 | Acc: 95.874% (8713/9088)
[Train] Epoch= 25  BatchID= 0 Loss: 0.215 | Acc: 95.312% (122/128)
[Train] Epoch= 25  BatchID= 10 Loss: 0.155 | Acc: 95.099% (1339/1408)
[Train] Epoch= 25  BatchID= 20 Loss: 0.142 | Acc: 95.536% (2568/2688)
[Train] Epoch= 25  BatchID= 30 Loss: 0.146 | Acc: 95.665% (3796/3968)
[Train] Epoch= 25  BatchID= 40 Loss: 0.147 | Acc: 95.541% (5014/5248)
[Train] Epoch= 25  BatchID= 50 Loss: 0.141 | Acc: 95.772% (6252/6528)
[Train] Epoch= 25  BatchID= 60 Loss: 0.144 | Acc: 95.620% (7466/7808)
[Train] Epoch= 25  BatchID= 70 Loss: 0.149 | Acc: 95.533% (8682/9088)
[Test] Epoch= 25  BatchID= 0 Loss: 0.103 | Acc: 97.656% (125/128)
[Test] Epoch= 25  BatchID= 10 Loss: 0.338 | Acc: 89.702% (1263/1408)
[Test] Epoch= 25  BatchID= 20 Loss: 0.293 | Acc: 90.811% (2441/2688)
[Test] Epoch= 25  BatchID= 30 Loss: 0.332 | Acc: 89.834% (3526/3925)

Epoch: 26
[Test] Epoch= 25  BatchID= 0 Loss: 0.114 | Acc: 98.438% (126/128)
[Test] Epoch= 25  BatchID= 10 Loss: 0.308 | Acc: 91.619% (1290/1408)
[Test] Epoch= 25  BatchID= 20 Loss: 0.298 | Acc: 91.369% (2456/2688)
[Test] Epoch= 25  BatchID= 30 Loss: 0.342 | Acc: 90.089% (3536/3925)

Epoch: 26
[Train] Epoch= 26  BatchID= 0 Loss: 0.092 | Acc: 96.875% (124/128)
[Train] Epoch= 26  BatchID= 10 Loss: 0.143 | Acc: 95.526% (1345/1408)
[Train] Epoch= 26  BatchID= 20 Loss: 0.143 | Acc: 95.685% (2572/2688)
[Train] Epoch= 26  BatchID= 30 Loss: 0.141 | Acc: 95.867% (3804/3968)
[Train] Epoch= 26  BatchID= 40 Loss: 0.139 | Acc: 95.960% (5036/5248)
[Train] Epoch= 26  BatchID= 50 Loss: 0.141 | Acc: 95.910% (6261/6528)
[Train] Epoch= 26  BatchID= 60 Loss: 0.142 | Acc: 95.876% (7486/7808)
[Train] Epoch= 26  BatchID= 70 Loss: 0.140 | Acc: 95.929% (8718/9088)
[Train] Epoch= 26  BatchID= 0 Loss: 0.096 | Acc: 96.875% (124/128)
[Train] Epoch= 26  BatchID= 10 Loss: 0.143 | Acc: 95.312% (1342/1408)
[Train] Epoch= 26  BatchID= 20 Loss: 0.134 | Acc: 95.610% (2570/2688)
[Train] Epoch= 26  BatchID= 30 Loss: 0.141 | Acc: 95.287% (3781/3968)
[Train] Epoch= 26  BatchID= 40 Loss: 0.142 | Acc: 95.274% (5000/5248)
[Train] Epoch= 26  BatchID= 50 Loss: 0.141 | Acc: 95.389% (6227/6528)
[Train] Epoch= 26  BatchID= 60 Loss: 0.140 | Acc: 95.492% (7456/7808)
[Train] Epoch= 26  BatchID= 70 Loss: 0.141 | Acc: 95.478% (8677/9088)
[Test] Epoch= 26  BatchID= 0 Loss: 0.123 | Acc: 97.656% (125/128)
[Test] Epoch= 26  BatchID= 10 Loss: 0.319 | Acc: 90.767% (1278/1408)
[Test] Epoch= 26  BatchID= 20 Loss: 0.303 | Acc: 90.513% (2433/2688)
[Test] Epoch= 26  BatchID= 30 Loss: 0.334 | Acc: 89.860% (3527/3925)

Epoch: 27
[Test] Epoch= 26  BatchID= 0 Loss: 0.108 | Acc: 97.656% (125/128)
[Test] Epoch= 26  BatchID= 10 Loss: 0.297 | Acc: 91.264% (1285/1408)
[Test] Epoch= 26  BatchID= 20 Loss: 0.291 | Acc: 91.220% (2452/2688)
[Test] Epoch= 26  BatchID= 30 Loss: 0.327 | Acc: 90.548% (3554/3925)

Epoch: 27
[Train] Epoch= 27  BatchID= 0 Loss: 0.211 | Acc: 92.188% (118/128)
[Train] Epoch= 27  BatchID= 10 Loss: 0.160 | Acc: 94.318% (1328/1408)
[Train] Epoch= 27  BatchID= 20 Loss: 0.141 | Acc: 95.536% (2568/2688)
[Train] Epoch= 27  BatchID= 30 Loss: 0.140 | Acc: 95.665% (3796/3968)
[Train] Epoch= 27  BatchID= 40 Loss: 0.138 | Acc: 95.675% (5021/5248)
[Train] Epoch= 27  BatchID= 50 Loss: 0.135 | Acc: 95.726% (6249/6528)
[Train] Epoch= 27  BatchID= 60 Loss: 0.139 | Acc: 95.607% (7465/7808)
[Train] Epoch= 27  BatchID= 70 Loss: 0.140 | Acc: 95.610% (8689/9088)
[Train] Epoch= 27  BatchID= 0 Loss: 0.177 | Acc: 95.312% (122/128)
[Train] Epoch= 27  BatchID= 10 Loss: 0.120 | Acc: 96.449% (1358/1408)
[Train] Epoch= 27  BatchID= 20 Loss: 0.125 | Acc: 96.168% (2585/2688)
[Train] Epoch= 27  BatchID= 30 Loss: 0.129 | Acc: 95.968% (3808/3968)
[Train] Epoch= 27  BatchID= 40 Loss: 0.132 | Acc: 95.884% (5032/5248)
[Train] Epoch= 27  BatchID= 50 Loss: 0.134 | Acc: 95.925% (6262/6528)
[Train] Epoch= 27  BatchID= 60 Loss: 0.132 | Acc: 96.017% (7497/7808)
[Train] Epoch= 27  BatchID= 70 Loss: 0.132 | Acc: 96.017% (8726/9088)
[Test] Epoch= 27  BatchID= 0 Loss: 0.111 | Acc: 98.438% (126/128)
[Test] Epoch= 27  BatchID= 10 Loss: 0.312 | Acc: 90.412% (1273/1408)
[Test] Epoch= 27  BatchID= 20 Loss: 0.293 | Acc: 90.885% (2443/2688)
[Test] Epoch= 27  BatchID= 30 Loss: 0.343 | Acc: 89.401% (3509/3925)

Epoch: 28
[Test] Epoch= 27  BatchID= 0 Loss: 0.125 | Acc: 96.875% (124/128)
[Test] Epoch= 27  BatchID= 10 Loss: 0.309 | Acc: 90.483% (1274/1408)
[Test] Epoch= 27  BatchID= 20 Loss: 0.294 | Acc: 91.034% (2447/2688)
[Test] Epoch= 27  BatchID= 30 Loss: 0.343 | Acc: 89.707% (3521/3925)

Epoch: 28
[Train] Epoch= 28  BatchID= 0 Loss: 0.171 | Acc: 96.875% (124/128)
[Train] Epoch= 28  BatchID= 10 Loss: 0.138 | Acc: 95.668% (1347/1408)
[Train] Epoch= 28  BatchID= 20 Loss: 0.126 | Acc: 95.871% (2577/2688)
[Train] Epoch= 28  BatchID= 30 Loss: 0.135 | Acc: 95.489% (3789/3968)
[Train] Epoch= 28  BatchID= 40 Loss: 0.133 | Acc: 95.732% (5024/5248)
[Train] Epoch= 28  BatchID= 50 Loss: 0.134 | Acc: 95.803% (6254/6528)
[Train] Epoch= 28  BatchID= 60 Loss: 0.134 | Acc: 95.914% (7489/7808)
[Train] Epoch= 28  BatchID= 70 Loss: 0.139 | Acc: 95.797% (8706/9088)
[Train] Epoch= 28  BatchID= 0 Loss: 0.187 | Acc: 92.188% (118/128)
[Train] Epoch= 28  BatchID= 10 Loss: 0.153 | Acc: 94.957% (1337/1408)
[Train] Epoch= 28  BatchID= 20 Loss: 0.140 | Acc: 95.796% (2575/2688)
[Train] Epoch= 28  BatchID= 30 Loss: 0.132 | Acc: 95.842% (3803/3968)
[Train] Epoch= 28  BatchID= 40 Loss: 0.128 | Acc: 95.960% (5036/5248)
[Train] Epoch= 28  BatchID= 50 Loss: 0.132 | Acc: 95.925% (6262/6528)
[Train] Epoch= 28  BatchID= 60 Loss: 0.130 | Acc: 95.940% (7491/7808)
[Train] Epoch= 28  BatchID= 70 Loss: 0.132 | Acc: 95.907% (8716/9088)
[Test] Epoch= 28  BatchID= 0 Loss: 0.138 | Acc: 96.094% (123/128)
[Test] Epoch= 28  BatchID= 10 Loss: 0.305 | Acc: 91.122% (1283/1408)
[Test] Epoch= 28  BatchID= 20 Loss: 0.285 | Acc: 91.146% (2450/2688)
[Test] Epoch= 28  BatchID= 30 Loss: 0.325 | Acc: 90.064% (3535/3925)

Epoch: 29
[Test] Epoch= 28  BatchID= 0 Loss: 0.103 | Acc: 97.656% (125/128)
[Test] Epoch= 28  BatchID= 10 Loss: 0.339 | Acc: 90.199% (1270/1408)
[Test] Epoch= 28  BatchID= 20 Loss: 0.296 | Acc: 91.220% (2452/2688)
[Test] Epoch= 28  BatchID= 30 Loss: 0.326 | Acc: 90.522% (3553/3925)

Epoch: 29
[Train] Epoch= 29  BatchID= 0 Loss: 0.109 | Acc: 96.875% (124/128)
[Train] Epoch= 29  BatchID= 10 Loss: 0.123 | Acc: 95.952% (1351/1408)
[Train] Epoch= 29  BatchID= 20 Loss: 0.130 | Acc: 95.759% (2574/2688)
[Train] Epoch= 29  BatchID= 30 Loss: 0.136 | Acc: 95.615% (3794/3968)
[Train] Epoch= 29  BatchID= 40 Loss: 0.138 | Acc: 95.655% (5020/5248)
[Train] Epoch= 29  BatchID= 50 Loss: 0.134 | Acc: 95.925% (6262/6528)
[Train] Epoch= 29  BatchID= 60 Loss: 0.131 | Acc: 95.953% (7492/7808)
[Train] Epoch= 29  BatchID= 70 Loss: 0.133 | Acc: 95.929% (8718/9088)
[Train] Epoch= 29  BatchID= 0 Loss: 0.075 | Acc: 96.875% (124/128)
[Train] Epoch= 29  BatchID= 10 Loss: 0.138 | Acc: 95.597% (1346/1408)
[Train] Epoch= 29  BatchID= 20 Loss: 0.132 | Acc: 95.796% (2575/2688)
[Train] Epoch= 29  BatchID= 30 Loss: 0.125 | Acc: 95.993% (3809/3968)
[Train] Epoch= 29  BatchID= 40 Loss: 0.127 | Acc: 95.941% (5035/5248)
[Train] Epoch= 29  BatchID= 50 Loss: 0.131 | Acc: 95.833% (6256/6528)
[Train] Epoch= 29  BatchID= 60 Loss: 0.131 | Acc: 95.914% (7489/7808)
[Train] Epoch= 29  BatchID= 70 Loss: 0.131 | Acc: 95.929% (8718/9088)
[Test] Epoch= 29  BatchID= 0 Loss: 0.154 | Acc: 96.094% (123/128)
[Test] Epoch= 29  BatchID= 10 Loss: 0.373 | Acc: 89.418% (1259/1408)
[Test] Epoch= 29  BatchID= 20 Loss: 0.316 | Acc: 90.774% (2440/2688)
[Test] Epoch= 29  BatchID= 30 Loss: 0.333 | Acc: 90.522% (3553/3925)
Saving..
Best accuracy:  90.52229299363057

Epoch: 30
[Test] Epoch= 29  BatchID= 0 Loss: 0.083 | Acc: 97.656% (125/128)
[Test] Epoch= 29  BatchID= 10 Loss: 0.335 | Acc: 90.483% (1274/1408)
[Test] Epoch= 29  BatchID= 20 Loss: 0.312 | Acc: 91.034% (2447/2688)
[Test] Epoch= 29  BatchID= 30 Loss: 0.325 | Acc: 90.675% (3559/3925)

Epoch: 30
[Train] Epoch= 30  BatchID= 0 Loss: 0.146 | Acc: 94.531% (121/128)
[Train] Epoch= 30  BatchID= 10 Loss: 0.142 | Acc: 95.739% (1348/1408)
[Train] Epoch= 30  BatchID= 20 Loss: 0.140 | Acc: 95.796% (2575/2688)
[Train] Epoch= 30  BatchID= 30 Loss: 0.142 | Acc: 95.917% (3806/3968)
[Train] Epoch= 30  BatchID= 40 Loss: 0.143 | Acc: 95.770% (5026/5248)
[Train] Epoch= 30  BatchID= 50 Loss: 0.145 | Acc: 95.726% (6249/6528)
[Train] Epoch= 30  BatchID= 60 Loss: 0.142 | Acc: 95.863% (7485/7808)
[Train] Epoch= 30  BatchID= 70 Loss: 0.140 | Acc: 95.830% (8709/9088)
[Train] Epoch= 30  BatchID= 0 Loss: 0.104 | Acc: 97.656% (125/128)
[Train] Epoch= 30  BatchID= 10 Loss: 0.151 | Acc: 95.028% (1338/1408)
[Train] Epoch= 30  BatchID= 20 Loss: 0.131 | Acc: 95.833% (2576/2688)
[Train] Epoch= 30  BatchID= 30 Loss: 0.138 | Acc: 95.791% (3801/3968)
[Train] Epoch= 30  BatchID= 40 Loss: 0.141 | Acc: 95.732% (5024/5248)
[Train] Epoch= 30  BatchID= 50 Loss: 0.142 | Acc: 95.695% (6247/6528)
[Train] Epoch= 30  BatchID= 60 Loss: 0.143 | Acc: 95.594% (7464/7808)
[Train] Epoch= 30  BatchID= 70 Loss: 0.146 | Acc: 95.555% (8684/9088)
[Test] Epoch= 30  BatchID= 0 Loss: 0.132 | Acc: 96.875% (124/128)
[Test] Epoch= 30  BatchID= 10 Loss: 0.349 | Acc: 89.986% (1267/1408)
[Test] Epoch= 30  BatchID= 20 Loss: 0.312 | Acc: 90.885% (2443/2688)
[Test] Epoch= 30  BatchID= 30 Loss: 0.340 | Acc: 90.115% (3537/3925)

Epoch: 31
[Test] Epoch= 30  BatchID= 0 Loss: 0.127 | Acc: 96.875% (124/128)
[Test] Epoch= 30  BatchID= 10 Loss: 0.300 | Acc: 90.980% (1281/1408)
[Test] Epoch= 30  BatchID= 20 Loss: 0.304 | Acc: 90.625% (2436/2688)
[Test] Epoch= 30  BatchID= 30 Loss: 0.335 | Acc: 90.140% (3538/3925)

Epoch: 31
[Train] Epoch= 31  BatchID= 0 Loss: 0.159 | Acc: 95.312% (122/128)
[Train] Epoch= 31  BatchID= 10 Loss: 0.131 | Acc: 96.236% (1355/1408)
[Train] Epoch= 31  BatchID= 20 Loss: 0.133 | Acc: 96.280% (2588/2688)
[Train] Epoch= 31  BatchID= 30 Loss: 0.127 | Acc: 96.371% (3824/3968)
[Train] Epoch= 31  BatchID= 40 Loss: 0.130 | Acc: 96.246% (5051/5248)
[Train] Epoch= 31  BatchID= 50 Loss: 0.132 | Acc: 96.232% (6282/6528)
[Train] Epoch= 31  BatchID= 60 Loss: 0.134 | Acc: 96.055% (7500/7808)
[Train] Epoch= 31  BatchID= 70 Loss: 0.133 | Acc: 96.017% (8726/9088)
[Train] Epoch= 31  BatchID= 0 Loss: 0.054 | Acc: 99.219% (127/128)
[Train] Epoch= 31  BatchID= 10 Loss: 0.121 | Acc: 96.733% (1362/1408)
[Train] Epoch= 31  BatchID= 20 Loss: 0.124 | Acc: 96.280% (2588/2688)
[Train] Epoch= 31  BatchID= 30 Loss: 0.129 | Acc: 96.144% (3815/3968)
[Train] Epoch= 31  BatchID= 40 Loss: 0.137 | Acc: 95.941% (5035/5248)
[Train] Epoch= 31  BatchID= 50 Loss: 0.138 | Acc: 95.772% (6252/6528)
[Train] Epoch= 31  BatchID= 60 Loss: 0.133 | Acc: 95.953% (7492/7808)
[Train] Epoch= 31  BatchID= 70 Loss: 0.135 | Acc: 95.896% (8715/9088)
[Test] Epoch= 31  BatchID= 0 Loss: 0.147 | Acc: 95.312% (122/128)
[Test] Epoch= 31  BatchID= 10 Loss: 0.341 | Acc: 89.844% (1265/1408)
[Test] Epoch= 31  BatchID= 20 Loss: 0.316 | Acc: 90.439% (2431/2688)
[Test] Epoch= 31  BatchID= 30 Loss: 0.334 | Acc: 90.140% (3538/3925)

Epoch: 32
[Test] Epoch= 31  BatchID= 0 Loss: 0.126 | Acc: 97.656% (125/128)
[Test] Epoch= 31  BatchID= 10 Loss: 0.323 | Acc: 90.767% (1278/1408)
[Test] Epoch= 31  BatchID= 20 Loss: 0.300 | Acc: 91.220% (2452/2688)
[Test] Epoch= 31  BatchID= 30 Loss: 0.328 | Acc: 90.701% (3560/3925)

Epoch: 32
[Train] Epoch= 32  BatchID= 0 Loss: 0.131 | Acc: 96.094% (123/128)
[Train] Epoch= 32  BatchID= 10 Loss: 0.137 | Acc: 95.952% (1351/1408)
[Train] Epoch= 32  BatchID= 20 Loss: 0.126 | Acc: 96.243% (2587/2688)
[Train] Epoch= 32  BatchID= 30 Loss: 0.125 | Acc: 96.346% (3823/3968)
[Train] Epoch= 32  BatchID= 40 Loss: 0.124 | Acc: 96.437% (5061/5248)
[Train] Epoch= 32  BatchID= 50 Loss: 0.131 | Acc: 96.078% (6272/6528)
[Train] Epoch= 32  BatchID= 60 Loss: 0.130 | Acc: 96.132% (7506/7808)
[Train] Epoch= 32  BatchID= 70 Loss: 0.134 | Acc: 96.061% (8730/9088)
[Train] Epoch= 32  BatchID= 0 Loss: 0.130 | Acc: 95.312% (122/128)
[Train] Epoch= 32  BatchID= 10 Loss: 0.115 | Acc: 96.520% (1359/1408)
[Train] Epoch= 32  BatchID= 20 Loss: 0.121 | Acc: 96.205% (2586/2688)
[Train] Epoch= 32  BatchID= 30 Loss: 0.124 | Acc: 96.094% (3813/3968)
[Train] Epoch= 32  BatchID= 40 Loss: 0.122 | Acc: 96.132% (5045/5248)
[Train] Epoch= 32  BatchID= 50 Loss: 0.128 | Acc: 96.048% (6270/6528)
[Train] Epoch= 32  BatchID= 60 Loss: 0.128 | Acc: 96.030% (7498/7808)
[Train] Epoch= 32  BatchID= 70 Loss: 0.131 | Acc: 95.995% (8724/9088)
[Test] Epoch= 32  BatchID= 0 Loss: 0.129 | Acc: 96.875% (124/128)
[Test] Epoch= 32  BatchID= 10 Loss: 0.347 | Acc: 90.057% (1268/1408)
[Test] Epoch= 32  BatchID= 20 Loss: 0.317 | Acc: 90.625% (2436/2688)
[Test] Epoch= 32  BatchID= 30 Loss: 0.335 | Acc: 90.166% (3539/3925)

Epoch: 33
[Test] Epoch= 32  BatchID= 0 Loss: 0.110 | Acc: 96.094% (123/128)
[Test] Epoch= 32  BatchID= 10 Loss: 0.286 | Acc: 91.335% (1286/1408)
[Test] Epoch= 32  BatchID= 20 Loss: 0.286 | Acc: 91.481% (2459/2688)
[Test] Epoch= 32  BatchID= 30 Loss: 0.322 | Acc: 90.573% (3555/3925)

Epoch: 33
[Train] Epoch= 33  BatchID= 0 Loss: 0.071 | Acc: 96.875% (124/128)
[Train] Epoch= 33  BatchID= 10 Loss: 0.128 | Acc: 96.733% (1362/1408)
[Train] Epoch= 33  BatchID= 20 Loss: 0.126 | Acc: 96.429% (2592/2688)
[Train] Epoch= 33  BatchID= 30 Loss: 0.134 | Acc: 95.892% (3805/3968)
[Train] Epoch= 33  BatchID= 40 Loss: 0.138 | Acc: 95.694% (5022/5248)
[Train] Epoch= 33  BatchID= 50 Loss: 0.130 | Acc: 96.063% (6271/6528)
[Train] Epoch= 33  BatchID= 60 Loss: 0.129 | Acc: 96.119% (7505/7808)
[Train] Epoch= 33  BatchID= 70 Loss: 0.128 | Acc: 96.094% (8733/9088)
[Train] Epoch= 33  BatchID= 0 Loss: 0.131 | Acc: 96.875% (124/128)
[Train] Epoch= 33  BatchID= 10 Loss: 0.124 | Acc: 96.236% (1355/1408)
[Train] Epoch= 33  BatchID= 20 Loss: 0.122 | Acc: 96.280% (2588/2688)
[Train] Epoch= 33  BatchID= 30 Loss: 0.125 | Acc: 96.396% (3825/3968)
[Train] Epoch= 33  BatchID= 40 Loss: 0.130 | Acc: 96.227% (5050/5248)
[Train] Epoch= 33  BatchID= 50 Loss: 0.132 | Acc: 96.140% (6276/6528)
[Train] Epoch= 33  BatchID= 60 Loss: 0.130 | Acc: 96.145% (7507/7808)
[Train] Epoch= 33  BatchID= 70 Loss: 0.128 | Acc: 96.171% (8740/9088)
[Test] Epoch= 33  BatchID= 0 Loss: 0.119 | Acc: 97.656% (125/128)
[Test] Epoch= 33  BatchID= 10 Loss: 0.342 | Acc: 90.909% (1280/1408)
[Test] Epoch= 33  BatchID= 20 Loss: 0.302 | Acc: 91.629% (2463/2688)
[Test] Epoch= 33  BatchID= 30 Loss: 0.331 | Acc: 90.548% (3554/3925)
Saving..
Best accuracy:  90.54777070063695

Epoch: 34
[Test] Epoch= 33  BatchID= 0 Loss: 0.107 | Acc: 97.656% (125/128)
[Test] Epoch= 33  BatchID= 10 Loss: 0.330 | Acc: 90.767% (1278/1408)
[Test] Epoch= 33  BatchID= 20 Loss: 0.301 | Acc: 91.109% (2449/2688)
[Test] Epoch= 33  BatchID= 30 Loss: 0.328 | Acc: 90.420% (3549/3925)

Epoch: 34
[Train] Epoch= 34  BatchID= 0 Loss: 0.077 | Acc: 98.438% (126/128)
[Train] Epoch= 34  BatchID= 10 Loss: 0.126 | Acc: 95.739% (1348/1408)
[Train] Epoch= 34  BatchID= 20 Loss: 0.128 | Acc: 95.982% (2580/2688)
[Train] Epoch= 34  BatchID= 30 Loss: 0.128 | Acc: 96.018% (3810/3968)
[Train] Epoch= 34  BatchID= 40 Loss: 0.128 | Acc: 96.056% (5041/5248)
[Train] Epoch= 34  BatchID= 50 Loss: 0.130 | Acc: 96.155% (6277/6528)
[Train] Epoch= 34  BatchID= 60 Loss: 0.126 | Acc: 96.311% (7520/7808)
[Train] Epoch= 34  BatchID= 70 Loss: 0.125 | Acc: 96.237% (8746/9088)
[Train] Epoch= 34  BatchID= 0 Loss: 0.157 | Acc: 93.750% (120/128)
[Train] Epoch= 34  BatchID= 10 Loss: 0.130 | Acc: 96.094% (1353/1408)
[Train] Epoch= 34  BatchID= 20 Loss: 0.129 | Acc: 95.982% (2580/2688)
[Train] Epoch= 34  BatchID= 30 Loss: 0.128 | Acc: 95.943% (3807/3968)
[Train] Epoch= 34  BatchID= 40 Loss: 0.129 | Acc: 95.998% (5038/5248)
[Train] Epoch= 34  BatchID= 50 Loss: 0.128 | Acc: 95.987% (6266/6528)
[Train] Epoch= 34  BatchID= 60 Loss: 0.130 | Acc: 95.876% (7486/7808)
[Train] Epoch= 34  BatchID= 70 Loss: 0.128 | Acc: 96.017% (8726/9088)
[Test] Epoch= 34  BatchID= 0 Loss: 0.141 | Acc: 96.875% (124/128)
[Test] Epoch= 34  BatchID= 10 Loss: 0.320 | Acc: 90.767% (1278/1408)
[Test] Epoch= 34  BatchID= 20 Loss: 0.300 | Acc: 91.220% (2452/2688)
[Test] Epoch= 34  BatchID= 30 Loss: 0.340 | Acc: 90.293% (3544/3925)

Epoch: 35
[Test] Epoch= 34  BatchID= 0 Loss: 0.116 | Acc: 96.875% (124/128)
[Test] Epoch= 34  BatchID= 10 Loss: 0.313 | Acc: 90.909% (1280/1408)
[Test] Epoch= 34  BatchID= 20 Loss: 0.288 | Acc: 91.295% (2454/2688)
[Test] Epoch= 34  BatchID= 30 Loss: 0.329 | Acc: 90.242% (3542/3925)

Epoch: 35
[Train] Epoch= 35  BatchID= 0 Loss: 0.147 | Acc: 96.875% (124/128)
[Train] Epoch= 35  BatchID= 10 Loss: 0.142 | Acc: 95.668% (1347/1408)
[Train] Epoch= 35  BatchID= 20 Loss: 0.150 | Acc: 95.573% (2569/2688)
[Train] Epoch= 35  BatchID= 30 Loss: 0.135 | Acc: 96.119% (3814/3968)
[Train] Epoch= 35  BatchID= 40 Loss: 0.136 | Acc: 95.903% (5033/5248)
[Train] Epoch= 35  BatchID= 50 Loss: 0.137 | Acc: 95.849% (6257/6528)
[Train] Epoch= 35  BatchID= 60 Loss: 0.138 | Acc: 95.774% (7478/7808)
[Train] Epoch= 35  BatchID= 70 Loss: 0.141 | Acc: 95.599% (8688/9088)
[Train] Epoch= 35  BatchID= 0 Loss: 0.080 | Acc: 98.438% (126/128)
[Train] Epoch= 35  BatchID= 10 Loss: 0.129 | Acc: 95.241% (1341/1408)
[Train] Epoch= 35  BatchID= 20 Loss: 0.130 | Acc: 95.871% (2577/2688)
[Train] Epoch= 35  BatchID= 30 Loss: 0.127 | Acc: 96.169% (3816/3968)
[Train] Epoch= 35  BatchID= 40 Loss: 0.128 | Acc: 96.094% (5043/5248)
[Train] Epoch= 35  BatchID= 50 Loss: 0.128 | Acc: 96.124% (6275/6528)
[Train] Epoch= 35  BatchID= 60 Loss: 0.127 | Acc: 96.145% (7507/7808)
[Train] Epoch= 35  BatchID= 70 Loss: 0.128 | Acc: 95.984% (8723/9088)
[Test] Epoch= 35  BatchID= 0 Loss: 0.150 | Acc: 96.094% (123/128)
[Test] Epoch= 35  BatchID= 10 Loss: 0.347 | Acc: 89.631% (1262/1408)
[Test] Epoch= 35  BatchID= 20 Loss: 0.318 | Acc: 90.885% (2443/2688)
[Test] Epoch= 35  BatchID= 30 Loss: 0.355 | Acc: 90.038% (3534/3925)

Epoch: 36
[Test] Epoch= 35  BatchID= 0 Loss: 0.114 | Acc: 96.875% (124/128)
[Test] Epoch= 35  BatchID= 10 Loss: 0.330 | Acc: 89.702% (1263/1408)
[Test] Epoch= 35  BatchID= 20 Loss: 0.312 | Acc: 90.625% (2436/2688)
[Test] Epoch= 35  BatchID= 30 Loss: 0.335 | Acc: 90.242% (3542/3925)

Epoch: 36
[Train] Epoch= 36  BatchID= 0 Loss: 0.178 | Acc: 95.312% (122/128)
[Train] Epoch= 36  BatchID= 10 Loss: 0.128 | Acc: 95.952% (1351/1408)
[Train] Epoch= 36  BatchID= 20 Loss: 0.133 | Acc: 95.833% (2576/2688)
[Train] Epoch= 36  BatchID= 30 Loss: 0.133 | Acc: 95.817% (3802/3968)
[Train] Epoch= 36  BatchID= 40 Loss: 0.140 | Acc: 95.484% (5011/5248)
[Train] Epoch= 36  BatchID= 50 Loss: 0.140 | Acc: 95.573% (6239/6528)
[Train] Epoch= 36  BatchID= 60 Loss: 0.140 | Acc: 95.594% (7464/7808)
[Train] Epoch= 36  BatchID= 70 Loss: 0.141 | Acc: 95.599% (8688/9088)
[Train] Epoch= 36  BatchID= 0 Loss: 0.141 | Acc: 95.312% (122/128)
[Train] Epoch= 36  BatchID= 10 Loss: 0.103 | Acc: 97.230% (1369/1408)
[Train] Epoch= 36  BatchID= 20 Loss: 0.124 | Acc: 96.577% (2596/2688)
[Train] Epoch= 36  BatchID= 30 Loss: 0.124 | Acc: 96.421% (3826/3968)
[Train] Epoch= 36  BatchID= 40 Loss: 0.131 | Acc: 96.113% (5044/5248)
[Train] Epoch= 36  BatchID= 50 Loss: 0.127 | Acc: 96.232% (6282/6528)
[Train] Epoch= 36  BatchID= 60 Loss: 0.128 | Acc: 96.183% (7510/7808)
[Train] Epoch= 36  BatchID= 70 Loss: 0.129 | Acc: 96.039% (8728/9088)
[Test] Epoch= 36  BatchID= 0 Loss: 0.142 | Acc: 96.875% (124/128)
[Test] Epoch= 36  BatchID= 10 Loss: 0.325 | Acc: 90.412% (1273/1408)
[Test] Epoch= 36  BatchID= 20 Loss: 0.304 | Acc: 90.662% (2437/2688)
[Test] Epoch= 36  BatchID= 30 Loss: 0.321 | Acc: 90.293% (3544/3925)

Epoch: 37
[Test] Epoch= 36  BatchID= 0 Loss: 0.096 | Acc: 96.875% (124/128)
[Test] Epoch= 36  BatchID= 10 Loss: 0.330 | Acc: 90.270% (1271/1408)
[Test] Epoch= 36  BatchID= 20 Loss: 0.305 | Acc: 90.960% (2445/2688)
[Test] Epoch= 36  BatchID= 30 Loss: 0.339 | Acc: 90.242% (3542/3925)

Epoch: 37
[Train] Epoch= 37  BatchID= 0 Loss: 0.086 | Acc: 96.094% (123/128)
[Train] Epoch= 37  BatchID= 10 Loss: 0.112 | Acc: 96.449% (1358/1408)
[Train] Epoch= 37  BatchID= 20 Loss: 0.120 | Acc: 96.243% (2587/2688)
[Train] Epoch= 37  BatchID= 30 Loss: 0.122 | Acc: 96.195% (3817/3968)
[Train] Epoch= 37  BatchID= 40 Loss: 0.116 | Acc: 96.399% (5059/5248)
[Train] Epoch= 37  BatchID= 50 Loss: 0.122 | Acc: 96.170% (6278/6528)
[Train] Epoch= 37  BatchID= 60 Loss: 0.122 | Acc: 96.247% (7515/7808)
[Train] Epoch= 37  BatchID= 70 Loss: 0.125 | Acc: 96.116% (8735/9088)
[Train] Epoch= 37  BatchID= 0 Loss: 0.133 | Acc: 95.312% (122/128)
[Train] Epoch= 37  BatchID= 10 Loss: 0.145 | Acc: 95.455% (1344/1408)
[Train] Epoch= 37  BatchID= 20 Loss: 0.133 | Acc: 95.685% (2572/2688)
[Train] Epoch= 37  BatchID= 30 Loss: 0.130 | Acc: 95.691% (3797/3968)
[Train] Epoch= 37  BatchID= 40 Loss: 0.130 | Acc: 95.789% (5027/5248)
[Train] Epoch= 37  BatchID= 50 Loss: 0.131 | Acc: 95.833% (6256/6528)
[Train] Epoch= 37  BatchID= 60 Loss: 0.129 | Acc: 95.863% (7485/7808)
[Train] Epoch= 37  BatchID= 70 Loss: 0.129 | Acc: 95.863% (8712/9088)
[Test] Epoch= 37  BatchID= 0 Loss: 0.110 | Acc: 98.438% (126/128)
[Test] Epoch= 37  BatchID= 10 Loss: 0.311 | Acc: 91.264% (1285/1408)
[Test] Epoch= 37  BatchID= 20 Loss: 0.290 | Acc: 91.443% (2458/2688)
[Test] Epoch= 37  BatchID= 30 Loss: 0.322 | Acc: 90.675% (3559/3925)
Saving..
Best accuracy:  90.67515923566879

Epoch: 38
[Test] Epoch= 37  BatchID= 0 Loss: 0.134 | Acc: 97.656% (125/128)
[Test] Epoch= 37  BatchID= 10 Loss: 0.336 | Acc: 90.270% (1271/1408)
[Test] Epoch= 37  BatchID= 20 Loss: 0.307 | Acc: 90.737% (2439/2688)
[Test] Epoch= 37  BatchID= 30 Loss: 0.339 | Acc: 90.191% (3540/3925)

Epoch: 38
[Train] Epoch= 38  BatchID= 0 Loss: 0.175 | Acc: 91.406% (117/128)
[Train] Epoch= 38  BatchID= 10 Loss: 0.142 | Acc: 95.952% (1351/1408)
[Train] Epoch= 38  BatchID= 20 Loss: 0.130 | Acc: 96.429% (2592/2688)
[Train] Epoch= 38  BatchID= 30 Loss: 0.133 | Acc: 96.245% (3819/3968)
[Train] Epoch= 38  BatchID= 40 Loss: 0.128 | Acc: 96.341% (5056/5248)
[Train] Epoch= 38  BatchID= 50 Loss: 0.132 | Acc: 96.201% (6280/6528)
[Train] Epoch= 38  BatchID= 60 Loss: 0.133 | Acc: 96.094% (7503/7808)
[Train] Epoch= 38  BatchID= 70 Loss: 0.132 | Acc: 96.171% (8740/9088)
[Train] Epoch= 38  BatchID= 0 Loss: 0.161 | Acc: 96.094% (123/128)
[Train] Epoch= 38  BatchID= 10 Loss: 0.123 | Acc: 96.449% (1358/1408)
[Train] Epoch= 38  BatchID= 20 Loss: 0.117 | Acc: 96.429% (2592/2688)
[Train] Epoch= 38  BatchID= 30 Loss: 0.117 | Acc: 96.321% (3822/3968)
[Train] Epoch= 38  BatchID= 40 Loss: 0.123 | Acc: 96.056% (5041/5248)
[Train] Epoch= 38  BatchID= 50 Loss: 0.127 | Acc: 95.971% (6265/6528)
[Train] Epoch= 38  BatchID= 60 Loss: 0.129 | Acc: 95.927% (7490/7808)
[Train] Epoch= 38  BatchID= 70 Loss: 0.134 | Acc: 95.819% (8708/9088)
[Test] Epoch= 38  BatchID= 0 Loss: 0.113 | Acc: 97.656% (125/128)
[Test] Epoch= 38  BatchID= 10 Loss: 0.293 | Acc: 91.761% (1292/1408)
[Test] Epoch= 38  BatchID= 20 Loss: 0.285 | Acc: 91.555% (2461/2688)
[Test] Epoch= 38  BatchID= 30 Loss: 0.325 | Acc: 90.726% (3561/3925)
Saving..
Best accuracy:  90.72611464968153

Epoch: 39
[Test] Epoch= 38  BatchID= 0 Loss: 0.128 | Acc: 97.656% (125/128)
[Test] Epoch= 38  BatchID= 10 Loss: 0.292 | Acc: 91.264% (1285/1408)
[Test] Epoch= 38  BatchID= 20 Loss: 0.276 | Acc: 91.741% (2466/2688)
[Test] Epoch= 38  BatchID= 30 Loss: 0.326 | Acc: 90.420% (3549/3925)

Epoch: 39
[Train] Epoch= 39  BatchID= 0 Loss: 0.095 | Acc: 97.656% (125/128)
[Train] Epoch= 39  BatchID= 10 Loss: 0.131 | Acc: 96.449% (1358/1408)
[Train] Epoch= 39  BatchID= 20 Loss: 0.120 | Acc: 96.801% (2602/2688)
[Train] Epoch= 39  BatchID= 30 Loss: 0.124 | Acc: 96.598% (3833/3968)
[Train] Epoch= 39  BatchID= 40 Loss: 0.124 | Acc: 96.532% (5066/5248)
[Train] Epoch= 39  BatchID= 50 Loss: 0.126 | Acc: 96.400% (6293/6528)
[Train] Epoch= 39  BatchID= 60 Loss: 0.124 | Acc: 96.427% (7529/7808)
[Train] Epoch= 39  BatchID= 70 Loss: 0.124 | Acc: 96.402% (8761/9088)
[Train] Epoch= 39  BatchID= 0 Loss: 0.159 | Acc: 93.750% (120/128)
[Train] Epoch= 39  BatchID= 10 Loss: 0.153 | Acc: 94.957% (1337/1408)
[Train] Epoch= 39  BatchID= 20 Loss: 0.133 | Acc: 96.057% (2582/2688)
[Train] Epoch= 39  BatchID= 30 Loss: 0.129 | Acc: 96.346% (3823/3968)
[Train] Epoch= 39  BatchID= 40 Loss: 0.128 | Acc: 96.475% (5063/5248)
[Train] Epoch= 39  BatchID= 50 Loss: 0.131 | Acc: 96.262% (6284/6528)
[Train] Epoch= 39  BatchID= 60 Loss: 0.129 | Acc: 96.337% (7522/7808)
[Train] Epoch= 39  BatchID= 70 Loss: 0.132 | Acc: 96.237% (8746/9088)
[Test] Epoch= 39  BatchID= 0 Loss: 0.105 | Acc: 96.875% (124/128)
[Test] Epoch= 39  BatchID= 10 Loss: 0.307 | Acc: 91.122% (1283/1408)
[Test] Epoch= 39  BatchID= 20 Loss: 0.292 | Acc: 91.183% (2451/2688)
[Test] Epoch= 39  BatchID= 30 Loss: 0.325 | Acc: 90.318% (3545/3925)

Epoch: 40
[Test] Epoch= 39  BatchID= 0 Loss: 0.083 | Acc: 98.438% (126/128)
[Test] Epoch= 39  BatchID= 10 Loss: 0.309 | Acc: 90.980% (1281/1408)
[Test] Epoch= 39  BatchID= 20 Loss: 0.286 | Acc: 91.443% (2458/2688)
[Test] Epoch= 39  BatchID= 30 Loss: 0.329 | Acc: 90.369% (3547/3925)

Epoch: 40
[Train] Epoch= 40  BatchID= 0 Loss: 0.062 | Acc: 99.219% (127/128)
[Train] Epoch= 40  BatchID= 10 Loss: 0.118 | Acc: 96.520% (1359/1408)
[Train] Epoch= 40  BatchID= 20 Loss: 0.128 | Acc: 96.280% (2588/2688)
[Train] Epoch= 40  BatchID= 30 Loss: 0.132 | Acc: 96.220% (3818/3968)
[Train] Epoch= 40  BatchID= 40 Loss: 0.127 | Acc: 96.341% (5056/5248)
[Train] Epoch= 40  BatchID= 50 Loss: 0.125 | Acc: 96.461% (6297/6528)
[Train] Epoch= 40  BatchID= 60 Loss: 0.125 | Acc: 96.401% (7527/7808)
[Train] Epoch= 40  BatchID= 70 Loss: 0.129 | Acc: 96.281% (8750/9088)
[Train] Epoch= 40  BatchID= 0 Loss: 0.249 | Acc: 93.750% (120/128)
[Train] Epoch= 40  BatchID= 10 Loss: 0.148 | Acc: 95.455% (1344/1408)
[Train] Epoch= 40  BatchID= 20 Loss: 0.142 | Acc: 95.759% (2574/2688)
[Train] Epoch= 40  BatchID= 30 Loss: 0.140 | Acc: 95.766% (3800/3968)
[Train] Epoch= 40  BatchID= 40 Loss: 0.140 | Acc: 95.808% (5028/5248)
[Train] Epoch= 40  BatchID= 50 Loss: 0.141 | Acc: 95.711% (6248/6528)
[Train] Epoch= 40  BatchID= 60 Loss: 0.142 | Acc: 95.710% (7473/7808)
[Train] Epoch= 40  BatchID= 70 Loss: 0.141 | Acc: 95.731% (8700/9088)
[Test] Epoch= 40  BatchID= 0 Loss: 0.123 | Acc: 96.875% (124/128)
[Test] Epoch= 40  BatchID= 10 Loss: 0.324 | Acc: 89.915% (1266/1408)
[Test] Epoch= 40  BatchID= 20 Loss: 0.290 | Acc: 91.146% (2450/2688)
[Test] Epoch= 40  BatchID= 30 Loss: 0.322 | Acc: 90.548% (3554/3925)

Epoch: 41
[Test] Epoch= 40  BatchID= 0 Loss: 0.107 | Acc: 97.656% (125/128)
[Test] Epoch= 40  BatchID= 10 Loss: 0.344 | Acc: 90.554% (1275/1408)
[Test] Epoch= 40  BatchID= 20 Loss: 0.311 | Acc: 91.257% (2453/2688)
[Test] Epoch= 40  BatchID= 30 Loss: 0.349 | Acc: 90.140% (3538/3925)

Epoch: 41
[Train] Epoch= 41  BatchID= 0 Loss: 0.066 | Acc: 98.438% (126/128)
[Train] Epoch= 41  BatchID= 10 Loss: 0.122 | Acc: 95.810% (1349/1408)
[Train] Epoch= 41  BatchID= 20 Loss: 0.130 | Acc: 96.019% (2581/2688)
[Train] Epoch= 41  BatchID= 30 Loss: 0.133 | Acc: 95.917% (3806/3968)
[Train] Epoch= 41  BatchID= 40 Loss: 0.133 | Acc: 95.941% (5035/5248)
[Train] Epoch= 41  BatchID= 50 Loss: 0.132 | Acc: 95.971% (6265/6528)
[Train] Epoch= 41  BatchID= 60 Loss: 0.131 | Acc: 95.991% (7495/7808)
[Train] Epoch= 41  BatchID= 70 Loss: 0.128 | Acc: 96.116% (8735/9088)
[Train] Epoch= 41  BatchID= 0 Loss: 0.128 | Acc: 96.094% (123/128)
[Train] Epoch= 41  BatchID= 10 Loss: 0.126 | Acc: 95.881% (1350/1408)
[Train] Epoch= 41  BatchID= 20 Loss: 0.125 | Acc: 95.908% (2578/2688)
[Train] Epoch= 41  BatchID= 30 Loss: 0.120 | Acc: 96.195% (3817/3968)
[Train] Epoch= 41  BatchID= 40 Loss: 0.125 | Acc: 96.037% (5040/5248)
[Train] Epoch= 41  BatchID= 50 Loss: 0.130 | Acc: 95.833% (6256/6528)
[Train] Epoch= 41  BatchID= 60 Loss: 0.131 | Acc: 95.863% (7485/7808)
[Train] Epoch= 41  BatchID= 70 Loss: 0.127 | Acc: 95.984% (8723/9088)
[Test] Epoch= 41  BatchID= 0 Loss: 0.124 | Acc: 96.094% (123/128)
[Test] Epoch= 41  BatchID= 10 Loss: 0.331 | Acc: 89.347% (1258/1408)
[Test] Epoch= 41  BatchID= 20 Loss: 0.293 | Acc: 90.625% (2436/2688)
[Test] Epoch= 41  BatchID= 30 Loss: 0.325 | Acc: 89.885% (3528/3925)

Epoch: 42
[Test] Epoch= 41  BatchID= 0 Loss: 0.073 | Acc: 97.656% (125/128)
[Test] Epoch= 41  BatchID= 10 Loss: 0.280 | Acc: 91.832% (1293/1408)
[Test] Epoch= 41  BatchID= 20 Loss: 0.276 | Acc: 91.443% (2458/2688)
[Test] Epoch= 41  BatchID= 30 Loss: 0.318 | Acc: 90.497% (3552/3925)

Epoch: 42
[Train] Epoch= 42  BatchID= 0 Loss: 0.136 | Acc: 94.531% (121/128)
[Train] Epoch= 42  BatchID= 10 Loss: 0.125 | Acc: 96.662% (1361/1408)
[Train] Epoch= 42  BatchID= 20 Loss: 0.134 | Acc: 96.205% (2586/2688)
[Train] Epoch= 42  BatchID= 30 Loss: 0.135 | Acc: 96.069% (3812/3968)
[Train] Epoch= 42  BatchID= 40 Loss: 0.131 | Acc: 96.094% (5043/5248)
[Train] Epoch= 42  BatchID= 50 Loss: 0.130 | Acc: 96.109% (6274/6528)
[Train] Epoch= 42  BatchID= 60 Loss: 0.134 | Acc: 95.978% (7494/7808)
[Train] Epoch= 42  BatchID= 70 Loss: 0.134 | Acc: 95.940% (8719/9088)
[Train] Epoch= 42  BatchID= 0 Loss: 0.173 | Acc: 94.531% (121/128)
[Train] Epoch= 42  BatchID= 10 Loss: 0.130 | Acc: 95.952% (1351/1408)
[Train] Epoch= 42  BatchID= 20 Loss: 0.118 | Acc: 96.429% (2592/2688)
[Train] Epoch= 42  BatchID= 30 Loss: 0.114 | Acc: 96.522% (3830/3968)
[Train] Epoch= 42  BatchID= 40 Loss: 0.112 | Acc: 96.627% (5071/5248)
[Train] Epoch= 42  BatchID= 50 Loss: 0.113 | Acc: 96.661% (6310/6528)
[Train] Epoch= 42  BatchID= 60 Loss: 0.113 | Acc: 96.580% (7541/7808)
[Train] Epoch= 42  BatchID= 70 Loss: 0.112 | Acc: 96.611% (8780/9088)
[Test] Epoch= 42  BatchID= 0 Loss: 0.086 | Acc: 96.875% (124/128)
[Test] Epoch= 42  BatchID= 10 Loss: 0.307 | Acc: 91.051% (1282/1408)
[Test] Epoch= 42  BatchID= 20 Loss: 0.290 | Acc: 91.406% (2457/2688)
[Test] Epoch= 42  BatchID= 30 Loss: 0.332 | Acc: 90.064% (3535/3925)

Epoch: 43
[Test] Epoch= 42  BatchID= 0 Loss: 0.097 | Acc: 98.438% (126/128)
[Test] Epoch= 42  BatchID= 10 Loss: 0.295 | Acc: 91.335% (1286/1408)
[Test] Epoch= 42  BatchID= 20 Loss: 0.275 | Acc: 91.704% (2465/2688)
[Test] Epoch= 42  BatchID= 30 Loss: 0.327 | Acc: 90.344% (3546/3925)

Epoch: 43
[Train] Epoch= 43  BatchID= 0 Loss: 0.064 | Acc: 98.438% (126/128)
[Train] Epoch= 43  BatchID= 10 Loss: 0.098 | Acc: 97.017% (1366/1408)
[Train] Epoch= 43  BatchID= 20 Loss: 0.117 | Acc: 96.429% (2592/2688)
[Train] Epoch= 43  BatchID= 30 Loss: 0.111 | Acc: 96.673% (3836/3968)
[Train] Epoch= 43  BatchID= 40 Loss: 0.114 | Acc: 96.589% (5069/5248)
[Train] Epoch= 43  BatchID= 50 Loss: 0.118 | Acc: 96.369% (6291/6528)
[Train] Epoch= 43  BatchID= 60 Loss: 0.120 | Acc: 96.363% (7524/7808)
[Train] Epoch= 43  BatchID= 70 Loss: 0.120 | Acc: 96.369% (8758/9088)
[Train] Epoch= 43  BatchID= 0 Loss: 0.122 | Acc: 96.875% (124/128)
[Train] Epoch= 43  BatchID= 10 Loss: 0.115 | Acc: 96.236% (1355/1408)
[Train] Epoch= 43  BatchID= 20 Loss: 0.119 | Acc: 96.540% (2595/2688)
[Train] Epoch= 43  BatchID= 30 Loss: 0.121 | Acc: 96.421% (3826/3968)
[Train] Epoch= 43  BatchID= 40 Loss: 0.126 | Acc: 96.361% (5057/5248)
[Train] Epoch= 43  BatchID= 50 Loss: 0.129 | Acc: 96.278% (6285/6528)
[Train] Epoch= 43  BatchID= 60 Loss: 0.129 | Acc: 96.311% (7520/7808)
[Train] Epoch= 43  BatchID= 70 Loss: 0.127 | Acc: 96.314% (8753/9088)
[Test] Epoch= 43  BatchID= 0 Loss: 0.117 | Acc: 96.875% (124/128)
[Test] Epoch= 43  BatchID= 10 Loss: 0.320 | Acc: 90.128% (1269/1408)
[Test] Epoch= 43  BatchID= 20 Loss: 0.302 | Acc: 90.699% (2438/2688)
[Test] Epoch= 43  BatchID= 30 Loss: 0.336 | Acc: 89.860% (3527/3925)

Epoch: 44
[Test] Epoch= 43  BatchID= 0 Loss: 0.109 | Acc: 97.656% (125/128)
[Test] Epoch= 43  BatchID= 10 Loss: 0.304 | Acc: 90.625% (1276/1408)
[Test] Epoch= 43  BatchID= 20 Loss: 0.283 | Acc: 91.109% (2449/2688)
[Test] Epoch= 43  BatchID= 30 Loss: 0.321 | Acc: 90.242% (3542/3925)

Epoch: 44
[Train] Epoch= 44  BatchID= 0 Loss: 0.094 | Acc: 97.656% (125/128)
[Train] Epoch= 44  BatchID= 10 Loss: 0.126 | Acc: 96.094% (1353/1408)
[Train] Epoch= 44  BatchID= 20 Loss: 0.128 | Acc: 95.833% (2576/2688)
[Train] Epoch= 44  BatchID= 30 Loss: 0.130 | Acc: 95.716% (3798/3968)
[Train] Epoch= 44  BatchID= 40 Loss: 0.128 | Acc: 95.789% (5027/5248)
[Train] Epoch= 44  BatchID= 50 Loss: 0.124 | Acc: 95.987% (6266/6528)
[Train] Epoch= 44  BatchID= 60 Loss: 0.123 | Acc: 96.145% (7507/7808)
[Train] Epoch= 44  BatchID= 70 Loss: 0.125 | Acc: 96.138% (8737/9088)
[Train] Epoch= 44  BatchID= 0 Loss: 0.100 | Acc: 96.875% (124/128)
[Train] Epoch= 44  BatchID= 10 Loss: 0.132 | Acc: 96.023% (1352/1408)
[Train] Epoch= 44  BatchID= 20 Loss: 0.119 | Acc: 96.354% (2590/2688)
[Train] Epoch= 44  BatchID= 30 Loss: 0.116 | Acc: 96.270% (3820/3968)
[Train] Epoch= 44  BatchID= 40 Loss: 0.117 | Acc: 96.170% (5047/5248)
[Train] Epoch= 44  BatchID= 50 Loss: 0.121 | Acc: 96.048% (6270/6528)
[Train] Epoch= 44  BatchID= 60 Loss: 0.119 | Acc: 96.196% (7511/7808)
[Train] Epoch= 44  BatchID= 70 Loss: 0.122 | Acc: 96.160% (8739/9088)
[Test] Epoch= 44  BatchID= 0 Loss: 0.138 | Acc: 96.094% (123/128)
[Test] Epoch= 44  BatchID= 10 Loss: 0.328 | Acc: 89.347% (1258/1408)
[Test] Epoch= 44  BatchID= 20 Loss: 0.295 | Acc: 90.551% (2434/2688)
[Test] Epoch= 44  BatchID= 30 Loss: 0.325 | Acc: 89.987% (3532/3925)

Epoch: 45
[Test] Epoch= 44  BatchID= 0 Loss: 0.099 | Acc: 97.656% (125/128)
[Test] Epoch= 44  BatchID= 10 Loss: 0.287 | Acc: 91.832% (1293/1408)
[Test] Epoch= 44  BatchID= 20 Loss: 0.282 | Acc: 91.778% (2467/2688)
[Test] Epoch= 44  BatchID= 30 Loss: 0.316 | Acc: 90.599% (3556/3925)

Epoch: 45
[Train] Epoch= 45  BatchID= 0 Loss: 0.054 | Acc: 98.438% (126/128)
[Train] Epoch= 45  BatchID= 10 Loss: 0.115 | Acc: 96.165% (1354/1408)
[Train] Epoch= 45  BatchID= 20 Loss: 0.125 | Acc: 95.945% (2579/2688)
[Train] Epoch= 45  BatchID= 30 Loss: 0.127 | Acc: 95.968% (3808/3968)
[Train] Epoch= 45  BatchID= 40 Loss: 0.123 | Acc: 96.132% (5045/5248)
[Train] Epoch= 45  BatchID= 50 Loss: 0.122 | Acc: 96.232% (6282/6528)
[Train] Epoch= 45  BatchID= 60 Loss: 0.123 | Acc: 96.171% (7509/7808)
[Train] Epoch= 45  BatchID= 70 Loss: 0.125 | Acc: 95.929% (8718/9088)
[Train] Epoch= 45  BatchID= 0 Loss: 0.092 | Acc: 98.438% (126/128)
[Train] Epoch= 45  BatchID= 10 Loss: 0.112 | Acc: 96.449% (1358/1408)
[Train] Epoch= 45  BatchID= 20 Loss: 0.122 | Acc: 95.945% (2579/2688)
[Train] Epoch= 45  BatchID= 30 Loss: 0.122 | Acc: 95.817% (3802/3968)
[Train] Epoch= 45  BatchID= 40 Loss: 0.125 | Acc: 95.941% (5035/5248)
[Train] Epoch= 45  BatchID= 50 Loss: 0.125 | Acc: 95.987% (6266/6528)
[Train] Epoch= 45  BatchID= 60 Loss: 0.124 | Acc: 96.107% (7504/7808)
[Train] Epoch= 45  BatchID= 70 Loss: 0.122 | Acc: 96.171% (8740/9088)
[Test] Epoch= 45  BatchID= 0 Loss: 0.090 | Acc: 97.656% (125/128)
[Test] Epoch= 45  BatchID= 10 Loss: 0.290 | Acc: 91.264% (1285/1408)
[Test] Epoch= 45  BatchID= 20 Loss: 0.285 | Acc: 91.071% (2448/2688)
[Test] Epoch= 45  BatchID= 30 Loss: 0.334 | Acc: 89.860% (3527/3925)

Epoch: 46
[Test] Epoch= 45  BatchID= 0 Loss: 0.118 | Acc: 96.094% (123/128)
[Test] Epoch= 45  BatchID= 10 Loss: 0.292 | Acc: 90.909% (1280/1408)
[Test] Epoch= 45  BatchID= 20 Loss: 0.283 | Acc: 91.183% (2451/2688)
[Test] Epoch= 45  BatchID= 30 Loss: 0.326 | Acc: 90.242% (3542/3925)

Epoch: 46
[Train] Epoch= 46  BatchID= 0 Loss: 0.136 | Acc: 96.094% (123/128)
[Train] Epoch= 46  BatchID= 10 Loss: 0.122 | Acc: 96.165% (1354/1408)
[Train] Epoch= 46  BatchID= 20 Loss: 0.122 | Acc: 96.131% (2584/2688)
[Train] Epoch= 46  BatchID= 30 Loss: 0.126 | Acc: 96.018% (3810/3968)
[Train] Epoch= 46  BatchID= 40 Loss: 0.134 | Acc: 95.789% (5027/5248)
[Train] Epoch= 46  BatchID= 50 Loss: 0.129 | Acc: 95.956% (6264/6528)
[Train] Epoch= 46  BatchID= 60 Loss: 0.129 | Acc: 95.978% (7494/7808)
[Train] Epoch= 46  BatchID= 70 Loss: 0.126 | Acc: 96.039% (8728/9088)
[Train] Epoch= 46  BatchID= 0 Loss: 0.118 | Acc: 96.875% (124/128)
[Train] Epoch= 46  BatchID= 10 Loss: 0.123 | Acc: 96.165% (1354/1408)
[Train] Epoch= 46  BatchID= 20 Loss: 0.122 | Acc: 96.280% (2588/2688)
[Train] Epoch= 46  BatchID= 30 Loss: 0.118 | Acc: 96.421% (3826/3968)
[Train] Epoch= 46  BatchID= 40 Loss: 0.117 | Acc: 96.418% (5060/5248)
[Train] Epoch= 46  BatchID= 50 Loss: 0.120 | Acc: 96.339% (6289/6528)
[Train] Epoch= 46  BatchID= 60 Loss: 0.123 | Acc: 96.311% (7520/7808)
[Train] Epoch= 46  BatchID= 70 Loss: 0.126 | Acc: 96.237% (8746/9088)
[Test] Epoch= 46  BatchID= 0 Loss: 0.084 | Acc: 97.656% (125/128)
[Test] Epoch= 46  BatchID= 10 Loss: 0.321 | Acc: 90.270% (1271/1408)
[Test] Epoch= 46  BatchID= 20 Loss: 0.287 | Acc: 91.109% (2449/2688)
[Test] Epoch= 46  BatchID= 30 Loss: 0.331 | Acc: 89.987% (3532/3925)

Epoch: 47
[Test] Epoch= 46  BatchID= 0 Loss: 0.131 | Acc: 96.875% (124/128)
[Test] Epoch= 46  BatchID= 10 Loss: 0.297 | Acc: 90.696% (1277/1408)
[Test] Epoch= 46  BatchID= 20 Loss: 0.272 | Acc: 91.481% (2459/2688)
[Test] Epoch= 46  BatchID= 30 Loss: 0.322 | Acc: 90.344% (3546/3925)

Epoch: 47
[Train] Epoch= 47  BatchID= 0 Loss: 0.201 | Acc: 92.188% (118/128)
[Train] Epoch= 47  BatchID= 10 Loss: 0.129 | Acc: 95.668% (1347/1408)
[Train] Epoch= 47  BatchID= 20 Loss: 0.134 | Acc: 95.871% (2577/2688)
[Train] Epoch= 47  BatchID= 30 Loss: 0.132 | Acc: 95.892% (3805/3968)
[Train] Epoch= 47  BatchID= 40 Loss: 0.128 | Acc: 96.075% (5042/5248)
[Train] Epoch= 47  BatchID= 50 Loss: 0.130 | Acc: 95.971% (6265/6528)
[Train] Epoch= 47  BatchID= 60 Loss: 0.129 | Acc: 96.017% (7497/7808)
[Train] Epoch= 47  BatchID= 70 Loss: 0.126 | Acc: 96.160% (8739/9088)
[Train] Epoch= 47  BatchID= 0 Loss: 0.168 | Acc: 94.531% (121/128)
[Train] Epoch= 47  BatchID= 10 Loss: 0.135 | Acc: 95.881% (1350/1408)
[Train] Epoch= 47  BatchID= 20 Loss: 0.126 | Acc: 96.057% (2582/2688)
[Train] Epoch= 47  BatchID= 30 Loss: 0.121 | Acc: 96.270% (3820/3968)
[Train] Epoch= 47  BatchID= 40 Loss: 0.126 | Acc: 96.075% (5042/5248)
[Train] Epoch= 47  BatchID= 50 Loss: 0.129 | Acc: 96.048% (6270/6528)
[Train] Epoch= 47  BatchID= 60 Loss: 0.126 | Acc: 96.119% (7505/7808)
[Train] Epoch= 47  BatchID= 70 Loss: 0.126 | Acc: 96.149% (8738/9088)
[Test] Epoch= 47  BatchID= 0 Loss: 0.080 | Acc: 97.656% (125/128)
[Test] Epoch= 47  BatchID= 10 Loss: 0.323 | Acc: 89.844% (1265/1408)
[Test] Epoch= 47  BatchID= 20 Loss: 0.291 | Acc: 90.885% (2443/2688)
[Test] Epoch= 47  BatchID= 30 Loss: 0.319 | Acc: 90.395% (3548/3925)

Epoch: 48
[Test] Epoch= 47  BatchID= 0 Loss: 0.096 | Acc: 97.656% (125/128)
[Test] Epoch= 47  BatchID= 10 Loss: 0.310 | Acc: 90.909% (1280/1408)
[Test] Epoch= 47  BatchID= 20 Loss: 0.281 | Acc: 91.629% (2463/2688)
[Test] Epoch= 47  BatchID= 30 Loss: 0.312 | Acc: 91.057% (3574/3925)
Saving..
Best accuracy:  91.05732484076434

Epoch: 48
[Train] Epoch= 48  BatchID= 0 Loss: 0.121 | Acc: 96.094% (123/128)
[Train] Epoch= 48  BatchID= 10 Loss: 0.109 | Acc: 97.088% (1367/1408)
[Train] Epoch= 48  BatchID= 20 Loss: 0.111 | Acc: 96.615% (2597/2688)
[Train] Epoch= 48  BatchID= 30 Loss: 0.112 | Acc: 96.547% (3831/3968)
[Train] Epoch= 48  BatchID= 40 Loss: 0.120 | Acc: 96.322% (5055/5248)
[Train] Epoch= 48  BatchID= 50 Loss: 0.122 | Acc: 96.324% (6288/6528)
[Train] Epoch= 48  BatchID= 60 Loss: 0.122 | Acc: 96.247% (7515/7808)
[Train] Epoch= 48  BatchID= 70 Loss: 0.124 | Acc: 96.215% (8744/9088)
[Train] Epoch= 48  BatchID= 0 Loss: 0.116 | Acc: 95.312% (122/128)
[Train] Epoch= 48  BatchID= 10 Loss: 0.135 | Acc: 95.455% (1344/1408)
[Train] Epoch= 48  BatchID= 20 Loss: 0.125 | Acc: 95.982% (2580/2688)
[Train] Epoch= 48  BatchID= 30 Loss: 0.117 | Acc: 96.270% (3820/3968)
[Train] Epoch= 48  BatchID= 40 Loss: 0.119 | Acc: 96.227% (5050/5248)
[Train] Epoch= 48  BatchID= 50 Loss: 0.122 | Acc: 96.201% (6280/6528)
[Train] Epoch= 48  BatchID= 60 Loss: 0.124 | Acc: 96.145% (7507/7808)
[Train] Epoch= 48  BatchID= 70 Loss: 0.122 | Acc: 96.204% (8743/9088)
[Test] Epoch= 48  BatchID= 0 Loss: 0.095 | Acc: 98.438% (126/128)
[Test] Epoch= 48  BatchID= 10 Loss: 0.307 | Acc: 91.051% (1282/1408)
[Test] Epoch= 48  BatchID= 20 Loss: 0.284 | Acc: 91.555% (2461/2688)
[Test] Epoch= 48  BatchID= 30 Loss: 0.317 | Acc: 90.803% (3564/3925)
Saving..
Best accuracy:  90.80254777070064

Epoch: 49
[Test] Epoch= 48  BatchID= 0 Loss: 0.098 | Acc: 98.438% (126/128)
[Test] Epoch= 48  BatchID= 10 Loss: 0.294 | Acc: 91.335% (1286/1408)
[Test] Epoch= 48  BatchID= 20 Loss: 0.284 | Acc: 91.332% (2455/2688)
[Test] Epoch= 48  BatchID= 30 Loss: 0.307 | Acc: 90.828% (3565/3925)

Epoch: 49
[Train] Epoch= 49  BatchID= 0 Loss: 0.054 | Acc: 98.438% (126/128)
[Train] Epoch= 49  BatchID= 10 Loss: 0.137 | Acc: 95.526% (1345/1408)
[Train] Epoch= 49  BatchID= 20 Loss: 0.135 | Acc: 95.536% (2568/2688)
[Train] Epoch= 49  BatchID= 30 Loss: 0.134 | Acc: 95.665% (3796/3968)
[Train] Epoch= 49  BatchID= 40 Loss: 0.131 | Acc: 95.903% (5033/5248)
[Train] Epoch= 49  BatchID= 50 Loss: 0.128 | Acc: 96.078% (6272/6528)
[Train] Epoch= 49  BatchID= 60 Loss: 0.125 | Acc: 96.260% (7516/7808)
[Train] Epoch= 49  BatchID= 70 Loss: 0.122 | Acc: 96.292% (8751/9088)
[Train] Epoch= 49  BatchID= 0 Loss: 0.115 | Acc: 96.875% (124/128)
[Train] Epoch= 49  BatchID= 10 Loss: 0.113 | Acc: 96.591% (1360/1408)
[Train] Epoch= 49  BatchID= 20 Loss: 0.112 | Acc: 96.615% (2597/2688)
[Train] Epoch= 49  BatchID= 30 Loss: 0.111 | Acc: 96.673% (3836/3968)
[Train] Epoch= 49  BatchID= 40 Loss: 0.117 | Acc: 96.437% (5061/5248)
[Train] Epoch= 49  BatchID= 50 Loss: 0.121 | Acc: 96.369% (6291/6528)
[Train] Epoch= 49  BatchID= 60 Loss: 0.120 | Acc: 96.465% (7532/7808)
[Train] Epoch= 49  BatchID= 70 Loss: 0.122 | Acc: 96.336% (8755/9088)
[Test] Epoch= 49  BatchID= 0 Loss: 0.084 | Acc: 97.656% (125/128)
[Test] Epoch= 49  BatchID= 10 Loss: 0.310 | Acc: 91.335% (1286/1408)
[Test] Epoch= 49  BatchID= 20 Loss: 0.287 | Acc: 91.704% (2465/2688)
[Test] Epoch= 49  BatchID= 30 Loss: 0.325 | Acc: 90.650% (3558/3925)

Epoch: 50
[Test] Epoch= 49  BatchID= 0 Loss: 0.100 | Acc: 97.656% (125/128)
[Test] Epoch= 49  BatchID= 10 Loss: 0.286 | Acc: 91.548% (1289/1408)
[Test] Epoch= 49  BatchID= 20 Loss: 0.279 | Acc: 91.406% (2457/2688)
[Test] Epoch= 49  BatchID= 30 Loss: 0.311 | Acc: 90.599% (3556/3925)

Epoch: 50
[Train] Epoch= 50  BatchID= 0 Loss: 0.125 | Acc: 93.750% (120/128)
[Train] Epoch= 50  BatchID= 10 Loss: 0.104 | Acc: 96.875% (1364/1408)
[Train] Epoch= 50  BatchID= 20 Loss: 0.113 | Acc: 96.652% (2598/2688)
[Train] Epoch= 50  BatchID= 30 Loss: 0.117 | Acc: 96.573% (3832/3968)
[Train] Epoch= 50  BatchID= 40 Loss: 0.119 | Acc: 96.494% (5064/5248)
[Train] Epoch= 50  BatchID= 50 Loss: 0.122 | Acc: 96.293% (6286/6528)
[Train] Epoch= 50  BatchID= 60 Loss: 0.122 | Acc: 96.273% (7517/7808)
[Train] Epoch= 50  BatchID= 70 Loss: 0.123 | Acc: 96.226% (8745/9088)
[Train] Epoch= 50  BatchID= 0 Loss: 0.081 | Acc: 96.875% (124/128)
[Train] Epoch= 50  BatchID= 10 Loss: 0.109 | Acc: 96.946% (1365/1408)
[Train] Epoch= 50  BatchID= 20 Loss: 0.115 | Acc: 96.503% (2594/2688)
[Train] Epoch= 50  BatchID= 30 Loss: 0.119 | Acc: 96.321% (3822/3968)
[Train] Epoch= 50  BatchID= 40 Loss: 0.120 | Acc: 96.361% (5057/5248)
[Train] Epoch= 50  BatchID= 50 Loss: 0.117 | Acc: 96.492% (6299/6528)
[Train] Epoch= 50  BatchID= 60 Loss: 0.114 | Acc: 96.555% (7539/7808)
[Train] Epoch= 50  BatchID= 70 Loss: 0.116 | Acc: 96.512% (8771/9088)
[Test] Epoch= 50  BatchID= 0 Loss: 0.097 | Acc: 97.656% (125/128)
[Test] Epoch= 50  BatchID= 10 Loss: 0.317 | Acc: 90.838% (1279/1408)
[Test] Epoch= 50  BatchID= 20 Loss: 0.288 | Acc: 91.406% (2457/2688)
[Test] Epoch= 50  BatchID= 30 Loss: 0.320 | Acc: 90.471% (3551/3925)

Epoch: 51
[Test] Epoch= 50  BatchID= 0 Loss: 0.107 | Acc: 98.438% (126/128)
[Test] Epoch= 50  BatchID= 10 Loss: 0.306 | Acc: 90.909% (1280/1408)
[Test] Epoch= 50  BatchID= 20 Loss: 0.285 | Acc: 91.257% (2453/2688)
[Test] Epoch= 50  BatchID= 30 Loss: 0.312 | Acc: 90.599% (3556/3925)

Epoch: 51
[Train] Epoch= 51  BatchID= 0 Loss: 0.068 | Acc: 97.656% (125/128)
[Train] Epoch= 51  BatchID= 10 Loss: 0.105 | Acc: 96.733% (1362/1408)
[Train] Epoch= 51  BatchID= 20 Loss: 0.116 | Acc: 96.243% (2587/2688)
[Train] Epoch= 51  BatchID= 30 Loss: 0.123 | Acc: 95.993% (3809/3968)
[Train] Epoch= 51  BatchID= 40 Loss: 0.120 | Acc: 96.170% (5047/5248)
[Train] Epoch= 51  BatchID= 50 Loss: 0.115 | Acc: 96.400% (6293/6528)
[Train] Epoch= 51  BatchID= 60 Loss: 0.116 | Acc: 96.337% (7522/7808)
[Train] Epoch= 51  BatchID= 70 Loss: 0.116 | Acc: 96.347% (8756/9088)
[Train] Epoch= 51  BatchID= 0 Loss: 0.065 | Acc: 97.656% (125/128)
[Train] Epoch= 51  BatchID= 10 Loss: 0.106 | Acc: 96.804% (1363/1408)
[Train] Epoch= 51  BatchID= 20 Loss: 0.108 | Acc: 96.689% (2599/2688)
[Train] Epoch= 51  BatchID= 30 Loss: 0.116 | Acc: 96.144% (3815/3968)
[Train] Epoch= 51  BatchID= 40 Loss: 0.114 | Acc: 96.341% (5056/5248)
[Train] Epoch= 51  BatchID= 50 Loss: 0.117 | Acc: 96.278% (6285/6528)
[Train] Epoch= 51  BatchID= 60 Loss: 0.117 | Acc: 96.260% (7516/7808)
[Train] Epoch= 51  BatchID= 70 Loss: 0.117 | Acc: 96.325% (8754/9088)
[Test] Epoch= 51  BatchID= 0 Loss: 0.081 | Acc: 98.438% (126/128)
[Test] Epoch= 51  BatchID= 10 Loss: 0.306 | Acc: 90.909% (1280/1408)
[Test] Epoch= 51  BatchID= 20 Loss: 0.283 | Acc: 91.481% (2459/2688)
[Test] Epoch= 51  BatchID= 30 Loss: 0.310 | Acc: 90.650% (3558/3925)

Epoch: 52
[Test] Epoch= 51  BatchID= 0 Loss: 0.086 | Acc: 98.438% (126/128)
[Test] Epoch= 51  BatchID= 10 Loss: 0.286 | Acc: 91.903% (1294/1408)
[Test] Epoch= 51  BatchID= 20 Loss: 0.279 | Acc: 91.890% (2470/2688)
[Test] Epoch= 51  BatchID= 30 Loss: 0.316 | Acc: 90.904% (3568/3925)

Epoch: 52
[Train] Epoch= 52  BatchID= 0 Loss: 0.167 | Acc: 92.969% (119/128)
[Train] Epoch= 52  BatchID= 10 Loss: 0.121 | Acc: 96.520% (1359/1408)
[Train] Epoch= 52  BatchID= 20 Loss: 0.117 | Acc: 96.652% (2598/2688)
[Train] Epoch= 52  BatchID= 30 Loss: 0.119 | Acc: 96.699% (3837/3968)
[Train] Epoch= 52  BatchID= 40 Loss: 0.118 | Acc: 96.513% (5065/5248)
[Train] Epoch= 52  BatchID= 50 Loss: 0.118 | Acc: 96.415% (6294/6528)
[Train] Epoch= 52  BatchID= 60 Loss: 0.117 | Acc: 96.401% (7527/7808)
[Train] Epoch= 52  BatchID= 70 Loss: 0.118 | Acc: 96.369% (8758/9088)
[Train] Epoch= 52  BatchID= 0 Loss: 0.131 | Acc: 96.094% (123/128)
[Train] Epoch= 52  BatchID= 10 Loss: 0.108 | Acc: 96.662% (1361/1408)
[Train] Epoch= 52  BatchID= 20 Loss: 0.103 | Acc: 96.912% (2605/2688)
[Train] Epoch= 52  BatchID= 30 Loss: 0.111 | Acc: 96.774% (3840/3968)
[Train] Epoch= 52  BatchID= 40 Loss: 0.110 | Acc: 96.799% (5080/5248)
[Train] Epoch= 52  BatchID= 50 Loss: 0.115 | Acc: 96.645% (6309/6528)
[Train] Epoch= 52  BatchID= 60 Loss: 0.113 | Acc: 96.593% (7542/7808)
[Train] Epoch= 52  BatchID= 70 Loss: 0.113 | Acc: 96.567% (8776/9088)
[Test] Epoch= 52  BatchID= 0 Loss: 0.104 | Acc: 96.875% (124/128)
[Test] Epoch= 52  BatchID= 10 Loss: 0.307 | Acc: 91.122% (1283/1408)
[Test] Epoch= 52  BatchID= 20 Loss: 0.282 | Acc: 91.853% (2469/2688)
[Test] Epoch= 52  BatchID= 30 Loss: 0.317 | Acc: 90.854% (3566/3925)
Saving..
Best accuracy:  90.85350318471338

Epoch: 53
[Test] Epoch= 52  BatchID= 0 Loss: 0.125 | Acc: 96.875% (124/128)
[Test] Epoch= 52  BatchID= 10 Loss: 0.311 | Acc: 91.193% (1284/1408)
[Test] Epoch= 52  BatchID= 20 Loss: 0.278 | Acc: 91.815% (2468/2688)
[Test] Epoch= 52  BatchID= 30 Loss: 0.320 | Acc: 90.675% (3559/3925)

Epoch: 53
[Train] Epoch= 53  BatchID= 0 Loss: 0.134 | Acc: 96.875% (124/128)
[Train] Epoch= 53  BatchID= 10 Loss: 0.126 | Acc: 96.520% (1359/1408)
[Train] Epoch= 53  BatchID= 20 Loss: 0.124 | Acc: 96.205% (2586/2688)
[Train] Epoch= 53  BatchID= 30 Loss: 0.121 | Acc: 96.245% (3819/3968)
[Train] Epoch= 53  BatchID= 40 Loss: 0.118 | Acc: 96.341% (5056/5248)
[Train] Epoch= 53  BatchID= 50 Loss: 0.122 | Acc: 96.094% (6273/6528)
[Train] Epoch= 53  BatchID= 60 Loss: 0.122 | Acc: 96.107% (7504/7808)
[Train] Epoch= 53  BatchID= 70 Loss: 0.122 | Acc: 96.083% (8732/9088)
[Train] Epoch= 53  BatchID= 0 Loss: 0.209 | Acc: 94.531% (121/128)
[Train] Epoch= 53  BatchID= 10 Loss: 0.113 | Acc: 96.733% (1362/1408)
[Train] Epoch= 53  BatchID= 20 Loss: 0.117 | Acc: 96.354% (2590/2688)
[Train] Epoch= 53  BatchID= 30 Loss: 0.119 | Acc: 96.321% (3822/3968)
[Train] Epoch= 53  BatchID= 40 Loss: 0.120 | Acc: 96.227% (5050/5248)
[Train] Epoch= 53  BatchID= 50 Loss: 0.120 | Acc: 96.201% (6280/6528)
[Train] Epoch= 53  BatchID= 60 Loss: 0.121 | Acc: 96.273% (7517/7808)
[Train] Epoch= 53  BatchID= 70 Loss: 0.119 | Acc: 96.369% (8758/9088)
[Test] Epoch= 53  BatchID= 0 Loss: 0.094 | Acc: 97.656% (125/128)
[Test] Epoch= 53  BatchID= 10 Loss: 0.321 | Acc: 90.554% (1275/1408)
[Test] Epoch= 53  BatchID= 20 Loss: 0.283 | Acc: 91.295% (2454/2688)
[Test] Epoch= 53  BatchID= 30 Loss: 0.318 | Acc: 90.420% (3549/3925)

Epoch: 54
[Test] Epoch= 53  BatchID= 0 Loss: 0.099 | Acc: 97.656% (125/128)
[Test] Epoch= 53  BatchID= 10 Loss: 0.309 | Acc: 90.696% (1277/1408)
[Test] Epoch= 53  BatchID= 20 Loss: 0.295 | Acc: 90.997% (2446/2688)
[Test] Epoch= 53  BatchID= 30 Loss: 0.316 | Acc: 90.548% (3554/3925)

Epoch: 54
[Train] Epoch= 54  BatchID= 0 Loss: 0.118 | Acc: 96.094% (123/128)
[Train] Epoch= 54  BatchID= 10 Loss: 0.114 | Acc: 96.591% (1360/1408)
[Train] Epoch= 54  BatchID= 20 Loss: 0.116 | Acc: 96.577% (2596/2688)
[Train] Epoch= 54  BatchID= 30 Loss: 0.112 | Acc: 96.799% (3841/3968)
[Train] Epoch= 54  BatchID= 40 Loss: 0.111 | Acc: 96.723% (5076/5248)
[Train] Epoch= 54  BatchID= 50 Loss: 0.108 | Acc: 96.798% (6319/6528)
[Train] Epoch= 54  BatchID= 60 Loss: 0.112 | Acc: 96.568% (7540/7808)
[Train] Epoch= 54  BatchID= 70 Loss: 0.114 | Acc: 96.545% (8774/9088)
[Train] Epoch= 54  BatchID= 0 Loss: 0.051 | Acc: 98.438% (126/128)
[Train] Epoch= 54  BatchID= 10 Loss: 0.097 | Acc: 97.443% (1372/1408)
[Train] Epoch= 54  BatchID= 20 Loss: 0.110 | Acc: 96.875% (2604/2688)
[Train] Epoch= 54  BatchID= 30 Loss: 0.107 | Acc: 96.925% (3846/3968)
[Train] Epoch= 54  BatchID= 40 Loss: 0.107 | Acc: 96.818% (5081/5248)
[Train] Epoch= 54  BatchID= 50 Loss: 0.109 | Acc: 96.615% (6307/6528)
[Train] Epoch= 54  BatchID= 60 Loss: 0.112 | Acc: 96.580% (7541/7808)
[Train] Epoch= 54  BatchID= 70 Loss: 0.113 | Acc: 96.600% (8779/9088)
[Test] Epoch= 54  BatchID= 0 Loss: 0.089 | Acc: 97.656% (125/128)
[Test] Epoch= 54  BatchID= 10 Loss: 0.312 | Acc: 91.193% (1284/1408)
[Test] Epoch= 54  BatchID= 20 Loss: 0.285 | Acc: 91.332% (2455/2688)
[Test] Epoch= 54  BatchID= 30 Loss: 0.314 | Acc: 90.573% (3555/3925)

Epoch: 55
[Test] Epoch= 54  BatchID= 0 Loss: 0.085 | Acc: 98.438% (126/128)
[Test] Epoch= 54  BatchID= 10 Loss: 0.291 | Acc: 91.761% (1292/1408)
[Test] Epoch= 54  BatchID= 20 Loss: 0.277 | Acc: 91.853% (2469/2688)
[Test] Epoch= 54  BatchID= 30 Loss: 0.311 | Acc: 91.006% (3572/3925)

Epoch: 55
[Train] Epoch= 55  BatchID= 0 Loss: 0.080 | Acc: 97.656% (125/128)
[Train] Epoch= 55  BatchID= 10 Loss: 0.099 | Acc: 97.159% (1368/1408)
[Train] Epoch= 55  BatchID= 20 Loss: 0.099 | Acc: 97.173% (2612/2688)
[Train] Epoch= 55  BatchID= 30 Loss: 0.110 | Acc: 96.774% (3840/3968)
[Train] Epoch= 55  BatchID= 40 Loss: 0.113 | Acc: 96.627% (5071/5248)
[Train] Epoch= 55  BatchID= 50 Loss: 0.114 | Acc: 96.584% (6305/6528)
[Train] Epoch= 55  BatchID= 60 Loss: 0.113 | Acc: 96.542% (7538/7808)
[Train] Epoch= 55  BatchID= 70 Loss: 0.114 | Acc: 96.523% (8772/9088)
[Train] Epoch= 55  BatchID= 0 Loss: 0.067 | Acc: 99.219% (127/128)
[Train] Epoch= 55  BatchID= 10 Loss: 0.094 | Acc: 97.230% (1369/1408)
[Train] Epoch= 55  BatchID= 20 Loss: 0.107 | Acc: 96.838% (2603/2688)
[Train] Epoch= 55  BatchID= 30 Loss: 0.115 | Acc: 96.598% (3833/3968)
[Train] Epoch= 55  BatchID= 40 Loss: 0.112 | Acc: 96.780% (5079/5248)
[Train] Epoch= 55  BatchID= 50 Loss: 0.111 | Acc: 96.844% (6322/6528)
[Train] Epoch= 55  BatchID= 60 Loss: 0.112 | Acc: 96.734% (7553/7808)
[Train] Epoch= 55  BatchID= 70 Loss: 0.113 | Acc: 96.688% (8787/9088)
[Test] Epoch= 55  BatchID= 0 Loss: 0.083 | Acc: 97.656% (125/128)
[Test] Epoch= 55  BatchID= 10 Loss: 0.282 | Acc: 92.259% (1299/1408)
[Test] Epoch= 55  BatchID= 20 Loss: 0.270 | Acc: 92.150% (2477/2688)
[Test] Epoch= 55  BatchID= 30 Loss: 0.311 | Acc: 90.879% (3567/3925)
Saving..
Best accuracy:  90.87898089171975

Epoch: 56
[Test] Epoch= 55  BatchID= 0 Loss: 0.110 | Acc: 96.094% (123/128)
[Test] Epoch= 55  BatchID= 10 Loss: 0.318 | Acc: 90.909% (1280/1408)
[Test] Epoch= 55  BatchID= 20 Loss: 0.297 | Acc: 91.369% (2456/2688)
[Test] Epoch= 55  BatchID= 30 Loss: 0.318 | Acc: 90.675% (3559/3925)

Epoch: 56
[Train] Epoch= 56  BatchID= 0 Loss: 0.149 | Acc: 96.875% (124/128)
[Train] Epoch= 56  BatchID= 10 Loss: 0.119 | Acc: 96.165% (1354/1408)
[Train] Epoch= 56  BatchID= 20 Loss: 0.122 | Acc: 96.094% (2583/2688)
[Train] Epoch= 56  BatchID= 30 Loss: 0.117 | Acc: 96.371% (3824/3968)
[Train] Epoch= 56  BatchID= 40 Loss: 0.123 | Acc: 96.189% (5048/5248)
[Train] Epoch= 56  BatchID= 50 Loss: 0.119 | Acc: 96.339% (6289/6528)
[Train] Epoch= 56  BatchID= 60 Loss: 0.120 | Acc: 96.311% (7520/7808)
[Train] Epoch= 56  BatchID= 70 Loss: 0.120 | Acc: 96.292% (8751/9088)
[Train] Epoch= 56  BatchID= 0 Loss: 0.080 | Acc: 97.656% (125/128)
[Train] Epoch= 56  BatchID= 10 Loss: 0.109 | Acc: 96.236% (1355/1408)
[Train] Epoch= 56  BatchID= 20 Loss: 0.117 | Acc: 96.094% (2583/2688)
[Train] Epoch= 56  BatchID= 30 Loss: 0.115 | Acc: 96.371% (3824/3968)
[Train] Epoch= 56  BatchID= 40 Loss: 0.115 | Acc: 96.341% (5056/5248)
[Train] Epoch= 56  BatchID= 50 Loss: 0.117 | Acc: 96.354% (6290/6528)
[Train] Epoch= 56  BatchID= 60 Loss: 0.118 | Acc: 96.350% (7523/7808)
[Train] Epoch= 56  BatchID= 70 Loss: 0.118 | Acc: 96.380% (8759/9088)
[Test] Epoch= 56  BatchID= 0 Loss: 0.089 | Acc: 98.438% (126/128)
[Test] Epoch= 56  BatchID= 10 Loss: 0.295 | Acc: 91.619% (1290/1408)
[Test] Epoch= 56  BatchID= 20 Loss: 0.282 | Acc: 91.443% (2458/2688)
[Test] Epoch= 56  BatchID= 30 Loss: 0.309 | Acc: 90.752% (3562/3925)

Epoch: 57
[Test] Epoch= 56  BatchID= 0 Loss: 0.095 | Acc: 96.875% (124/128)
[Test] Epoch= 56  BatchID= 10 Loss: 0.319 | Acc: 90.767% (1278/1408)
[Test] Epoch= 56  BatchID= 20 Loss: 0.287 | Acc: 91.443% (2458/2688)
[Test] Epoch= 56  BatchID= 30 Loss: 0.316 | Acc: 90.726% (3561/3925)

Epoch: 57
[Train] Epoch= 57  BatchID= 0 Loss: 0.061 | Acc: 99.219% (127/128)
[Train] Epoch= 57  BatchID= 10 Loss: 0.115 | Acc: 96.804% (1363/1408)
[Train] Epoch= 57  BatchID= 20 Loss: 0.105 | Acc: 96.987% (2607/2688)
[Train] Epoch= 57  BatchID= 30 Loss: 0.106 | Acc: 97.026% (3850/3968)
[Train] Epoch= 57  BatchID= 40 Loss: 0.106 | Acc: 96.951% (5088/5248)
[Train] Epoch= 57  BatchID= 50 Loss: 0.106 | Acc: 96.952% (6329/6528)
[Train] Epoch= 57  BatchID= 60 Loss: 0.108 | Acc: 96.811% (7559/7808)
[Train] Epoch= 57  BatchID= 70 Loss: 0.109 | Acc: 96.732% (8791/9088)
[Train] Epoch= 57  BatchID= 0 Loss: 0.097 | Acc: 96.875% (124/128)
[Train] Epoch= 57  BatchID= 10 Loss: 0.123 | Acc: 95.739% (1348/1408)
[Train] Epoch= 57  BatchID= 20 Loss: 0.116 | Acc: 96.057% (2582/2688)
[Train] Epoch= 57  BatchID= 30 Loss: 0.121 | Acc: 95.892% (3805/3968)
[Train] Epoch= 57  BatchID= 40 Loss: 0.121 | Acc: 95.960% (5036/5248)
[Train] Epoch= 57  BatchID= 50 Loss: 0.123 | Acc: 96.002% (6267/6528)
[Train] Epoch= 57  BatchID= 60 Loss: 0.121 | Acc: 96.081% (7502/7808)
[Train] Epoch= 57  BatchID= 70 Loss: 0.119 | Acc: 96.226% (8745/9088)
[Test] Epoch= 57  BatchID= 0 Loss: 0.097 | Acc: 98.438% (126/128)
[Test] Epoch= 57  BatchID= 10 Loss: 0.296 | Acc: 91.335% (1286/1408)
[Test] Epoch= 57  BatchID= 20 Loss: 0.276 | Acc: 91.518% (2460/2688)
[Test] Epoch= 57  BatchID= 30 Loss: 0.315 | Acc: 90.446% (3550/3925)

Epoch: 58
[Test] Epoch= 57  BatchID= 0 Loss: 0.120 | Acc: 96.094% (123/128)
[Test] Epoch= 57  BatchID= 10 Loss: 0.304 | Acc: 91.477% (1288/1408)
[Test] Epoch= 57  BatchID= 20 Loss: 0.294 | Acc: 91.369% (2456/2688)
[Test] Epoch= 57  BatchID= 30 Loss: 0.322 | Acc: 90.650% (3558/3925)

Epoch: 58
[Train] Epoch= 58  BatchID= 0 Loss: 0.116 | Acc: 96.875% (124/128)
[Train] Epoch= 58  BatchID= 10 Loss: 0.116 | Acc: 96.591% (1360/1408)
[Train] Epoch= 58  BatchID= 20 Loss: 0.121 | Acc: 96.317% (2589/2688)
[Train] Epoch= 58  BatchID= 30 Loss: 0.117 | Acc: 96.497% (3829/3968)
[Train] Epoch= 58  BatchID= 40 Loss: 0.115 | Acc: 96.513% (5065/5248)
[Train] Epoch= 58  BatchID= 50 Loss: 0.115 | Acc: 96.538% (6302/6528)
[Train] Epoch= 58  BatchID= 60 Loss: 0.114 | Acc: 96.516% (7536/7808)
[Train] Epoch= 58  BatchID= 70 Loss: 0.112 | Acc: 96.534% (8773/9088)
[Train] Epoch= 58  BatchID= 0 Loss: 0.143 | Acc: 95.312% (122/128)
[Train] Epoch= 58  BatchID= 10 Loss: 0.118 | Acc: 96.449% (1358/1408)
[Train] Epoch= 58  BatchID= 20 Loss: 0.112 | Acc: 96.912% (2605/2688)
[Train] Epoch= 58  BatchID= 30 Loss: 0.112 | Acc: 96.900% (3845/3968)
[Train] Epoch= 58  BatchID= 40 Loss: 0.111 | Acc: 96.780% (5079/5248)
[Train] Epoch= 58  BatchID= 50 Loss: 0.111 | Acc: 96.798% (6319/6528)
[Train] Epoch= 58  BatchID= 60 Loss: 0.109 | Acc: 96.837% (7561/7808)
[Train] Epoch= 58  BatchID= 70 Loss: 0.110 | Acc: 96.776% (8795/9088)
[Test] Epoch= 58  BatchID= 0 Loss: 0.112 | Acc: 96.094% (123/128)
[Test] Epoch= 58  BatchID= 10 Loss: 0.290 | Acc: 91.193% (1284/1408)
[Test] Epoch= 58  BatchID= 20 Loss: 0.280 | Acc: 91.555% (2461/2688)
[Test] Epoch= 58  BatchID= 30 Loss: 0.311 | Acc: 90.650% (3558/3925)

Epoch: 59
[Test] Epoch= 58  BatchID= 0 Loss: 0.100 | Acc: 97.656% (125/128)
[Test] Epoch= 58  BatchID= 10 Loss: 0.300 | Acc: 90.838% (1279/1408)
[Test] Epoch= 58  BatchID= 20 Loss: 0.281 | Acc: 91.332% (2455/2688)
[Test] Epoch= 58  BatchID= 30 Loss: 0.313 | Acc: 90.522% (3553/3925)

Epoch: 59
[Train] Epoch= 59  BatchID= 0 Loss: 0.156 | Acc: 94.531% (121/128)
[Train] Epoch= 59  BatchID= 10 Loss: 0.134 | Acc: 95.739% (1348/1408)
[Train] Epoch= 59  BatchID= 20 Loss: 0.113 | Acc: 96.652% (2598/2688)
[Train] Epoch= 59  BatchID= 30 Loss: 0.111 | Acc: 96.699% (3837/3968)
[Train] Epoch= 59  BatchID= 40 Loss: 0.108 | Acc: 96.684% (5074/5248)
[Train] Epoch= 59  BatchID= 50 Loss: 0.112 | Acc: 96.553% (6303/6528)
[Train] Epoch= 59  BatchID= 60 Loss: 0.108 | Acc: 96.657% (7547/7808)
[Train] Epoch= 59  BatchID= 70 Loss: 0.106 | Acc: 96.798% (8797/9088)
[Train] Epoch= 59  BatchID= 0 Loss: 0.070 | Acc: 99.219% (127/128)
[Train] Epoch= 59  BatchID= 10 Loss: 0.118 | Acc: 96.165% (1354/1408)
[Train] Epoch= 59  BatchID= 20 Loss: 0.109 | Acc: 96.838% (2603/2688)
[Train] Epoch= 59  BatchID= 30 Loss: 0.107 | Acc: 96.925% (3846/3968)
[Train] Epoch= 59  BatchID= 40 Loss: 0.113 | Acc: 96.723% (5076/5248)
[Train] Epoch= 59  BatchID= 50 Loss: 0.110 | Acc: 96.722% (6314/6528)
[Train] Epoch= 59  BatchID= 60 Loss: 0.113 | Acc: 96.568% (7540/7808)
[Train] Epoch= 59  BatchID= 70 Loss: 0.117 | Acc: 96.468% (8767/9088)
[Test] Epoch= 59  BatchID= 0 Loss: 0.085 | Acc: 96.875% (124/128)
[Test] Epoch= 59  BatchID= 10 Loss: 0.308 | Acc: 90.980% (1281/1408)
[Test] Epoch= 59  BatchID= 20 Loss: 0.287 | Acc: 91.481% (2459/2688)
[Test] Epoch= 59  BatchID= 30 Loss: 0.315 | Acc: 90.726% (3561/3925)

Epoch: 60
[Test] Epoch= 59  BatchID= 0 Loss: 0.089 | Acc: 98.438% (126/128)
[Test] Epoch= 59  BatchID= 10 Loss: 0.295 | Acc: 91.335% (1286/1408)
[Test] Epoch= 59  BatchID= 20 Loss: 0.276 | Acc: 91.741% (2466/2688)
[Test] Epoch= 59  BatchID= 30 Loss: 0.306 | Acc: 90.904% (3568/3925)

Epoch: 60
[Train] Epoch= 60  BatchID= 0 Loss: 0.172 | Acc: 95.312% (122/128)
[Train] Epoch= 60  BatchID= 10 Loss: 0.120 | Acc: 96.449% (1358/1408)
[Train] Epoch= 60  BatchID= 20 Loss: 0.112 | Acc: 96.801% (2602/2688)
[Train] Epoch= 60  BatchID= 30 Loss: 0.112 | Acc: 96.749% (3839/3968)
[Train] Epoch= 60  BatchID= 40 Loss: 0.110 | Acc: 96.780% (5079/5248)
[Train] Epoch= 60  BatchID= 50 Loss: 0.109 | Acc: 96.783% (6318/6528)
[Train] Epoch= 60  BatchID= 60 Loss: 0.107 | Acc: 96.773% (7556/7808)
[Train] Epoch= 60  BatchID= 70 Loss: 0.110 | Acc: 96.633% (8782/9088)
[Train] Epoch= 60  BatchID= 0 Loss: 0.137 | Acc: 94.531% (121/128)
[Train] Epoch= 60  BatchID= 10 Loss: 0.121 | Acc: 96.662% (1361/1408)
[Train] Epoch= 60  BatchID= 20 Loss: 0.125 | Acc: 96.503% (2594/2688)
[Train] Epoch= 60  BatchID= 30 Loss: 0.120 | Acc: 96.547% (3831/3968)
[Train] Epoch= 60  BatchID= 40 Loss: 0.117 | Acc: 96.570% (5068/5248)
[Train] Epoch= 60  BatchID= 50 Loss: 0.114 | Acc: 96.553% (6303/6528)
[Train] Epoch= 60  BatchID= 60 Loss: 0.113 | Acc: 96.529% (7537/7808)
[Train] Epoch= 60  BatchID= 70 Loss: 0.114 | Acc: 96.556% (8775/9088)
[Test] Epoch= 60  BatchID= 0 Loss: 0.087 | Acc: 97.656% (125/128)
[Test] Epoch= 60  BatchID= 10 Loss: 0.278 | Acc: 91.832% (1293/1408)
[Test] Epoch= 60  BatchID= 20 Loss: 0.274 | Acc: 91.964% (2472/2688)
[Test] Epoch= 60  BatchID= 30 Loss: 0.315 | Acc: 90.752% (3562/3925)

Epoch: 61
[Test] Epoch= 60  BatchID= 0 Loss: 0.079 | Acc: 97.656% (125/128)
[Test] Epoch= 60  BatchID= 10 Loss: 0.273 | Acc: 92.188% (1298/1408)
[Test] Epoch= 60  BatchID= 20 Loss: 0.263 | Acc: 92.150% (2477/2688)
[Test] Epoch= 60  BatchID= 30 Loss: 0.314 | Acc: 90.777% (3563/3925)

Epoch: 61
[Train] Epoch= 61  BatchID= 0 Loss: 0.121 | Acc: 95.312% (122/128)
[Train] Epoch= 61  BatchID= 10 Loss: 0.100 | Acc: 96.733% (1362/1408)
[Train] Epoch= 61  BatchID= 20 Loss: 0.103 | Acc: 96.577% (2596/2688)
[Train] Epoch= 61  BatchID= 30 Loss: 0.109 | Acc: 96.547% (3831/3968)
[Train] Epoch= 61  BatchID= 40 Loss: 0.114 | Acc: 96.399% (5059/5248)
[Train] Epoch= 61  BatchID= 50 Loss: 0.116 | Acc: 96.308% (6287/6528)
[Train] Epoch= 61  BatchID= 60 Loss: 0.112 | Acc: 96.478% (7533/7808)
[Train] Epoch= 61  BatchID= 70 Loss: 0.118 | Acc: 96.358% (8757/9088)
[Train] Epoch= 61  BatchID= 0 Loss: 0.158 | Acc: 94.531% (121/128)
[Train] Epoch= 61  BatchID= 10 Loss: 0.126 | Acc: 96.307% (1356/1408)
[Train] Epoch= 61  BatchID= 20 Loss: 0.116 | Acc: 96.689% (2599/2688)
[Train] Epoch= 61  BatchID= 30 Loss: 0.118 | Acc: 96.472% (3828/3968)
[Train] Epoch= 61  BatchID= 40 Loss: 0.119 | Acc: 96.418% (5060/5248)
[Train] Epoch= 61  BatchID= 50 Loss: 0.117 | Acc: 96.599% (6306/6528)
[Train] Epoch= 61  BatchID= 60 Loss: 0.116 | Acc: 96.619% (7544/7808)
[Train] Epoch= 61  BatchID= 70 Loss: 0.117 | Acc: 96.556% (8775/9088)
[Test] Epoch= 61  BatchID= 0 Loss: 0.073 | Acc: 98.438% (126/128)
[Test] Epoch= 61  BatchID= 10 Loss: 0.289 | Acc: 91.761% (1292/1408)
[Test] Epoch= 61  BatchID= 20 Loss: 0.273 | Acc: 92.001% (2473/2688)
[Test] Epoch= 61  BatchID= 30 Loss: 0.317 | Acc: 90.981% (3571/3925)

Epoch: 62
[Test] Epoch= 61  BatchID= 0 Loss: 0.115 | Acc: 96.875% (124/128)
[Test] Epoch= 61  BatchID= 10 Loss: 0.303 | Acc: 91.051% (1282/1408)
[Test] Epoch= 61  BatchID= 20 Loss: 0.277 | Acc: 91.518% (2460/2688)
[Test] Epoch= 61  BatchID= 30 Loss: 0.317 | Acc: 90.293% (3544/3925)

Epoch: 62
[Train] Epoch= 62  BatchID= 0 Loss: 0.091 | Acc: 96.875% (124/128)
[Train] Epoch= 62  BatchID= 10 Loss: 0.124 | Acc: 95.810% (1349/1408)
[Train] Epoch= 62  BatchID= 20 Loss: 0.122 | Acc: 96.057% (2582/2688)
[Train] Epoch= 62  BatchID= 30 Loss: 0.119 | Acc: 96.371% (3824/3968)
[Train] Epoch= 62  BatchID= 40 Loss: 0.118 | Acc: 96.437% (5061/5248)
[Train] Epoch= 62  BatchID= 50 Loss: 0.124 | Acc: 96.216% (6281/6528)
[Train] Epoch= 62  BatchID= 60 Loss: 0.120 | Acc: 96.299% (7519/7808)
[Train] Epoch= 62  BatchID= 70 Loss: 0.121 | Acc: 96.215% (8744/9088)
[Train] Epoch= 62  BatchID= 0 Loss: 0.132 | Acc: 96.094% (123/128)
[Train] Epoch= 62  BatchID= 10 Loss: 0.109 | Acc: 96.591% (1360/1408)
[Train] Epoch= 62  BatchID= 20 Loss: 0.115 | Acc: 96.726% (2600/2688)
[Train] Epoch= 62  BatchID= 30 Loss: 0.107 | Acc: 97.026% (3850/3968)
[Train] Epoch= 62  BatchID= 40 Loss: 0.109 | Acc: 96.894% (5085/5248)
[Train] Epoch= 62  BatchID= 50 Loss: 0.106 | Acc: 96.952% (6329/6528)
[Train] Epoch= 62  BatchID= 60 Loss: 0.111 | Acc: 96.721% (7552/7808)
[Train] Epoch= 62  BatchID= 70 Loss: 0.111 | Acc: 96.677% (8786/9088)
[Test] Epoch= 62  BatchID= 0 Loss: 0.118 | Acc: 97.656% (125/128)
[Test] Epoch= 62  BatchID= 10 Loss: 0.318 | Acc: 90.412% (1273/1408)
[Test] Epoch= 62  BatchID= 20 Loss: 0.284 | Acc: 91.220% (2452/2688)
[Test] Epoch= 62  BatchID= 30 Loss: 0.319 | Acc: 90.599% (3556/3925)

Epoch: 63
[Test] Epoch= 62  BatchID= 0 Loss: 0.080 | Acc: 97.656% (125/128)
[Test] Epoch= 62  BatchID= 10 Loss: 0.295 | Acc: 91.761% (1292/1408)
[Test] Epoch= 62  BatchID= 20 Loss: 0.275 | Acc: 91.964% (2472/2688)
[Test] Epoch= 62  BatchID= 30 Loss: 0.314 | Acc: 90.955% (3570/3925)
Saving..
Best accuracy:  90.95541401273886

Epoch: 63
[Train] Epoch= 63  BatchID= 0 Loss: 0.134 | Acc: 95.312% (122/128)
[Train] Epoch= 63  BatchID= 10 Loss: 0.104 | Acc: 96.733% (1362/1408)
[Train] Epoch= 63  BatchID= 20 Loss: 0.107 | Acc: 96.652% (2598/2688)
[Train] Epoch= 63  BatchID= 30 Loss: 0.115 | Acc: 96.346% (3823/3968)
[Train] Epoch= 63  BatchID= 40 Loss: 0.113 | Acc: 96.551% (5067/5248)
[Train] Epoch= 63  BatchID= 50 Loss: 0.111 | Acc: 96.553% (6303/6528)
[Train] Epoch= 63  BatchID= 60 Loss: 0.113 | Acc: 96.516% (7536/7808)
[Train] Epoch= 63  BatchID= 70 Loss: 0.112 | Acc: 96.523% (8772/9088)
[Train] Epoch= 63  BatchID= 0 Loss: 0.127 | Acc: 96.094% (123/128)
[Train] Epoch= 63  BatchID= 10 Loss: 0.106 | Acc: 96.733% (1362/1408)
[Train] Epoch= 63  BatchID= 20 Loss: 0.114 | Acc: 96.615% (2597/2688)
[Train] Epoch= 63  BatchID= 30 Loss: 0.110 | Acc: 96.648% (3835/3968)
[Train] Epoch= 63  BatchID= 40 Loss: 0.109 | Acc: 96.646% (5072/5248)
[Train] Epoch= 63  BatchID= 50 Loss: 0.107 | Acc: 96.691% (6312/6528)
[Train] Epoch= 63  BatchID= 60 Loss: 0.107 | Acc: 96.632% (7545/7808)
[Train] Epoch= 63  BatchID= 70 Loss: 0.107 | Acc: 96.622% (8781/9088)
[Test] Epoch= 63  BatchID= 0 Loss: 0.112 | Acc: 96.875% (124/128)
[Test] Epoch= 63  BatchID= 10 Loss: 0.307 | Acc: 90.696% (1277/1408)
[Test] Epoch= 63  BatchID= 20 Loss: 0.287 | Acc: 91.257% (2453/2688)
[Test] Epoch= 63  BatchID= 30 Loss: 0.312 | Acc: 90.675% (3559/3925)

Epoch: 64
[Test] Epoch= 63  BatchID= 0 Loss: 0.087 | Acc: 97.656% (125/128)
[Test] Epoch= 63  BatchID= 10 Loss: 0.292 | Acc: 91.903% (1294/1408)
[Test] Epoch= 63  BatchID= 20 Loss: 0.277 | Acc: 91.890% (2470/2688)
[Test] Epoch= 63  BatchID= 30 Loss: 0.307 | Acc: 91.032% (3573/3925)
Saving..
Best accuracy:  91.03184713375796

Epoch: 64
[Train] Epoch= 64  BatchID= 0 Loss: 0.124 | Acc: 96.094% (123/128)
[Train] Epoch= 64  BatchID= 10 Loss: 0.117 | Acc: 96.449% (1358/1408)
[Train] Epoch= 64  BatchID= 20 Loss: 0.135 | Acc: 96.057% (2582/2688)
[Train] Epoch= 64  BatchID= 30 Loss: 0.129 | Acc: 96.346% (3823/3968)
[Train] Epoch= 64  BatchID= 40 Loss: 0.125 | Acc: 96.475% (5063/5248)
[Train] Epoch= 64  BatchID= 50 Loss: 0.123 | Acc: 96.492% (6299/6528)
[Train] Epoch= 64  BatchID= 60 Loss: 0.118 | Acc: 96.696% (7550/7808)
[Train] Epoch= 64  BatchID= 70 Loss: 0.115 | Acc: 96.798% (8797/9088)
[Train] Epoch= 64  BatchID= 0 Loss: 0.155 | Acc: 95.312% (122/128)
[Train] Epoch= 64  BatchID= 10 Loss: 0.144 | Acc: 95.312% (1342/1408)
[Train] Epoch= 64  BatchID= 20 Loss: 0.119 | Acc: 96.243% (2587/2688)
[Train] Epoch= 64  BatchID= 30 Loss: 0.114 | Acc: 96.396% (3825/3968)
[Train] Epoch= 64  BatchID= 40 Loss: 0.111 | Acc: 96.551% (5067/5248)
[Train] Epoch= 64  BatchID= 50 Loss: 0.109 | Acc: 96.722% (6314/6528)
[Train] Epoch= 64  BatchID= 60 Loss: 0.111 | Acc: 96.644% (7546/7808)
[Train] Epoch= 64  BatchID= 70 Loss: 0.110 | Acc: 96.655% (8784/9088)
[Test] Epoch= 64  BatchID= 0 Loss: 0.087 | Acc: 97.656% (125/128)
[Test] Epoch= 64  BatchID= 10 Loss: 0.297 | Acc: 91.264% (1285/1408)
[Test] Epoch= 64  BatchID= 20 Loss: 0.280 | Acc: 91.332% (2455/2688)
[Test] Epoch= 64  BatchID= 30 Loss: 0.309 | Acc: 90.752% (3562/3925)

Epoch: 65
[Test] Epoch= 64  BatchID= 0 Loss: 0.082 | Acc: 98.438% (126/128)
[Test] Epoch= 64  BatchID= 10 Loss: 0.300 | Acc: 91.193% (1284/1408)
[Test] Epoch= 64  BatchID= 20 Loss: 0.283 | Acc: 91.704% (2465/2688)
[Test] Epoch= 64  BatchID= 30 Loss: 0.312 | Acc: 90.701% (3560/3925)

Epoch: 65
[Train] Epoch= 65  BatchID= 0 Loss: 0.172 | Acc: 93.750% (120/128)
[Train] Epoch= 65  BatchID= 10 Loss: 0.137 | Acc: 95.881% (1350/1408)
[Train] Epoch= 65  BatchID= 20 Loss: 0.127 | Acc: 96.391% (2591/2688)
[Train] Epoch= 65  BatchID= 30 Loss: 0.125 | Acc: 96.270% (3820/3968)
[Train] Epoch= 65  BatchID= 40 Loss: 0.117 | Acc: 96.513% (5065/5248)
[Train] Epoch= 65  BatchID= 50 Loss: 0.119 | Acc: 96.415% (6294/6528)
[Train] Epoch= 65  BatchID= 60 Loss: 0.117 | Acc: 96.440% (7530/7808)
[Train] Epoch= 65  BatchID= 70 Loss: 0.116 | Acc: 96.468% (8767/9088)
[Train] Epoch= 65  BatchID= 0 Loss: 0.128 | Acc: 96.094% (123/128)
[Train] Epoch= 65  BatchID= 10 Loss: 0.122 | Acc: 96.023% (1352/1408)
[Train] Epoch= 65  BatchID= 20 Loss: 0.109 | Acc: 96.577% (2596/2688)
[Train] Epoch= 65  BatchID= 30 Loss: 0.118 | Acc: 96.321% (3822/3968)
[Train] Epoch= 65  BatchID= 40 Loss: 0.122 | Acc: 96.284% (5053/5248)
[Train] Epoch= 65  BatchID= 50 Loss: 0.120 | Acc: 96.339% (6289/6528)
[Train] Epoch= 65  BatchID= 60 Loss: 0.117 | Acc: 96.350% (7523/7808)
[Train] Epoch= 65  BatchID= 70 Loss: 0.115 | Acc: 96.424% (8763/9088)
[Test] Epoch= 65  BatchID= 0 Loss: 0.105 | Acc: 97.656% (125/128)
[Test] Epoch= 65  BatchID= 10 Loss: 0.302 | Acc: 90.909% (1280/1408)
[Test] Epoch= 65  BatchID= 20 Loss: 0.273 | Acc: 91.481% (2459/2688)
[Test] Epoch= 65  BatchID= 30 Loss: 0.311 | Acc: 90.624% (3557/3925)

Epoch: 66
[Test] Epoch= 65  BatchID= 0 Loss: 0.099 | Acc: 97.656% (125/128)
[Test] Epoch= 65  BatchID= 10 Loss: 0.283 | Acc: 91.903% (1294/1408)
[Test] Epoch= 65  BatchID= 20 Loss: 0.264 | Acc: 92.113% (2476/2688)
[Test] Epoch= 65  BatchID= 30 Loss: 0.311 | Acc: 90.854% (3566/3925)

Epoch: 66
[Train] Epoch= 66  BatchID= 0 Loss: 0.045 | Acc: 98.438% (126/128)
[Train] Epoch= 66  BatchID= 10 Loss: 0.117 | Acc: 97.088% (1367/1408)
[Train] Epoch= 66  BatchID= 20 Loss: 0.112 | Acc: 96.763% (2601/2688)
[Train] Epoch= 66  BatchID= 30 Loss: 0.111 | Acc: 96.699% (3837/3968)
[Train] Epoch= 66  BatchID= 40 Loss: 0.112 | Acc: 96.723% (5076/5248)
[Train] Epoch= 66  BatchID= 50 Loss: 0.113 | Acc: 96.722% (6314/6528)
[Train] Epoch= 66  BatchID= 60 Loss: 0.111 | Acc: 96.747% (7554/7808)
[Train] Epoch= 66  BatchID= 70 Loss: 0.113 | Acc: 96.699% (8788/9088)
[Train] Epoch= 66  BatchID= 0 Loss: 0.115 | Acc: 96.094% (123/128)
[Train] Epoch= 66  BatchID= 10 Loss: 0.106 | Acc: 96.875% (1364/1408)
[Train] Epoch= 66  BatchID= 20 Loss: 0.101 | Acc: 96.949% (2606/2688)
[Train] Epoch= 66  BatchID= 30 Loss: 0.111 | Acc: 96.522% (3830/3968)
[Train] Epoch= 66  BatchID= 40 Loss: 0.112 | Acc: 96.456% (5062/5248)
[Train] Epoch= 66  BatchID= 50 Loss: 0.112 | Acc: 96.446% (6296/6528)
[Train] Epoch= 66  BatchID= 60 Loss: 0.114 | Acc: 96.388% (7526/7808)
[Train] Epoch= 66  BatchID= 70 Loss: 0.114 | Acc: 96.347% (8756/9088)
[Test] Epoch= 66  BatchID= 0 Loss: 0.104 | Acc: 97.656% (125/128)
[Test] Epoch= 66  BatchID= 10 Loss: 0.303 | Acc: 91.122% (1283/1408)
[Test] Epoch= 66  BatchID= 20 Loss: 0.274 | Acc: 91.704% (2465/2688)
[Test] Epoch= 66  BatchID= 30 Loss: 0.312 | Acc: 90.752% (3562/3925)

Epoch: 67
[Test] Epoch= 66  BatchID= 0 Loss: 0.089 | Acc: 97.656% (125/128)
[Test] Epoch= 66  BatchID= 10 Loss: 0.311 | Acc: 90.767% (1278/1408)
[Test] Epoch= 66  BatchID= 20 Loss: 0.278 | Acc: 91.592% (2462/2688)
[Test] Epoch= 66  BatchID= 30 Loss: 0.309 | Acc: 90.548% (3554/3925)

Epoch: 67
[Train] Epoch= 67  BatchID= 0 Loss: 0.054 | Acc: 98.438% (126/128)
[Train] Epoch= 67  BatchID= 10 Loss: 0.113 | Acc: 96.946% (1365/1408)
[Train] Epoch= 67  BatchID= 20 Loss: 0.101 | Acc: 97.210% (2613/2688)
[Train] Epoch= 67  BatchID= 30 Loss: 0.108 | Acc: 96.951% (3847/3968)
[Train] Epoch= 67  BatchID= 40 Loss: 0.110 | Acc: 96.799% (5080/5248)
[Train] Epoch= 67  BatchID= 50 Loss: 0.114 | Acc: 96.706% (6313/6528)
[Train] Epoch= 67  BatchID= 60 Loss: 0.114 | Acc: 96.721% (7552/7808)
[Train] Epoch= 67  BatchID= 70 Loss: 0.114 | Acc: 96.732% (8791/9088)
[Train] Epoch= 67  BatchID= 0 Loss: 0.070 | Acc: 97.656% (125/128)
[Train] Epoch= 67  BatchID= 10 Loss: 0.115 | Acc: 96.875% (1364/1408)
[Train] Epoch= 67  BatchID= 20 Loss: 0.100 | Acc: 97.396% (2618/2688)
[Train] Epoch= 67  BatchID= 30 Loss: 0.100 | Acc: 97.303% (3861/3968)
[Train] Epoch= 67  BatchID= 40 Loss: 0.103 | Acc: 97.085% (5095/5248)
[Train] Epoch= 67  BatchID= 50 Loss: 0.101 | Acc: 97.151% (6342/6528)
[Train] Epoch= 67  BatchID= 60 Loss: 0.109 | Acc: 96.849% (7562/7808)
[Train] Epoch= 67  BatchID= 70 Loss: 0.106 | Acc: 96.875% (8804/9088)
[Test] Epoch= 67  BatchID= 0 Loss: 0.084 | Acc: 97.656% (125/128)
[Test] Epoch= 67  BatchID= 10 Loss: 0.303 | Acc: 91.406% (1287/1408)
[Test] Epoch= 67  BatchID= 20 Loss: 0.275 | Acc: 91.853% (2469/2688)
[Test] Epoch= 67  BatchID= 30 Loss: 0.304 | Acc: 90.930% (3569/3925)

Epoch: 68
[Test] Epoch= 67  BatchID= 0 Loss: 0.097 | Acc: 98.438% (126/128)
[Test] Epoch= 67  BatchID= 10 Loss: 0.288 | Acc: 90.838% (1279/1408)
[Test] Epoch= 67  BatchID= 20 Loss: 0.268 | Acc: 91.481% (2459/2688)
[Test] Epoch= 67  BatchID= 30 Loss: 0.311 | Acc: 90.497% (3552/3925)

Epoch: 68
[Train] Epoch= 68  BatchID= 0 Loss: 0.053 | Acc: 98.438% (126/128)
[Train] Epoch= 68  BatchID= 10 Loss: 0.091 | Acc: 97.514% (1373/1408)
[Train] Epoch= 68  BatchID= 20 Loss: 0.097 | Acc: 97.321% (2616/2688)
[Train] Epoch= 68  BatchID= 30 Loss: 0.100 | Acc: 97.001% (3849/3968)
[Train] Epoch= 68  BatchID= 40 Loss: 0.101 | Acc: 97.008% (5091/5248)
[Train] Epoch= 68  BatchID= 50 Loss: 0.103 | Acc: 97.059% (6336/6528)
[Train] Epoch= 68  BatchID= 60 Loss: 0.100 | Acc: 97.118% (7583/7808)
[Train] Epoch= 68  BatchID= 70 Loss: 0.104 | Acc: 96.985% (8814/9088)
[Train] Epoch= 68  BatchID= 0 Loss: 0.162 | Acc: 93.750% (120/128)
[Train] Epoch= 68  BatchID= 10 Loss: 0.096 | Acc: 96.449% (1358/1408)
[Train] Epoch= 68  BatchID= 20 Loss: 0.096 | Acc: 96.763% (2601/2688)
[Train] Epoch= 68  BatchID= 30 Loss: 0.098 | Acc: 96.799% (3841/3968)
[Train] Epoch= 68  BatchID= 40 Loss: 0.101 | Acc: 96.589% (5069/5248)
[Train] Epoch= 68  BatchID= 50 Loss: 0.101 | Acc: 96.691% (6312/6528)
[Train] Epoch= 68  BatchID= 60 Loss: 0.104 | Acc: 96.568% (7540/7808)
[Train] Epoch= 68  BatchID= 70 Loss: 0.103 | Acc: 96.600% (8779/9088)
[Test] Epoch= 68  BatchID= 0 Loss: 0.100 | Acc: 96.875% (124/128)
[Test] Epoch= 68  BatchID= 10 Loss: 0.307 | Acc: 91.193% (1284/1408)
[Test] Epoch= 68  BatchID= 20 Loss: 0.281 | Acc: 91.741% (2466/2688)
[Test] Epoch= 68  BatchID= 30 Loss: 0.307 | Acc: 90.803% (3564/3925)

Epoch: 69
[Test] Epoch= 68  BatchID= 0 Loss: 0.105 | Acc: 98.438% (126/128)
[Test] Epoch= 68  BatchID= 10 Loss: 0.305 | Acc: 91.122% (1283/1408)
[Test] Epoch= 68  BatchID= 20 Loss: 0.281 | Acc: 91.704% (2465/2688)
[Test] Epoch= 68  BatchID= 30 Loss: 0.308 | Acc: 91.057% (3574/3925)

Epoch: 69
[Train] Epoch= 69  BatchID= 0 Loss: 0.110 | Acc: 97.656% (125/128)
[Train] Epoch= 69  BatchID= 10 Loss: 0.108 | Acc: 97.230% (1369/1408)
[Train] Epoch= 69  BatchID= 20 Loss: 0.115 | Acc: 96.763% (2601/2688)
[Train] Epoch= 69  BatchID= 30 Loss: 0.114 | Acc: 96.749% (3839/3968)
[Train] Epoch= 69  BatchID= 40 Loss: 0.111 | Acc: 96.818% (5081/5248)
[Train] Epoch= 69  BatchID= 50 Loss: 0.110 | Acc: 96.783% (6318/6528)
[Train] Epoch= 69  BatchID= 60 Loss: 0.107 | Acc: 96.926% (7568/7808)
[Train] Epoch= 69  BatchID= 70 Loss: 0.109 | Acc: 96.875% (8804/9088)
[Train] Epoch= 69  BatchID= 0 Loss: 0.051 | Acc: 98.438% (126/128)
[Train] Epoch= 69  BatchID= 10 Loss: 0.074 | Acc: 97.798% (1377/1408)
[Train] Epoch= 69  BatchID= 20 Loss: 0.097 | Acc: 97.247% (2614/2688)
[Train] Epoch= 69  BatchID= 30 Loss: 0.093 | Acc: 97.329% (3862/3968)
[Train] Epoch= 69  BatchID= 40 Loss: 0.095 | Acc: 97.294% (5106/5248)
[Train] Epoch= 69  BatchID= 50 Loss: 0.097 | Acc: 97.197% (6345/6528)
[Train] Epoch= 69  BatchID= 60 Loss: 0.097 | Acc: 97.131% (7584/7808)
[Train] Epoch= 69  BatchID= 70 Loss: 0.098 | Acc: 97.150% (8829/9088)
[Test] Epoch= 69  BatchID= 0 Loss: 0.099 | Acc: 97.656% (125/128)
[Test] Epoch= 69  BatchID= 10 Loss: 0.301 | Acc: 91.406% (1287/1408)
[Test] Epoch= 69  BatchID= 20 Loss: 0.277 | Acc: 91.815% (2468/2688)
[Test] Epoch= 69  BatchID= 30 Loss: 0.311 | Acc: 90.879% (3567/3925)

Epoch: 70
[Test] Epoch= 69  BatchID= 0 Loss: 0.094 | Acc: 97.656% (125/128)
[Test] Epoch= 69  BatchID= 10 Loss: 0.281 | Acc: 91.690% (1291/1408)
[Test] Epoch= 69  BatchID= 20 Loss: 0.268 | Acc: 91.815% (2468/2688)
[Test] Epoch= 69  BatchID= 30 Loss: 0.308 | Acc: 90.675% (3559/3925)

Epoch: 70
[Train] Epoch= 70  BatchID= 0 Loss: 0.124 | Acc: 96.094% (123/128)
[Train] Epoch= 70  BatchID= 10 Loss: 0.093 | Acc: 96.662% (1361/1408)
[Train] Epoch= 70  BatchID= 20 Loss: 0.098 | Acc: 96.689% (2599/2688)
[Train] Epoch= 70  BatchID= 30 Loss: 0.103 | Acc: 96.749% (3839/3968)
[Train] Epoch= 70  BatchID= 40 Loss: 0.105 | Acc: 96.684% (5074/5248)
[Train] Epoch= 70  BatchID= 50 Loss: 0.106 | Acc: 96.584% (6305/6528)
[Train] Epoch= 70  BatchID= 60 Loss: 0.104 | Acc: 96.721% (7552/7808)
[Train] Epoch= 70  BatchID= 70 Loss: 0.106 | Acc: 96.710% (8789/9088)
[Train] Epoch= 70  BatchID= 0 Loss: 0.179 | Acc: 95.312% (122/128)
[Train] Epoch= 70  BatchID= 10 Loss: 0.130 | Acc: 96.591% (1360/1408)
[Train] Epoch= 70  BatchID= 20 Loss: 0.113 | Acc: 96.987% (2607/2688)
[Train] Epoch= 70  BatchID= 30 Loss: 0.111 | Acc: 97.026% (3850/3968)
[Train] Epoch= 70  BatchID= 40 Loss: 0.111 | Acc: 97.027% (5092/5248)
[Train] Epoch= 70  BatchID= 50 Loss: 0.108 | Acc: 97.105% (6339/6528)
[Train] Epoch= 70  BatchID= 60 Loss: 0.109 | Acc: 97.016% (7575/7808)
[Train] Epoch= 70  BatchID= 70 Loss: 0.111 | Acc: 96.853% (8802/9088)
[Test] Epoch= 70  BatchID= 0 Loss: 0.091 | Acc: 98.438% (126/128)
[Test] Epoch= 70  BatchID= 10 Loss: 0.284 | Acc: 92.045% (1296/1408)
[Test] Epoch= 70  BatchID= 20 Loss: 0.278 | Acc: 91.815% (2468/2688)
[Test] Epoch= 70  BatchID= 30 Loss: 0.307 | Acc: 90.904% (3568/3925)

Epoch: 71
[Test] Epoch= 70  BatchID= 0 Loss: 0.107 | Acc: 96.875% (124/128)
[Test] Epoch= 70  BatchID= 10 Loss: 0.293 | Acc: 91.690% (1291/1408)
[Test] Epoch= 70  BatchID= 20 Loss: 0.277 | Acc: 91.704% (2465/2688)
[Test] Epoch= 70  BatchID= 30 Loss: 0.311 | Acc: 91.032% (3573/3925)

Epoch: 71
[Train] Epoch= 71  BatchID= 0 Loss: 0.097 | Acc: 96.875% (124/128)
[Train] Epoch= 71  BatchID= 10 Loss: 0.100 | Acc: 97.088% (1367/1408)
[Train] Epoch= 71  BatchID= 20 Loss: 0.097 | Acc: 97.135% (2611/2688)
[Train] Epoch= 71  BatchID= 30 Loss: 0.097 | Acc: 97.177% (3856/3968)
[Train] Epoch= 71  BatchID= 40 Loss: 0.103 | Acc: 96.932% (5087/5248)
[Train] Epoch= 71  BatchID= 50 Loss: 0.104 | Acc: 96.814% (6320/6528)
[Train] Epoch= 71  BatchID= 60 Loss: 0.105 | Acc: 96.773% (7556/7808)
[Train] Epoch= 71  BatchID= 70 Loss: 0.106 | Acc: 96.776% (8795/9088)
[Train] Epoch= 71  BatchID= 0 Loss: 0.094 | Acc: 96.875% (124/128)
[Train] Epoch= 71  BatchID= 10 Loss: 0.098 | Acc: 96.520% (1359/1408)
[Train] Epoch= 71  BatchID= 20 Loss: 0.097 | Acc: 96.912% (2605/2688)
[Train] Epoch= 71  BatchID= 30 Loss: 0.104 | Acc: 96.825% (3842/3968)
[Train] Epoch= 71  BatchID= 40 Loss: 0.100 | Acc: 96.951% (5088/5248)
[Train] Epoch= 71  BatchID= 50 Loss: 0.103 | Acc: 96.844% (6322/6528)
[Train] Epoch= 71  BatchID= 60 Loss: 0.106 | Acc: 96.837% (7561/7808)
[Train] Epoch= 71  BatchID= 70 Loss: 0.105 | Acc: 96.853% (8802/9088)
[Test] Epoch= 71  BatchID= 0 Loss: 0.079 | Acc: 97.656% (125/128)
[Test] Epoch= 71  BatchID= 10 Loss: 0.294 | Acc: 91.974% (1295/1408)
[Test] Epoch= 71  BatchID= 20 Loss: 0.276 | Acc: 91.815% (2468/2688)
[Test] Epoch= 71  BatchID= 30 Loss: 0.304 | Acc: 90.955% (3570/3925)

Epoch: 72
[Test] Epoch= 71  BatchID= 0 Loss: 0.106 | Acc: 98.438% (126/128)
[Test] Epoch= 71  BatchID= 10 Loss: 0.276 | Acc: 91.761% (1292/1408)
[Test] Epoch= 71  BatchID= 20 Loss: 0.268 | Acc: 91.741% (2466/2688)
[Test] Epoch= 71  BatchID= 30 Loss: 0.304 | Acc: 90.803% (3564/3925)

Epoch: 72
[Train] Epoch= 72  BatchID= 0 Loss: 0.112 | Acc: 96.875% (124/128)
[Train] Epoch= 72  BatchID= 10 Loss: 0.098 | Acc: 96.804% (1363/1408)
[Train] Epoch= 72  BatchID= 20 Loss: 0.098 | Acc: 96.949% (2606/2688)
[Train] Epoch= 72  BatchID= 30 Loss: 0.098 | Acc: 96.951% (3847/3968)
[Train] Epoch= 72  BatchID= 40 Loss: 0.100 | Acc: 96.894% (5085/5248)
[Train] Epoch= 72  BatchID= 50 Loss: 0.099 | Acc: 96.860% (6323/6528)
[Train] Epoch= 72  BatchID= 60 Loss: 0.101 | Acc: 96.798% (7558/7808)
[Train] Epoch= 72  BatchID= 70 Loss: 0.102 | Acc: 96.721% (8790/9088)
[Train] Epoch= 72  BatchID= 0 Loss: 0.117 | Acc: 96.094% (123/128)
[Train] Epoch= 72  BatchID= 10 Loss: 0.129 | Acc: 95.952% (1351/1408)
[Train] Epoch= 72  BatchID= 20 Loss: 0.122 | Acc: 96.317% (2589/2688)
[Train] Epoch= 72  BatchID= 30 Loss: 0.125 | Acc: 96.094% (3813/3968)
[Train] Epoch= 72  BatchID= 40 Loss: 0.118 | Acc: 96.380% (5058/5248)
[Train] Epoch= 72  BatchID= 50 Loss: 0.113 | Acc: 96.523% (6301/6528)
[Train] Epoch= 72  BatchID= 60 Loss: 0.111 | Acc: 96.619% (7544/7808)
[Train] Epoch= 72  BatchID= 70 Loss: 0.112 | Acc: 96.611% (8780/9088)
[Test] Epoch= 72  BatchID= 0 Loss: 0.099 | Acc: 97.656% (125/128)
[Test] Epoch= 72  BatchID= 10 Loss: 0.302 | Acc: 91.477% (1288/1408)
[Test] Epoch= 72  BatchID= 20 Loss: 0.281 | Acc: 91.778% (2467/2688)
[Test] Epoch= 72  BatchID= 30 Loss: 0.301 | Acc: 91.057% (3574/3925)
Saving..
Best accuracy:  91.05732484076434

Epoch: 73
[Test] Epoch= 72  BatchID= 0 Loss: 0.096 | Acc: 97.656% (125/128)
[Test] Epoch= 72  BatchID= 10 Loss: 0.287 | Acc: 91.335% (1286/1408)
[Test] Epoch= 72  BatchID= 20 Loss: 0.276 | Acc: 91.183% (2451/2688)
[Test] Epoch= 72  BatchID= 30 Loss: 0.308 | Acc: 90.573% (3555/3925)

Epoch: 73
[Train] Epoch= 73  BatchID= 0 Loss: 0.190 | Acc: 92.188% (118/128)
[Train] Epoch= 73  BatchID= 10 Loss: 0.127 | Acc: 95.668% (1347/1408)
[Train] Epoch= 73  BatchID= 20 Loss: 0.114 | Acc: 96.205% (2586/2688)
[Train] Epoch= 73  BatchID= 30 Loss: 0.121 | Acc: 96.169% (3816/3968)
[Train] Epoch= 73  BatchID= 40 Loss: 0.119 | Acc: 96.208% (5049/5248)
[Train] Epoch= 73  BatchID= 50 Loss: 0.121 | Acc: 96.216% (6281/6528)
[Train] Epoch= 73  BatchID= 60 Loss: 0.117 | Acc: 96.363% (7524/7808)
[Train] Epoch= 73  BatchID= 70 Loss: 0.116 | Acc: 96.358% (8757/9088)
[Train] Epoch= 73  BatchID= 0 Loss: 0.093 | Acc: 96.875% (124/128)
[Train] Epoch= 73  BatchID= 10 Loss: 0.096 | Acc: 97.301% (1370/1408)
[Train] Epoch= 73  BatchID= 20 Loss: 0.109 | Acc: 96.726% (2600/2688)
[Train] Epoch= 73  BatchID= 30 Loss: 0.119 | Acc: 96.346% (3823/3968)
[Train] Epoch= 73  BatchID= 40 Loss: 0.113 | Acc: 96.532% (5066/5248)
[Train] Epoch= 73  BatchID= 50 Loss: 0.111 | Acc: 96.507% (6300/6528)
[Train] Epoch= 73  BatchID= 60 Loss: 0.112 | Acc: 96.568% (7540/7808)
[Train] Epoch= 73  BatchID= 70 Loss: 0.111 | Acc: 96.589% (8778/9088)
[Test] Epoch= 73  BatchID= 0 Loss: 0.093 | Acc: 97.656% (125/128)
[Test] Epoch= 73  BatchID= 10 Loss: 0.280 | Acc: 92.045% (1296/1408)
[Test] Epoch= 73  BatchID= 20 Loss: 0.269 | Acc: 91.890% (2470/2688)
[Test] Epoch= 73  BatchID= 30 Loss: 0.316 | Acc: 90.573% (3555/3925)

Epoch: 74
[Test] Epoch= 73  BatchID= 0 Loss: 0.100 | Acc: 97.656% (125/128)
[Test] Epoch= 73  BatchID= 10 Loss: 0.301 | Acc: 91.193% (1284/1408)
[Test] Epoch= 73  BatchID= 20 Loss: 0.282 | Acc: 91.667% (2464/2688)
[Test] Epoch= 73  BatchID= 30 Loss: 0.301 | Acc: 91.032% (3573/3925)

Epoch: 74
[Train] Epoch= 74  BatchID= 0 Loss: 0.086 | Acc: 98.438% (126/128)
[Train] Epoch= 74  BatchID= 10 Loss: 0.091 | Acc: 97.017% (1366/1408)
[Train] Epoch= 74  BatchID= 20 Loss: 0.095 | Acc: 96.987% (2607/2688)
[Train] Epoch= 74  BatchID= 30 Loss: 0.101 | Acc: 96.976% (3848/3968)
[Train] Epoch= 74  BatchID= 40 Loss: 0.107 | Acc: 96.665% (5073/5248)
[Train] Epoch= 74  BatchID= 50 Loss: 0.109 | Acc: 96.615% (6307/6528)
[Train] Epoch= 74  BatchID= 60 Loss: 0.108 | Acc: 96.747% (7554/7808)
[Train] Epoch= 74  BatchID= 70 Loss: 0.107 | Acc: 96.754% (8793/9088)
[Train] Epoch= 74  BatchID= 0 Loss: 0.155 | Acc: 95.312% (122/128)
[Train] Epoch= 74  BatchID= 10 Loss: 0.104 | Acc: 96.804% (1363/1408)
[Train] Epoch= 74  BatchID= 20 Loss: 0.111 | Acc: 96.429% (2592/2688)
[Train] Epoch= 74  BatchID= 30 Loss: 0.109 | Acc: 96.573% (3832/3968)
[Train] Epoch= 74  BatchID= 40 Loss: 0.110 | Acc: 96.589% (5069/5248)
[Train] Epoch= 74  BatchID= 50 Loss: 0.108 | Acc: 96.645% (6309/6528)
[Train] Epoch= 74  BatchID= 60 Loss: 0.104 | Acc: 96.760% (7555/7808)
[Train] Epoch= 74  BatchID= 70 Loss: 0.104 | Acc: 96.732% (8791/9088)
[Test] Epoch= 74  BatchID= 0 Loss: 0.076 | Acc: 97.656% (125/128)
[Test] Epoch= 74  BatchID= 10 Loss: 0.307 | Acc: 90.909% (1280/1408)
[Test] Epoch= 74  BatchID= 20 Loss: 0.281 | Acc: 91.406% (2457/2688)
[Test] Epoch= 74  BatchID= 30 Loss: 0.301 | Acc: 90.752% (3562/3925)

Epoch: 75
[Test] Epoch= 74  BatchID= 0 Loss: 0.105 | Acc: 97.656% (125/128)
[Test] Epoch= 74  BatchID= 10 Loss: 0.295 | Acc: 91.264% (1285/1408)
[Test] Epoch= 74  BatchID= 20 Loss: 0.279 | Acc: 91.518% (2460/2688)
[Test] Epoch= 74  BatchID= 30 Loss: 0.303 | Acc: 91.057% (3574/3925)

Epoch: 75
[Train] Epoch= 75  BatchID= 0 Loss: 0.090 | Acc: 97.656% (125/128)
[Train] Epoch= 75  BatchID= 10 Loss: 0.110 | Acc: 96.307% (1356/1408)
[Train] Epoch= 75  BatchID= 20 Loss: 0.098 | Acc: 96.949% (2606/2688)
[Train] Epoch= 75  BatchID= 30 Loss: 0.098 | Acc: 97.026% (3850/3968)
[Train] Epoch= 75  BatchID= 40 Loss: 0.099 | Acc: 96.913% (5086/5248)
[Train] Epoch= 75  BatchID= 50 Loss: 0.104 | Acc: 96.706% (6313/6528)
[Train] Epoch= 75  BatchID= 60 Loss: 0.099 | Acc: 96.888% (7565/7808)
[Train] Epoch= 75  BatchID= 70 Loss: 0.100 | Acc: 96.853% (8802/9088)
[Train] Epoch= 75  BatchID= 0 Loss: 0.107 | Acc: 95.312% (122/128)
[Train] Epoch= 75  BatchID= 10 Loss: 0.109 | Acc: 96.662% (1361/1408)
[Train] Epoch= 75  BatchID= 20 Loss: 0.108 | Acc: 96.838% (2603/2688)
[Train] Epoch= 75  BatchID= 30 Loss: 0.103 | Acc: 97.026% (3850/3968)
[Train] Epoch= 75  BatchID= 40 Loss: 0.103 | Acc: 97.085% (5095/5248)
[Train] Epoch= 75  BatchID= 50 Loss: 0.103 | Acc: 96.952% (6329/6528)
[Train] Epoch= 75  BatchID= 60 Loss: 0.098 | Acc: 97.067% (7579/7808)
[Train] Epoch= 75  BatchID= 70 Loss: 0.095 | Acc: 97.139% (8828/9088)
[Test] Epoch= 75  BatchID= 0 Loss: 0.126 | Acc: 96.875% (124/128)
[Test] Epoch= 75  BatchID= 10 Loss: 0.296 | Acc: 91.193% (1284/1408)
[Test] Epoch= 75  BatchID= 20 Loss: 0.274 | Acc: 91.518% (2460/2688)
[Test] Epoch= 75  BatchID= 30 Loss: 0.314 | Acc: 90.675% (3559/3925)

Epoch: 76
[Test] Epoch= 75  BatchID= 0 Loss: 0.094 | Acc: 97.656% (125/128)
[Test] Epoch= 75  BatchID= 10 Loss: 0.288 | Acc: 91.193% (1284/1408)
[Test] Epoch= 75  BatchID= 20 Loss: 0.272 | Acc: 91.518% (2460/2688)
[Test] Epoch= 75  BatchID= 30 Loss: 0.310 | Acc: 90.522% (3553/3925)

Epoch: 76
[Train] Epoch= 76  BatchID= 0 Loss: 0.121 | Acc: 94.531% (121/128)
[Train] Epoch= 76  BatchID= 10 Loss: 0.096 | Acc: 96.662% (1361/1408)
[Train] Epoch= 76  BatchID= 20 Loss: 0.091 | Acc: 97.135% (2611/2688)
[Train] Epoch= 76  BatchID= 30 Loss: 0.097 | Acc: 96.951% (3847/3968)
[Train] Epoch= 76  BatchID= 40 Loss: 0.099 | Acc: 96.970% (5089/5248)
[Train] Epoch= 76  BatchID= 50 Loss: 0.097 | Acc: 97.089% (6338/6528)
[Train] Epoch= 76  BatchID= 60 Loss: 0.098 | Acc: 97.093% (7581/7808)
[Train] Epoch= 76  BatchID= 70 Loss: 0.099 | Acc: 97.040% (8819/9088)
[Train] Epoch= 76  BatchID= 0 Loss: 0.112 | Acc: 97.656% (125/128)
[Train] Epoch= 76  BatchID= 10 Loss: 0.113 | Acc: 96.591% (1360/1408)
[Train] Epoch= 76  BatchID= 20 Loss: 0.110 | Acc: 96.838% (2603/2688)
[Train] Epoch= 76  BatchID= 30 Loss: 0.116 | Acc: 96.699% (3837/3968)
[Train] Epoch= 76  BatchID= 40 Loss: 0.118 | Acc: 96.513% (5065/5248)
[Train] Epoch= 76  BatchID= 50 Loss: 0.120 | Acc: 96.461% (6297/6528)
[Train] Epoch= 76  BatchID= 60 Loss: 0.118 | Acc: 96.529% (7537/7808)
[Train] Epoch= 76  BatchID= 70 Loss: 0.114 | Acc: 96.688% (8787/9088)
[Test] Epoch= 76  BatchID= 0 Loss: 0.098 | Acc: 97.656% (125/128)
[Test] Epoch= 76  BatchID= 10 Loss: 0.293 | Acc: 91.193% (1284/1408)
[Test] Epoch= 76  BatchID= 20 Loss: 0.282 | Acc: 91.332% (2455/2688)
[Test] Epoch= 76  BatchID= 30 Loss: 0.310 | Acc: 90.726% (3561/3925)

Epoch: 77
[Test] Epoch= 76  BatchID= 0 Loss: 0.112 | Acc: 97.656% (125/128)
[Test] Epoch= 76  BatchID= 10 Loss: 0.307 | Acc: 91.122% (1283/1408)
[Test] Epoch= 76  BatchID= 20 Loss: 0.280 | Acc: 91.741% (2466/2688)
[Test] Epoch= 76  BatchID= 30 Loss: 0.306 | Acc: 91.032% (3573/3925)

Epoch: 77
[Train] Epoch= 77  BatchID= 0 Loss: 0.121 | Acc: 96.094% (123/128)
[Train] Epoch= 77  BatchID= 10 Loss: 0.088 | Acc: 97.372% (1371/1408)
[Train] Epoch= 77  BatchID= 20 Loss: 0.104 | Acc: 96.838% (2603/2688)
[Train] Epoch= 77  BatchID= 30 Loss: 0.101 | Acc: 96.875% (3844/3968)
[Train] Epoch= 77  BatchID= 40 Loss: 0.098 | Acc: 96.970% (5089/5248)
[Train] Epoch= 77  BatchID= 50 Loss: 0.105 | Acc: 96.783% (6318/6528)
[Train] Epoch= 77  BatchID= 60 Loss: 0.108 | Acc: 96.670% (7548/7808)
[Train] Epoch= 77  BatchID= 70 Loss: 0.107 | Acc: 96.776% (8795/9088)
[Train] Epoch= 77  BatchID= 0 Loss: 0.056 | Acc: 98.438% (126/128)
[Train] Epoch= 77  BatchID= 10 Loss: 0.096 | Acc: 97.372% (1371/1408)
[Train] Epoch= 77  BatchID= 20 Loss: 0.097 | Acc: 97.210% (2613/2688)
[Train] Epoch= 77  BatchID= 30 Loss: 0.097 | Acc: 97.253% (3859/3968)
[Train] Epoch= 77  BatchID= 40 Loss: 0.100 | Acc: 97.180% (5100/5248)
[Train] Epoch= 77  BatchID= 50 Loss: 0.100 | Acc: 97.151% (6342/6528)
[Train] Epoch= 77  BatchID= 60 Loss: 0.097 | Acc: 97.221% (7591/7808)
[Train] Epoch= 77  BatchID= 70 Loss: 0.100 | Acc: 97.095% (8824/9088)
[Test] Epoch= 77  BatchID= 0 Loss: 0.096 | Acc: 97.656% (125/128)
[Test] Epoch= 77  BatchID= 10 Loss: 0.306 | Acc: 91.335% (1286/1408)
[Test] Epoch= 77  BatchID= 20 Loss: 0.281 | Acc: 91.741% (2466/2688)
[Test] Epoch= 77  BatchID= 30 Loss: 0.307 | Acc: 90.879% (3567/3925)

Epoch: 78
[Test] Epoch= 77  BatchID= 0 Loss: 0.093 | Acc: 97.656% (125/128)
[Test] Epoch= 77  BatchID= 10 Loss: 0.295 | Acc: 90.909% (1280/1408)
[Test] Epoch= 77  BatchID= 20 Loss: 0.275 | Acc: 91.332% (2455/2688)
[Test] Epoch= 77  BatchID= 30 Loss: 0.307 | Acc: 90.828% (3565/3925)

Epoch: 78
[Train] Epoch= 78  BatchID= 0 Loss: 0.067 | Acc: 97.656% (125/128)
[Train] Epoch= 78  BatchID= 10 Loss: 0.089 | Acc: 97.514% (1373/1408)
[Train] Epoch= 78  BatchID= 20 Loss: 0.091 | Acc: 97.247% (2614/2688)
[Train] Epoch= 78  BatchID= 30 Loss: 0.092 | Acc: 97.177% (3856/3968)
[Train] Epoch= 78  BatchID= 40 Loss: 0.095 | Acc: 97.046% (5093/5248)
[Train] Epoch= 78  BatchID= 50 Loss: 0.098 | Acc: 96.998% (6332/6528)
[Train] Epoch= 78  BatchID= 60 Loss: 0.101 | Acc: 96.862% (7563/7808)
[Train] Epoch= 78  BatchID= 70 Loss: 0.102 | Acc: 96.798% (8797/9088)
[Train] Epoch= 78  BatchID= 0 Loss: 0.084 | Acc: 97.656% (125/128)
[Train] Epoch= 78  BatchID= 10 Loss: 0.117 | Acc: 96.378% (1357/1408)
[Train] Epoch= 78  BatchID= 20 Loss: 0.112 | Acc: 96.689% (2599/2688)
[Train] Epoch= 78  BatchID= 30 Loss: 0.111 | Acc: 96.699% (3837/3968)
[Train] Epoch= 78  BatchID= 40 Loss: 0.110 | Acc: 96.818% (5081/5248)
[Train] Epoch= 78  BatchID= 50 Loss: 0.107 | Acc: 96.798% (6319/6528)
[Train] Epoch= 78  BatchID= 60 Loss: 0.109 | Acc: 96.760% (7555/7808)
[Train] Epoch= 78  BatchID= 70 Loss: 0.110 | Acc: 96.622% (8781/9088)
[Test] Epoch= 78  BatchID= 0 Loss: 0.091 | Acc: 96.875% (124/128)
[Test] Epoch= 78  BatchID= 10 Loss: 0.292 | Acc: 91.122% (1283/1408)
[Test] Epoch= 78  BatchID= 20 Loss: 0.272 | Acc: 91.629% (2463/2688)
[Test] Epoch= 78  BatchID= 30 Loss: 0.305 | Acc: 90.904% (3568/3925)

Epoch: 79
[Test] Epoch= 78  BatchID= 0 Loss: 0.088 | Acc: 97.656% (125/128)
[Test] Epoch= 78  BatchID= 10 Loss: 0.304 | Acc: 91.193% (1284/1408)
[Test] Epoch= 78  BatchID= 20 Loss: 0.283 | Acc: 91.741% (2466/2688)
[Test] Epoch= 78  BatchID= 30 Loss: 0.305 | Acc: 91.032% (3573/3925)

Epoch: 79
[Train] Epoch= 79  BatchID= 0 Loss: 0.104 | Acc: 96.094% (123/128)
[Train] Epoch= 79  BatchID= 10 Loss: 0.106 | Acc: 96.946% (1365/1408)
[Train] Epoch= 79  BatchID= 20 Loss: 0.100 | Acc: 97.173% (2612/2688)
[Train] Epoch= 79  BatchID= 30 Loss: 0.106 | Acc: 97.026% (3850/3968)
[Train] Epoch= 79  BatchID= 40 Loss: 0.103 | Acc: 96.951% (5088/5248)
[Train] Epoch= 79  BatchID= 50 Loss: 0.103 | Acc: 96.982% (6331/6528)
[Train] Epoch= 79  BatchID= 60 Loss: 0.105 | Acc: 96.965% (7571/7808)
[Train] Epoch= 79  BatchID= 70 Loss: 0.103 | Acc: 96.974% (8813/9088)
[Train] Epoch= 79  BatchID= 0 Loss: 0.063 | Acc: 97.656% (125/128)
[Train] Epoch= 79  BatchID= 10 Loss: 0.113 | Acc: 96.378% (1357/1408)
[Train] Epoch= 79  BatchID= 20 Loss: 0.112 | Acc: 96.503% (2594/2688)
[Train] Epoch= 79  BatchID= 30 Loss: 0.112 | Acc: 96.447% (3827/3968)
[Train] Epoch= 79  BatchID= 40 Loss: 0.116 | Acc: 96.494% (5064/5248)
[Train] Epoch= 79  BatchID= 50 Loss: 0.111 | Acc: 96.661% (6310/6528)
[Train] Epoch= 79  BatchID= 60 Loss: 0.108 | Acc: 96.747% (7554/7808)
[Train] Epoch= 79  BatchID= 70 Loss: 0.109 | Acc: 96.721% (8790/9088)
[Test] Epoch= 79  BatchID= 0 Loss: 0.085 | Acc: 97.656% (125/128)
[Test] Epoch= 79  BatchID= 10 Loss: 0.280 | Acc: 91.903% (1294/1408)
[Test] Epoch= 79  BatchID= 20 Loss: 0.269 | Acc: 92.001% (2473/2688)
[Test] Epoch= 79  BatchID= 30 Loss: 0.308 | Acc: 90.879% (3567/3925)

Epoch: 80
[Test] Epoch= 79  BatchID= 0 Loss: 0.082 | Acc: 98.438% (126/128)
[Test] Epoch= 79  BatchID= 10 Loss: 0.293 | Acc: 91.264% (1285/1408)
[Test] Epoch= 79  BatchID= 20 Loss: 0.277 | Acc: 91.592% (2462/2688)
[Test] Epoch= 79  BatchID= 30 Loss: 0.309 | Acc: 90.828% (3565/3925)

Epoch: 80
[Train] Epoch= 80  BatchID= 0 Loss: 0.075 | Acc: 98.438% (126/128)
[Train] Epoch= 80  BatchID= 10 Loss: 0.111 | Acc: 96.662% (1361/1408)
[Train] Epoch= 80  BatchID= 20 Loss: 0.105 | Acc: 97.024% (2608/2688)
[Train] Epoch= 80  BatchID= 30 Loss: 0.107 | Acc: 96.951% (3847/3968)
[Train] Epoch= 80  BatchID= 40 Loss: 0.106 | Acc: 97.046% (5093/5248)
[Train] Epoch= 80  BatchID= 50 Loss: 0.106 | Acc: 96.875% (6324/6528)
[Train] Epoch= 80  BatchID= 60 Loss: 0.105 | Acc: 96.875% (7564/7808)
[Train] Epoch= 80  BatchID= 70 Loss: 0.104 | Acc: 96.820% (8799/9088)
[Train] Epoch= 80  BatchID= 0 Loss: 0.089 | Acc: 96.875% (124/128)
[Train] Epoch= 80  BatchID= 10 Loss: 0.114 | Acc: 96.165% (1354/1408)
[Train] Epoch= 80  BatchID= 20 Loss: 0.108 | Acc: 96.354% (2590/2688)
[Train] Epoch= 80  BatchID= 30 Loss: 0.105 | Acc: 96.522% (3830/3968)
[Train] Epoch= 80  BatchID= 40 Loss: 0.106 | Acc: 96.665% (5073/5248)
[Train] Epoch= 80  BatchID= 50 Loss: 0.106 | Acc: 96.768% (6317/6528)
[Train] Epoch= 80  BatchID= 60 Loss: 0.108 | Acc: 96.734% (7553/7808)
[Train] Epoch= 80  BatchID= 70 Loss: 0.106 | Acc: 96.721% (8790/9088)
[Test] Epoch= 80  BatchID= 0 Loss: 0.090 | Acc: 97.656% (125/128)
[Test] Epoch= 80  BatchID= 10 Loss: 0.282 | Acc: 91.548% (1289/1408)
[Test] Epoch= 80  BatchID= 20 Loss: 0.270 | Acc: 91.815% (2468/2688)
[Test] Epoch= 80  BatchID= 30 Loss: 0.299 | Acc: 91.108% (3576/3925)
Saving..
Best accuracy:  91.10828025477707

Epoch: 81
[Test] Epoch= 80  BatchID= 0 Loss: 0.112 | Acc: 97.656% (125/128)
[Test] Epoch= 80  BatchID= 10 Loss: 0.293 | Acc: 91.335% (1286/1408)
[Test] Epoch= 80  BatchID= 20 Loss: 0.279 | Acc: 91.629% (2463/2688)
[Test] Epoch= 80  BatchID= 30 Loss: 0.303 | Acc: 90.879% (3567/3925)

Epoch: 81
[Train] Epoch= 81  BatchID= 0 Loss: 0.090 | Acc: 96.875% (124/128)
[Train] Epoch= 81  BatchID= 10 Loss: 0.114 | Acc: 97.159% (1368/1408)
[Train] Epoch= 81  BatchID= 20 Loss: 0.107 | Acc: 97.061% (2609/2688)
[Train] Epoch= 81  BatchID= 30 Loss: 0.104 | Acc: 97.051% (3851/3968)
[Train] Epoch= 81  BatchID= 40 Loss: 0.112 | Acc: 96.704% (5075/5248)
[Train] Epoch= 81  BatchID= 50 Loss: 0.114 | Acc: 96.553% (6303/6528)
[Train] Epoch= 81  BatchID= 60 Loss: 0.112 | Acc: 96.632% (7545/7808)
[Train] Epoch= 81  BatchID= 70 Loss: 0.110 | Acc: 96.666% (8785/9088)
[Train] Epoch= 81  BatchID= 0 Loss: 0.150 | Acc: 96.094% (123/128)
[Train] Epoch= 81  BatchID= 10 Loss: 0.097 | Acc: 97.230% (1369/1408)
[Train] Epoch= 81  BatchID= 20 Loss: 0.095 | Acc: 97.173% (2612/2688)
[Train] Epoch= 81  BatchID= 30 Loss: 0.097 | Acc: 97.051% (3851/3968)
[Train] Epoch= 81  BatchID= 40 Loss: 0.101 | Acc: 97.046% (5093/5248)
[Train] Epoch= 81  BatchID= 50 Loss: 0.098 | Acc: 97.135% (6341/6528)
[Train] Epoch= 81  BatchID= 60 Loss: 0.102 | Acc: 97.029% (7576/7808)
[Train] Epoch= 81  BatchID= 70 Loss: 0.102 | Acc: 97.051% (8820/9088)
[Test] Epoch= 81  BatchID= 0 Loss: 0.103 | Acc: 96.875% (124/128)
[Test] Epoch= 81  BatchID= 10 Loss: 0.286 | Acc: 91.974% (1295/1408)
[Test] Epoch= 81  BatchID= 20 Loss: 0.274 | Acc: 92.039% (2474/2688)
[Test] Epoch= 81  BatchID= 30 Loss: 0.307 | Acc: 91.210% (3580/3925)
Saving..
Best accuracy:  91.21019108280255

Epoch: 82
[Test] Epoch= 81  BatchID= 0 Loss: 0.088 | Acc: 97.656% (125/128)
[Test] Epoch= 81  BatchID= 10 Loss: 0.300 | Acc: 91.477% (1288/1408)
[Test] Epoch= 81  BatchID= 20 Loss: 0.283 | Acc: 91.592% (2462/2688)
[Test] Epoch= 81  BatchID= 30 Loss: 0.303 | Acc: 90.803% (3564/3925)

Epoch: 82
[Train] Epoch= 82  BatchID= 0 Loss: 0.067 | Acc: 98.438% (126/128)
[Train] Epoch= 82  BatchID= 10 Loss: 0.098 | Acc: 97.088% (1367/1408)
[Train] Epoch= 82  BatchID= 20 Loss: 0.104 | Acc: 96.689% (2599/2688)
[Train] Epoch= 82  BatchID= 30 Loss: 0.108 | Acc: 96.421% (3826/3968)
[Train] Epoch= 82  BatchID= 40 Loss: 0.107 | Acc: 96.494% (5064/5248)
[Train] Epoch= 82  BatchID= 50 Loss: 0.105 | Acc: 96.538% (6302/6528)
[Train] Epoch= 82  BatchID= 60 Loss: 0.107 | Acc: 96.632% (7545/7808)
[Train] Epoch= 82  BatchID= 70 Loss: 0.105 | Acc: 96.699% (8788/9088)
[Train] Epoch= 82  BatchID= 0 Loss: 0.128 | Acc: 96.094% (123/128)
[Train] Epoch= 82  BatchID= 10 Loss: 0.108 | Acc: 96.946% (1365/1408)
[Train] Epoch= 82  BatchID= 20 Loss: 0.104 | Acc: 96.987% (2607/2688)
[Train] Epoch= 82  BatchID= 30 Loss: 0.114 | Acc: 96.699% (3837/3968)
[Train] Epoch= 82  BatchID= 40 Loss: 0.110 | Acc: 96.761% (5078/5248)
[Train] Epoch= 82  BatchID= 50 Loss: 0.107 | Acc: 96.814% (6320/6528)
[Train] Epoch= 82  BatchID= 60 Loss: 0.105 | Acc: 96.888% (7565/7808)
[Train] Epoch= 82  BatchID= 70 Loss: 0.104 | Acc: 96.952% (8811/9088)
[Test] Epoch= 82  BatchID= 0 Loss: 0.100 | Acc: 96.875% (124/128)
[Test] Epoch= 82  BatchID= 10 Loss: 0.290 | Acc: 91.903% (1294/1408)
[Test] Epoch= 82  BatchID= 20 Loss: 0.273 | Acc: 91.927% (2471/2688)
[Test] Epoch= 82  BatchID= 30 Loss: 0.308 | Acc: 91.134% (3577/3925)

Epoch: 83
[Test] Epoch= 82  BatchID= 0 Loss: 0.098 | Acc: 97.656% (125/128)
[Test] Epoch= 82  BatchID= 10 Loss: 0.299 | Acc: 90.909% (1280/1408)
[Test] Epoch= 82  BatchID= 20 Loss: 0.277 | Acc: 91.481% (2459/2688)
[Test] Epoch= 82  BatchID= 30 Loss: 0.309 | Acc: 90.548% (3554/3925)

Epoch: 83
[Train] Epoch= 83  BatchID= 0 Loss: 0.077 | Acc: 97.656% (125/128)
[Train] Epoch= 83  BatchID= 10 Loss: 0.101 | Acc: 96.804% (1363/1408)
[Train] Epoch= 83  BatchID= 20 Loss: 0.101 | Acc: 96.912% (2605/2688)
[Train] Epoch= 83  BatchID= 30 Loss: 0.100 | Acc: 97.152% (3855/3968)
[Train] Epoch= 83  BatchID= 40 Loss: 0.100 | Acc: 97.180% (5100/5248)
[Train] Epoch= 83  BatchID= 50 Loss: 0.103 | Acc: 97.120% (6340/6528)
[Train] Epoch= 83  BatchID= 60 Loss: 0.107 | Acc: 96.990% (7573/7808)
[Train] Epoch= 83  BatchID= 70 Loss: 0.107 | Acc: 96.996% (8815/9088)
[Train] Epoch= 83  BatchID= 0 Loss: 0.095 | Acc: 96.875% (124/128)
[Train] Epoch= 83  BatchID= 10 Loss: 0.095 | Acc: 97.230% (1369/1408)
[Train] Epoch= 83  BatchID= 20 Loss: 0.108 | Acc: 96.652% (2598/2688)
[Train] Epoch= 83  BatchID= 30 Loss: 0.112 | Acc: 96.724% (3838/3968)
[Train] Epoch= 83  BatchID= 40 Loss: 0.108 | Acc: 96.856% (5083/5248)
[Train] Epoch= 83  BatchID= 50 Loss: 0.107 | Acc: 96.860% (6323/6528)
[Train] Epoch= 83  BatchID= 60 Loss: 0.108 | Acc: 96.734% (7553/7808)
[Train] Epoch= 83  BatchID= 70 Loss: 0.106 | Acc: 96.787% (8796/9088)
[Test] Epoch= 83  BatchID= 0 Loss: 0.098 | Acc: 97.656% (125/128)
[Test] Epoch= 83  BatchID= 10 Loss: 0.283 | Acc: 91.335% (1286/1408)
[Test] Epoch= 83  BatchID= 20 Loss: 0.270 | Acc: 91.704% (2465/2688)
[Test] Epoch= 83  BatchID= 30 Loss: 0.306 | Acc: 90.981% (3571/3925)

Epoch: 84
[Test] Epoch= 83  BatchID= 0 Loss: 0.087 | Acc: 97.656% (125/128)
[Test] Epoch= 83  BatchID= 10 Loss: 0.290 | Acc: 91.335% (1286/1408)
[Test] Epoch= 83  BatchID= 20 Loss: 0.272 | Acc: 91.592% (2462/2688)
[Test] Epoch= 83  BatchID= 30 Loss: 0.301 | Acc: 90.726% (3561/3925)

Epoch: 84
[Train] Epoch= 84  BatchID= 0 Loss: 0.150 | Acc: 94.531% (121/128)
[Train] Epoch= 84  BatchID= 10 Loss: 0.097 | Acc: 96.946% (1365/1408)
[Train] Epoch= 84  BatchID= 20 Loss: 0.088 | Acc: 97.396% (2618/2688)
[Train] Epoch= 84  BatchID= 30 Loss: 0.094 | Acc: 97.329% (3862/3968)
[Train] Epoch= 84  BatchID= 40 Loss: 0.094 | Acc: 97.409% (5112/5248)
[Train] Epoch= 84  BatchID= 50 Loss: 0.095 | Acc: 97.304% (6352/6528)
[Train] Epoch= 84  BatchID= 60 Loss: 0.096 | Acc: 97.272% (7595/7808)
[Train] Epoch= 84  BatchID= 70 Loss: 0.096 | Acc: 97.271% (8840/9088)
[Train] Epoch= 84  BatchID= 0 Loss: 0.042 | Acc: 99.219% (127/128)
[Train] Epoch= 84  BatchID= 10 Loss: 0.125 | Acc: 96.165% (1354/1408)
[Train] Epoch= 84  BatchID= 20 Loss: 0.108 | Acc: 96.652% (2598/2688)
[Train] Epoch= 84  BatchID= 30 Loss: 0.105 | Acc: 96.774% (3840/3968)
[Train] Epoch= 84  BatchID= 40 Loss: 0.104 | Acc: 96.894% (5085/5248)
[Train] Epoch= 84  BatchID= 50 Loss: 0.107 | Acc: 96.691% (6312/6528)
[Train] Epoch= 84  BatchID= 60 Loss: 0.108 | Acc: 96.734% (7553/7808)
[Train] Epoch= 84  BatchID= 70 Loss: 0.106 | Acc: 96.754% (8793/9088)
[Test] Epoch= 84  BatchID= 0 Loss: 0.086 | Acc: 97.656% (125/128)
[Test] Epoch= 84  BatchID= 10 Loss: 0.294 | Acc: 91.264% (1285/1408)
[Test] Epoch= 84  BatchID= 20 Loss: 0.272 | Acc: 91.667% (2464/2688)
[Test] Epoch= 84  BatchID= 30 Loss: 0.310 | Acc: 90.828% (3565/3925)

Epoch: 85
[Test] Epoch= 84  BatchID= 0 Loss: 0.094 | Acc: 97.656% (125/128)
[Test] Epoch= 84  BatchID= 10 Loss: 0.298 | Acc: 91.690% (1291/1408)
[Test] Epoch= 84  BatchID= 20 Loss: 0.279 | Acc: 91.890% (2470/2688)
[Test] Epoch= 84  BatchID= 30 Loss: 0.309 | Acc: 90.904% (3568/3925)

Epoch: 85
[Train] Epoch= 85  BatchID= 0 Loss: 0.159 | Acc: 96.875% (124/128)
[Train] Epoch= 85  BatchID= 10 Loss: 0.113 | Acc: 97.159% (1368/1408)
[Train] Epoch= 85  BatchID= 20 Loss: 0.112 | Acc: 96.838% (2603/2688)
[Train] Epoch= 85  BatchID= 30 Loss: 0.112 | Acc: 96.774% (3840/3968)
[Train] Epoch= 85  BatchID= 40 Loss: 0.112 | Acc: 96.704% (5075/5248)
[Train] Epoch= 85  BatchID= 50 Loss: 0.109 | Acc: 96.783% (6318/6528)
[Train] Epoch= 85  BatchID= 60 Loss: 0.109 | Acc: 96.747% (7554/7808)
[Train] Epoch= 85  BatchID= 70 Loss: 0.106 | Acc: 96.842% (8801/9088)
[Train] Epoch= 85  BatchID= 0 Loss: 0.125 | Acc: 96.094% (123/128)
[Train] Epoch= 85  BatchID= 10 Loss: 0.112 | Acc: 96.520% (1359/1408)
[Train] Epoch= 85  BatchID= 20 Loss: 0.108 | Acc: 96.689% (2599/2688)
[Train] Epoch= 85  BatchID= 30 Loss: 0.114 | Acc: 96.623% (3834/3968)
[Train] Epoch= 85  BatchID= 40 Loss: 0.116 | Acc: 96.551% (5067/5248)
[Train] Epoch= 85  BatchID= 50 Loss: 0.113 | Acc: 96.615% (6307/6528)
[Train] Epoch= 85  BatchID= 60 Loss: 0.113 | Acc: 96.657% (7547/7808)
[Train] Epoch= 85  BatchID= 70 Loss: 0.111 | Acc: 96.721% (8790/9088)
[Test] Epoch= 85  BatchID= 0 Loss: 0.093 | Acc: 98.438% (126/128)
[Test] Epoch= 85  BatchID= 10 Loss: 0.280 | Acc: 91.690% (1291/1408)
[Test] Epoch= 85  BatchID= 20 Loss: 0.269 | Acc: 92.076% (2475/2688)
[Test] Epoch= 85  BatchID= 30 Loss: 0.308 | Acc: 91.108% (3576/3925)

Epoch: 86
[Test] Epoch= 85  BatchID= 0 Loss: 0.102 | Acc: 97.656% (125/128)
[Test] Epoch= 85  BatchID= 10 Loss: 0.308 | Acc: 91.122% (1283/1408)
[Test] Epoch= 85  BatchID= 20 Loss: 0.282 | Acc: 91.592% (2462/2688)
[Test] Epoch= 85  BatchID= 30 Loss: 0.305 | Acc: 90.803% (3564/3925)

Epoch: 86
[Train] Epoch= 86  BatchID= 0 Loss: 0.099 | Acc: 96.875% (124/128)
[Train] Epoch= 86  BatchID= 10 Loss: 0.109 | Acc: 96.662% (1361/1408)
[Train] Epoch= 86  BatchID= 20 Loss: 0.099 | Acc: 96.801% (2602/2688)
[Train] Epoch= 86  BatchID= 30 Loss: 0.104 | Acc: 96.573% (3832/3968)
[Train] Epoch= 86  BatchID= 40 Loss: 0.105 | Acc: 96.761% (5078/5248)
[Train] Epoch= 86  BatchID= 50 Loss: 0.102 | Acc: 96.860% (6323/6528)
[Train] Epoch= 86  BatchID= 60 Loss: 0.103 | Acc: 96.875% (7564/7808)
[Train] Epoch= 86  BatchID= 70 Loss: 0.103 | Acc: 96.798% (8797/9088)
[Train] Epoch= 86  BatchID= 0 Loss: 0.175 | Acc: 94.531% (121/128)
[Train] Epoch= 86  BatchID= 10 Loss: 0.114 | Acc: 96.875% (1364/1408)
[Train] Epoch= 86  BatchID= 20 Loss: 0.111 | Acc: 96.801% (2602/2688)
[Train] Epoch= 86  BatchID= 30 Loss: 0.118 | Acc: 96.447% (3827/3968)
[Train] Epoch= 86  BatchID= 40 Loss: 0.121 | Acc: 96.303% (5054/5248)
[Train] Epoch= 86  BatchID= 50 Loss: 0.119 | Acc: 96.293% (6286/6528)
[Train] Epoch= 86  BatchID= 60 Loss: 0.115 | Acc: 96.440% (7530/7808)
[Train] Epoch= 86  BatchID= 70 Loss: 0.111 | Acc: 96.534% (8773/9088)
[Test] Epoch= 86  BatchID= 0 Loss: 0.104 | Acc: 97.656% (125/128)
[Test] Epoch= 86  BatchID= 10 Loss: 0.287 | Acc: 91.548% (1289/1408)
[Test] Epoch= 86  BatchID= 20 Loss: 0.274 | Acc: 91.815% (2468/2688)
[Test] Epoch= 86  BatchID= 30 Loss: 0.307 | Acc: 91.006% (3572/3925)

Epoch: 87
[Test] Epoch= 86  BatchID= 0 Loss: 0.096 | Acc: 97.656% (125/128)
[Test] Epoch= 86  BatchID= 10 Loss: 0.305 | Acc: 90.909% (1280/1408)
[Test] Epoch= 86  BatchID= 20 Loss: 0.285 | Acc: 91.443% (2458/2688)
[Test] Epoch= 86  BatchID= 30 Loss: 0.306 | Acc: 90.904% (3568/3925)

Epoch: 87
[Train] Epoch= 87  BatchID= 0 Loss: 0.108 | Acc: 97.656% (125/128)
[Train] Epoch= 87  BatchID= 10 Loss: 0.117 | Acc: 95.952% (1351/1408)
[Train] Epoch= 87  BatchID= 20 Loss: 0.109 | Acc: 96.466% (2593/2688)
[Train] Epoch= 87  BatchID= 30 Loss: 0.110 | Acc: 96.648% (3835/3968)
[Train] Epoch= 87  BatchID= 40 Loss: 0.110 | Acc: 96.684% (5074/5248)
[Train] Epoch= 87  BatchID= 50 Loss: 0.111 | Acc: 96.645% (6309/6528)
[Train] Epoch= 87  BatchID= 60 Loss: 0.112 | Acc: 96.555% (7539/7808)
[Train] Epoch= 87  BatchID= 70 Loss: 0.113 | Acc: 96.578% (8777/9088)
[Train] Epoch= 87  BatchID= 0 Loss: 0.115 | Acc: 96.094% (123/128)
[Train] Epoch= 87  BatchID= 10 Loss: 0.108 | Acc: 96.378% (1357/1408)
[Train] Epoch= 87  BatchID= 20 Loss: 0.105 | Acc: 96.540% (2595/2688)
[Train] Epoch= 87  BatchID= 30 Loss: 0.106 | Acc: 96.648% (3835/3968)
[Train] Epoch= 87  BatchID= 40 Loss: 0.109 | Acc: 96.627% (5071/5248)
[Train] Epoch= 87  BatchID= 50 Loss: 0.111 | Acc: 96.553% (6303/6528)
[Train] Epoch= 87  BatchID= 60 Loss: 0.111 | Acc: 96.555% (7539/7808)
[Train] Epoch= 87  BatchID= 70 Loss: 0.110 | Acc: 96.589% (8778/9088)
[Test] Epoch= 87  BatchID= 0 Loss: 0.095 | Acc: 97.656% (125/128)
[Test] Epoch= 87  BatchID= 10 Loss: 0.298 | Acc: 90.767% (1278/1408)
[Test] Epoch= 87  BatchID= 20 Loss: 0.279 | Acc: 91.183% (2451/2688)
[Test] Epoch= 87  BatchID= 30 Loss: 0.306 | Acc: 90.650% (3558/3925)

Epoch: 88
[Test] Epoch= 87  BatchID= 0 Loss: 0.082 | Acc: 98.438% (126/128)
[Test] Epoch= 87  BatchID= 10 Loss: 0.297 | Acc: 91.477% (1288/1408)
[Test] Epoch= 87  BatchID= 20 Loss: 0.279 | Acc: 91.815% (2468/2688)
[Test] Epoch= 87  BatchID= 30 Loss: 0.306 | Acc: 90.904% (3568/3925)

Epoch: 88
[Train] Epoch= 88  BatchID= 0 Loss: 0.137 | Acc: 96.875% (124/128)
[Train] Epoch= 88  BatchID= 10 Loss: 0.133 | Acc: 96.165% (1354/1408)
[Train] Epoch= 88  BatchID= 20 Loss: 0.112 | Acc: 96.652% (2598/2688)
[Train] Epoch= 88  BatchID= 30 Loss: 0.116 | Acc: 96.497% (3829/3968)
[Train] Epoch= 88  BatchID= 40 Loss: 0.114 | Acc: 96.608% (5070/5248)
[Train] Epoch= 88  BatchID= 50 Loss: 0.113 | Acc: 96.645% (6309/6528)
[Train] Epoch= 88  BatchID= 60 Loss: 0.112 | Acc: 96.580% (7541/7808)
[Train] Epoch= 88  BatchID= 70 Loss: 0.110 | Acc: 96.578% (8777/9088)
[Train] Epoch= 88  BatchID= 0 Loss: 0.059 | Acc: 97.656% (125/128)
[Train] Epoch= 88  BatchID= 10 Loss: 0.101 | Acc: 97.372% (1371/1408)
[Train] Epoch= 88  BatchID= 20 Loss: 0.103 | Acc: 97.173% (2612/2688)
[Train] Epoch= 88  BatchID= 30 Loss: 0.105 | Acc: 97.177% (3856/3968)
[Train] Epoch= 88  BatchID= 40 Loss: 0.105 | Acc: 97.123% (5097/5248)
[Train] Epoch= 88  BatchID= 50 Loss: 0.106 | Acc: 96.998% (6332/6528)
[Train] Epoch= 88  BatchID= 60 Loss: 0.108 | Acc: 96.888% (7565/7808)
[Train] Epoch= 88  BatchID= 70 Loss: 0.105 | Acc: 96.941% (8810/9088)
[Test] Epoch= 88  BatchID= 0 Loss: 0.101 | Acc: 96.875% (124/128)
[Test] Epoch= 88  BatchID= 10 Loss: 0.289 | Acc: 91.690% (1291/1408)
[Test] Epoch= 88  BatchID= 20 Loss: 0.273 | Acc: 91.667% (2464/2688)
[Test] Epoch= 88  BatchID= 30 Loss: 0.307 | Acc: 91.134% (3577/3925)

Epoch: 89
[Test] Epoch= 88  BatchID= 0 Loss: 0.107 | Acc: 97.656% (125/128)
[Test] Epoch= 88  BatchID= 10 Loss: 0.314 | Acc: 90.980% (1281/1408)
[Test] Epoch= 88  BatchID= 20 Loss: 0.284 | Acc: 91.555% (2461/2688)
[Test] Epoch= 88  BatchID= 30 Loss: 0.310 | Acc: 90.777% (3563/3925)

Epoch: 89
[Train] Epoch= 89  BatchID= 0 Loss: 0.087 | Acc: 96.875% (124/128)
[Train] Epoch= 89  BatchID= 10 Loss: 0.103 | Acc: 96.591% (1360/1408)
[Train] Epoch= 89  BatchID= 20 Loss: 0.104 | Acc: 96.726% (2600/2688)
[Train] Epoch= 89  BatchID= 30 Loss: 0.109 | Acc: 96.825% (3842/3968)
[Train] Epoch= 89  BatchID= 40 Loss: 0.111 | Acc: 96.742% (5077/5248)
[Train] Epoch= 89  BatchID= 50 Loss: 0.110 | Acc: 96.752% (6316/6528)
[Train] Epoch= 89  BatchID= 60 Loss: 0.107 | Acc: 96.798% (7558/7808)
[Train] Epoch= 89  BatchID= 70 Loss: 0.105 | Acc: 96.886% (8805/9088)
[Train] Epoch= 89  BatchID= 0 Loss: 0.146 | Acc: 96.094% (123/128)
[Train] Epoch= 89  BatchID= 10 Loss: 0.136 | Acc: 96.023% (1352/1408)
[Train] Epoch= 89  BatchID= 20 Loss: 0.111 | Acc: 96.726% (2600/2688)
[Train] Epoch= 89  BatchID= 30 Loss: 0.109 | Acc: 96.724% (3838/3968)
[Train] Epoch= 89  BatchID= 40 Loss: 0.110 | Acc: 96.723% (5076/5248)
[Train] Epoch= 89  BatchID= 50 Loss: 0.109 | Acc: 96.829% (6321/6528)
[Train] Epoch= 89  BatchID= 60 Loss: 0.111 | Acc: 96.734% (7553/7808)
[Train] Epoch= 89  BatchID= 70 Loss: 0.109 | Acc: 96.754% (8793/9088)
[Test] Epoch= 89  BatchID= 0 Loss: 0.095 | Acc: 97.656% (125/128)
[Test] Epoch= 89  BatchID= 10 Loss: 0.294 | Acc: 91.264% (1285/1408)
[Test] Epoch= 89  BatchID= 20 Loss: 0.273 | Acc: 91.555% (2461/2688)
[Test] Epoch= 89  BatchID= 30 Loss: 0.311 | Acc: 90.752% (3562/3925)
[Test] Epoch= 89  BatchID= 0 Loss: 0.082 | Acc: 98.438% (126/128)
[Test] Epoch= 89  BatchID= 10 Loss: 0.289 | Acc: 91.832% (1293/1408)
[Test] Epoch= 89  BatchID= 20 Loss: 0.271 | Acc: 92.039% (2474/2688)
[Test] Epoch= 89  BatchID= 30 Loss: 0.306 | Acc: 90.955% (3570/3925)
