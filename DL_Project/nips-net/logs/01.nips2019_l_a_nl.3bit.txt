==> Preparing data..
==> Building model..
DataParallel(
  (module): ResNet(
    (conv1): InputConv2dLSQ(
      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): LinearLSQ(
      in_features=512, out_features=10, bias=True
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=True)
    )
  )
)
==> Initializing from checkpoint..

Epoch: 0
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
[Train] Epoch= 0  BatchID= 0 Loss: 0.263 | Acc: 91.406% (117/128)
[Train] Epoch= 0  BatchID= 10 Loss: 0.739 | Acc: 78.480% (1105/1408)
[Train] Epoch= 0  BatchID= 20 Loss: 0.700 | Acc: 79.539% (2138/2688)
[Train] Epoch= 0  BatchID= 30 Loss: 0.691 | Acc: 79.461% (3153/3968)
[Train] Epoch= 0  BatchID= 40 Loss: 0.667 | Acc: 80.069% (4202/5248)
[Train] Epoch= 0  BatchID= 50 Loss: 0.657 | Acc: 80.101% (5229/6528)
[Train] Epoch= 0  BatchID= 60 Loss: 0.640 | Acc: 80.571% (6291/7808)
[Train] Epoch= 0  BatchID= 70 Loss: 0.624 | Acc: 80.865% (7349/9088)
[Test] Epoch= 0  BatchID= 0 Loss: 0.260 | Acc: 92.188% (118/128)
[Test] Epoch= 0  BatchID= 10 Loss: 0.390 | Acc: 87.216% (1228/1408)
[Test] Epoch= 0  BatchID= 20 Loss: 0.482 | Acc: 84.487% (2271/2688)
[Test] Epoch= 0  BatchID= 30 Loss: 0.547 | Acc: 83.032% (3259/3925)
Saving..
Best accuracy:  83.03184713375796

Epoch: 1
[Train] Epoch= 1  BatchID= 0 Loss: 0.404 | Acc: 88.281% (113/128)
[Train] Epoch= 1  BatchID= 10 Loss: 0.488 | Acc: 84.020% (1183/1408)
[Train] Epoch= 1  BatchID= 20 Loss: 0.518 | Acc: 83.110% (2234/2688)
[Train] Epoch= 1  BatchID= 30 Loss: 0.545 | Acc: 82.258% (3264/3968)
[Train] Epoch= 1  BatchID= 40 Loss: 0.548 | Acc: 82.260% (4317/5248)
[Train] Epoch= 1  BatchID= 50 Loss: 0.546 | Acc: 82.246% (5369/6528)
[Train] Epoch= 1  BatchID= 60 Loss: 0.550 | Acc: 82.236% (6421/7808)
[Train] Epoch= 1  BatchID= 70 Loss: 0.550 | Acc: 82.130% (7464/9088)
[Test] Epoch= 1  BatchID= 0 Loss: 0.326 | Acc: 92.188% (118/128)
[Test] Epoch= 1  BatchID= 10 Loss: 0.446 | Acc: 85.369% (1202/1408)
[Test] Epoch= 1  BatchID= 20 Loss: 0.497 | Acc: 84.375% (2268/2688)
[Test] Epoch= 1  BatchID= 30 Loss: 0.590 | Acc: 81.885% (3214/3925)

Epoch: 2
[Train] Epoch= 2  BatchID= 0 Loss: 0.506 | Acc: 84.375% (108/128)
[Train] Epoch= 2  BatchID= 10 Loss: 0.550 | Acc: 80.753% (1137/1408)
[Train] Epoch= 2  BatchID= 20 Loss: 0.540 | Acc: 81.399% (2188/2688)
[Train] Epoch= 2  BatchID= 30 Loss: 0.531 | Acc: 82.233% (3263/3968)
[Train] Epoch= 2  BatchID= 40 Loss: 0.535 | Acc: 82.184% (4313/5248)
[Train] Epoch= 2  BatchID= 50 Loss: 0.543 | Acc: 81.801% (5340/6528)
[Train] Epoch= 2  BatchID= 60 Loss: 0.550 | Acc: 81.429% (6358/7808)
[Train] Epoch= 2  BatchID= 70 Loss: 0.563 | Acc: 81.129% (7373/9088)
[Test] Epoch= 2  BatchID= 0 Loss: 0.398 | Acc: 88.281% (113/128)
[Test] Epoch= 2  BatchID= 10 Loss: 0.485 | Acc: 85.156% (1199/1408)
[Test] Epoch= 2  BatchID= 20 Loss: 0.672 | Acc: 80.208% (2156/2688)
[Test] Epoch= 2  BatchID= 30 Loss: 0.605 | Acc: 81.911% (3215/3925)

Epoch: 3
[Train] Epoch= 3  BatchID= 0 Loss: 0.589 | Acc: 83.594% (107/128)
[Train] Epoch= 3  BatchID= 10 Loss: 0.502 | Acc: 83.665% (1178/1408)
[Train] Epoch= 3  BatchID= 20 Loss: 0.531 | Acc: 83.185% (2236/2688)
[Train] Epoch= 3  BatchID= 30 Loss: 0.567 | Acc: 81.578% (3237/3968)
[Train] Epoch= 3  BatchID= 40 Loss: 0.557 | Acc: 81.803% (4293/5248)
[Train] Epoch= 3  BatchID= 50 Loss: 0.561 | Acc: 81.771% (5338/6528)
[Train] Epoch= 3  BatchID= 60 Loss: 0.559 | Acc: 81.788% (6386/7808)
[Train] Epoch= 3  BatchID= 70 Loss: 0.569 | Acc: 81.701% (7425/9088)
[Test] Epoch= 3  BatchID= 0 Loss: 0.176 | Acc: 94.531% (121/128)
[Test] Epoch= 3  BatchID= 10 Loss: 0.378 | Acc: 87.429% (1231/1408)
[Test] Epoch= 3  BatchID= 20 Loss: 0.464 | Acc: 84.933% (2283/2688)
[Test] Epoch= 3  BatchID= 30 Loss: 0.525 | Acc: 82.854% (3252/3925)

Epoch: 4
[Train] Epoch= 4  BatchID= 0 Loss: 0.565 | Acc: 80.469% (103/128)
[Train] Epoch= 4  BatchID= 10 Loss: 0.603 | Acc: 79.332% (1117/1408)
[Train] Epoch= 4  BatchID= 20 Loss: 0.606 | Acc: 80.357% (2160/2688)
[Train] Epoch= 4  BatchID= 30 Loss: 0.593 | Acc: 81.023% (3215/3968)
[Train] Epoch= 4  BatchID= 40 Loss: 0.609 | Acc: 80.450% (4222/5248)
[Train] Epoch= 4  BatchID= 50 Loss: 0.608 | Acc: 80.515% (5256/6528)
[Train] Epoch= 4  BatchID= 60 Loss: 0.601 | Acc: 80.571% (6291/7808)
[Train] Epoch= 4  BatchID= 70 Loss: 0.598 | Acc: 80.678% (7332/9088)
[Test] Epoch= 4  BatchID= 0 Loss: 0.267 | Acc: 92.188% (118/128)
[Test] Epoch= 4  BatchID= 10 Loss: 0.384 | Acc: 87.287% (1229/1408)
[Test] Epoch= 4  BatchID= 20 Loss: 0.501 | Acc: 83.854% (2254/2688)
[Test] Epoch= 4  BatchID= 30 Loss: 0.565 | Acc: 81.783% (3210/3925)

Epoch: 5
[Train] Epoch= 5  BatchID= 0 Loss: 0.524 | Acc: 82.031% (105/128)
[Train] Epoch= 5  BatchID= 10 Loss: 0.585 | Acc: 80.398% (1132/1408)
[Train] Epoch= 5  BatchID= 20 Loss: 0.575 | Acc: 81.324% (2186/2688)
[Train] Epoch= 5  BatchID= 30 Loss: 0.577 | Acc: 81.477% (3233/3968)
[Train] Epoch= 5  BatchID= 40 Loss: 0.577 | Acc: 81.441% (4274/5248)
[Train] Epoch= 5  BatchID= 50 Loss: 0.574 | Acc: 81.495% (5320/6528)
[Train] Epoch= 5  BatchID= 60 Loss: 0.586 | Acc: 81.058% (6329/7808)
[Train] Epoch= 5  BatchID= 70 Loss: 0.589 | Acc: 80.997% (7361/9088)
[Test] Epoch= 5  BatchID= 0 Loss: 0.140 | Acc: 96.875% (124/128)
[Test] Epoch= 5  BatchID= 10 Loss: 0.549 | Acc: 82.884% (1167/1408)
[Test] Epoch= 5  BatchID= 20 Loss: 0.535 | Acc: 83.222% (2237/2688)
[Test] Epoch= 5  BatchID= 30 Loss: 0.559 | Acc: 82.191% (3226/3925)

Epoch: 6
[Train] Epoch= 6  BatchID= 0 Loss: 0.557 | Acc: 82.031% (105/128)
[Train] Epoch= 6  BatchID= 10 Loss: 0.538 | Acc: 82.599% (1163/1408)
[Train] Epoch= 6  BatchID= 20 Loss: 0.561 | Acc: 81.957% (2203/2688)
[Train] Epoch= 6  BatchID= 30 Loss: 0.545 | Acc: 82.863% (3288/3968)
[Train] Epoch= 6  BatchID= 40 Loss: 0.551 | Acc: 82.470% (4328/5248)
[Train] Epoch= 6  BatchID= 50 Loss: 0.562 | Acc: 82.123% (5361/6528)
[Train] Epoch= 6  BatchID= 60 Loss: 0.581 | Acc: 81.775% (6385/7808)
[Train] Epoch= 6  BatchID= 70 Loss: 0.590 | Acc: 81.525% (7409/9088)
[Test] Epoch= 6  BatchID= 0 Loss: 0.239 | Acc: 90.625% (116/128)
[Test] Epoch= 6  BatchID= 10 Loss: 0.441 | Acc: 84.801% (1194/1408)
[Test] Epoch= 6  BatchID= 20 Loss: 0.512 | Acc: 83.147% (2235/2688)
[Test] Epoch= 6  BatchID= 30 Loss: 0.523 | Acc: 83.006% (3258/3925)

Epoch: 7
[Train] Epoch= 7  BatchID= 0 Loss: 0.617 | Acc: 80.469% (103/128)
[Train] Epoch= 7  BatchID= 10 Loss: 0.564 | Acc: 81.463% (1147/1408)
[Train] Epoch= 7  BatchID= 20 Loss: 0.579 | Acc: 81.250% (2184/2688)
[Train] Epoch= 7  BatchID= 30 Loss: 0.584 | Acc: 81.200% (3222/3968)
[Train] Epoch= 7  BatchID= 40 Loss: 0.584 | Acc: 81.079% (4255/5248)
[Train] Epoch= 7  BatchID= 50 Loss: 0.590 | Acc: 80.668% (5266/6528)
[Train] Epoch= 7  BatchID= 60 Loss: 0.592 | Acc: 80.469% (6283/7808)
[Train] Epoch= 7  BatchID= 70 Loss: 0.603 | Acc: 80.304% (7298/9088)
[Test] Epoch= 7  BatchID= 0 Loss: 0.109 | Acc: 94.531% (121/128)
[Test] Epoch= 7  BatchID= 10 Loss: 0.567 | Acc: 79.901% (1125/1408)
[Test] Epoch= 7  BatchID= 20 Loss: 0.578 | Acc: 80.841% (2173/2688)
[Test] Epoch= 7  BatchID= 30 Loss: 0.562 | Acc: 81.223% (3188/3925)

Epoch: 8
[Train] Epoch= 8  BatchID= 0 Loss: 0.527 | Acc: 83.594% (107/128)
[Train] Epoch= 8  BatchID= 10 Loss: 0.548 | Acc: 82.599% (1163/1408)
[Train] Epoch= 8  BatchID= 20 Loss: 0.551 | Acc: 82.031% (2205/2688)
[Train] Epoch= 8  BatchID= 30 Loss: 0.566 | Acc: 81.905% (3250/3968)
[Train] Epoch= 8  BatchID= 40 Loss: 0.590 | Acc: 80.907% (4246/5248)
[Train] Epoch= 8  BatchID= 50 Loss: 0.607 | Acc: 80.377% (5247/6528)
[Train] Epoch= 8  BatchID= 60 Loss: 0.610 | Acc: 80.187% (6261/7808)
[Train] Epoch= 8  BatchID= 70 Loss: 0.619 | Acc: 80.018% (7272/9088)
[Test] Epoch= 8  BatchID= 0 Loss: 0.417 | Acc: 90.625% (116/128)
[Test] Epoch= 8  BatchID= 10 Loss: 0.592 | Acc: 80.469% (1133/1408)
[Test] Epoch= 8  BatchID= 20 Loss: 0.751 | Acc: 76.265% (2050/2688)
[Test] Epoch= 8  BatchID= 30 Loss: 0.652 | Acc: 79.006% (3101/3925)

Epoch: 9
[Train] Epoch= 9  BatchID= 0 Loss: 0.781 | Acc: 71.094% (91/128)
[Train] Epoch= 9  BatchID= 10 Loss: 0.600 | Acc: 79.901% (1125/1408)
[Train] Epoch= 9  BatchID= 20 Loss: 0.607 | Acc: 80.097% (2153/2688)
[Train] Epoch= 9  BatchID= 30 Loss: 0.613 | Acc: 79.713% (3163/3968)
[Train] Epoch= 9  BatchID= 40 Loss: 0.612 | Acc: 79.726% (4184/5248)
[Train] Epoch= 9  BatchID= 50 Loss: 0.615 | Acc: 79.749% (5206/6528)
[Train] Epoch= 9  BatchID= 60 Loss: 0.616 | Acc: 79.969% (6244/7808)
[Train] Epoch= 9  BatchID= 70 Loss: 0.611 | Acc: 80.172% (7286/9088)
[Test] Epoch= 9  BatchID= 0 Loss: 0.210 | Acc: 93.750% (120/128)
[Test] Epoch= 9  BatchID= 10 Loss: 0.593 | Acc: 80.540% (1134/1408)
[Test] Epoch= 9  BatchID= 20 Loss: 0.544 | Acc: 82.403% (2215/2688)
[Test] Epoch= 9  BatchID= 30 Loss: 0.677 | Acc: 78.446% (3079/3925)

Epoch: 10
[Train] Epoch= 10  BatchID= 0 Loss: 0.568 | Acc: 80.469% (103/128)
[Train] Epoch= 10  BatchID= 10 Loss: 0.573 | Acc: 81.392% (1146/1408)
[Train] Epoch= 10  BatchID= 20 Loss: 0.544 | Acc: 82.552% (2219/2688)
[Train] Epoch= 10  BatchID= 30 Loss: 0.548 | Acc: 82.132% (3259/3968)
[Train] Epoch= 10  BatchID= 40 Loss: 0.560 | Acc: 81.631% (4284/5248)
[Train] Epoch= 10  BatchID= 50 Loss: 0.567 | Acc: 81.327% (5309/6528)
[Train] Epoch= 10  BatchID= 60 Loss: 0.577 | Acc: 81.019% (6326/7808)
[Train] Epoch= 10  BatchID= 70 Loss: 0.580 | Acc: 81.030% (7364/9088)
[Test] Epoch= 10  BatchID= 0 Loss: 0.321 | Acc: 89.062% (114/128)
[Test] Epoch= 10  BatchID= 10 Loss: 0.495 | Acc: 83.381% (1174/1408)
[Test] Epoch= 10  BatchID= 20 Loss: 0.649 | Acc: 79.427% (2135/2688)
[Test] Epoch= 10  BatchID= 30 Loss: 0.654 | Acc: 78.904% (3097/3925)

Epoch: 11
[Train] Epoch= 11  BatchID= 0 Loss: 0.682 | Acc: 78.125% (100/128)
[Train] Epoch= 11  BatchID= 10 Loss: 0.560 | Acc: 81.463% (1147/1408)
[Train] Epoch= 11  BatchID= 20 Loss: 0.560 | Acc: 80.804% (2172/2688)
[Train] Epoch= 11  BatchID= 30 Loss: 0.564 | Acc: 80.897% (3210/3968)
[Train] Epoch= 11  BatchID= 40 Loss: 0.570 | Acc: 80.850% (4243/5248)
[Train] Epoch= 11  BatchID= 50 Loss: 0.571 | Acc: 80.959% (5285/6528)
[Train] Epoch= 11  BatchID= 60 Loss: 0.575 | Acc: 81.135% (6335/7808)
[Train] Epoch= 11  BatchID= 70 Loss: 0.578 | Acc: 81.008% (7362/9088)
[Test] Epoch= 11  BatchID= 0 Loss: 0.208 | Acc: 92.969% (119/128)
[Test] Epoch= 11  BatchID= 10 Loss: 0.509 | Acc: 83.381% (1174/1408)
[Test] Epoch= 11  BatchID= 20 Loss: 0.559 | Acc: 82.143% (2208/2688)
[Test] Epoch= 11  BatchID= 30 Loss: 0.655 | Acc: 79.261% (3111/3925)

Epoch: 12
[Train] Epoch= 12  BatchID= 0 Loss: 0.577 | Acc: 79.688% (102/128)
[Train] Epoch= 12  BatchID= 10 Loss: 0.621 | Acc: 79.545% (1120/1408)
[Train] Epoch= 12  BatchID= 20 Loss: 0.607 | Acc: 80.432% (2162/2688)
[Train] Epoch= 12  BatchID= 30 Loss: 0.606 | Acc: 79.940% (3172/3968)
[Train] Epoch= 12  BatchID= 40 Loss: 0.589 | Acc: 80.488% (4224/5248)
[Train] Epoch= 12  BatchID= 50 Loss: 0.587 | Acc: 80.668% (5266/6528)
[Train] Epoch= 12  BatchID= 60 Loss: 0.596 | Acc: 80.648% (6297/7808)
[Train] Epoch= 12  BatchID= 70 Loss: 0.594 | Acc: 80.700% (7334/9088)
[Test] Epoch= 12  BatchID= 0 Loss: 0.442 | Acc: 86.719% (111/128)
[Test] Epoch= 12  BatchID= 10 Loss: 0.600 | Acc: 80.185% (1129/1408)
[Test] Epoch= 12  BatchID= 20 Loss: 0.602 | Acc: 80.171% (2155/2688)
[Test] Epoch= 12  BatchID= 30 Loss: 0.586 | Acc: 80.917% (3176/3925)

Epoch: 13
[Train] Epoch= 13  BatchID= 0 Loss: 0.535 | Acc: 82.812% (106/128)
[Train] Epoch= 13  BatchID= 10 Loss: 0.581 | Acc: 81.534% (1148/1408)
[Train] Epoch= 13  BatchID= 20 Loss: 0.587 | Acc: 81.585% (2193/2688)
[Train] Epoch= 13  BatchID= 30 Loss: 0.583 | Acc: 81.678% (3241/3968)
[Train] Epoch= 13  BatchID= 40 Loss: 0.568 | Acc: 82.127% (4310/5248)
[Train] Epoch= 13  BatchID= 50 Loss: 0.576 | Acc: 81.664% (5331/6528)
[Train] Epoch= 13  BatchID= 60 Loss: 0.576 | Acc: 81.557% (6368/7808)
[Train] Epoch= 13  BatchID= 70 Loss: 0.588 | Acc: 80.964% (7358/9088)
[Test] Epoch= 13  BatchID= 0 Loss: 0.118 | Acc: 96.875% (124/128)
[Test] Epoch= 13  BatchID= 10 Loss: 0.465 | Acc: 85.511% (1204/1408)
[Test] Epoch= 13  BatchID= 20 Loss: 0.517 | Acc: 84.152% (2262/2688)
[Test] Epoch= 13  BatchID= 30 Loss: 0.555 | Acc: 82.599% (3242/3925)

Epoch: 14
[Train] Epoch= 14  BatchID= 0 Loss: 0.674 | Acc: 78.906% (101/128)
[Train] Epoch= 14  BatchID= 10 Loss: 0.516 | Acc: 83.097% (1170/1408)
[Train] Epoch= 14  BatchID= 20 Loss: 0.519 | Acc: 82.961% (2230/2688)
[Train] Epoch= 14  BatchID= 30 Loss: 0.538 | Acc: 82.510% (3274/3968)
[Train] Epoch= 14  BatchID= 40 Loss: 0.542 | Acc: 82.222% (4315/5248)
[Train] Epoch= 14  BatchID= 50 Loss: 0.555 | Acc: 81.817% (5341/6528)
[Train] Epoch= 14  BatchID= 60 Loss: 0.567 | Acc: 81.698% (6379/7808)
[Train] Epoch= 14  BatchID= 70 Loss: 0.565 | Acc: 81.701% (7425/9088)
[Test] Epoch= 14  BatchID= 0 Loss: 0.354 | Acc: 89.062% (114/128)
[Test] Epoch= 14  BatchID= 10 Loss: 0.471 | Acc: 84.801% (1194/1408)
[Test] Epoch= 14  BatchID= 20 Loss: 0.511 | Acc: 84.301% (2266/2688)
[Test] Epoch= 14  BatchID= 30 Loss: 0.616 | Acc: 81.172% (3186/3925)

Epoch: 15
[Train] Epoch= 15  BatchID= 0 Loss: 0.646 | Acc: 80.469% (103/128)
[Train] Epoch= 15  BatchID= 10 Loss: 0.552 | Acc: 82.315% (1159/1408)
[Train] Epoch= 15  BatchID= 20 Loss: 0.546 | Acc: 82.143% (2208/2688)
[Train] Epoch= 15  BatchID= 30 Loss: 0.545 | Acc: 81.678% (3241/3968)
[Train] Epoch= 15  BatchID= 40 Loss: 0.546 | Acc: 81.841% (4295/5248)
[Train] Epoch= 15  BatchID= 50 Loss: 0.563 | Acc: 81.587% (5326/6528)
[Train] Epoch= 15  BatchID= 60 Loss: 0.565 | Acc: 81.416% (6357/7808)
[Train] Epoch= 15  BatchID= 70 Loss: 0.567 | Acc: 81.272% (7386/9088)
[Test] Epoch= 15  BatchID= 0 Loss: 0.442 | Acc: 90.625% (116/128)
[Test] Epoch= 15  BatchID= 10 Loss: 0.544 | Acc: 82.528% (1162/1408)
[Test] Epoch= 15  BatchID= 20 Loss: 0.571 | Acc: 82.254% (2211/2688)
[Test] Epoch= 15  BatchID= 30 Loss: 0.558 | Acc: 82.420% (3235/3925)

Epoch: 16
[Train] Epoch= 16  BatchID= 0 Loss: 0.457 | Acc: 86.719% (111/128)
[Train] Epoch= 16  BatchID= 10 Loss: 0.518 | Acc: 82.741% (1165/1408)
[Train] Epoch= 16  BatchID= 20 Loss: 0.511 | Acc: 83.371% (2241/2688)
[Train] Epoch= 16  BatchID= 30 Loss: 0.529 | Acc: 82.888% (3289/3968)
[Train] Epoch= 16  BatchID= 40 Loss: 0.550 | Acc: 82.222% (4315/5248)
[Train] Epoch= 16  BatchID= 50 Loss: 0.560 | Acc: 81.878% (5345/6528)
[Train] Epoch= 16  BatchID= 60 Loss: 0.560 | Acc: 81.749% (6383/7808)
[Train] Epoch= 16  BatchID= 70 Loss: 0.562 | Acc: 81.668% (7422/9088)
[Test] Epoch= 16  BatchID= 0 Loss: 0.187 | Acc: 95.312% (122/128)
[Test] Epoch= 16  BatchID= 10 Loss: 0.389 | Acc: 87.145% (1227/1408)
[Test] Epoch= 16  BatchID= 20 Loss: 0.431 | Acc: 86.235% (2318/2688)
[Test] Epoch= 16  BatchID= 30 Loss: 0.461 | Acc: 85.197% (3344/3925)
Saving..
Best accuracy:  85.19745222929936

Epoch: 17
[Train] Epoch= 17  BatchID= 0 Loss: 0.598 | Acc: 80.469% (103/128)
[Train] Epoch= 17  BatchID= 10 Loss: 0.506 | Acc: 83.594% (1177/1408)
[Train] Epoch= 17  BatchID= 20 Loss: 0.545 | Acc: 82.180% (2209/2688)
[Train] Epoch= 17  BatchID= 30 Loss: 0.532 | Acc: 82.359% (3268/3968)
[Train] Epoch= 17  BatchID= 40 Loss: 0.541 | Acc: 82.050% (4306/5248)
[Train] Epoch= 17  BatchID= 50 Loss: 0.542 | Acc: 82.154% (5363/6528)
[Train] Epoch= 17  BatchID= 60 Loss: 0.547 | Acc: 82.095% (6410/7808)
[Train] Epoch= 17  BatchID= 70 Loss: 0.555 | Acc: 81.877% (7441/9088)
[Test] Epoch= 17  BatchID= 0 Loss: 0.074 | Acc: 97.656% (125/128)
[Test] Epoch= 17  BatchID= 10 Loss: 0.470 | Acc: 83.452% (1175/1408)
[Test] Epoch= 17  BatchID= 20 Loss: 0.556 | Acc: 81.473% (2190/2688)
[Test] Epoch= 17  BatchID= 30 Loss: 0.623 | Acc: 79.236% (3110/3925)

Epoch: 18
[Train] Epoch= 18  BatchID= 0 Loss: 0.497 | Acc: 85.156% (109/128)
[Train] Epoch= 18  BatchID= 10 Loss: 0.478 | Acc: 84.872% (1195/1408)
[Train] Epoch= 18  BatchID= 20 Loss: 0.538 | Acc: 82.738% (2224/2688)
[Train] Epoch= 18  BatchID= 30 Loss: 0.550 | Acc: 82.359% (3268/3968)
[Train] Epoch= 18  BatchID= 40 Loss: 0.557 | Acc: 82.050% (4306/5248)
[Train] Epoch= 18  BatchID= 50 Loss: 0.573 | Acc: 81.725% (5335/6528)
[Train] Epoch= 18  BatchID= 60 Loss: 0.574 | Acc: 81.352% (6352/7808)
[Train] Epoch= 18  BatchID= 70 Loss: 0.573 | Acc: 81.525% (7409/9088)
[Test] Epoch= 18  BatchID= 0 Loss: 0.053 | Acc: 97.656% (125/128)
[Test] Epoch= 18  BatchID= 10 Loss: 0.436 | Acc: 85.938% (1210/1408)
[Test] Epoch= 18  BatchID= 20 Loss: 0.487 | Acc: 84.263% (2265/2688)
[Test] Epoch= 18  BatchID= 30 Loss: 0.594 | Acc: 82.115% (3223/3925)

Epoch: 19
[Train] Epoch= 19  BatchID= 0 Loss: 0.518 | Acc: 82.812% (106/128)
[Train] Epoch= 19  BatchID= 10 Loss: 0.529 | Acc: 82.457% (1161/1408)
[Train] Epoch= 19  BatchID= 20 Loss: 0.520 | Acc: 82.999% (2231/2688)
[Train] Epoch= 19  BatchID= 30 Loss: 0.535 | Acc: 82.485% (3273/3968)
[Train] Epoch= 19  BatchID= 40 Loss: 0.548 | Acc: 81.936% (4300/5248)
[Train] Epoch= 19  BatchID= 50 Loss: 0.553 | Acc: 81.909% (5347/6528)
[Train] Epoch= 19  BatchID= 60 Loss: 0.559 | Acc: 81.865% (6392/7808)
[Train] Epoch= 19  BatchID= 70 Loss: 0.556 | Acc: 82.163% (7467/9088)
[Test] Epoch= 19  BatchID= 0 Loss: 0.140 | Acc: 96.094% (123/128)
[Test] Epoch= 19  BatchID= 10 Loss: 0.497 | Acc: 83.736% (1179/1408)
[Test] Epoch= 19  BatchID= 20 Loss: 0.537 | Acc: 82.292% (2212/2688)
[Test] Epoch= 19  BatchID= 30 Loss: 0.570 | Acc: 81.325% (3192/3925)

Epoch: 20
[Train] Epoch= 20  BatchID= 0 Loss: 0.565 | Acc: 85.938% (110/128)
[Train] Epoch= 20  BatchID= 10 Loss: 0.556 | Acc: 83.026% (1169/1408)
[Train] Epoch= 20  BatchID= 20 Loss: 0.548 | Acc: 82.850% (2227/2688)
[Train] Epoch= 20  BatchID= 30 Loss: 0.549 | Acc: 82.787% (3285/3968)
[Train] Epoch= 20  BatchID= 40 Loss: 0.551 | Acc: 82.565% (4333/5248)
[Train] Epoch= 20  BatchID= 50 Loss: 0.545 | Acc: 82.767% (5403/6528)
[Train] Epoch= 20  BatchID= 60 Loss: 0.545 | Acc: 82.633% (6452/7808)
[Train] Epoch= 20  BatchID= 70 Loss: 0.547 | Acc: 82.592% (7506/9088)
[Test] Epoch= 20  BatchID= 0 Loss: 0.247 | Acc: 92.969% (119/128)
[Test] Epoch= 20  BatchID= 10 Loss: 0.423 | Acc: 86.648% (1220/1408)
[Test] Epoch= 20  BatchID= 20 Loss: 0.460 | Acc: 85.342% (2294/2688)
[Test] Epoch= 20  BatchID= 30 Loss: 0.462 | Acc: 85.070% (3339/3925)

Epoch: 21
[Train] Epoch= 21  BatchID= 0 Loss: 0.607 | Acc: 79.688% (102/128)
[Train] Epoch= 21  BatchID= 10 Loss: 0.575 | Acc: 80.895% (1139/1408)
[Train] Epoch= 21  BatchID= 20 Loss: 0.530 | Acc: 82.254% (2211/2688)
[Train] Epoch= 21  BatchID= 30 Loss: 0.529 | Acc: 82.157% (3260/3968)
[Train] Epoch= 21  BatchID= 40 Loss: 0.529 | Acc: 82.336% (4321/5248)
[Train] Epoch= 21  BatchID= 50 Loss: 0.537 | Acc: 82.491% (5385/6528)
[Train] Epoch= 21  BatchID= 60 Loss: 0.555 | Acc: 81.890% (6394/7808)
[Train] Epoch= 21  BatchID= 70 Loss: 0.561 | Acc: 81.646% (7420/9088)
[Test] Epoch= 21  BatchID= 0 Loss: 0.245 | Acc: 94.531% (121/128)
[Test] Epoch= 21  BatchID= 10 Loss: 0.439 | Acc: 86.364% (1216/1408)
[Test] Epoch= 21  BatchID= 20 Loss: 0.483 | Acc: 84.933% (2283/2688)
[Test] Epoch= 21  BatchID= 30 Loss: 0.533 | Acc: 83.134% (3263/3925)

Epoch: 22
[Train] Epoch= 22  BatchID= 0 Loss: 0.551 | Acc: 84.375% (108/128)
[Train] Epoch= 22  BatchID= 10 Loss: 0.552 | Acc: 82.528% (1162/1408)
[Train] Epoch= 22  BatchID= 20 Loss: 0.540 | Acc: 82.775% (2225/2688)
[Train] Epoch= 22  BatchID= 30 Loss: 0.538 | Acc: 82.686% (3281/3968)
[Train] Epoch= 22  BatchID= 40 Loss: 0.533 | Acc: 83.022% (4357/5248)
[Train] Epoch= 22  BatchID= 50 Loss: 0.544 | Acc: 82.521% (5387/6528)
[Train] Epoch= 22  BatchID= 60 Loss: 0.539 | Acc: 82.556% (6446/7808)
[Train] Epoch= 22  BatchID= 70 Loss: 0.541 | Acc: 82.548% (7502/9088)
[Test] Epoch= 22  BatchID= 0 Loss: 0.072 | Acc: 96.875% (124/128)
[Test] Epoch= 22  BatchID= 10 Loss: 0.411 | Acc: 85.866% (1209/1408)
[Test] Epoch= 22  BatchID= 20 Loss: 0.498 | Acc: 83.817% (2253/2688)
[Test] Epoch= 22  BatchID= 30 Loss: 0.543 | Acc: 82.217% (3227/3925)

Epoch: 23
[Train] Epoch= 23  BatchID= 0 Loss: 0.508 | Acc: 82.031% (105/128)
[Train] Epoch= 23  BatchID= 10 Loss: 0.504 | Acc: 83.310% (1173/1408)
[Train] Epoch= 23  BatchID= 20 Loss: 0.499 | Acc: 83.185% (2236/2688)
[Train] Epoch= 23  BatchID= 30 Loss: 0.512 | Acc: 82.964% (3292/3968)
[Train] Epoch= 23  BatchID= 40 Loss: 0.526 | Acc: 82.793% (4345/5248)
[Train] Epoch= 23  BatchID= 50 Loss: 0.526 | Acc: 82.812% (5406/6528)
[Train] Epoch= 23  BatchID= 60 Loss: 0.530 | Acc: 82.723% (6459/7808)
[Train] Epoch= 23  BatchID= 70 Loss: 0.538 | Acc: 82.416% (7490/9088)
[Test] Epoch= 23  BatchID= 0 Loss: 0.354 | Acc: 89.062% (114/128)
[Test] Epoch= 23  BatchID= 10 Loss: 0.608 | Acc: 81.108% (1142/1408)
[Test] Epoch= 23  BatchID= 20 Loss: 0.549 | Acc: 82.850% (2227/2688)
[Test] Epoch= 23  BatchID= 30 Loss: 0.706 | Acc: 79.414% (3117/3925)

Epoch: 24
[Train] Epoch= 24  BatchID= 0 Loss: 0.454 | Acc: 85.938% (110/128)
[Train] Epoch= 24  BatchID= 10 Loss: 0.477 | Acc: 84.659% (1192/1408)
[Train] Epoch= 24  BatchID= 20 Loss: 0.496 | Acc: 84.375% (2268/2688)
[Train] Epoch= 24  BatchID= 30 Loss: 0.510 | Acc: 83.720% (3322/3968)
[Train] Epoch= 24  BatchID= 40 Loss: 0.525 | Acc: 83.232% (4368/5248)
[Train] Epoch= 24  BatchID= 50 Loss: 0.533 | Acc: 83.012% (5419/6528)
[Train] Epoch= 24  BatchID= 60 Loss: 0.522 | Acc: 83.171% (6494/7808)
[Train] Epoch= 24  BatchID= 70 Loss: 0.528 | Acc: 82.967% (7540/9088)
[Test] Epoch= 24  BatchID= 0 Loss: 0.445 | Acc: 86.719% (111/128)
[Test] Epoch= 24  BatchID= 10 Loss: 0.554 | Acc: 81.818% (1152/1408)
[Test] Epoch= 24  BatchID= 20 Loss: 0.565 | Acc: 81.994% (2204/2688)
[Test] Epoch= 24  BatchID= 30 Loss: 0.564 | Acc: 82.064% (3221/3925)

Epoch: 25
[Train] Epoch= 25  BatchID= 0 Loss: 0.296 | Acc: 90.625% (116/128)
[Train] Epoch= 25  BatchID= 10 Loss: 0.566 | Acc: 82.031% (1155/1408)
[Train] Epoch= 25  BatchID= 20 Loss: 0.536 | Acc: 82.552% (2219/2688)
[Train] Epoch= 25  BatchID= 30 Loss: 0.534 | Acc: 82.712% (3282/3968)
[Train] Epoch= 25  BatchID= 40 Loss: 0.536 | Acc: 82.679% (4339/5248)
[Train] Epoch= 25  BatchID= 50 Loss: 0.541 | Acc: 82.460% (5383/6528)
[Train] Epoch= 25  BatchID= 60 Loss: 0.538 | Acc: 82.620% (6451/7808)
[Train] Epoch= 25  BatchID= 70 Loss: 0.534 | Acc: 82.746% (7520/9088)
[Test] Epoch= 25  BatchID= 0 Loss: 0.187 | Acc: 95.312% (122/128)
[Test] Epoch= 25  BatchID= 10 Loss: 0.384 | Acc: 87.500% (1232/1408)
[Test] Epoch= 25  BatchID= 20 Loss: 0.396 | Acc: 87.351% (2348/2688)
[Test] Epoch= 25  BatchID= 30 Loss: 0.508 | Acc: 84.000% (3297/3925)

Epoch: 26
[Train] Epoch= 26  BatchID= 0 Loss: 0.472 | Acc: 79.688% (102/128)
[Train] Epoch= 26  BatchID= 10 Loss: 0.469 | Acc: 84.162% (1185/1408)
[Train] Epoch= 26  BatchID= 20 Loss: 0.497 | Acc: 83.780% (2252/2688)
[Train] Epoch= 26  BatchID= 30 Loss: 0.500 | Acc: 83.846% (3327/3968)
[Train] Epoch= 26  BatchID= 40 Loss: 0.505 | Acc: 83.537% (4384/5248)
[Train] Epoch= 26  BatchID= 50 Loss: 0.503 | Acc: 83.900% (5477/6528)
[Train] Epoch= 26  BatchID= 60 Loss: 0.514 | Acc: 83.555% (6524/7808)
[Train] Epoch= 26  BatchID= 70 Loss: 0.517 | Acc: 83.451% (7584/9088)
[Test] Epoch= 26  BatchID= 0 Loss: 0.310 | Acc: 91.406% (117/128)
[Test] Epoch= 26  BatchID= 10 Loss: 0.756 | Acc: 76.491% (1077/1408)
[Test] Epoch= 26  BatchID= 20 Loss: 0.595 | Acc: 81.585% (2193/2688)
[Test] Epoch= 26  BatchID= 30 Loss: 0.619 | Acc: 80.662% (3166/3925)

Epoch: 27
[Train] Epoch= 27  BatchID= 0 Loss: 0.451 | Acc: 83.594% (107/128)
[Train] Epoch= 27  BatchID= 10 Loss: 0.460 | Acc: 85.085% (1198/1408)
[Train] Epoch= 27  BatchID= 20 Loss: 0.474 | Acc: 84.524% (2272/2688)
[Train] Epoch= 27  BatchID= 30 Loss: 0.506 | Acc: 83.417% (3310/3968)
[Train] Epoch= 27  BatchID= 40 Loss: 0.515 | Acc: 83.117% (4362/5248)
[Train] Epoch= 27  BatchID= 50 Loss: 0.519 | Acc: 83.134% (5427/6528)
[Train] Epoch= 27  BatchID= 60 Loss: 0.512 | Acc: 83.274% (6502/7808)
[Train] Epoch= 27  BatchID= 70 Loss: 0.510 | Acc: 83.275% (7568/9088)
[Test] Epoch= 27  BatchID= 0 Loss: 0.353 | Acc: 90.625% (116/128)
[Test] Epoch= 27  BatchID= 10 Loss: 0.493 | Acc: 84.162% (1185/1408)
[Test] Epoch= 27  BatchID= 20 Loss: 0.555 | Acc: 82.143% (2208/2688)
[Test] Epoch= 27  BatchID= 30 Loss: 0.533 | Acc: 83.185% (3265/3925)

Epoch: 28
[Train] Epoch= 28  BatchID= 0 Loss: 0.552 | Acc: 82.812% (106/128)
[Train] Epoch= 28  BatchID= 10 Loss: 0.485 | Acc: 83.310% (1173/1408)
[Train] Epoch= 28  BatchID= 20 Loss: 0.486 | Acc: 83.296% (2239/2688)
[Train] Epoch= 28  BatchID= 30 Loss: 0.496 | Acc: 83.443% (3311/3968)
[Train] Epoch= 28  BatchID= 40 Loss: 0.508 | Acc: 83.232% (4368/5248)
[Train] Epoch= 28  BatchID= 50 Loss: 0.514 | Acc: 83.318% (5439/6528)
[Train] Epoch= 28  BatchID= 60 Loss: 0.508 | Acc: 83.530% (6522/7808)
[Train] Epoch= 28  BatchID= 70 Loss: 0.511 | Acc: 83.495% (7588/9088)
[Test] Epoch= 28  BatchID= 0 Loss: 0.196 | Acc: 92.188% (118/128)
[Test] Epoch= 28  BatchID= 10 Loss: 0.304 | Acc: 89.062% (1254/1408)
[Test] Epoch= 28  BatchID= 20 Loss: 0.395 | Acc: 86.756% (2332/2688)
[Test] Epoch= 28  BatchID= 30 Loss: 0.481 | Acc: 83.439% (3275/3925)

Epoch: 29
[Train] Epoch= 29  BatchID= 0 Loss: 0.472 | Acc: 84.375% (108/128)
[Train] Epoch= 29  BatchID= 10 Loss: 0.477 | Acc: 84.730% (1193/1408)
[Train] Epoch= 29  BatchID= 20 Loss: 0.459 | Acc: 85.491% (2298/2688)
[Train] Epoch= 29  BatchID= 30 Loss: 0.472 | Acc: 84.778% (3364/3968)
[Train] Epoch= 29  BatchID= 40 Loss: 0.479 | Acc: 84.604% (4440/5248)
[Train] Epoch= 29  BatchID= 50 Loss: 0.479 | Acc: 84.329% (5505/6528)
[Train] Epoch= 29  BatchID= 60 Loss: 0.479 | Acc: 84.349% (6586/7808)
[Train] Epoch= 29  BatchID= 70 Loss: 0.481 | Acc: 84.364% (7667/9088)
[Test] Epoch= 29  BatchID= 0 Loss: 0.271 | Acc: 92.969% (119/128)
[Test] Epoch= 29  BatchID= 10 Loss: 0.390 | Acc: 87.074% (1226/1408)
[Test] Epoch= 29  BatchID= 20 Loss: 0.396 | Acc: 87.351% (2348/2688)
[Test] Epoch= 29  BatchID= 30 Loss: 0.444 | Acc: 85.707% (3364/3925)
Saving..
Best accuracy:  85.70700636942675

Epoch: 30
[Train] Epoch= 30  BatchID= 0 Loss: 0.321 | Acc: 91.406% (117/128)
[Train] Epoch= 30  BatchID= 10 Loss: 0.518 | Acc: 83.381% (1174/1408)
[Train] Epoch= 30  BatchID= 20 Loss: 0.529 | Acc: 83.147% (2235/2688)
[Train] Epoch= 30  BatchID= 30 Loss: 0.519 | Acc: 83.241% (3303/3968)
[Train] Epoch= 30  BatchID= 40 Loss: 0.508 | Acc: 83.518% (4383/5248)
[Train] Epoch= 30  BatchID= 50 Loss: 0.503 | Acc: 83.900% (5477/6528)
[Train] Epoch= 30  BatchID= 60 Loss: 0.498 | Acc: 84.170% (6572/7808)
[Train] Epoch= 30  BatchID= 70 Loss: 0.505 | Acc: 83.858% (7621/9088)
[Test] Epoch= 30  BatchID= 0 Loss: 0.269 | Acc: 94.531% (121/128)
[Test] Epoch= 30  BatchID= 10 Loss: 0.619 | Acc: 80.469% (1133/1408)
[Test] Epoch= 30  BatchID= 20 Loss: 0.635 | Acc: 79.874% (2147/2688)
[Test] Epoch= 30  BatchID= 30 Loss: 0.573 | Acc: 81.809% (3211/3925)

Epoch: 31
[Train] Epoch= 31  BatchID= 0 Loss: 0.558 | Acc: 79.688% (102/128)
[Train] Epoch= 31  BatchID= 10 Loss: 0.470 | Acc: 85.014% (1197/1408)
[Train] Epoch= 31  BatchID= 20 Loss: 0.481 | Acc: 84.598% (2274/2688)
[Train] Epoch= 31  BatchID= 30 Loss: 0.508 | Acc: 83.493% (3313/3968)
[Train] Epoch= 31  BatchID= 40 Loss: 0.499 | Acc: 83.956% (4406/5248)
[Train] Epoch= 31  BatchID= 50 Loss: 0.496 | Acc: 83.854% (5474/6528)
[Train] Epoch= 31  BatchID= 60 Loss: 0.493 | Acc: 84.042% (6562/7808)
[Train] Epoch= 31  BatchID= 70 Loss: 0.494 | Acc: 83.935% (7628/9088)
[Test] Epoch= 31  BatchID= 0 Loss: 0.225 | Acc: 94.531% (121/128)
[Test] Epoch= 31  BatchID= 10 Loss: 0.440 | Acc: 85.938% (1210/1408)
[Test] Epoch= 31  BatchID= 20 Loss: 0.429 | Acc: 86.496% (2325/2688)
[Test] Epoch= 31  BatchID= 30 Loss: 0.452 | Acc: 85.936% (3373/3925)
Saving..
Best accuracy:  85.93630573248407

Epoch: 32
[Train] Epoch= 32  BatchID= 0 Loss: 0.405 | Acc: 88.281% (113/128)
[Train] Epoch= 32  BatchID= 10 Loss: 0.454 | Acc: 84.943% (1196/1408)
[Train] Epoch= 32  BatchID= 20 Loss: 0.453 | Acc: 84.710% (2277/2688)
[Train] Epoch= 32  BatchID= 30 Loss: 0.455 | Acc: 84.904% (3369/3968)
[Train] Epoch= 32  BatchID= 40 Loss: 0.472 | Acc: 84.661% (4443/5248)
[Train] Epoch= 32  BatchID= 50 Loss: 0.481 | Acc: 84.498% (5516/6528)
[Train] Epoch= 32  BatchID= 60 Loss: 0.484 | Acc: 84.349% (6586/7808)
[Train] Epoch= 32  BatchID= 70 Loss: 0.487 | Acc: 84.243% (7656/9088)
[Test] Epoch= 32  BatchID= 0 Loss: 0.212 | Acc: 94.531% (121/128)
[Test] Epoch= 32  BatchID= 10 Loss: 0.674 | Acc: 79.119% (1114/1408)
[Test] Epoch= 32  BatchID= 20 Loss: 0.703 | Acc: 78.311% (2105/2688)
[Test] Epoch= 32  BatchID= 30 Loss: 0.584 | Acc: 81.758% (3209/3925)

Epoch: 33
[Train] Epoch= 33  BatchID= 0 Loss: 0.617 | Acc: 82.812% (106/128)
[Train] Epoch= 33  BatchID= 10 Loss: 0.475 | Acc: 84.872% (1195/1408)
[Train] Epoch= 33  BatchID= 20 Loss: 0.478 | Acc: 83.817% (2253/2688)
[Train] Epoch= 33  BatchID= 30 Loss: 0.492 | Acc: 83.443% (3311/3968)
[Train] Epoch= 33  BatchID= 40 Loss: 0.484 | Acc: 83.899% (4403/5248)
[Train] Epoch= 33  BatchID= 50 Loss: 0.492 | Acc: 83.609% (5458/6528)
[Train] Epoch= 33  BatchID= 60 Loss: 0.485 | Acc: 84.119% (6568/7808)
[Train] Epoch= 33  BatchID= 70 Loss: 0.481 | Acc: 84.144% (7647/9088)
[Test] Epoch= 33  BatchID= 0 Loss: 0.091 | Acc: 96.094% (123/128)
[Test] Epoch= 33  BatchID= 10 Loss: 0.305 | Acc: 89.986% (1267/1408)
[Test] Epoch= 33  BatchID= 20 Loss: 0.409 | Acc: 86.868% (2335/2688)
[Test] Epoch= 33  BatchID= 30 Loss: 0.481 | Acc: 84.841% (3330/3925)

Epoch: 34
[Train] Epoch= 34  BatchID= 0 Loss: 0.453 | Acc: 87.500% (112/128)
[Train] Epoch= 34  BatchID= 10 Loss: 0.425 | Acc: 86.719% (1221/1408)
[Train] Epoch= 34  BatchID= 20 Loss: 0.446 | Acc: 85.900% (2309/2688)
[Train] Epoch= 34  BatchID= 30 Loss: 0.450 | Acc: 85.257% (3383/3968)
[Train] Epoch= 34  BatchID= 40 Loss: 0.450 | Acc: 85.271% (4475/5248)
[Train] Epoch= 34  BatchID= 50 Loss: 0.462 | Acc: 84.911% (5543/6528)
[Train] Epoch= 34  BatchID= 60 Loss: 0.464 | Acc: 84.900% (6629/7808)
[Train] Epoch= 34  BatchID= 70 Loss: 0.462 | Acc: 84.870% (7713/9088)
[Test] Epoch= 34  BatchID= 0 Loss: 0.140 | Acc: 96.875% (124/128)
[Test] Epoch= 34  BatchID= 10 Loss: 0.560 | Acc: 82.173% (1157/1408)
[Test] Epoch= 34  BatchID= 20 Loss: 0.544 | Acc: 82.589% (2220/2688)
[Test] Epoch= 34  BatchID= 30 Loss: 0.589 | Acc: 80.764% (3170/3925)

Epoch: 35
[Train] Epoch= 35  BatchID= 0 Loss: 0.384 | Acc: 89.062% (114/128)
[Train] Epoch= 35  BatchID= 10 Loss: 0.475 | Acc: 84.872% (1195/1408)
[Train] Epoch= 35  BatchID= 20 Loss: 0.463 | Acc: 85.342% (2294/2688)
[Train] Epoch= 35  BatchID= 30 Loss: 0.441 | Acc: 86.114% (3417/3968)
[Train] Epoch= 35  BatchID= 40 Loss: 0.450 | Acc: 85.995% (4513/5248)
[Train] Epoch= 35  BatchID= 50 Loss: 0.452 | Acc: 85.800% (5601/6528)
[Train] Epoch= 35  BatchID= 60 Loss: 0.450 | Acc: 85.669% (6689/7808)
[Train] Epoch= 35  BatchID= 70 Loss: 0.452 | Acc: 85.552% (7775/9088)
[Test] Epoch= 35  BatchID= 0 Loss: 0.226 | Acc: 92.969% (119/128)
[Test] Epoch= 35  BatchID= 10 Loss: 0.480 | Acc: 84.943% (1196/1408)
[Test] Epoch= 35  BatchID= 20 Loss: 0.492 | Acc: 83.891% (2255/2688)
[Test] Epoch= 35  BatchID= 30 Loss: 0.526 | Acc: 83.108% (3262/3925)

Epoch: 36
[Train] Epoch= 36  BatchID= 0 Loss: 0.444 | Acc: 82.812% (106/128)
[Train] Epoch= 36  BatchID= 10 Loss: 0.436 | Acc: 85.511% (1204/1408)
[Train] Epoch= 36  BatchID= 20 Loss: 0.441 | Acc: 85.714% (2304/2688)
[Train] Epoch= 36  BatchID= 30 Loss: 0.441 | Acc: 85.837% (3406/3968)
[Train] Epoch= 36  BatchID= 40 Loss: 0.456 | Acc: 85.366% (4480/5248)
[Train] Epoch= 36  BatchID= 50 Loss: 0.454 | Acc: 85.463% (5579/6528)
[Train] Epoch= 36  BatchID= 60 Loss: 0.458 | Acc: 85.425% (6670/7808)
[Train] Epoch= 36  BatchID= 70 Loss: 0.458 | Acc: 85.442% (7765/9088)
[Test] Epoch= 36  BatchID= 0 Loss: 0.107 | Acc: 95.312% (122/128)
[Test] Epoch= 36  BatchID= 10 Loss: 0.322 | Acc: 89.134% (1255/1408)
[Test] Epoch= 36  BatchID= 20 Loss: 0.381 | Acc: 87.277% (2346/2688)
[Test] Epoch= 36  BatchID= 30 Loss: 0.446 | Acc: 85.452% (3354/3925)

Epoch: 37
[Train] Epoch= 37  BatchID= 0 Loss: 0.412 | Acc: 86.719% (111/128)
[Train] Epoch= 37  BatchID= 10 Loss: 0.408 | Acc: 87.074% (1226/1408)
[Train] Epoch= 37  BatchID= 20 Loss: 0.412 | Acc: 86.830% (2334/2688)
[Train] Epoch= 37  BatchID= 30 Loss: 0.412 | Acc: 86.719% (3441/3968)
[Train] Epoch= 37  BatchID= 40 Loss: 0.417 | Acc: 86.662% (4548/5248)
[Train] Epoch= 37  BatchID= 50 Loss: 0.416 | Acc: 86.749% (5663/6528)
[Train] Epoch= 37  BatchID= 60 Loss: 0.423 | Acc: 86.450% (6750/7808)
[Train] Epoch= 37  BatchID= 70 Loss: 0.427 | Acc: 86.389% (7851/9088)
[Test] Epoch= 37  BatchID= 0 Loss: 0.316 | Acc: 89.062% (114/128)
[Test] Epoch= 37  BatchID= 10 Loss: 0.427 | Acc: 86.648% (1220/1408)
[Test] Epoch= 37  BatchID= 20 Loss: 0.469 | Acc: 86.012% (2312/2688)
[Test] Epoch= 37  BatchID= 30 Loss: 0.484 | Acc: 85.045% (3338/3925)

Epoch: 38
[Train] Epoch= 38  BatchID= 0 Loss: 0.498 | Acc: 85.156% (109/128)
[Train] Epoch= 38  BatchID= 10 Loss: 0.419 | Acc: 87.003% (1225/1408)
[Train] Epoch= 38  BatchID= 20 Loss: 0.443 | Acc: 85.900% (2309/2688)
[Train] Epoch= 38  BatchID= 30 Loss: 0.441 | Acc: 85.912% (3409/3968)
[Train] Epoch= 38  BatchID= 40 Loss: 0.436 | Acc: 85.995% (4513/5248)
[Train] Epoch= 38  BatchID= 50 Loss: 0.431 | Acc: 86.106% (5621/6528)
[Train] Epoch= 38  BatchID= 60 Loss: 0.434 | Acc: 85.861% (6704/7808)
[Train] Epoch= 38  BatchID= 70 Loss: 0.437 | Acc: 85.585% (7778/9088)
[Test] Epoch= 38  BatchID= 0 Loss: 0.407 | Acc: 88.281% (113/128)
[Test] Epoch= 38  BatchID= 10 Loss: 0.485 | Acc: 84.588% (1191/1408)
[Test] Epoch= 38  BatchID= 20 Loss: 0.447 | Acc: 85.789% (2306/2688)
[Test] Epoch= 38  BatchID= 30 Loss: 0.491 | Acc: 84.204% (3305/3925)

Epoch: 39
[Train] Epoch= 39  BatchID= 0 Loss: 0.435 | Acc: 86.719% (111/128)
[Train] Epoch= 39  BatchID= 10 Loss: 0.433 | Acc: 85.227% (1200/1408)
[Train] Epoch= 39  BatchID= 20 Loss: 0.413 | Acc: 86.049% (2313/2688)
[Train] Epoch= 39  BatchID= 30 Loss: 0.416 | Acc: 86.038% (3414/3968)
[Train] Epoch= 39  BatchID= 40 Loss: 0.430 | Acc: 85.861% (4506/5248)
[Train] Epoch= 39  BatchID= 50 Loss: 0.440 | Acc: 85.570% (5586/6528)
[Train] Epoch= 39  BatchID= 60 Loss: 0.441 | Acc: 85.617% (6685/7808)
[Train] Epoch= 39  BatchID= 70 Loss: 0.439 | Acc: 85.706% (7789/9088)
[Test] Epoch= 39  BatchID= 0 Loss: 0.195 | Acc: 95.312% (122/128)
[Test] Epoch= 39  BatchID= 10 Loss: 0.351 | Acc: 88.565% (1247/1408)
[Test] Epoch= 39  BatchID= 20 Loss: 0.335 | Acc: 88.839% (2388/2688)
[Test] Epoch= 39  BatchID= 30 Loss: 0.372 | Acc: 87.924% (3451/3925)
Saving..
Best accuracy:  87.92356687898089

Epoch: 40
[Train] Epoch= 40  BatchID= 0 Loss: 0.498 | Acc: 85.938% (110/128)
[Train] Epoch= 40  BatchID= 10 Loss: 0.436 | Acc: 87.003% (1225/1408)
[Train] Epoch= 40  BatchID= 20 Loss: 0.431 | Acc: 86.793% (2333/2688)
[Train] Epoch= 40  BatchID= 30 Loss: 0.426 | Acc: 86.542% (3434/3968)
[Train] Epoch= 40  BatchID= 40 Loss: 0.423 | Acc: 86.700% (4550/5248)
[Train] Epoch= 40  BatchID= 50 Loss: 0.416 | Acc: 86.903% (5673/6528)
[Train] Epoch= 40  BatchID= 60 Loss: 0.415 | Acc: 86.885% (6784/7808)
[Train] Epoch= 40  BatchID= 70 Loss: 0.415 | Acc: 86.785% (7887/9088)
[Test] Epoch= 40  BatchID= 0 Loss: 0.196 | Acc: 93.750% (120/128)
[Test] Epoch= 40  BatchID= 10 Loss: 0.369 | Acc: 88.139% (1241/1408)
[Test] Epoch= 40  BatchID= 20 Loss: 0.418 | Acc: 86.570% (2327/2688)
[Test] Epoch= 40  BatchID= 30 Loss: 0.407 | Acc: 86.828% (3408/3925)

Epoch: 41
[Train] Epoch= 41  BatchID= 0 Loss: 0.611 | Acc: 80.469% (103/128)
[Train] Epoch= 41  BatchID= 10 Loss: 0.423 | Acc: 85.653% (1206/1408)
[Train] Epoch= 41  BatchID= 20 Loss: 0.417 | Acc: 86.049% (2313/2688)
[Train] Epoch= 41  BatchID= 30 Loss: 0.417 | Acc: 86.391% (3428/3968)
[Train] Epoch= 41  BatchID= 40 Loss: 0.413 | Acc: 86.471% (4538/5248)
[Train] Epoch= 41  BatchID= 50 Loss: 0.426 | Acc: 86.091% (5620/6528)
[Train] Epoch= 41  BatchID= 60 Loss: 0.421 | Acc: 86.373% (6744/7808)
[Train] Epoch= 41  BatchID= 70 Loss: 0.424 | Acc: 86.147% (7829/9088)
[Test] Epoch= 41  BatchID= 0 Loss: 0.187 | Acc: 95.312% (122/128)
[Test] Epoch= 41  BatchID= 10 Loss: 0.348 | Acc: 88.707% (1249/1408)
[Test] Epoch= 41  BatchID= 20 Loss: 0.377 | Acc: 87.984% (2365/2688)
[Test] Epoch= 41  BatchID= 30 Loss: 0.415 | Acc: 86.752% (3405/3925)

Epoch: 42
[Train] Epoch= 42  BatchID= 0 Loss: 0.393 | Acc: 87.500% (112/128)
[Train] Epoch= 42  BatchID= 10 Loss: 0.418 | Acc: 87.074% (1226/1408)
[Train] Epoch= 42  BatchID= 20 Loss: 0.413 | Acc: 87.091% (2341/2688)
[Train] Epoch= 42  BatchID= 30 Loss: 0.424 | Acc: 86.542% (3434/3968)
[Train] Epoch= 42  BatchID= 40 Loss: 0.444 | Acc: 86.052% (4516/5248)
[Train] Epoch= 42  BatchID= 50 Loss: 0.451 | Acc: 85.907% (5608/6528)
[Train] Epoch= 42  BatchID= 60 Loss: 0.457 | Acc: 85.720% (6693/7808)
[Train] Epoch= 42  BatchID= 70 Loss: 0.455 | Acc: 85.772% (7795/9088)
[Test] Epoch= 42  BatchID= 0 Loss: 0.124 | Acc: 96.875% (124/128)
[Test] Epoch= 42  BatchID= 10 Loss: 0.365 | Acc: 88.352% (1244/1408)
[Test] Epoch= 42  BatchID= 20 Loss: 0.383 | Acc: 88.021% (2366/2688)
[Test] Epoch= 42  BatchID= 30 Loss: 0.403 | Acc: 86.930% (3412/3925)

Epoch: 43
[Train] Epoch= 43  BatchID= 0 Loss: 0.320 | Acc: 90.625% (116/128)
[Train] Epoch= 43  BatchID= 10 Loss: 0.377 | Acc: 87.358% (1230/1408)
[Train] Epoch= 43  BatchID= 20 Loss: 0.382 | Acc: 87.351% (2348/2688)
[Train] Epoch= 43  BatchID= 30 Loss: 0.407 | Acc: 86.568% (3435/3968)
[Train] Epoch= 43  BatchID= 40 Loss: 0.400 | Acc: 86.966% (4564/5248)
[Train] Epoch= 43  BatchID= 50 Loss: 0.407 | Acc: 86.688% (5659/6528)
[Train] Epoch= 43  BatchID= 60 Loss: 0.414 | Acc: 86.565% (6759/7808)
[Train] Epoch= 43  BatchID= 70 Loss: 0.418 | Acc: 86.455% (7857/9088)
[Test] Epoch= 43  BatchID= 0 Loss: 0.226 | Acc: 94.531% (121/128)
[Test] Epoch= 43  BatchID= 10 Loss: 0.348 | Acc: 88.565% (1247/1408)
[Test] Epoch= 43  BatchID= 20 Loss: 0.388 | Acc: 87.872% (2362/2688)
[Test] Epoch= 43  BatchID= 30 Loss: 0.379 | Acc: 88.127% (3459/3925)
Saving..
Best accuracy:  88.12738853503184

Epoch: 44
[Train] Epoch= 44  BatchID= 0 Loss: 0.453 | Acc: 85.156% (109/128)
[Train] Epoch= 44  BatchID= 10 Loss: 0.370 | Acc: 87.642% (1234/1408)
[Train] Epoch= 44  BatchID= 20 Loss: 0.398 | Acc: 87.463% (2351/2688)
[Train] Epoch= 44  BatchID= 30 Loss: 0.391 | Acc: 87.601% (3476/3968)
[Train] Epoch= 44  BatchID= 40 Loss: 0.402 | Acc: 87.062% (4569/5248)
[Train] Epoch= 44  BatchID= 50 Loss: 0.396 | Acc: 87.331% (5701/6528)
[Train] Epoch= 44  BatchID= 60 Loss: 0.399 | Acc: 87.231% (6811/7808)
[Train] Epoch= 44  BatchID= 70 Loss: 0.396 | Acc: 87.291% (7933/9088)
[Test] Epoch= 44  BatchID= 0 Loss: 0.109 | Acc: 96.875% (124/128)
[Test] Epoch= 44  BatchID= 10 Loss: 0.462 | Acc: 84.801% (1194/1408)
[Test] Epoch= 44  BatchID= 20 Loss: 0.417 | Acc: 86.607% (2328/2688)
[Test] Epoch= 44  BatchID= 30 Loss: 0.422 | Acc: 86.599% (3399/3925)

Epoch: 45
[Train] Epoch= 45  BatchID= 0 Loss: 0.418 | Acc: 87.500% (112/128)
[Train] Epoch= 45  BatchID= 10 Loss: 0.388 | Acc: 87.642% (1234/1408)
[Train] Epoch= 45  BatchID= 20 Loss: 0.383 | Acc: 87.760% (2359/2688)
[Train] Epoch= 45  BatchID= 30 Loss: 0.379 | Acc: 87.954% (3490/3968)
[Train] Epoch= 45  BatchID= 40 Loss: 0.364 | Acc: 88.453% (4642/5248)
[Train] Epoch= 45  BatchID= 50 Loss: 0.379 | Acc: 88.174% (5756/6528)
[Train] Epoch= 45  BatchID= 60 Loss: 0.384 | Acc: 87.923% (6865/7808)
[Train] Epoch= 45  BatchID= 70 Loss: 0.389 | Acc: 87.742% (7974/9088)
[Test] Epoch= 45  BatchID= 0 Loss: 0.181 | Acc: 94.531% (121/128)
[Test] Epoch= 45  BatchID= 10 Loss: 0.363 | Acc: 87.926% (1238/1408)
[Test] Epoch= 45  BatchID= 20 Loss: 0.392 | Acc: 87.240% (2345/2688)
[Test] Epoch= 45  BatchID= 30 Loss: 0.459 | Acc: 85.376% (3351/3925)

Epoch: 46
[Train] Epoch= 46  BatchID= 0 Loss: 0.410 | Acc: 89.062% (114/128)
[Train] Epoch= 46  BatchID= 10 Loss: 0.385 | Acc: 87.500% (1232/1408)
[Train] Epoch= 46  BatchID= 20 Loss: 0.396 | Acc: 87.202% (2344/2688)
[Train] Epoch= 46  BatchID= 30 Loss: 0.405 | Acc: 87.147% (3458/3968)
[Train] Epoch= 46  BatchID= 40 Loss: 0.391 | Acc: 87.614% (4598/5248)
[Train] Epoch= 46  BatchID= 50 Loss: 0.387 | Acc: 87.714% (5726/6528)
[Train] Epoch= 46  BatchID= 60 Loss: 0.387 | Acc: 87.615% (6841/7808)
[Train] Epoch= 46  BatchID= 70 Loss: 0.390 | Acc: 87.621% (7963/9088)
[Test] Epoch= 46  BatchID= 0 Loss: 0.211 | Acc: 95.312% (122/128)
[Test] Epoch= 46  BatchID= 10 Loss: 0.397 | Acc: 86.932% (1224/1408)
[Test] Epoch= 46  BatchID= 20 Loss: 0.436 | Acc: 85.528% (2299/2688)
[Test] Epoch= 46  BatchID= 30 Loss: 0.424 | Acc: 85.707% (3364/3925)

Epoch: 47
[Train] Epoch= 47  BatchID= 0 Loss: 0.350 | Acc: 91.406% (117/128)
[Train] Epoch= 47  BatchID= 10 Loss: 0.404 | Acc: 86.506% (1218/1408)
[Train] Epoch= 47  BatchID= 20 Loss: 0.405 | Acc: 86.756% (2332/2688)
[Train] Epoch= 47  BatchID= 30 Loss: 0.408 | Acc: 87.046% (3454/3968)
[Train] Epoch= 47  BatchID= 40 Loss: 0.399 | Acc: 87.195% (4576/5248)
[Train] Epoch= 47  BatchID= 50 Loss: 0.400 | Acc: 87.025% (5681/6528)
[Train] Epoch= 47  BatchID= 60 Loss: 0.394 | Acc: 87.231% (6811/7808)
[Train] Epoch= 47  BatchID= 70 Loss: 0.398 | Acc: 87.192% (7924/9088)
[Test] Epoch= 47  BatchID= 0 Loss: 0.130 | Acc: 96.875% (124/128)
[Test] Epoch= 47  BatchID= 10 Loss: 0.420 | Acc: 86.364% (1216/1408)
[Test] Epoch= 47  BatchID= 20 Loss: 0.374 | Acc: 88.058% (2367/2688)
[Test] Epoch= 47  BatchID= 30 Loss: 0.372 | Acc: 88.306% (3466/3925)
Saving..
Best accuracy:  88.30573248407643

Epoch: 48
[Train] Epoch= 48  BatchID= 0 Loss: 0.394 | Acc: 87.500% (112/128)
[Train] Epoch= 48  BatchID= 10 Loss: 0.375 | Acc: 87.784% (1236/1408)
[Train] Epoch= 48  BatchID= 20 Loss: 0.372 | Acc: 88.021% (2366/2688)
[Train] Epoch= 48  BatchID= 30 Loss: 0.364 | Acc: 88.155% (3498/3968)
[Train] Epoch= 48  BatchID= 40 Loss: 0.361 | Acc: 88.377% (4638/5248)
[Train] Epoch= 48  BatchID= 50 Loss: 0.368 | Acc: 88.189% (5757/6528)
[Train] Epoch= 48  BatchID= 60 Loss: 0.374 | Acc: 87.999% (6871/7808)
[Train] Epoch= 48  BatchID= 70 Loss: 0.375 | Acc: 87.973% (7995/9088)
[Test] Epoch= 48  BatchID= 0 Loss: 0.072 | Acc: 97.656% (125/128)
[Test] Epoch= 48  BatchID= 10 Loss: 0.309 | Acc: 90.057% (1268/1408)
[Test] Epoch= 48  BatchID= 20 Loss: 0.313 | Acc: 89.955% (2418/2688)
[Test] Epoch= 48  BatchID= 30 Loss: 0.359 | Acc: 88.586% (3477/3925)
Saving..
Best accuracy:  88.5859872611465

Epoch: 49
[Train] Epoch= 49  BatchID= 0 Loss: 0.419 | Acc: 85.938% (110/128)
[Train] Epoch= 49  BatchID= 10 Loss: 0.367 | Acc: 87.855% (1237/1408)
[Train] Epoch= 49  BatchID= 20 Loss: 0.367 | Acc: 87.798% (2360/2688)
[Train] Epoch= 49  BatchID= 30 Loss: 0.366 | Acc: 88.105% (3496/3968)
[Train] Epoch= 49  BatchID= 40 Loss: 0.368 | Acc: 88.014% (4619/5248)
[Train] Epoch= 49  BatchID= 50 Loss: 0.373 | Acc: 87.914% (5739/6528)
[Train] Epoch= 49  BatchID= 60 Loss: 0.380 | Acc: 87.705% (6848/7808)
[Train] Epoch= 49  BatchID= 70 Loss: 0.382 | Acc: 87.610% (7962/9088)
[Test] Epoch= 49  BatchID= 0 Loss: 0.269 | Acc: 93.750% (120/128)
[Test] Epoch= 49  BatchID= 10 Loss: 0.434 | Acc: 87.500% (1232/1408)
[Test] Epoch= 49  BatchID= 20 Loss: 0.430 | Acc: 87.426% (2350/2688)
[Test] Epoch= 49  BatchID= 30 Loss: 0.486 | Acc: 85.682% (3363/3925)

Epoch: 50
[Train] Epoch= 50  BatchID= 0 Loss: 0.416 | Acc: 87.500% (112/128)
[Train] Epoch= 50  BatchID= 10 Loss: 0.386 | Acc: 87.997% (1239/1408)
[Train] Epoch= 50  BatchID= 20 Loss: 0.382 | Acc: 88.318% (2374/2688)
[Train] Epoch= 50  BatchID= 30 Loss: 0.382 | Acc: 87.727% (3481/3968)
[Train] Epoch= 50  BatchID= 40 Loss: 0.389 | Acc: 87.462% (4590/5248)
[Train] Epoch= 50  BatchID= 50 Loss: 0.383 | Acc: 87.883% (5737/6528)
[Train] Epoch= 50  BatchID= 60 Loss: 0.383 | Acc: 87.923% (6865/7808)
[Train] Epoch= 50  BatchID= 70 Loss: 0.390 | Acc: 87.632% (7964/9088)
[Test] Epoch= 50  BatchID= 0 Loss: 0.128 | Acc: 96.094% (123/128)
[Test] Epoch= 50  BatchID= 10 Loss: 0.369 | Acc: 88.352% (1244/1408)
[Test] Epoch= 50  BatchID= 20 Loss: 0.382 | Acc: 88.170% (2370/2688)
[Test] Epoch= 50  BatchID= 30 Loss: 0.416 | Acc: 86.904% (3411/3925)

Epoch: 51
[Train] Epoch= 51  BatchID= 0 Loss: 0.492 | Acc: 83.594% (107/128)
[Train] Epoch= 51  BatchID= 10 Loss: 0.393 | Acc: 87.358% (1230/1408)
[Train] Epoch= 51  BatchID= 20 Loss: 0.358 | Acc: 89.211% (2398/2688)
[Train] Epoch= 51  BatchID= 30 Loss: 0.374 | Acc: 88.810% (3524/3968)
[Train] Epoch= 51  BatchID= 40 Loss: 0.384 | Acc: 88.167% (4627/5248)
[Train] Epoch= 51  BatchID= 50 Loss: 0.386 | Acc: 88.128% (5753/6528)
[Train] Epoch= 51  BatchID= 60 Loss: 0.386 | Acc: 88.025% (6873/7808)
[Train] Epoch= 51  BatchID= 70 Loss: 0.383 | Acc: 88.072% (8004/9088)
[Test] Epoch= 51  BatchID= 0 Loss: 0.146 | Acc: 96.094% (123/128)
[Test] Epoch= 51  BatchID= 10 Loss: 0.319 | Acc: 89.276% (1257/1408)
[Test] Epoch= 51  BatchID= 20 Loss: 0.339 | Acc: 89.174% (2397/2688)
[Test] Epoch= 51  BatchID= 30 Loss: 0.359 | Acc: 88.586% (3477/3925)

Epoch: 52
[Train] Epoch= 52  BatchID= 0 Loss: 0.404 | Acc: 88.281% (113/128)
[Train] Epoch= 52  BatchID= 10 Loss: 0.382 | Acc: 87.429% (1231/1408)
[Train] Epoch= 52  BatchID= 20 Loss: 0.393 | Acc: 86.868% (2335/2688)
[Train] Epoch= 52  BatchID= 30 Loss: 0.377 | Acc: 87.651% (3478/3968)
[Train] Epoch= 52  BatchID= 40 Loss: 0.356 | Acc: 88.529% (4646/5248)
[Train] Epoch= 52  BatchID= 50 Loss: 0.353 | Acc: 88.680% (5789/6528)
[Train] Epoch= 52  BatchID= 60 Loss: 0.357 | Acc: 88.730% (6928/7808)
[Train] Epoch= 52  BatchID= 70 Loss: 0.356 | Acc: 88.721% (8063/9088)
[Test] Epoch= 52  BatchID= 0 Loss: 0.174 | Acc: 94.531% (121/128)
[Test] Epoch= 52  BatchID= 10 Loss: 0.286 | Acc: 90.980% (1281/1408)
[Test] Epoch= 52  BatchID= 20 Loss: 0.345 | Acc: 89.360% (2402/2688)
[Test] Epoch= 52  BatchID= 30 Loss: 0.379 | Acc: 88.204% (3462/3925)

Epoch: 53
[Train] Epoch= 53  BatchID= 0 Loss: 0.245 | Acc: 89.844% (115/128)
[Train] Epoch= 53  BatchID= 10 Loss: 0.333 | Acc: 89.418% (1259/1408)
[Train] Epoch= 53  BatchID= 20 Loss: 0.326 | Acc: 89.509% (2406/2688)
[Train] Epoch= 53  BatchID= 30 Loss: 0.326 | Acc: 89.516% (3552/3968)
[Train] Epoch= 53  BatchID= 40 Loss: 0.330 | Acc: 89.482% (4696/5248)
[Train] Epoch= 53  BatchID= 50 Loss: 0.333 | Acc: 89.354% (5833/6528)
[Train] Epoch= 53  BatchID= 60 Loss: 0.335 | Acc: 89.267% (6970/7808)
[Train] Epoch= 53  BatchID= 70 Loss: 0.337 | Acc: 89.107% (8098/9088)
[Test] Epoch= 53  BatchID= 0 Loss: 0.194 | Acc: 94.531% (121/128)
[Test] Epoch= 53  BatchID= 10 Loss: 0.307 | Acc: 90.199% (1270/1408)
[Test] Epoch= 53  BatchID= 20 Loss: 0.351 | Acc: 89.211% (2398/2688)
[Test] Epoch= 53  BatchID= 30 Loss: 0.378 | Acc: 88.280% (3465/3925)

Epoch: 54
[Train] Epoch= 54  BatchID= 0 Loss: 0.415 | Acc: 87.500% (112/128)
[Train] Epoch= 54  BatchID= 10 Loss: 0.325 | Acc: 89.844% (1265/1408)
[Train] Epoch= 54  BatchID= 20 Loss: 0.326 | Acc: 89.769% (2413/2688)
[Train] Epoch= 54  BatchID= 30 Loss: 0.323 | Acc: 89.718% (3560/3968)
[Train] Epoch= 54  BatchID= 40 Loss: 0.328 | Acc: 89.615% (4703/5248)
[Train] Epoch= 54  BatchID= 50 Loss: 0.329 | Acc: 89.568% (5847/6528)
[Train] Epoch= 54  BatchID= 60 Loss: 0.338 | Acc: 89.255% (6969/7808)
[Train] Epoch= 54  BatchID= 70 Loss: 0.342 | Acc: 89.074% (8095/9088)
[Test] Epoch= 54  BatchID= 0 Loss: 0.206 | Acc: 95.312% (122/128)
[Test] Epoch= 54  BatchID= 10 Loss: 0.394 | Acc: 88.707% (1249/1408)
[Test] Epoch= 54  BatchID= 20 Loss: 0.373 | Acc: 89.062% (2394/2688)
[Test] Epoch= 54  BatchID= 30 Loss: 0.380 | Acc: 88.892% (3489/3925)
Saving..
Best accuracy:  88.89171974522293

Epoch: 55
[Train] Epoch= 55  BatchID= 0 Loss: 0.406 | Acc: 86.719% (111/128)
[Train] Epoch= 55  BatchID= 10 Loss: 0.337 | Acc: 89.560% (1261/1408)
[Train] Epoch= 55  BatchID= 20 Loss: 0.337 | Acc: 89.397% (2403/2688)
[Train] Epoch= 55  BatchID= 30 Loss: 0.330 | Acc: 89.617% (3556/3968)
[Train] Epoch= 55  BatchID= 40 Loss: 0.318 | Acc: 89.806% (4713/5248)
[Train] Epoch= 55  BatchID= 50 Loss: 0.323 | Acc: 89.400% (5836/6528)
[Train] Epoch= 55  BatchID= 60 Loss: 0.328 | Acc: 89.242% (6968/7808)
[Train] Epoch= 55  BatchID= 70 Loss: 0.325 | Acc: 89.426% (8127/9088)
[Test] Epoch= 55  BatchID= 0 Loss: 0.155 | Acc: 94.531% (121/128)
[Test] Epoch= 55  BatchID= 10 Loss: 0.418 | Acc: 86.435% (1217/1408)
[Test] Epoch= 55  BatchID= 20 Loss: 0.400 | Acc: 86.830% (2334/2688)
[Test] Epoch= 55  BatchID= 30 Loss: 0.376 | Acc: 87.669% (3441/3925)

Epoch: 56
[Train] Epoch= 56  BatchID= 0 Loss: 0.318 | Acc: 89.844% (115/128)
[Train] Epoch= 56  BatchID= 10 Loss: 0.290 | Acc: 90.128% (1269/1408)
[Train] Epoch= 56  BatchID= 20 Loss: 0.300 | Acc: 90.327% (2428/2688)
[Train] Epoch= 56  BatchID= 30 Loss: 0.310 | Acc: 90.071% (3574/3968)
[Train] Epoch= 56  BatchID= 40 Loss: 0.321 | Acc: 89.691% (4707/5248)
[Train] Epoch= 56  BatchID= 50 Loss: 0.326 | Acc: 89.491% (5842/6528)
[Train] Epoch= 56  BatchID= 60 Loss: 0.322 | Acc: 89.677% (7002/7808)
[Train] Epoch= 56  BatchID= 70 Loss: 0.332 | Acc: 89.338% (8119/9088)
[Test] Epoch= 56  BatchID= 0 Loss: 0.093 | Acc: 97.656% (125/128)
[Test] Epoch= 56  BatchID= 10 Loss: 0.339 | Acc: 89.489% (1260/1408)
[Test] Epoch= 56  BatchID= 20 Loss: 0.362 | Acc: 88.914% (2390/2688)
[Test] Epoch= 56  BatchID= 30 Loss: 0.366 | Acc: 88.790% (3485/3925)

Epoch: 57
[Train] Epoch= 57  BatchID= 0 Loss: 0.380 | Acc: 88.281% (113/128)
[Train] Epoch= 57  BatchID= 10 Loss: 0.328 | Acc: 89.347% (1258/1408)
[Train] Epoch= 57  BatchID= 20 Loss: 0.322 | Acc: 89.807% (2414/2688)
[Train] Epoch= 57  BatchID= 30 Loss: 0.306 | Acc: 90.247% (3581/3968)
[Train] Epoch= 57  BatchID= 40 Loss: 0.309 | Acc: 89.996% (4723/5248)
[Train] Epoch= 57  BatchID= 50 Loss: 0.310 | Acc: 90.012% (5876/6528)
[Train] Epoch= 57  BatchID= 60 Loss: 0.309 | Acc: 90.190% (7042/7808)
[Train] Epoch= 57  BatchID= 70 Loss: 0.313 | Acc: 90.130% (8191/9088)
[Test] Epoch= 57  BatchID= 0 Loss: 0.099 | Acc: 96.875% (124/128)
[Test] Epoch= 57  BatchID= 10 Loss: 0.316 | Acc: 90.128% (1269/1408)
[Test] Epoch= 57  BatchID= 20 Loss: 0.309 | Acc: 90.327% (2428/2688)
[Test] Epoch= 57  BatchID= 30 Loss: 0.343 | Acc: 89.376% (3508/3925)
Saving..
Best accuracy:  89.37579617834395

Epoch: 58
[Train] Epoch= 58  BatchID= 0 Loss: 0.159 | Acc: 94.531% (121/128)
[Train] Epoch= 58  BatchID= 10 Loss: 0.297 | Acc: 90.128% (1269/1408)
[Train] Epoch= 58  BatchID= 20 Loss: 0.299 | Acc: 89.955% (2418/2688)
[Train] Epoch= 58  BatchID= 30 Loss: 0.308 | Acc: 90.197% (3579/3968)
[Train] Epoch= 58  BatchID= 40 Loss: 0.301 | Acc: 90.434% (4746/5248)
[Train] Epoch= 58  BatchID= 50 Loss: 0.304 | Acc: 90.334% (5897/6528)
[Train] Epoch= 58  BatchID= 60 Loss: 0.304 | Acc: 90.305% (7051/7808)
[Train] Epoch= 58  BatchID= 70 Loss: 0.297 | Acc: 90.581% (8232/9088)
[Test] Epoch= 58  BatchID= 0 Loss: 0.118 | Acc: 96.094% (123/128)
[Test] Epoch= 58  BatchID= 10 Loss: 0.285 | Acc: 91.335% (1286/1408)
[Test] Epoch= 58  BatchID= 20 Loss: 0.388 | Acc: 88.988% (2392/2688)
[Test] Epoch= 58  BatchID= 30 Loss: 0.418 | Acc: 87.873% (3449/3925)

Epoch: 59
[Train] Epoch= 59  BatchID= 0 Loss: 0.375 | Acc: 86.719% (111/128)
[Train] Epoch= 59  BatchID= 10 Loss: 0.311 | Acc: 90.412% (1273/1408)
[Train] Epoch= 59  BatchID= 20 Loss: 0.280 | Acc: 90.960% (2445/2688)
[Train] Epoch= 59  BatchID= 30 Loss: 0.286 | Acc: 91.053% (3613/3968)
[Train] Epoch= 59  BatchID= 40 Loss: 0.293 | Acc: 90.873% (4769/5248)
[Train] Epoch= 59  BatchID= 50 Loss: 0.292 | Acc: 90.885% (5933/6528)
[Train] Epoch= 59  BatchID= 60 Loss: 0.300 | Acc: 90.625% (7076/7808)
[Train] Epoch= 59  BatchID= 70 Loss: 0.300 | Acc: 90.669% (8240/9088)
[Test] Epoch= 59  BatchID= 0 Loss: 0.141 | Acc: 96.094% (123/128)
[Test] Epoch= 59  BatchID= 10 Loss: 0.325 | Acc: 89.844% (1265/1408)
[Test] Epoch= 59  BatchID= 20 Loss: 0.315 | Acc: 90.030% (2420/2688)
[Test] Epoch= 59  BatchID= 30 Loss: 0.321 | Acc: 89.809% (3525/3925)
Saving..
Best accuracy:  89.80891719745223

Epoch: 60
[Train] Epoch= 60  BatchID= 0 Loss: 0.351 | Acc: 87.500% (112/128)
[Train] Epoch= 60  BatchID= 10 Loss: 0.289 | Acc: 91.122% (1283/1408)
[Train] Epoch= 60  BatchID= 20 Loss: 0.290 | Acc: 91.071% (2448/2688)
[Train] Epoch= 60  BatchID= 30 Loss: 0.290 | Acc: 90.852% (3605/3968)
[Train] Epoch= 60  BatchID= 40 Loss: 0.300 | Acc: 90.663% (4758/5248)
[Train] Epoch= 60  BatchID= 50 Loss: 0.303 | Acc: 90.365% (5899/6528)
[Train] Epoch= 60  BatchID= 60 Loss: 0.303 | Acc: 90.484% (7065/7808)
[Train] Epoch= 60  BatchID= 70 Loss: 0.305 | Acc: 90.372% (8213/9088)
[Test] Epoch= 60  BatchID= 0 Loss: 0.192 | Acc: 94.531% (121/128)
[Test] Epoch= 60  BatchID= 10 Loss: 0.336 | Acc: 89.773% (1264/1408)
[Test] Epoch= 60  BatchID= 20 Loss: 0.319 | Acc: 90.179% (2424/2688)
[Test] Epoch= 60  BatchID= 30 Loss: 0.329 | Acc: 89.732% (3522/3925)

Epoch: 61
[Train] Epoch= 61  BatchID= 0 Loss: 0.284 | Acc: 91.406% (117/128)
[Train] Epoch= 61  BatchID= 10 Loss: 0.272 | Acc: 91.264% (1285/1408)
[Train] Epoch= 61  BatchID= 20 Loss: 0.281 | Acc: 90.811% (2441/2688)
[Train] Epoch= 61  BatchID= 30 Loss: 0.282 | Acc: 90.827% (3604/3968)
[Train] Epoch= 61  BatchID= 40 Loss: 0.280 | Acc: 90.777% (4764/5248)
[Train] Epoch= 61  BatchID= 50 Loss: 0.290 | Acc: 90.395% (5901/6528)
[Train] Epoch= 61  BatchID= 60 Loss: 0.292 | Acc: 90.241% (7046/7808)
[Train] Epoch= 61  BatchID= 70 Loss: 0.290 | Acc: 90.295% (8206/9088)
[Test] Epoch= 61  BatchID= 0 Loss: 0.076 | Acc: 96.875% (124/128)
[Test] Epoch= 61  BatchID= 10 Loss: 0.294 | Acc: 90.412% (1273/1408)
[Test] Epoch= 61  BatchID= 20 Loss: 0.286 | Acc: 90.885% (2443/2688)
[Test] Epoch= 61  BatchID= 30 Loss: 0.310 | Acc: 90.471% (3551/3925)
Saving..
Best accuracy:  90.47133757961784

Epoch: 62
[Train] Epoch= 62  BatchID= 0 Loss: 0.383 | Acc: 88.281% (113/128)
[Train] Epoch= 62  BatchID= 10 Loss: 0.281 | Acc: 91.335% (1286/1408)
[Train] Epoch= 62  BatchID= 20 Loss: 0.267 | Acc: 91.629% (2463/2688)
[Train] Epoch= 62  BatchID= 30 Loss: 0.277 | Acc: 91.305% (3623/3968)
[Train] Epoch= 62  BatchID= 40 Loss: 0.280 | Acc: 91.330% (4793/5248)
[Train] Epoch= 62  BatchID= 50 Loss: 0.272 | Acc: 91.759% (5990/6528)
[Train] Epoch= 62  BatchID= 60 Loss: 0.276 | Acc: 91.611% (7153/7808)
[Train] Epoch= 62  BatchID= 70 Loss: 0.279 | Acc: 91.417% (8308/9088)
[Test] Epoch= 62  BatchID= 0 Loss: 0.067 | Acc: 96.875% (124/128)
[Test] Epoch= 62  BatchID= 10 Loss: 0.343 | Acc: 89.773% (1264/1408)
[Test] Epoch= 62  BatchID= 20 Loss: 0.330 | Acc: 90.179% (2424/2688)
[Test] Epoch= 62  BatchID= 30 Loss: 0.330 | Acc: 90.038% (3534/3925)

Epoch: 63
[Train] Epoch= 63  BatchID= 0 Loss: 0.400 | Acc: 89.062% (114/128)
[Train] Epoch= 63  BatchID= 10 Loss: 0.294 | Acc: 91.051% (1282/1408)
[Train] Epoch= 63  BatchID= 20 Loss: 0.281 | Acc: 91.146% (2450/2688)
[Train] Epoch= 63  BatchID= 30 Loss: 0.276 | Acc: 91.003% (3611/3968)
[Train] Epoch= 63  BatchID= 40 Loss: 0.277 | Acc: 91.159% (4784/5248)
[Train] Epoch= 63  BatchID= 50 Loss: 0.271 | Acc: 91.299% (5960/6528)
[Train] Epoch= 63  BatchID= 60 Loss: 0.271 | Acc: 91.278% (7127/7808)
[Train] Epoch= 63  BatchID= 70 Loss: 0.274 | Acc: 91.219% (8290/9088)
[Test] Epoch= 63  BatchID= 0 Loss: 0.229 | Acc: 93.750% (120/128)
[Test] Epoch= 63  BatchID= 10 Loss: 0.317 | Acc: 90.270% (1271/1408)
[Test] Epoch= 63  BatchID= 20 Loss: 0.339 | Acc: 89.435% (2404/2688)
[Test] Epoch= 63  BatchID= 30 Loss: 0.355 | Acc: 89.019% (3494/3925)

Epoch: 64
[Train] Epoch= 64  BatchID= 0 Loss: 0.184 | Acc: 94.531% (121/128)
[Train] Epoch= 64  BatchID= 10 Loss: 0.277 | Acc: 91.690% (1291/1408)
[Train] Epoch= 64  BatchID= 20 Loss: 0.289 | Acc: 90.960% (2445/2688)
[Train] Epoch= 64  BatchID= 30 Loss: 0.275 | Acc: 91.305% (3623/3968)
[Train] Epoch= 64  BatchID= 40 Loss: 0.271 | Acc: 91.349% (4794/5248)
[Train] Epoch= 64  BatchID= 50 Loss: 0.270 | Acc: 91.330% (5962/6528)
[Train] Epoch= 64  BatchID= 60 Loss: 0.270 | Acc: 91.317% (7130/7808)
[Train] Epoch= 64  BatchID= 70 Loss: 0.272 | Acc: 91.263% (8294/9088)
[Test] Epoch= 64  BatchID= 0 Loss: 0.164 | Acc: 95.312% (122/128)
[Test] Epoch= 64  BatchID= 10 Loss: 0.346 | Acc: 88.849% (1251/1408)
[Test] Epoch= 64  BatchID= 20 Loss: 0.314 | Acc: 90.141% (2423/2688)
[Test] Epoch= 64  BatchID= 30 Loss: 0.334 | Acc: 89.299% (3505/3925)

Epoch: 65
[Train] Epoch= 65  BatchID= 0 Loss: 0.250 | Acc: 92.188% (118/128)
[Train] Epoch= 65  BatchID= 10 Loss: 0.256 | Acc: 91.690% (1291/1408)
[Train] Epoch= 65  BatchID= 20 Loss: 0.245 | Acc: 92.188% (2478/2688)
[Train] Epoch= 65  BatchID= 30 Loss: 0.268 | Acc: 91.305% (3623/3968)
[Train] Epoch= 65  BatchID= 40 Loss: 0.260 | Acc: 91.387% (4796/5248)
[Train] Epoch= 65  BatchID= 50 Loss: 0.263 | Acc: 91.406% (5967/6528)
[Train] Epoch= 65  BatchID= 60 Loss: 0.266 | Acc: 91.496% (7144/7808)
[Train] Epoch= 65  BatchID= 70 Loss: 0.263 | Acc: 91.659% (8330/9088)
[Test] Epoch= 65  BatchID= 0 Loss: 0.155 | Acc: 94.531% (121/128)
[Test] Epoch= 65  BatchID= 10 Loss: 0.285 | Acc: 90.696% (1277/1408)
[Test] Epoch= 65  BatchID= 20 Loss: 0.326 | Acc: 89.807% (2414/2688)
[Test] Epoch= 65  BatchID= 30 Loss: 0.326 | Acc: 89.605% (3517/3925)

Epoch: 66
[Train] Epoch= 66  BatchID= 0 Loss: 0.292 | Acc: 92.188% (118/128)
[Train] Epoch= 66  BatchID= 10 Loss: 0.259 | Acc: 91.477% (1288/1408)
[Train] Epoch= 66  BatchID= 20 Loss: 0.269 | Acc: 91.629% (2463/2688)
[Train] Epoch= 66  BatchID= 30 Loss: 0.266 | Acc: 91.734% (3640/3968)
[Train] Epoch= 66  BatchID= 40 Loss: 0.260 | Acc: 92.035% (4830/5248)
[Train] Epoch= 66  BatchID= 50 Loss: 0.259 | Acc: 92.034% (6008/6528)
[Train] Epoch= 66  BatchID= 60 Loss: 0.259 | Acc: 92.149% (7195/7808)
[Train] Epoch= 66  BatchID= 70 Loss: 0.262 | Acc: 92.077% (8368/9088)
[Test] Epoch= 66  BatchID= 0 Loss: 0.154 | Acc: 95.312% (122/128)
[Test] Epoch= 66  BatchID= 10 Loss: 0.259 | Acc: 92.045% (1296/1408)
[Test] Epoch= 66  BatchID= 20 Loss: 0.274 | Acc: 91.406% (2457/2688)
[Test] Epoch= 66  BatchID= 30 Loss: 0.320 | Acc: 90.115% (3537/3925)

Epoch: 67
[Train] Epoch= 67  BatchID= 0 Loss: 0.252 | Acc: 89.844% (115/128)
[Train] Epoch= 67  BatchID= 10 Loss: 0.245 | Acc: 92.188% (1298/1408)
[Train] Epoch= 67  BatchID= 20 Loss: 0.232 | Acc: 92.634% (2490/2688)
[Train] Epoch= 67  BatchID= 30 Loss: 0.238 | Acc: 92.591% (3674/3968)
[Train] Epoch= 67  BatchID= 40 Loss: 0.237 | Acc: 92.511% (4855/5248)
[Train] Epoch= 67  BatchID= 50 Loss: 0.246 | Acc: 92.295% (6025/6528)
[Train] Epoch= 67  BatchID= 60 Loss: 0.249 | Acc: 92.162% (7196/7808)
[Train] Epoch= 67  BatchID= 70 Loss: 0.250 | Acc: 92.088% (8369/9088)
[Test] Epoch= 67  BatchID= 0 Loss: 0.148 | Acc: 95.312% (122/128)
[Test] Epoch= 67  BatchID= 10 Loss: 0.263 | Acc: 91.832% (1293/1408)
[Test] Epoch= 67  BatchID= 20 Loss: 0.270 | Acc: 91.406% (2457/2688)
[Test] Epoch= 67  BatchID= 30 Loss: 0.295 | Acc: 90.624% (3557/3925)
Saving..
Best accuracy:  90.62420382165605

Epoch: 68
[Train] Epoch= 68  BatchID= 0 Loss: 0.287 | Acc: 89.844% (115/128)
[Train] Epoch= 68  BatchID= 10 Loss: 0.235 | Acc: 91.974% (1295/1408)
[Train] Epoch= 68  BatchID= 20 Loss: 0.256 | Acc: 91.518% (2460/2688)
[Train] Epoch= 68  BatchID= 30 Loss: 0.258 | Acc: 91.431% (3628/3968)
[Train] Epoch= 68  BatchID= 40 Loss: 0.259 | Acc: 91.387% (4796/5248)
[Train] Epoch= 68  BatchID= 50 Loss: 0.253 | Acc: 91.759% (5990/6528)
[Train] Epoch= 68  BatchID= 60 Loss: 0.252 | Acc: 91.944% (7179/7808)
[Train] Epoch= 68  BatchID= 70 Loss: 0.249 | Acc: 92.000% (8361/9088)
[Test] Epoch= 68  BatchID= 0 Loss: 0.110 | Acc: 96.094% (123/128)
[Test] Epoch= 68  BatchID= 10 Loss: 0.277 | Acc: 90.909% (1280/1408)
[Test] Epoch= 68  BatchID= 20 Loss: 0.269 | Acc: 91.257% (2453/2688)
[Test] Epoch= 68  BatchID= 30 Loss: 0.301 | Acc: 90.497% (3552/3925)

Epoch: 69
[Train] Epoch= 69  BatchID= 0 Loss: 0.172 | Acc: 95.312% (122/128)
[Train] Epoch= 69  BatchID= 10 Loss: 0.214 | Acc: 92.827% (1307/1408)
[Train] Epoch= 69  BatchID= 20 Loss: 0.228 | Acc: 92.374% (2483/2688)
[Train] Epoch= 69  BatchID= 30 Loss: 0.227 | Acc: 92.717% (3679/3968)
[Train] Epoch= 69  BatchID= 40 Loss: 0.229 | Acc: 92.778% (4869/5248)
[Train] Epoch= 69  BatchID= 50 Loss: 0.233 | Acc: 92.570% (6043/6528)
[Train] Epoch= 69  BatchID= 60 Loss: 0.230 | Acc: 92.636% (7233/7808)
[Train] Epoch= 69  BatchID= 70 Loss: 0.227 | Acc: 92.760% (8430/9088)
[Test] Epoch= 69  BatchID= 0 Loss: 0.119 | Acc: 96.094% (123/128)
[Test] Epoch= 69  BatchID= 10 Loss: 0.261 | Acc: 91.690% (1291/1408)
[Test] Epoch= 69  BatchID= 20 Loss: 0.267 | Acc: 91.257% (2453/2688)
[Test] Epoch= 69  BatchID= 30 Loss: 0.300 | Acc: 90.344% (3546/3925)

Epoch: 70
[Train] Epoch= 70  BatchID= 0 Loss: 0.305 | Acc: 88.281% (113/128)
[Train] Epoch= 70  BatchID= 10 Loss: 0.216 | Acc: 93.111% (1311/1408)
[Train] Epoch= 70  BatchID= 20 Loss: 0.231 | Acc: 92.932% (2498/2688)
[Train] Epoch= 70  BatchID= 30 Loss: 0.223 | Acc: 93.322% (3703/3968)
[Train] Epoch= 70  BatchID= 40 Loss: 0.230 | Acc: 92.988% (4880/5248)
[Train] Epoch= 70  BatchID= 50 Loss: 0.234 | Acc: 92.647% (6048/6528)
[Train] Epoch= 70  BatchID= 60 Loss: 0.235 | Acc: 92.661% (7235/7808)
[Train] Epoch= 70  BatchID= 70 Loss: 0.228 | Acc: 92.848% (8438/9088)
[Test] Epoch= 70  BatchID= 0 Loss: 0.143 | Acc: 95.312% (122/128)
[Test] Epoch= 70  BatchID= 10 Loss: 0.273 | Acc: 90.696% (1277/1408)
[Test] Epoch= 70  BatchID= 20 Loss: 0.275 | Acc: 90.923% (2444/2688)
[Test] Epoch= 70  BatchID= 30 Loss: 0.291 | Acc: 90.497% (3552/3925)

Epoch: 71
[Train] Epoch= 71  BatchID= 0 Loss: 0.225 | Acc: 95.312% (122/128)
[Train] Epoch= 71  BatchID= 10 Loss: 0.230 | Acc: 93.111% (1311/1408)
[Train] Epoch= 71  BatchID= 20 Loss: 0.244 | Acc: 92.746% (2493/2688)
[Train] Epoch= 71  BatchID= 30 Loss: 0.243 | Acc: 92.818% (3683/3968)
[Train] Epoch= 71  BatchID= 40 Loss: 0.235 | Acc: 92.854% (4873/5248)
[Train] Epoch= 71  BatchID= 50 Loss: 0.235 | Acc: 92.907% (6065/6528)
[Train] Epoch= 71  BatchID= 60 Loss: 0.239 | Acc: 92.713% (7239/7808)
[Train] Epoch= 71  BatchID= 70 Loss: 0.233 | Acc: 92.859% (8439/9088)
[Test] Epoch= 71  BatchID= 0 Loss: 0.117 | Acc: 96.094% (123/128)
[Test] Epoch= 71  BatchID= 10 Loss: 0.278 | Acc: 91.193% (1284/1408)
[Test] Epoch= 71  BatchID= 20 Loss: 0.276 | Acc: 91.071% (2448/2688)
[Test] Epoch= 71  BatchID= 30 Loss: 0.293 | Acc: 90.650% (3558/3925)
Saving..
Best accuracy:  90.64968152866243

Epoch: 72
[Train] Epoch= 72  BatchID= 0 Loss: 0.165 | Acc: 92.188% (118/128)
[Train] Epoch= 72  BatchID= 10 Loss: 0.196 | Acc: 93.182% (1312/1408)
[Train] Epoch= 72  BatchID= 20 Loss: 0.216 | Acc: 92.225% (2479/2688)
[Train] Epoch= 72  BatchID= 30 Loss: 0.205 | Acc: 92.893% (3686/3968)
[Train] Epoch= 72  BatchID= 40 Loss: 0.211 | Acc: 92.759% (4868/5248)
[Train] Epoch= 72  BatchID= 50 Loss: 0.209 | Acc: 93.061% (6075/6528)
[Train] Epoch= 72  BatchID= 60 Loss: 0.212 | Acc: 93.033% (7264/7808)
[Train] Epoch= 72  BatchID= 70 Loss: 0.219 | Acc: 92.870% (8440/9088)
[Test] Epoch= 72  BatchID= 0 Loss: 0.162 | Acc: 95.312% (122/128)
[Test] Epoch= 72  BatchID= 10 Loss: 0.295 | Acc: 90.909% (1280/1408)
[Test] Epoch= 72  BatchID= 20 Loss: 0.292 | Acc: 90.811% (2441/2688)
[Test] Epoch= 72  BatchID= 30 Loss: 0.300 | Acc: 90.522% (3553/3925)

Epoch: 73
[Train] Epoch= 73  BatchID= 0 Loss: 0.133 | Acc: 96.094% (123/128)
[Train] Epoch= 73  BatchID= 10 Loss: 0.203 | Acc: 93.679% (1319/1408)
[Train] Epoch= 73  BatchID= 20 Loss: 0.232 | Acc: 92.708% (2492/2688)
[Train] Epoch= 73  BatchID= 30 Loss: 0.222 | Acc: 93.246% (3700/3968)
[Train] Epoch= 73  BatchID= 40 Loss: 0.216 | Acc: 93.464% (4905/5248)
[Train] Epoch= 73  BatchID= 50 Loss: 0.216 | Acc: 93.428% (6099/6528)
[Train] Epoch= 73  BatchID= 60 Loss: 0.211 | Acc: 93.532% (7303/7808)
[Train] Epoch= 73  BatchID= 70 Loss: 0.213 | Acc: 93.431% (8491/9088)
[Test] Epoch= 73  BatchID= 0 Loss: 0.142 | Acc: 96.094% (123/128)
[Test] Epoch= 73  BatchID= 10 Loss: 0.268 | Acc: 92.116% (1297/1408)
[Test] Epoch= 73  BatchID= 20 Loss: 0.269 | Acc: 92.039% (2474/2688)
[Test] Epoch= 73  BatchID= 30 Loss: 0.291 | Acc: 91.210% (3580/3925)
Saving..
Best accuracy:  91.21019108280255

Epoch: 74
[Train] Epoch= 74  BatchID= 0 Loss: 0.215 | Acc: 93.750% (120/128)
[Train] Epoch= 74  BatchID= 10 Loss: 0.211 | Acc: 93.679% (1319/1408)
[Train] Epoch= 74  BatchID= 20 Loss: 0.224 | Acc: 92.969% (2499/2688)
[Train] Epoch= 74  BatchID= 30 Loss: 0.216 | Acc: 93.296% (3702/3968)
[Train] Epoch= 74  BatchID= 40 Loss: 0.203 | Acc: 93.693% (4917/5248)
[Train] Epoch= 74  BatchID= 50 Loss: 0.208 | Acc: 93.520% (6105/6528)
[Train] Epoch= 74  BatchID= 60 Loss: 0.215 | Acc: 93.366% (7290/7808)
[Train] Epoch= 74  BatchID= 70 Loss: 0.209 | Acc: 93.530% (8500/9088)
[Test] Epoch= 74  BatchID= 0 Loss: 0.110 | Acc: 96.875% (124/128)
[Test] Epoch= 74  BatchID= 10 Loss: 0.275 | Acc: 92.045% (1296/1408)
[Test] Epoch= 74  BatchID= 20 Loss: 0.264 | Acc: 92.113% (2476/2688)
[Test] Epoch= 74  BatchID= 30 Loss: 0.291 | Acc: 91.083% (3575/3925)

Epoch: 75
[Train] Epoch= 75  BatchID= 0 Loss: 0.221 | Acc: 92.188% (118/128)
[Train] Epoch= 75  BatchID= 10 Loss: 0.205 | Acc: 92.827% (1307/1408)
[Train] Epoch= 75  BatchID= 20 Loss: 0.187 | Acc: 93.824% (2522/2688)
[Train] Epoch= 75  BatchID= 30 Loss: 0.187 | Acc: 93.926% (3727/3968)
[Train] Epoch= 75  BatchID= 40 Loss: 0.192 | Acc: 93.636% (4914/5248)
[Train] Epoch= 75  BatchID= 50 Loss: 0.194 | Acc: 93.704% (6117/6528)
[Train] Epoch= 75  BatchID= 60 Loss: 0.194 | Acc: 93.712% (7317/7808)
[Train] Epoch= 75  BatchID= 70 Loss: 0.193 | Acc: 93.651% (8511/9088)
[Test] Epoch= 75  BatchID= 0 Loss: 0.129 | Acc: 96.875% (124/128)
[Test] Epoch= 75  BatchID= 10 Loss: 0.265 | Acc: 91.832% (1293/1408)
[Test] Epoch= 75  BatchID= 20 Loss: 0.270 | Acc: 91.109% (2449/2688)
[Test] Epoch= 75  BatchID= 30 Loss: 0.287 | Acc: 90.599% (3556/3925)

Epoch: 76
[Train] Epoch= 76  BatchID= 0 Loss: 0.120 | Acc: 96.875% (124/128)
[Train] Epoch= 76  BatchID= 10 Loss: 0.196 | Acc: 93.679% (1319/1408)
[Train] Epoch= 76  BatchID= 20 Loss: 0.198 | Acc: 93.713% (2519/2688)
[Train] Epoch= 76  BatchID= 30 Loss: 0.200 | Acc: 93.624% (3715/3968)
[Train] Epoch= 76  BatchID= 40 Loss: 0.203 | Acc: 93.540% (4909/5248)
[Train] Epoch= 76  BatchID= 50 Loss: 0.202 | Acc: 93.643% (6113/6528)
[Train] Epoch= 76  BatchID= 60 Loss: 0.203 | Acc: 93.571% (7306/7808)
[Train] Epoch= 76  BatchID= 70 Loss: 0.199 | Acc: 93.618% (8508/9088)
[Test] Epoch= 76  BatchID= 0 Loss: 0.107 | Acc: 96.875% (124/128)
[Test] Epoch= 76  BatchID= 10 Loss: 0.270 | Acc: 91.974% (1295/1408)
[Test] Epoch= 76  BatchID= 20 Loss: 0.259 | Acc: 91.964% (2472/2688)
[Test] Epoch= 76  BatchID= 30 Loss: 0.274 | Acc: 91.541% (3593/3925)
Saving..
Best accuracy:  91.54140127388536

Epoch: 77
[Train] Epoch= 77  BatchID= 0 Loss: 0.175 | Acc: 94.531% (121/128)
[Train] Epoch= 77  BatchID= 10 Loss: 0.195 | Acc: 93.679% (1319/1408)
[Train] Epoch= 77  BatchID= 20 Loss: 0.197 | Acc: 93.601% (2516/2688)
[Train] Epoch= 77  BatchID= 30 Loss: 0.196 | Acc: 93.725% (3719/3968)
[Train] Epoch= 77  BatchID= 40 Loss: 0.199 | Acc: 93.807% (4923/5248)
[Train] Epoch= 77  BatchID= 50 Loss: 0.199 | Acc: 93.796% (6123/6528)
[Train] Epoch= 77  BatchID= 60 Loss: 0.201 | Acc: 93.712% (7317/7808)
[Train] Epoch= 77  BatchID= 70 Loss: 0.201 | Acc: 93.717% (8517/9088)
[Test] Epoch= 77  BatchID= 0 Loss: 0.101 | Acc: 96.875% (124/128)
[Test] Epoch= 77  BatchID= 10 Loss: 0.292 | Acc: 91.051% (1282/1408)
[Test] Epoch= 77  BatchID= 20 Loss: 0.266 | Acc: 91.704% (2465/2688)
[Test] Epoch= 77  BatchID= 30 Loss: 0.280 | Acc: 91.236% (3581/3925)

Epoch: 78
[Train] Epoch= 78  BatchID= 0 Loss: 0.169 | Acc: 96.094% (123/128)
[Train] Epoch= 78  BatchID= 10 Loss: 0.212 | Acc: 93.821% (1321/1408)
[Train] Epoch= 78  BatchID= 20 Loss: 0.207 | Acc: 93.750% (2520/2688)
[Train] Epoch= 78  BatchID= 30 Loss: 0.195 | Acc: 94.178% (3737/3968)
[Train] Epoch= 78  BatchID= 40 Loss: 0.195 | Acc: 94.112% (4939/5248)
[Train] Epoch= 78  BatchID= 50 Loss: 0.188 | Acc: 94.286% (6155/6528)
[Train] Epoch= 78  BatchID= 60 Loss: 0.191 | Acc: 94.147% (7351/7808)
[Train] Epoch= 78  BatchID= 70 Loss: 0.193 | Acc: 94.124% (8554/9088)
[Test] Epoch= 78  BatchID= 0 Loss: 0.130 | Acc: 96.875% (124/128)
[Test] Epoch= 78  BatchID= 10 Loss: 0.265 | Acc: 92.045% (1296/1408)
[Test] Epoch= 78  BatchID= 20 Loss: 0.263 | Acc: 91.815% (2468/2688)
[Test] Epoch= 78  BatchID= 30 Loss: 0.281 | Acc: 91.083% (3575/3925)

Epoch: 79
[Train] Epoch= 79  BatchID= 0 Loss: 0.155 | Acc: 96.875% (124/128)
[Train] Epoch= 79  BatchID= 10 Loss: 0.158 | Acc: 95.739% (1348/1408)
[Train] Epoch= 79  BatchID= 20 Loss: 0.178 | Acc: 94.792% (2548/2688)
[Train] Epoch= 79  BatchID= 30 Loss: 0.178 | Acc: 94.708% (3758/3968)
[Train] Epoch= 79  BatchID= 40 Loss: 0.185 | Acc: 94.436% (4956/5248)
[Train] Epoch= 79  BatchID= 50 Loss: 0.184 | Acc: 94.393% (6162/6528)
[Train] Epoch= 79  BatchID= 60 Loss: 0.187 | Acc: 94.301% (7363/7808)
[Train] Epoch= 79  BatchID= 70 Loss: 0.191 | Acc: 94.179% (8559/9088)
[Test] Epoch= 79  BatchID= 0 Loss: 0.106 | Acc: 96.875% (124/128)
[Test] Epoch= 79  BatchID= 10 Loss: 0.272 | Acc: 91.903% (1294/1408)
[Test] Epoch= 79  BatchID= 20 Loss: 0.261 | Acc: 92.076% (2475/2688)
[Test] Epoch= 79  BatchID= 30 Loss: 0.281 | Acc: 91.108% (3576/3925)

Epoch: 80
[Train] Epoch= 80  BatchID= 0 Loss: 0.118 | Acc: 96.094% (123/128)
[Train] Epoch= 80  BatchID= 10 Loss: 0.181 | Acc: 94.460% (1330/1408)
[Train] Epoch= 80  BatchID= 20 Loss: 0.183 | Acc: 94.568% (2542/2688)
[Train] Epoch= 80  BatchID= 30 Loss: 0.180 | Acc: 94.657% (3756/3968)
[Train] Epoch= 80  BatchID= 40 Loss: 0.189 | Acc: 94.417% (4955/5248)
[Train] Epoch= 80  BatchID= 50 Loss: 0.187 | Acc: 94.424% (6164/6528)
[Train] Epoch= 80  BatchID= 60 Loss: 0.188 | Acc: 94.416% (7372/7808)
[Train] Epoch= 80  BatchID= 70 Loss: 0.189 | Acc: 94.476% (8586/9088)
[Test] Epoch= 80  BatchID= 0 Loss: 0.115 | Acc: 96.875% (124/128)
[Test] Epoch= 80  BatchID= 10 Loss: 0.255 | Acc: 92.045% (1296/1408)
[Test] Epoch= 80  BatchID= 20 Loss: 0.249 | Acc: 92.262% (2480/2688)
[Test] Epoch= 80  BatchID= 30 Loss: 0.272 | Acc: 91.541% (3593/3925)

Epoch: 81
[Train] Epoch= 81  BatchID= 0 Loss: 0.172 | Acc: 96.094% (123/128)
[Train] Epoch= 81  BatchID= 10 Loss: 0.192 | Acc: 93.821% (1321/1408)
[Train] Epoch= 81  BatchID= 20 Loss: 0.189 | Acc: 94.085% (2529/2688)
[Train] Epoch= 81  BatchID= 30 Loss: 0.194 | Acc: 93.876% (3725/3968)
[Train] Epoch= 81  BatchID= 40 Loss: 0.190 | Acc: 93.979% (4932/5248)
[Train] Epoch= 81  BatchID= 50 Loss: 0.186 | Acc: 94.102% (6143/6528)
[Train] Epoch= 81  BatchID= 60 Loss: 0.187 | Acc: 94.109% (7348/7808)
[Train] Epoch= 81  BatchID= 70 Loss: 0.184 | Acc: 94.267% (8567/9088)
[Test] Epoch= 81  BatchID= 0 Loss: 0.098 | Acc: 96.875% (124/128)
[Test] Epoch= 81  BatchID= 10 Loss: 0.254 | Acc: 92.045% (1296/1408)
[Test] Epoch= 81  BatchID= 20 Loss: 0.247 | Acc: 92.336% (2482/2688)
[Test] Epoch= 81  BatchID= 30 Loss: 0.276 | Acc: 91.592% (3595/3925)
Saving..
Best accuracy:  91.59235668789809

Epoch: 82
[Train] Epoch= 82  BatchID= 0 Loss: 0.295 | Acc: 92.188% (118/128)
[Train] Epoch= 82  BatchID= 10 Loss: 0.211 | Acc: 93.963% (1323/1408)
[Train] Epoch= 82  BatchID= 20 Loss: 0.201 | Acc: 94.048% (2528/2688)
[Train] Epoch= 82  BatchID= 30 Loss: 0.194 | Acc: 94.355% (3744/3968)
[Train] Epoch= 82  BatchID= 40 Loss: 0.201 | Acc: 94.055% (4936/5248)
[Train] Epoch= 82  BatchID= 50 Loss: 0.193 | Acc: 94.286% (6155/6528)
[Train] Epoch= 82  BatchID= 60 Loss: 0.191 | Acc: 94.365% (7368/7808)
[Train] Epoch= 82  BatchID= 70 Loss: 0.188 | Acc: 94.531% (8591/9088)
[Test] Epoch= 82  BatchID= 0 Loss: 0.115 | Acc: 96.875% (124/128)
[Test] Epoch= 82  BatchID= 10 Loss: 0.257 | Acc: 92.045% (1296/1408)
[Test] Epoch= 82  BatchID= 20 Loss: 0.260 | Acc: 91.853% (2469/2688)
[Test] Epoch= 82  BatchID= 30 Loss: 0.275 | Acc: 91.261% (3582/3925)

Epoch: 83
[Train] Epoch= 83  BatchID= 0 Loss: 0.105 | Acc: 97.656% (125/128)
[Train] Epoch= 83  BatchID= 10 Loss: 0.180 | Acc: 94.389% (1329/1408)
[Train] Epoch= 83  BatchID= 20 Loss: 0.177 | Acc: 94.159% (2531/2688)
[Train] Epoch= 83  BatchID= 30 Loss: 0.172 | Acc: 94.531% (3751/3968)
[Train] Epoch= 83  BatchID= 40 Loss: 0.173 | Acc: 94.455% (4957/5248)
[Train] Epoch= 83  BatchID= 50 Loss: 0.174 | Acc: 94.301% (6156/6528)
[Train] Epoch= 83  BatchID= 60 Loss: 0.182 | Acc: 94.096% (7347/7808)
[Train] Epoch= 83  BatchID= 70 Loss: 0.181 | Acc: 94.300% (8570/9088)
[Test] Epoch= 83  BatchID= 0 Loss: 0.125 | Acc: 96.875% (124/128)
[Test] Epoch= 83  BatchID= 10 Loss: 0.264 | Acc: 91.548% (1289/1408)
[Test] Epoch= 83  BatchID= 20 Loss: 0.248 | Acc: 92.188% (2478/2688)
[Test] Epoch= 83  BatchID= 30 Loss: 0.275 | Acc: 91.312% (3584/3925)

Epoch: 84
[Train] Epoch= 84  BatchID= 0 Loss: 0.112 | Acc: 96.875% (124/128)
[Train] Epoch= 84  BatchID= 10 Loss: 0.147 | Acc: 96.094% (1353/1408)
[Train] Epoch= 84  BatchID= 20 Loss: 0.157 | Acc: 95.610% (2570/2688)
[Train] Epoch= 84  BatchID= 30 Loss: 0.175 | Acc: 94.884% (3765/3968)
[Train] Epoch= 84  BatchID= 40 Loss: 0.180 | Acc: 94.607% (4965/5248)
[Train] Epoch= 84  BatchID= 50 Loss: 0.184 | Acc: 94.409% (6163/6528)
[Train] Epoch= 84  BatchID= 60 Loss: 0.182 | Acc: 94.544% (7382/7808)
[Train] Epoch= 84  BatchID= 70 Loss: 0.182 | Acc: 94.520% (8590/9088)
[Test] Epoch= 84  BatchID= 0 Loss: 0.124 | Acc: 96.875% (124/128)
[Test] Epoch= 84  BatchID= 10 Loss: 0.253 | Acc: 92.259% (1299/1408)
[Test] Epoch= 84  BatchID= 20 Loss: 0.247 | Acc: 92.299% (2481/2688)
[Test] Epoch= 84  BatchID= 30 Loss: 0.267 | Acc: 91.567% (3594/3925)

Epoch: 85
[Train] Epoch= 85  BatchID= 0 Loss: 0.155 | Acc: 94.531% (121/128)
[Train] Epoch= 85  BatchID= 10 Loss: 0.187 | Acc: 94.389% (1329/1408)
[Train] Epoch= 85  BatchID= 20 Loss: 0.194 | Acc: 93.936% (2525/2688)
[Train] Epoch= 85  BatchID= 30 Loss: 0.183 | Acc: 94.330% (3743/3968)
[Train] Epoch= 85  BatchID= 40 Loss: 0.188 | Acc: 94.074% (4937/5248)
[Train] Epoch= 85  BatchID= 50 Loss: 0.191 | Acc: 94.179% (6148/6528)
[Train] Epoch= 85  BatchID= 60 Loss: 0.197 | Acc: 94.019% (7341/7808)
[Train] Epoch= 85  BatchID= 70 Loss: 0.194 | Acc: 94.179% (8559/9088)
[Test] Epoch= 85  BatchID= 0 Loss: 0.099 | Acc: 96.875% (124/128)
[Test] Epoch= 85  BatchID= 10 Loss: 0.272 | Acc: 91.335% (1286/1408)
[Test] Epoch= 85  BatchID= 20 Loss: 0.256 | Acc: 91.853% (2469/2688)
[Test] Epoch= 85  BatchID= 30 Loss: 0.277 | Acc: 91.287% (3583/3925)

Epoch: 86
[Train] Epoch= 86  BatchID= 0 Loss: 0.103 | Acc: 96.875% (124/128)
[Train] Epoch= 86  BatchID= 10 Loss: 0.194 | Acc: 93.253% (1313/1408)
[Train] Epoch= 86  BatchID= 20 Loss: 0.179 | Acc: 94.271% (2534/2688)
[Train] Epoch= 86  BatchID= 30 Loss: 0.189 | Acc: 94.052% (3732/3968)
[Train] Epoch= 86  BatchID= 40 Loss: 0.185 | Acc: 94.284% (4948/5248)
[Train] Epoch= 86  BatchID= 50 Loss: 0.188 | Acc: 94.087% (6142/6528)
[Train] Epoch= 86  BatchID= 60 Loss: 0.186 | Acc: 94.237% (7358/7808)
[Train] Epoch= 86  BatchID= 70 Loss: 0.185 | Acc: 94.278% (8568/9088)
[Test] Epoch= 86  BatchID= 0 Loss: 0.136 | Acc: 96.094% (123/128)
[Test] Epoch= 86  BatchID= 10 Loss: 0.263 | Acc: 91.548% (1289/1408)
[Test] Epoch= 86  BatchID= 20 Loss: 0.257 | Acc: 91.741% (2466/2688)
[Test] Epoch= 86  BatchID= 30 Loss: 0.275 | Acc: 91.159% (3578/3925)

Epoch: 87
[Train] Epoch= 87  BatchID= 0 Loss: 0.132 | Acc: 95.312% (122/128)
[Train] Epoch= 87  BatchID= 10 Loss: 0.157 | Acc: 95.170% (1340/1408)
[Train] Epoch= 87  BatchID= 20 Loss: 0.159 | Acc: 95.164% (2558/2688)
[Train] Epoch= 87  BatchID= 30 Loss: 0.173 | Acc: 94.506% (3750/3968)
[Train] Epoch= 87  BatchID= 40 Loss: 0.179 | Acc: 94.455% (4957/5248)
[Train] Epoch= 87  BatchID= 50 Loss: 0.179 | Acc: 94.332% (6158/6528)
[Train] Epoch= 87  BatchID= 60 Loss: 0.180 | Acc: 94.390% (7370/7808)
[Train] Epoch= 87  BatchID= 70 Loss: 0.175 | Acc: 94.575% (8595/9088)
[Test] Epoch= 87  BatchID= 0 Loss: 0.130 | Acc: 96.875% (124/128)
[Test] Epoch= 87  BatchID= 10 Loss: 0.257 | Acc: 92.401% (1301/1408)
[Test] Epoch= 87  BatchID= 20 Loss: 0.242 | Acc: 92.411% (2484/2688)
[Test] Epoch= 87  BatchID= 30 Loss: 0.272 | Acc: 91.490% (3591/3925)

Epoch: 88
[Train] Epoch= 88  BatchID= 0 Loss: 0.237 | Acc: 93.750% (120/128)
[Train] Epoch= 88  BatchID= 10 Loss: 0.190 | Acc: 94.105% (1325/1408)
[Train] Epoch= 88  BatchID= 20 Loss: 0.188 | Acc: 94.085% (2529/2688)
[Train] Epoch= 88  BatchID= 30 Loss: 0.180 | Acc: 94.229% (3739/3968)
[Train] Epoch= 88  BatchID= 40 Loss: 0.182 | Acc: 94.245% (4946/5248)
[Train] Epoch= 88  BatchID= 50 Loss: 0.179 | Acc: 94.531% (6171/6528)
[Train] Epoch= 88  BatchID= 60 Loss: 0.178 | Acc: 94.570% (7384/7808)
[Train] Epoch= 88  BatchID= 70 Loss: 0.178 | Acc: 94.586% (8596/9088)
[Test] Epoch= 88  BatchID= 0 Loss: 0.133 | Acc: 96.094% (123/128)
[Test] Epoch= 88  BatchID= 10 Loss: 0.257 | Acc: 91.477% (1288/1408)
[Test] Epoch= 88  BatchID= 20 Loss: 0.253 | Acc: 91.853% (2469/2688)
[Test] Epoch= 88  BatchID= 30 Loss: 0.275 | Acc: 91.261% (3582/3925)

Epoch: 89
[Train] Epoch= 89  BatchID= 0 Loss: 0.154 | Acc: 95.312% (122/128)
[Train] Epoch= 89  BatchID= 10 Loss: 0.191 | Acc: 94.176% (1326/1408)
[Train] Epoch= 89  BatchID= 20 Loss: 0.177 | Acc: 94.606% (2543/2688)
[Train] Epoch= 89  BatchID= 30 Loss: 0.175 | Acc: 94.708% (3758/3968)
[Train] Epoch= 89  BatchID= 40 Loss: 0.178 | Acc: 94.417% (4955/5248)
[Train] Epoch= 89  BatchID= 50 Loss: 0.179 | Acc: 94.363% (6160/6528)
[Train] Epoch= 89  BatchID= 60 Loss: 0.180 | Acc: 94.262% (7360/7808)
[Train] Epoch= 89  BatchID= 70 Loss: 0.180 | Acc: 94.212% (8562/9088)
[Test] Epoch= 89  BatchID= 0 Loss: 0.141 | Acc: 96.094% (123/128)
[Test] Epoch= 89  BatchID= 10 Loss: 0.263 | Acc: 91.477% (1288/1408)
[Test] Epoch= 89  BatchID= 20 Loss: 0.252 | Acc: 91.964% (2472/2688)
[Test] Epoch= 89  BatchID= 30 Loss: 0.272 | Acc: 91.389% (3587/3925)
