==> Preparing data..
==> Building model..
DataParallel(
  (module): ResNet(
    (conv1): InputConv2dLSQ(
      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): LinearLSQ(
      in_features=512, out_features=10, bias=True
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=True)
    )
  )
)
==> Initializing from checkpoint..

Epoch: 0
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
[Train] Epoch= 0  BatchID= 0 Loss: 0.199 | Acc: 92.188% (118/128)
[Train] Epoch= 0  BatchID= 10 Loss: 0.681 | Acc: 80.469% (1133/1408)
[Train] Epoch= 0  BatchID= 20 Loss: 0.691 | Acc: 79.055% (2125/2688)
[Train] Epoch= 0  BatchID= 30 Loss: 0.678 | Acc: 78.730% (3124/3968)
[Train] Epoch= 0  BatchID= 40 Loss: 0.660 | Acc: 79.116% (4152/5248)
[Train] Epoch= 0  BatchID= 50 Loss: 0.640 | Acc: 79.688% (5202/6528)
[Train] Epoch= 0  BatchID= 60 Loss: 0.638 | Acc: 79.726% (6225/7808)
[Train] Epoch= 0  BatchID= 70 Loss: 0.632 | Acc: 79.897% (7261/9088)
[Test] Epoch= 0  BatchID= 0 Loss: 0.325 | Acc: 91.406% (117/128)
[Test] Epoch= 0  BatchID= 10 Loss: 0.577 | Acc: 81.747% (1151/1408)
[Test] Epoch= 0  BatchID= 20 Loss: 0.572 | Acc: 82.292% (2212/2688)
[Test] Epoch= 0  BatchID= 30 Loss: 0.530 | Acc: 83.669% (3284/3925)
Saving..
Best accuracy:  83.6687898089172

Epoch: 1
[Train] Epoch= 1  BatchID= 0 Loss: 0.662 | Acc: 75.781% (97/128)
[Train] Epoch= 1  BatchID= 10 Loss: 0.543 | Acc: 82.741% (1165/1408)
[Train] Epoch= 1  BatchID= 20 Loss: 0.554 | Acc: 82.031% (2205/2688)
[Train] Epoch= 1  BatchID= 30 Loss: 0.554 | Acc: 81.981% (3253/3968)
[Train] Epoch= 1  BatchID= 40 Loss: 0.560 | Acc: 81.898% (4298/5248)
[Train] Epoch= 1  BatchID= 50 Loss: 0.575 | Acc: 81.633% (5329/6528)
[Train] Epoch= 1  BatchID= 60 Loss: 0.578 | Acc: 81.583% (6370/7808)
[Train] Epoch= 1  BatchID= 70 Loss: 0.578 | Acc: 81.459% (7403/9088)
[Test] Epoch= 1  BatchID= 0 Loss: 0.137 | Acc: 94.531% (121/128)
[Test] Epoch= 1  BatchID= 10 Loss: 0.352 | Acc: 87.926% (1238/1408)
[Test] Epoch= 1  BatchID= 20 Loss: 0.445 | Acc: 85.193% (2290/2688)
[Test] Epoch= 1  BatchID= 30 Loss: 0.655 | Acc: 78.879% (3096/3925)

Epoch: 2
[Train] Epoch= 2  BatchID= 0 Loss: 0.636 | Acc: 78.125% (100/128)
[Train] Epoch= 2  BatchID= 10 Loss: 0.553 | Acc: 81.392% (1146/1408)
[Train] Epoch= 2  BatchID= 20 Loss: 0.527 | Acc: 82.850% (2227/2688)
[Train] Epoch= 2  BatchID= 30 Loss: 0.546 | Acc: 82.359% (3268/3968)
[Train] Epoch= 2  BatchID= 40 Loss: 0.556 | Acc: 81.974% (4302/5248)
[Train] Epoch= 2  BatchID= 50 Loss: 0.558 | Acc: 81.878% (5345/6528)
[Train] Epoch= 2  BatchID= 60 Loss: 0.560 | Acc: 81.814% (6388/7808)
[Train] Epoch= 2  BatchID= 70 Loss: 0.553 | Acc: 81.888% (7442/9088)
[Test] Epoch= 2  BatchID= 0 Loss: 1.016 | Acc: 75.000% (96/128)
[Test] Epoch= 2  BatchID= 10 Loss: 0.959 | Acc: 73.793% (1039/1408)
[Test] Epoch= 2  BatchID= 20 Loss: 0.879 | Acc: 76.562% (2058/2688)
[Test] Epoch= 2  BatchID= 30 Loss: 0.759 | Acc: 79.796% (3132/3925)

Epoch: 3
[Train] Epoch= 3  BatchID= 0 Loss: 0.527 | Acc: 84.375% (108/128)
[Train] Epoch= 3  BatchID= 10 Loss: 0.527 | Acc: 83.239% (1172/1408)
[Train] Epoch= 3  BatchID= 20 Loss: 0.538 | Acc: 82.478% (2217/2688)
[Train] Epoch= 3  BatchID= 30 Loss: 0.550 | Acc: 82.233% (3263/3968)
[Train] Epoch= 3  BatchID= 40 Loss: 0.562 | Acc: 81.688% (4287/5248)
[Train] Epoch= 3  BatchID= 50 Loss: 0.579 | Acc: 81.342% (5310/6528)
[Train] Epoch= 3  BatchID= 60 Loss: 0.583 | Acc: 81.160% (6337/7808)
[Train] Epoch= 3  BatchID= 70 Loss: 0.587 | Acc: 80.997% (7361/9088)
[Test] Epoch= 3  BatchID= 0 Loss: 0.298 | Acc: 90.625% (116/128)
[Test] Epoch= 3  BatchID= 10 Loss: 0.696 | Acc: 77.841% (1096/1408)
[Test] Epoch= 3  BatchID= 20 Loss: 0.659 | Acc: 79.836% (2146/2688)
[Test] Epoch= 3  BatchID= 30 Loss: 0.651 | Acc: 80.178% (3147/3925)

Epoch: 4
[Train] Epoch= 4  BatchID= 0 Loss: 0.565 | Acc: 78.906% (101/128)
[Train] Epoch= 4  BatchID= 10 Loss: 0.528 | Acc: 83.523% (1176/1408)
[Train] Epoch= 4  BatchID= 20 Loss: 0.539 | Acc: 83.185% (2236/2688)
[Train] Epoch= 4  BatchID= 30 Loss: 0.570 | Acc: 81.628% (3239/3968)
[Train] Epoch= 4  BatchID= 40 Loss: 0.582 | Acc: 81.231% (4263/5248)
[Train] Epoch= 4  BatchID= 50 Loss: 0.583 | Acc: 81.158% (5298/6528)
[Train] Epoch= 4  BatchID= 60 Loss: 0.583 | Acc: 81.160% (6337/7808)
[Train] Epoch= 4  BatchID= 70 Loss: 0.586 | Acc: 80.931% (7355/9088)
[Test] Epoch= 4  BatchID= 0 Loss: 0.288 | Acc: 90.625% (116/128)
[Test] Epoch= 4  BatchID= 10 Loss: 0.469 | Acc: 84.659% (1192/1408)
[Test] Epoch= 4  BatchID= 20 Loss: 0.547 | Acc: 82.589% (2220/2688)
[Test] Epoch= 4  BatchID= 30 Loss: 0.635 | Acc: 80.102% (3144/3925)

Epoch: 5
[Train] Epoch= 5  BatchID= 0 Loss: 0.692 | Acc: 78.125% (100/128)
[Train] Epoch= 5  BatchID= 10 Loss: 0.558 | Acc: 82.244% (1158/1408)
[Train] Epoch= 5  BatchID= 20 Loss: 0.549 | Acc: 82.478% (2217/2688)
[Train] Epoch= 5  BatchID= 30 Loss: 0.561 | Acc: 81.653% (3240/3968)
[Train] Epoch= 5  BatchID= 40 Loss: 0.576 | Acc: 81.136% (4258/5248)
[Train] Epoch= 5  BatchID= 50 Loss: 0.580 | Acc: 81.081% (5293/6528)
[Train] Epoch= 5  BatchID= 60 Loss: 0.580 | Acc: 81.378% (6354/7808)
[Train] Epoch= 5  BatchID= 70 Loss: 0.583 | Acc: 81.239% (7383/9088)
[Test] Epoch= 5  BatchID= 0 Loss: 0.352 | Acc: 90.625% (116/128)
[Test] Epoch= 5  BatchID= 10 Loss: 0.653 | Acc: 78.125% (1100/1408)
[Test] Epoch= 5  BatchID= 20 Loss: 0.555 | Acc: 81.659% (2195/2688)
[Test] Epoch= 5  BatchID= 30 Loss: 0.609 | Acc: 80.637% (3165/3925)

Epoch: 6
[Train] Epoch= 6  BatchID= 0 Loss: 0.588 | Acc: 81.250% (104/128)
[Train] Epoch= 6  BatchID= 10 Loss: 0.587 | Acc: 82.741% (1165/1408)
[Train] Epoch= 6  BatchID= 20 Loss: 0.574 | Acc: 82.515% (2218/2688)
[Train] Epoch= 6  BatchID= 30 Loss: 0.587 | Acc: 81.981% (3253/3968)
[Train] Epoch= 6  BatchID= 40 Loss: 0.585 | Acc: 81.650% (4285/5248)
[Train] Epoch= 6  BatchID= 50 Loss: 0.586 | Acc: 81.449% (5317/6528)
[Train] Epoch= 6  BatchID= 60 Loss: 0.588 | Acc: 81.327% (6350/7808)
[Train] Epoch= 6  BatchID= 70 Loss: 0.597 | Acc: 81.041% (7365/9088)
[Test] Epoch= 6  BatchID= 0 Loss: 0.700 | Acc: 82.031% (105/128)
[Test] Epoch= 6  BatchID= 10 Loss: 0.762 | Acc: 76.705% (1080/1408)
[Test] Epoch= 6  BatchID= 20 Loss: 0.730 | Acc: 77.307% (2078/2688)
[Test] Epoch= 6  BatchID= 30 Loss: 0.705 | Acc: 78.573% (3084/3925)

Epoch: 7
[Train] Epoch= 7  BatchID= 0 Loss: 0.403 | Acc: 86.719% (111/128)
[Train] Epoch= 7  BatchID= 10 Loss: 0.609 | Acc: 79.545% (1120/1408)
[Train] Epoch= 7  BatchID= 20 Loss: 0.585 | Acc: 80.097% (2153/2688)
[Train] Epoch= 7  BatchID= 30 Loss: 0.570 | Acc: 80.721% (3203/3968)
[Train] Epoch= 7  BatchID= 40 Loss: 0.583 | Acc: 80.526% (4226/5248)
[Train] Epoch= 7  BatchID= 50 Loss: 0.580 | Acc: 80.790% (5274/6528)
[Train] Epoch= 7  BatchID= 60 Loss: 0.581 | Acc: 80.751% (6305/7808)
[Train] Epoch= 7  BatchID= 70 Loss: 0.593 | Acc: 80.381% (7305/9088)
[Test] Epoch= 7  BatchID= 0 Loss: 0.650 | Acc: 82.031% (105/128)
[Test] Epoch= 7  BatchID= 10 Loss: 0.758 | Acc: 75.710% (1066/1408)
[Test] Epoch= 7  BatchID= 20 Loss: 0.814 | Acc: 73.772% (1983/2688)
[Test] Epoch= 7  BatchID= 30 Loss: 0.757 | Acc: 75.949% (2981/3925)

Epoch: 8
[Train] Epoch= 8  BatchID= 0 Loss: 0.654 | Acc: 81.250% (104/128)
[Train] Epoch= 8  BatchID= 10 Loss: 0.606 | Acc: 80.327% (1131/1408)
[Train] Epoch= 8  BatchID= 20 Loss: 0.598 | Acc: 81.138% (2181/2688)
[Train] Epoch= 8  BatchID= 30 Loss: 0.598 | Acc: 81.023% (3215/3968)
[Train] Epoch= 8  BatchID= 40 Loss: 0.598 | Acc: 80.640% (4232/5248)
[Train] Epoch= 8  BatchID= 50 Loss: 0.606 | Acc: 80.423% (5250/6528)
[Train] Epoch= 8  BatchID= 60 Loss: 0.612 | Acc: 80.097% (6254/7808)
[Train] Epoch= 8  BatchID= 70 Loss: 0.610 | Acc: 80.216% (7290/9088)
[Test] Epoch= 8  BatchID= 0 Loss: 0.246 | Acc: 91.406% (117/128)
[Test] Epoch= 8  BatchID= 10 Loss: 0.558 | Acc: 81.889% (1153/1408)
[Test] Epoch= 8  BatchID= 20 Loss: 0.656 | Acc: 78.832% (2119/2688)
[Test] Epoch= 8  BatchID= 30 Loss: 0.734 | Acc: 76.764% (3013/3925)

Epoch: 9
[Train] Epoch= 9  BatchID= 0 Loss: 0.690 | Acc: 75.000% (96/128)
[Train] Epoch= 9  BatchID= 10 Loss: 0.602 | Acc: 80.469% (1133/1408)
[Train] Epoch= 9  BatchID= 20 Loss: 0.598 | Acc: 80.320% (2159/2688)
[Train] Epoch= 9  BatchID= 30 Loss: 0.600 | Acc: 80.519% (3195/3968)
[Train] Epoch= 9  BatchID= 40 Loss: 0.601 | Acc: 80.469% (4223/5248)
[Train] Epoch= 9  BatchID= 50 Loss: 0.594 | Acc: 80.821% (5276/6528)
[Train] Epoch= 9  BatchID= 60 Loss: 0.607 | Acc: 80.456% (6282/7808)
[Train] Epoch= 9  BatchID= 70 Loss: 0.607 | Acc: 80.458% (7312/9088)
[Test] Epoch= 9  BatchID= 0 Loss: 1.082 | Acc: 68.750% (88/128)
[Test] Epoch= 9  BatchID= 10 Loss: 0.706 | Acc: 79.830% (1124/1408)
[Test] Epoch= 9  BatchID= 20 Loss: 0.788 | Acc: 76.488% (2056/2688)
[Test] Epoch= 9  BatchID= 30 Loss: 0.831 | Acc: 75.287% (2955/3925)

Epoch: 10
[Train] Epoch= 10  BatchID= 0 Loss: 0.652 | Acc: 74.219% (95/128)
[Train] Epoch= 10  BatchID= 10 Loss: 0.592 | Acc: 79.830% (1124/1408)
[Train] Epoch= 10  BatchID= 20 Loss: 0.586 | Acc: 80.394% (2161/2688)
[Train] Epoch= 10  BatchID= 30 Loss: 0.607 | Acc: 80.066% (3177/3968)
[Train] Epoch= 10  BatchID= 40 Loss: 0.603 | Acc: 80.488% (4224/5248)
[Train] Epoch= 10  BatchID= 50 Loss: 0.598 | Acc: 80.607% (5262/6528)
[Train] Epoch= 10  BatchID= 60 Loss: 0.597 | Acc: 80.648% (6297/7808)
[Train] Epoch= 10  BatchID= 70 Loss: 0.599 | Acc: 80.513% (7317/9088)
[Test] Epoch= 10  BatchID= 0 Loss: 0.277 | Acc: 92.969% (119/128)
[Test] Epoch= 10  BatchID= 10 Loss: 0.579 | Acc: 81.676% (1150/1408)
[Test] Epoch= 10  BatchID= 20 Loss: 0.591 | Acc: 81.585% (2193/2688)
[Test] Epoch= 10  BatchID= 30 Loss: 0.569 | Acc: 82.268% (3229/3925)

Epoch: 11
[Train] Epoch= 11  BatchID= 0 Loss: 0.598 | Acc: 81.250% (104/128)
[Train] Epoch= 11  BatchID= 10 Loss: 0.589 | Acc: 80.682% (1136/1408)
[Train] Epoch= 11  BatchID= 20 Loss: 0.585 | Acc: 80.990% (2177/2688)
[Train] Epoch= 11  BatchID= 30 Loss: 0.584 | Acc: 81.200% (3222/3968)
[Train] Epoch= 11  BatchID= 40 Loss: 0.588 | Acc: 80.774% (4239/5248)
[Train] Epoch= 11  BatchID= 50 Loss: 0.589 | Acc: 80.699% (5268/6528)
[Train] Epoch= 11  BatchID= 60 Loss: 0.591 | Acc: 80.699% (6301/7808)
[Train] Epoch= 11  BatchID= 70 Loss: 0.602 | Acc: 80.469% (7313/9088)
[Test] Epoch= 11  BatchID= 0 Loss: 0.243 | Acc: 93.750% (120/128)
[Test] Epoch= 11  BatchID= 10 Loss: 0.527 | Acc: 81.676% (1150/1408)
[Test] Epoch= 11  BatchID= 20 Loss: 0.639 | Acc: 79.241% (2130/2688)
[Test] Epoch= 11  BatchID= 30 Loss: 0.609 | Acc: 79.771% (3131/3925)

Epoch: 12
[Train] Epoch= 12  BatchID= 0 Loss: 0.507 | Acc: 85.156% (109/128)
[Train] Epoch= 12  BatchID= 10 Loss: 0.584 | Acc: 82.244% (1158/1408)
[Train] Epoch= 12  BatchID= 20 Loss: 0.580 | Acc: 81.287% (2185/2688)
[Train] Epoch= 12  BatchID= 30 Loss: 0.573 | Acc: 81.376% (3229/3968)
[Train] Epoch= 12  BatchID= 40 Loss: 0.566 | Acc: 81.593% (4282/5248)
[Train] Epoch= 12  BatchID= 50 Loss: 0.571 | Acc: 81.526% (5322/6528)
[Train] Epoch= 12  BatchID= 60 Loss: 0.570 | Acc: 81.481% (6362/7808)
[Train] Epoch= 12  BatchID= 70 Loss: 0.577 | Acc: 81.316% (7390/9088)
[Test] Epoch= 12  BatchID= 0 Loss: 0.225 | Acc: 92.969% (119/128)
[Test] Epoch= 12  BatchID= 10 Loss: 0.796 | Acc: 76.065% (1071/1408)
[Test] Epoch= 12  BatchID= 20 Loss: 0.882 | Acc: 73.140% (1966/2688)
[Test] Epoch= 12  BatchID= 30 Loss: 0.772 | Acc: 76.280% (2994/3925)

Epoch: 13
[Train] Epoch= 13  BatchID= 0 Loss: 0.673 | Acc: 80.469% (103/128)
[Train] Epoch= 13  BatchID= 10 Loss: 0.609 | Acc: 80.611% (1135/1408)
[Train] Epoch= 13  BatchID= 20 Loss: 0.615 | Acc: 80.097% (2153/2688)
[Train] Epoch= 13  BatchID= 30 Loss: 0.619 | Acc: 80.166% (3181/3968)
[Train] Epoch= 13  BatchID= 40 Loss: 0.616 | Acc: 80.373% (4218/5248)
[Train] Epoch= 13  BatchID= 50 Loss: 0.609 | Acc: 80.744% (5271/6528)
[Train] Epoch= 13  BatchID= 60 Loss: 0.606 | Acc: 80.827% (6311/7808)
[Train] Epoch= 13  BatchID= 70 Loss: 0.603 | Acc: 80.755% (7339/9088)
[Test] Epoch= 13  BatchID= 0 Loss: 0.251 | Acc: 95.312% (122/128)
[Test] Epoch= 13  BatchID= 10 Loss: 0.574 | Acc: 80.469% (1133/1408)
[Test] Epoch= 13  BatchID= 20 Loss: 0.591 | Acc: 81.176% (2182/2688)
[Test] Epoch= 13  BatchID= 30 Loss: 0.617 | Acc: 80.357% (3154/3925)

Epoch: 14
[Train] Epoch= 14  BatchID= 0 Loss: 0.824 | Acc: 72.656% (93/128)
[Train] Epoch= 14  BatchID= 10 Loss: 0.594 | Acc: 80.469% (1133/1408)
[Train] Epoch= 14  BatchID= 20 Loss: 0.590 | Acc: 80.915% (2175/2688)
[Train] Epoch= 14  BatchID= 30 Loss: 0.579 | Acc: 81.099% (3218/3968)
[Train] Epoch= 14  BatchID= 40 Loss: 0.578 | Acc: 81.212% (4262/5248)
[Train] Epoch= 14  BatchID= 50 Loss: 0.579 | Acc: 80.699% (5268/6528)
[Train] Epoch= 14  BatchID= 60 Loss: 0.578 | Acc: 80.776% (6307/7808)
[Train] Epoch= 14  BatchID= 70 Loss: 0.569 | Acc: 81.096% (7370/9088)
[Test] Epoch= 14  BatchID= 0 Loss: 0.262 | Acc: 92.188% (118/128)
[Test] Epoch= 14  BatchID= 10 Loss: 0.549 | Acc: 81.889% (1153/1408)
[Test] Epoch= 14  BatchID= 20 Loss: 0.511 | Acc: 83.743% (2251/2688)
[Test] Epoch= 14  BatchID= 30 Loss: 0.493 | Acc: 84.000% (3297/3925)
Saving..
Best accuracy:  84.0

Epoch: 15
[Train] Epoch= 15  BatchID= 0 Loss: 0.414 | Acc: 85.156% (109/128)
[Train] Epoch= 15  BatchID= 10 Loss: 0.555 | Acc: 82.173% (1157/1408)
[Train] Epoch= 15  BatchID= 20 Loss: 0.554 | Acc: 82.217% (2210/2688)
[Train] Epoch= 15  BatchID= 30 Loss: 0.537 | Acc: 82.712% (3282/3968)
[Train] Epoch= 15  BatchID= 40 Loss: 0.539 | Acc: 82.622% (4336/5248)
[Train] Epoch= 15  BatchID= 50 Loss: 0.541 | Acc: 82.659% (5396/6528)
[Train] Epoch= 15  BatchID= 60 Loss: 0.548 | Acc: 82.467% (6439/7808)
[Train] Epoch= 15  BatchID= 70 Loss: 0.554 | Acc: 82.020% (7454/9088)
[Test] Epoch= 15  BatchID= 0 Loss: 0.101 | Acc: 95.312% (122/128)
[Test] Epoch= 15  BatchID= 10 Loss: 0.456 | Acc: 84.943% (1196/1408)
[Test] Epoch= 15  BatchID= 20 Loss: 0.452 | Acc: 84.710% (2277/2688)
[Test] Epoch= 15  BatchID= 30 Loss: 0.548 | Acc: 82.675% (3245/3925)

Epoch: 16
[Train] Epoch= 16  BatchID= 0 Loss: 0.535 | Acc: 83.594% (107/128)
[Train] Epoch= 16  BatchID= 10 Loss: 0.565 | Acc: 81.179% (1143/1408)
[Train] Epoch= 16  BatchID= 20 Loss: 0.562 | Acc: 81.548% (2192/2688)
[Train] Epoch= 16  BatchID= 30 Loss: 0.562 | Acc: 81.351% (3228/3968)
[Train] Epoch= 16  BatchID= 40 Loss: 0.551 | Acc: 81.841% (4295/5248)
[Train] Epoch= 16  BatchID= 50 Loss: 0.549 | Acc: 81.970% (5351/6528)
[Train] Epoch= 16  BatchID= 60 Loss: 0.555 | Acc: 81.762% (6384/7808)
[Train] Epoch= 16  BatchID= 70 Loss: 0.562 | Acc: 81.459% (7403/9088)
[Test] Epoch= 16  BatchID= 0 Loss: 0.267 | Acc: 89.844% (115/128)
[Test] Epoch= 16  BatchID= 10 Loss: 0.519 | Acc: 82.528% (1162/1408)
[Test] Epoch= 16  BatchID= 20 Loss: 0.473 | Acc: 84.784% (2279/2688)
[Test] Epoch= 16  BatchID= 30 Loss: 0.507 | Acc: 83.541% (3279/3925)

Epoch: 17
[Train] Epoch= 17  BatchID= 0 Loss: 0.524 | Acc: 83.594% (107/128)
[Train] Epoch= 17  BatchID= 10 Loss: 0.595 | Acc: 81.392% (1146/1408)
[Train] Epoch= 17  BatchID= 20 Loss: 0.562 | Acc: 82.217% (2210/2688)
[Train] Epoch= 17  BatchID= 30 Loss: 0.561 | Acc: 81.956% (3252/3968)
[Train] Epoch= 17  BatchID= 40 Loss: 0.566 | Acc: 81.745% (4290/5248)
[Train] Epoch= 17  BatchID= 50 Loss: 0.562 | Acc: 81.893% (5346/6528)
[Train] Epoch= 17  BatchID= 60 Loss: 0.557 | Acc: 82.070% (6408/7808)
[Train] Epoch= 17  BatchID= 70 Loss: 0.560 | Acc: 81.855% (7439/9088)
[Test] Epoch= 17  BatchID= 0 Loss: 0.243 | Acc: 92.969% (119/128)
[Test] Epoch= 17  BatchID= 10 Loss: 0.384 | Acc: 88.210% (1242/1408)
[Test] Epoch= 17  BatchID= 20 Loss: 0.477 | Acc: 85.640% (2302/2688)
[Test] Epoch= 17  BatchID= 30 Loss: 0.497 | Acc: 84.713% (3325/3925)
Saving..
Best accuracy:  84.71337579617834

Epoch: 18
[Train] Epoch= 18  BatchID= 0 Loss: 0.391 | Acc: 88.281% (113/128)
[Train] Epoch= 18  BatchID= 10 Loss: 0.536 | Acc: 83.097% (1170/1408)
[Train] Epoch= 18  BatchID= 20 Loss: 0.567 | Acc: 82.106% (2207/2688)
[Train] Epoch= 18  BatchID= 30 Loss: 0.569 | Acc: 82.056% (3256/3968)
[Train] Epoch= 18  BatchID= 40 Loss: 0.572 | Acc: 81.917% (4299/5248)
[Train] Epoch= 18  BatchID= 50 Loss: 0.582 | Acc: 81.449% (5317/6528)
[Train] Epoch= 18  BatchID= 60 Loss: 0.583 | Acc: 81.545% (6367/7808)
[Train] Epoch= 18  BatchID= 70 Loss: 0.580 | Acc: 81.481% (7405/9088)
[Test] Epoch= 18  BatchID= 0 Loss: 0.239 | Acc: 92.969% (119/128)
[Test] Epoch= 18  BatchID= 10 Loss: 0.499 | Acc: 85.085% (1198/1408)
[Test] Epoch= 18  BatchID= 20 Loss: 0.544 | Acc: 83.333% (2240/2688)
[Test] Epoch= 18  BatchID= 30 Loss: 0.619 | Acc: 81.274% (3190/3925)

Epoch: 19
[Train] Epoch= 19  BatchID= 0 Loss: 0.413 | Acc: 90.625% (116/128)
[Train] Epoch= 19  BatchID= 10 Loss: 0.574 | Acc: 82.741% (1165/1408)
[Train] Epoch= 19  BatchID= 20 Loss: 0.568 | Acc: 82.478% (2217/2688)
[Train] Epoch= 19  BatchID= 30 Loss: 0.562 | Acc: 82.737% (3283/3968)
[Train] Epoch= 19  BatchID= 40 Loss: 0.569 | Acc: 82.374% (4323/5248)
[Train] Epoch= 19  BatchID= 50 Loss: 0.575 | Acc: 81.909% (5347/6528)
[Train] Epoch= 19  BatchID= 60 Loss: 0.581 | Acc: 81.596% (6371/7808)
[Train] Epoch= 19  BatchID= 70 Loss: 0.589 | Acc: 81.349% (7393/9088)
[Test] Epoch= 19  BatchID= 0 Loss: 0.285 | Acc: 90.625% (116/128)
[Test] Epoch= 19  BatchID= 10 Loss: 0.603 | Acc: 80.185% (1129/1408)
[Test] Epoch= 19  BatchID= 20 Loss: 0.527 | Acc: 82.961% (2230/2688)
[Test] Epoch= 19  BatchID= 30 Loss: 0.539 | Acc: 82.777% (3249/3925)

Epoch: 20
[Train] Epoch= 20  BatchID= 0 Loss: 0.553 | Acc: 78.125% (100/128)
[Train] Epoch= 20  BatchID= 10 Loss: 0.578 | Acc: 80.895% (1139/1408)
[Train] Epoch= 20  BatchID= 20 Loss: 0.541 | Acc: 82.106% (2207/2688)
[Train] Epoch= 20  BatchID= 30 Loss: 0.550 | Acc: 81.956% (3252/3968)
[Train] Epoch= 20  BatchID= 40 Loss: 0.553 | Acc: 82.088% (4308/5248)
[Train] Epoch= 20  BatchID= 50 Loss: 0.560 | Acc: 81.939% (5349/6528)
[Train] Epoch= 20  BatchID= 60 Loss: 0.568 | Acc: 81.737% (6382/7808)
[Train] Epoch= 20  BatchID= 70 Loss: 0.568 | Acc: 81.624% (7418/9088)
[Test] Epoch= 20  BatchID= 0 Loss: 0.239 | Acc: 92.969% (119/128)
[Test] Epoch= 20  BatchID= 10 Loss: 0.456 | Acc: 85.298% (1201/1408)
[Test] Epoch= 20  BatchID= 20 Loss: 0.437 | Acc: 86.086% (2314/2688)
[Test] Epoch= 20  BatchID= 30 Loss: 0.549 | Acc: 83.006% (3258/3925)

Epoch: 21
[Train] Epoch= 21  BatchID= 0 Loss: 0.392 | Acc: 86.719% (111/128)
[Train] Epoch= 21  BatchID= 10 Loss: 0.474 | Acc: 85.440% (1203/1408)
[Train] Epoch= 21  BatchID= 20 Loss: 0.484 | Acc: 84.896% (2282/2688)
[Train] Epoch= 21  BatchID= 30 Loss: 0.492 | Acc: 84.350% (3347/3968)
[Train] Epoch= 21  BatchID= 40 Loss: 0.502 | Acc: 84.108% (4414/5248)
[Train] Epoch= 21  BatchID= 50 Loss: 0.514 | Acc: 83.624% (5459/6528)
[Train] Epoch= 21  BatchID= 60 Loss: 0.511 | Acc: 83.658% (6532/7808)
[Train] Epoch= 21  BatchID= 70 Loss: 0.516 | Acc: 83.517% (7590/9088)
[Test] Epoch= 21  BatchID= 0 Loss: 0.313 | Acc: 92.188% (118/128)
[Test] Epoch= 21  BatchID= 10 Loss: 0.448 | Acc: 85.795% (1208/1408)
[Test] Epoch= 21  BatchID= 20 Loss: 0.571 | Acc: 82.515% (2218/2688)
[Test] Epoch= 21  BatchID= 30 Loss: 0.561 | Acc: 82.293% (3230/3925)

Epoch: 22
[Train] Epoch= 22  BatchID= 0 Loss: 0.591 | Acc: 80.469% (103/128)
[Train] Epoch= 22  BatchID= 10 Loss: 0.531 | Acc: 83.310% (1173/1408)
[Train] Epoch= 22  BatchID= 20 Loss: 0.531 | Acc: 83.408% (2242/2688)
[Train] Epoch= 22  BatchID= 30 Loss: 0.534 | Acc: 83.090% (3297/3968)
[Train] Epoch= 22  BatchID= 40 Loss: 0.549 | Acc: 82.393% (4324/5248)
[Train] Epoch= 22  BatchID= 50 Loss: 0.555 | Acc: 82.138% (5362/6528)
[Train] Epoch= 22  BatchID= 60 Loss: 0.555 | Acc: 82.211% (6419/7808)
[Train] Epoch= 22  BatchID= 70 Loss: 0.563 | Acc: 81.954% (7448/9088)
[Test] Epoch= 22  BatchID= 0 Loss: 0.149 | Acc: 94.531% (121/128)
[Test] Epoch= 22  BatchID= 10 Loss: 0.460 | Acc: 84.375% (1188/1408)
[Test] Epoch= 22  BatchID= 20 Loss: 0.524 | Acc: 83.185% (2236/2688)
[Test] Epoch= 22  BatchID= 30 Loss: 0.611 | Acc: 80.331% (3153/3925)

Epoch: 23
[Train] Epoch= 23  BatchID= 0 Loss: 0.580 | Acc: 81.250% (104/128)
[Train] Epoch= 23  BatchID= 10 Loss: 0.549 | Acc: 81.747% (1151/1408)
[Train] Epoch= 23  BatchID= 20 Loss: 0.556 | Acc: 81.957% (2203/2688)
[Train] Epoch= 23  BatchID= 30 Loss: 0.554 | Acc: 82.031% (3255/3968)
[Train] Epoch= 23  BatchID= 40 Loss: 0.548 | Acc: 82.260% (4317/5248)
[Train] Epoch= 23  BatchID= 50 Loss: 0.552 | Acc: 82.184% (5365/6528)
[Train] Epoch= 23  BatchID= 60 Loss: 0.550 | Acc: 82.480% (6440/7808)
[Train] Epoch= 23  BatchID= 70 Loss: 0.543 | Acc: 82.504% (7498/9088)
[Test] Epoch= 23  BatchID= 0 Loss: 0.352 | Acc: 86.719% (111/128)
[Test] Epoch= 23  BatchID= 10 Loss: 0.614 | Acc: 78.480% (1105/1408)
[Test] Epoch= 23  BatchID= 20 Loss: 0.623 | Acc: 79.092% (2126/2688)
[Test] Epoch= 23  BatchID= 30 Loss: 0.580 | Acc: 80.535% (3161/3925)

Epoch: 24
[Train] Epoch= 24  BatchID= 0 Loss: 0.611 | Acc: 81.250% (104/128)
[Train] Epoch= 24  BatchID= 10 Loss: 0.530 | Acc: 82.884% (1167/1408)
[Train] Epoch= 24  BatchID= 20 Loss: 0.531 | Acc: 82.440% (2216/2688)
[Train] Epoch= 24  BatchID= 30 Loss: 0.514 | Acc: 83.266% (3304/3968)
[Train] Epoch= 24  BatchID= 40 Loss: 0.507 | Acc: 83.575% (4386/5248)
[Train] Epoch= 24  BatchID= 50 Loss: 0.507 | Acc: 83.349% (5441/6528)
[Train] Epoch= 24  BatchID= 60 Loss: 0.510 | Acc: 83.350% (6508/7808)
[Train] Epoch= 24  BatchID= 70 Loss: 0.522 | Acc: 82.790% (7524/9088)
[Test] Epoch= 24  BatchID= 0 Loss: 0.203 | Acc: 92.188% (118/128)
[Test] Epoch= 24  BatchID= 10 Loss: 0.438 | Acc: 85.085% (1198/1408)
[Test] Epoch= 24  BatchID= 20 Loss: 0.607 | Acc: 79.464% (2136/2688)
[Test] Epoch= 24  BatchID= 30 Loss: 0.564 | Acc: 81.376% (3194/3925)

Epoch: 25
[Train] Epoch= 25  BatchID= 0 Loss: 0.384 | Acc: 86.719% (111/128)
[Train] Epoch= 25  BatchID= 10 Loss: 0.509 | Acc: 83.168% (1171/1408)
[Train] Epoch= 25  BatchID= 20 Loss: 0.545 | Acc: 81.882% (2201/2688)
[Train] Epoch= 25  BatchID= 30 Loss: 0.537 | Acc: 81.930% (3251/3968)
[Train] Epoch= 25  BatchID= 40 Loss: 0.526 | Acc: 82.069% (4307/5248)
[Train] Epoch= 25  BatchID= 50 Loss: 0.528 | Acc: 82.276% (5371/6528)
[Train] Epoch= 25  BatchID= 60 Loss: 0.527 | Acc: 82.441% (6437/7808)
[Train] Epoch= 25  BatchID= 70 Loss: 0.525 | Acc: 82.614% (7508/9088)
[Test] Epoch= 25  BatchID= 0 Loss: 0.174 | Acc: 93.750% (120/128)
[Test] Epoch= 25  BatchID= 10 Loss: 0.457 | Acc: 85.227% (1200/1408)
[Test] Epoch= 25  BatchID= 20 Loss: 0.490 | Acc: 84.710% (2277/2688)
[Test] Epoch= 25  BatchID= 30 Loss: 0.514 | Acc: 83.541% (3279/3925)

Epoch: 26
[Train] Epoch= 26  BatchID= 0 Loss: 0.436 | Acc: 87.500% (112/128)
[Train] Epoch= 26  BatchID= 10 Loss: 0.478 | Acc: 85.653% (1206/1408)
[Train] Epoch= 26  BatchID= 20 Loss: 0.484 | Acc: 85.491% (2298/2688)
[Train] Epoch= 26  BatchID= 30 Loss: 0.510 | Acc: 84.073% (3336/3968)
[Train] Epoch= 26  BatchID= 40 Loss: 0.519 | Acc: 83.670% (4391/5248)
[Train] Epoch= 26  BatchID= 50 Loss: 0.519 | Acc: 83.640% (5460/6528)
[Train] Epoch= 26  BatchID= 60 Loss: 0.522 | Acc: 83.543% (6523/7808)
[Train] Epoch= 26  BatchID= 70 Loss: 0.530 | Acc: 83.286% (7569/9088)
[Test] Epoch= 26  BatchID= 0 Loss: 0.303 | Acc: 91.406% (117/128)
[Test] Epoch= 26  BatchID= 10 Loss: 0.548 | Acc: 82.599% (1163/1408)
[Test] Epoch= 26  BatchID= 20 Loss: 0.693 | Acc: 79.278% (2131/2688)
[Test] Epoch= 26  BatchID= 30 Loss: 0.615 | Acc: 81.350% (3193/3925)

Epoch: 27
[Train] Epoch= 27  BatchID= 0 Loss: 0.507 | Acc: 78.906% (101/128)
[Train] Epoch= 27  BatchID= 10 Loss: 0.462 | Acc: 84.446% (1189/1408)
[Train] Epoch= 27  BatchID= 20 Loss: 0.481 | Acc: 83.594% (2247/2688)
[Train] Epoch= 27  BatchID= 30 Loss: 0.493 | Acc: 83.518% (3314/3968)
[Train] Epoch= 27  BatchID= 40 Loss: 0.496 | Acc: 83.575% (4386/5248)
[Train] Epoch= 27  BatchID= 50 Loss: 0.500 | Acc: 83.670% (5462/6528)
[Train] Epoch= 27  BatchID= 60 Loss: 0.506 | Acc: 83.350% (6508/7808)
[Train] Epoch= 27  BatchID= 70 Loss: 0.510 | Acc: 83.374% (7577/9088)
[Test] Epoch= 27  BatchID= 0 Loss: 0.296 | Acc: 89.062% (114/128)
[Test] Epoch= 27  BatchID= 10 Loss: 0.429 | Acc: 86.222% (1214/1408)
[Test] Epoch= 27  BatchID= 20 Loss: 0.399 | Acc: 87.314% (2347/2688)
[Test] Epoch= 27  BatchID= 30 Loss: 0.495 | Acc: 84.331% (3310/3925)

Epoch: 28
[Train] Epoch= 28  BatchID= 0 Loss: 0.464 | Acc: 84.375% (108/128)
[Train] Epoch= 28  BatchID= 10 Loss: 0.472 | Acc: 84.446% (1189/1408)
[Train] Epoch= 28  BatchID= 20 Loss: 0.484 | Acc: 84.449% (2270/2688)
[Train] Epoch= 28  BatchID= 30 Loss: 0.480 | Acc: 84.350% (3347/3968)
[Train] Epoch= 28  BatchID= 40 Loss: 0.486 | Acc: 83.822% (4399/5248)
[Train] Epoch= 28  BatchID= 50 Loss: 0.487 | Acc: 83.701% (5464/6528)
[Train] Epoch= 28  BatchID= 60 Loss: 0.494 | Acc: 83.530% (6522/7808)
[Train] Epoch= 28  BatchID= 70 Loss: 0.499 | Acc: 83.440% (7583/9088)
[Test] Epoch= 28  BatchID= 0 Loss: 0.367 | Acc: 92.188% (118/128)
[Test] Epoch= 28  BatchID= 10 Loss: 0.595 | Acc: 81.108% (1142/1408)
[Test] Epoch= 28  BatchID= 20 Loss: 0.544 | Acc: 82.701% (2223/2688)
[Test] Epoch= 28  BatchID= 30 Loss: 0.522 | Acc: 83.567% (3280/3925)

Epoch: 29
[Train] Epoch= 29  BatchID= 0 Loss: 0.443 | Acc: 86.719% (111/128)
[Train] Epoch= 29  BatchID= 10 Loss: 0.469 | Acc: 84.233% (1186/1408)
[Train] Epoch= 29  BatchID= 20 Loss: 0.506 | Acc: 83.445% (2243/2688)
[Train] Epoch= 29  BatchID= 30 Loss: 0.497 | Acc: 83.821% (3326/3968)
[Train] Epoch= 29  BatchID= 40 Loss: 0.494 | Acc: 83.899% (4403/5248)
[Train] Epoch= 29  BatchID= 50 Loss: 0.492 | Acc: 84.191% (5496/6528)
[Train] Epoch= 29  BatchID= 60 Loss: 0.504 | Acc: 83.594% (6527/7808)
[Train] Epoch= 29  BatchID= 70 Loss: 0.504 | Acc: 83.495% (7588/9088)
[Test] Epoch= 29  BatchID= 0 Loss: 0.271 | Acc: 92.188% (118/128)
[Test] Epoch= 29  BatchID= 10 Loss: 0.434 | Acc: 85.298% (1201/1408)
[Test] Epoch= 29  BatchID= 20 Loss: 0.449 | Acc: 85.231% (2291/2688)
[Test] Epoch= 29  BatchID= 30 Loss: 0.466 | Acc: 84.739% (3326/3925)
Saving..
Best accuracy:  84.73885350318471

Epoch: 30
[Train] Epoch= 30  BatchID= 0 Loss: 0.427 | Acc: 85.938% (110/128)
[Train] Epoch= 30  BatchID= 10 Loss: 0.449 | Acc: 85.866% (1209/1408)
[Train] Epoch= 30  BatchID= 20 Loss: 0.486 | Acc: 84.747% (2278/2688)
[Train] Epoch= 30  BatchID= 30 Loss: 0.482 | Acc: 84.879% (3368/3968)
[Train] Epoch= 30  BatchID= 40 Loss: 0.476 | Acc: 84.928% (4457/5248)
[Train] Epoch= 30  BatchID= 50 Loss: 0.484 | Acc: 84.727% (5531/6528)
[Train] Epoch= 30  BatchID= 60 Loss: 0.491 | Acc: 84.503% (6598/7808)
[Train] Epoch= 30  BatchID= 70 Loss: 0.491 | Acc: 84.496% (7679/9088)
[Test] Epoch= 30  BatchID= 0 Loss: 0.123 | Acc: 96.094% (123/128)
[Test] Epoch= 30  BatchID= 10 Loss: 0.522 | Acc: 82.102% (1156/1408)
[Test] Epoch= 30  BatchID= 20 Loss: 0.493 | Acc: 83.743% (2251/2688)
[Test] Epoch= 30  BatchID= 30 Loss: 0.493 | Acc: 84.000% (3297/3925)

Epoch: 31
[Train] Epoch= 31  BatchID= 0 Loss: 0.428 | Acc: 82.812% (106/128)
[Train] Epoch= 31  BatchID= 10 Loss: 0.453 | Acc: 85.085% (1198/1408)
[Train] Epoch= 31  BatchID= 20 Loss: 0.464 | Acc: 85.007% (2285/2688)
[Train] Epoch= 31  BatchID= 30 Loss: 0.464 | Acc: 85.030% (3374/3968)
[Train] Epoch= 31  BatchID= 40 Loss: 0.468 | Acc: 84.737% (4447/5248)
[Train] Epoch= 31  BatchID= 50 Loss: 0.469 | Acc: 84.559% (5520/6528)
[Train] Epoch= 31  BatchID= 60 Loss: 0.475 | Acc: 84.477% (6596/7808)
[Train] Epoch= 31  BatchID= 70 Loss: 0.476 | Acc: 84.386% (7669/9088)
[Test] Epoch= 31  BatchID= 0 Loss: 0.324 | Acc: 88.281% (113/128)
[Test] Epoch= 31  BatchID= 10 Loss: 0.513 | Acc: 83.097% (1170/1408)
[Test] Epoch= 31  BatchID= 20 Loss: 0.443 | Acc: 86.161% (2316/2688)
[Test] Epoch= 31  BatchID= 30 Loss: 0.473 | Acc: 85.401% (3352/3925)
Saving..
Best accuracy:  85.40127388535032

Epoch: 32
[Train] Epoch= 32  BatchID= 0 Loss: 0.438 | Acc: 89.844% (115/128)
[Train] Epoch= 32  BatchID= 10 Loss: 0.431 | Acc: 86.719% (1221/1408)
[Train] Epoch= 32  BatchID= 20 Loss: 0.447 | Acc: 85.491% (2298/2688)
[Train] Epoch= 32  BatchID= 30 Loss: 0.460 | Acc: 85.282% (3384/3968)
[Train] Epoch= 32  BatchID= 40 Loss: 0.461 | Acc: 85.271% (4475/5248)
[Train] Epoch= 32  BatchID= 50 Loss: 0.455 | Acc: 85.309% (5569/6528)
[Train] Epoch= 32  BatchID= 60 Loss: 0.457 | Acc: 85.169% (6650/7808)
[Train] Epoch= 32  BatchID= 70 Loss: 0.461 | Acc: 84.936% (7719/9088)
[Test] Epoch= 32  BatchID= 0 Loss: 0.129 | Acc: 95.312% (122/128)
[Test] Epoch= 32  BatchID= 10 Loss: 0.495 | Acc: 83.736% (1179/1408)
[Test] Epoch= 32  BatchID= 20 Loss: 0.504 | Acc: 84.003% (2258/2688)
[Test] Epoch= 32  BatchID= 30 Loss: 0.540 | Acc: 83.236% (3267/3925)

Epoch: 33
[Train] Epoch= 33  BatchID= 0 Loss: 0.479 | Acc: 84.375% (108/128)
[Train] Epoch= 33  BatchID= 10 Loss: 0.459 | Acc: 85.227% (1200/1408)
[Train] Epoch= 33  BatchID= 20 Loss: 0.494 | Acc: 83.854% (2254/2688)
[Train] Epoch= 33  BatchID= 30 Loss: 0.497 | Acc: 83.594% (3317/3968)
[Train] Epoch= 33  BatchID= 40 Loss: 0.496 | Acc: 83.651% (4390/5248)
[Train] Epoch= 33  BatchID= 50 Loss: 0.494 | Acc: 83.839% (5473/6528)
[Train] Epoch= 33  BatchID= 60 Loss: 0.482 | Acc: 84.221% (6576/7808)
[Train] Epoch= 33  BatchID= 70 Loss: 0.480 | Acc: 84.364% (7667/9088)
[Test] Epoch= 33  BatchID= 0 Loss: 0.228 | Acc: 94.531% (121/128)
[Test] Epoch= 33  BatchID= 10 Loss: 0.414 | Acc: 86.435% (1217/1408)
[Test] Epoch= 33  BatchID= 20 Loss: 0.397 | Acc: 87.388% (2349/2688)
[Test] Epoch= 33  BatchID= 30 Loss: 0.424 | Acc: 86.038% (3377/3925)
Saving..
Best accuracy:  86.03821656050955

Epoch: 34
[Train] Epoch= 34  BatchID= 0 Loss: 0.395 | Acc: 88.281% (113/128)
[Train] Epoch= 34  BatchID= 10 Loss: 0.428 | Acc: 85.724% (1207/1408)
[Train] Epoch= 34  BatchID= 20 Loss: 0.434 | Acc: 85.938% (2310/2688)
[Train] Epoch= 34  BatchID= 30 Loss: 0.449 | Acc: 85.383% (3388/3968)
[Train] Epoch= 34  BatchID= 40 Loss: 0.455 | Acc: 85.309% (4477/5248)
[Train] Epoch= 34  BatchID= 50 Loss: 0.466 | Acc: 84.957% (5546/6528)
[Train] Epoch= 34  BatchID= 60 Loss: 0.466 | Acc: 84.862% (6626/7808)
[Train] Epoch= 34  BatchID= 70 Loss: 0.467 | Acc: 84.991% (7724/9088)
[Test] Epoch= 34  BatchID= 0 Loss: 0.230 | Acc: 92.969% (119/128)
[Test] Epoch= 34  BatchID= 10 Loss: 0.400 | Acc: 87.145% (1227/1408)
[Test] Epoch= 34  BatchID= 20 Loss: 0.416 | Acc: 86.793% (2333/2688)
[Test] Epoch= 34  BatchID= 30 Loss: 0.402 | Acc: 87.210% (3423/3925)
Saving..
Best accuracy:  87.21019108280255

Epoch: 35
[Train] Epoch= 35  BatchID= 0 Loss: 0.528 | Acc: 83.594% (107/128)
[Train] Epoch= 35  BatchID= 10 Loss: 0.447 | Acc: 85.938% (1210/1408)
[Train] Epoch= 35  BatchID= 20 Loss: 0.452 | Acc: 85.677% (2303/2688)
[Train] Epoch= 35  BatchID= 30 Loss: 0.451 | Acc: 85.660% (3399/3968)
[Train] Epoch= 35  BatchID= 40 Loss: 0.438 | Acc: 85.880% (4507/5248)
[Train] Epoch= 35  BatchID= 50 Loss: 0.445 | Acc: 85.708% (5595/6528)
[Train] Epoch= 35  BatchID= 60 Loss: 0.448 | Acc: 85.438% (6671/7808)
[Train] Epoch= 35  BatchID= 70 Loss: 0.459 | Acc: 85.013% (7726/9088)
[Test] Epoch= 35  BatchID= 0 Loss: 0.146 | Acc: 94.531% (121/128)
[Test] Epoch= 35  BatchID= 10 Loss: 0.365 | Acc: 88.565% (1247/1408)
[Test] Epoch= 35  BatchID= 20 Loss: 0.430 | Acc: 87.091% (2341/2688)
[Test] Epoch= 35  BatchID= 30 Loss: 0.445 | Acc: 86.191% (3383/3925)

Epoch: 36
[Train] Epoch= 36  BatchID= 0 Loss: 0.496 | Acc: 85.156% (109/128)
[Train] Epoch= 36  BatchID= 10 Loss: 0.415 | Acc: 86.364% (1216/1408)
[Train] Epoch= 36  BatchID= 20 Loss: 0.422 | Acc: 86.012% (2312/2688)
[Train] Epoch= 36  BatchID= 30 Loss: 0.431 | Acc: 86.038% (3414/3968)
[Train] Epoch= 36  BatchID= 40 Loss: 0.437 | Acc: 85.957% (4511/5248)
[Train] Epoch= 36  BatchID= 50 Loss: 0.443 | Acc: 85.754% (5598/6528)
[Train] Epoch= 36  BatchID= 60 Loss: 0.444 | Acc: 85.643% (6687/7808)
[Train] Epoch= 36  BatchID= 70 Loss: 0.446 | Acc: 85.629% (7782/9088)
[Test] Epoch= 36  BatchID= 0 Loss: 0.249 | Acc: 92.188% (118/128)
[Test] Epoch= 36  BatchID= 10 Loss: 0.431 | Acc: 85.938% (1210/1408)
[Test] Epoch= 36  BatchID= 20 Loss: 0.438 | Acc: 86.533% (2326/2688)
[Test] Epoch= 36  BatchID= 30 Loss: 0.448 | Acc: 85.758% (3366/3925)

Epoch: 37
[Train] Epoch= 37  BatchID= 0 Loss: 0.462 | Acc: 84.375% (108/128)
[Train] Epoch= 37  BatchID= 10 Loss: 0.423 | Acc: 86.364% (1216/1408)
[Train] Epoch= 37  BatchID= 20 Loss: 0.432 | Acc: 86.086% (2314/2688)
[Train] Epoch= 37  BatchID= 30 Loss: 0.437 | Acc: 85.938% (3410/3968)
[Train] Epoch= 37  BatchID= 40 Loss: 0.447 | Acc: 85.575% (4491/5248)
[Train] Epoch= 37  BatchID= 50 Loss: 0.455 | Acc: 85.509% (5582/6528)
[Train] Epoch= 37  BatchID= 60 Loss: 0.459 | Acc: 85.323% (6662/7808)
[Train] Epoch= 37  BatchID= 70 Loss: 0.456 | Acc: 85.486% (7769/9088)
[Test] Epoch= 37  BatchID= 0 Loss: 0.161 | Acc: 95.312% (122/128)
[Test] Epoch= 37  BatchID= 10 Loss: 0.418 | Acc: 87.358% (1230/1408)
[Test] Epoch= 37  BatchID= 20 Loss: 0.463 | Acc: 85.863% (2308/2688)
[Test] Epoch= 37  BatchID= 30 Loss: 0.461 | Acc: 85.758% (3366/3925)

Epoch: 38
[Train] Epoch= 38  BatchID= 0 Loss: 0.391 | Acc: 86.719% (111/128)
[Train] Epoch= 38  BatchID= 10 Loss: 0.417 | Acc: 86.790% (1222/1408)
[Train] Epoch= 38  BatchID= 20 Loss: 0.438 | Acc: 86.235% (2318/2688)
[Train] Epoch= 38  BatchID= 30 Loss: 0.443 | Acc: 86.064% (3415/3968)
[Train] Epoch= 38  BatchID= 40 Loss: 0.448 | Acc: 85.861% (4506/5248)
[Train] Epoch= 38  BatchID= 50 Loss: 0.446 | Acc: 85.769% (5599/6528)
[Train] Epoch= 38  BatchID= 60 Loss: 0.453 | Acc: 85.374% (6666/7808)
[Train] Epoch= 38  BatchID= 70 Loss: 0.452 | Acc: 85.431% (7764/9088)
[Test] Epoch= 38  BatchID= 0 Loss: 0.144 | Acc: 96.875% (124/128)
[Test] Epoch= 38  BatchID= 10 Loss: 0.362 | Acc: 88.849% (1251/1408)
[Test] Epoch= 38  BatchID= 20 Loss: 0.353 | Acc: 88.914% (2390/2688)
[Test] Epoch= 38  BatchID= 30 Loss: 0.432 | Acc: 86.522% (3396/3925)

Epoch: 39
[Train] Epoch= 39  BatchID= 0 Loss: 0.402 | Acc: 85.156% (109/128)
[Train] Epoch= 39  BatchID= 10 Loss: 0.470 | Acc: 83.878% (1181/1408)
[Train] Epoch= 39  BatchID= 20 Loss: 0.476 | Acc: 84.040% (2259/2688)
[Train] Epoch= 39  BatchID= 30 Loss: 0.460 | Acc: 84.703% (3361/3968)
[Train] Epoch= 39  BatchID= 40 Loss: 0.461 | Acc: 84.889% (4455/5248)
[Train] Epoch= 39  BatchID= 50 Loss: 0.456 | Acc: 84.972% (5547/6528)
[Train] Epoch= 39  BatchID= 60 Loss: 0.455 | Acc: 84.964% (6634/7808)
[Train] Epoch= 39  BatchID= 70 Loss: 0.456 | Acc: 85.090% (7733/9088)
[Test] Epoch= 39  BatchID= 0 Loss: 0.292 | Acc: 91.406% (117/128)
[Test] Epoch= 39  BatchID= 10 Loss: 0.392 | Acc: 86.506% (1218/1408)
[Test] Epoch= 39  BatchID= 20 Loss: 0.501 | Acc: 83.519% (2245/2688)
[Test] Epoch= 39  BatchID= 30 Loss: 0.480 | Acc: 84.255% (3307/3925)

Epoch: 40
[Train] Epoch= 40  BatchID= 0 Loss: 0.452 | Acc: 85.938% (110/128)
[Train] Epoch= 40  BatchID= 10 Loss: 0.459 | Acc: 84.730% (1193/1408)
[Train] Epoch= 40  BatchID= 20 Loss: 0.437 | Acc: 86.049% (2313/2688)
[Train] Epoch= 40  BatchID= 30 Loss: 0.443 | Acc: 85.484% (3392/3968)
[Train] Epoch= 40  BatchID= 40 Loss: 0.443 | Acc: 85.442% (4484/5248)
[Train] Epoch= 40  BatchID= 50 Loss: 0.443 | Acc: 85.524% (5583/6528)
[Train] Epoch= 40  BatchID= 60 Loss: 0.448 | Acc: 85.272% (6658/7808)
[Train] Epoch= 40  BatchID= 70 Loss: 0.442 | Acc: 85.475% (7768/9088)
[Test] Epoch= 40  BatchID= 0 Loss: 0.249 | Acc: 92.969% (119/128)
[Test] Epoch= 40  BatchID= 10 Loss: 0.401 | Acc: 86.009% (1211/1408)
[Test] Epoch= 40  BatchID= 20 Loss: 0.439 | Acc: 85.193% (2290/2688)
[Test] Epoch= 40  BatchID= 30 Loss: 0.455 | Acc: 84.739% (3326/3925)

Epoch: 41
[Train] Epoch= 41  BatchID= 0 Loss: 0.380 | Acc: 86.719% (111/128)
[Train] Epoch= 41  BatchID= 10 Loss: 0.420 | Acc: 86.364% (1216/1408)
[Train] Epoch= 41  BatchID= 20 Loss: 0.418 | Acc: 86.049% (2313/2688)
[Train] Epoch= 41  BatchID= 30 Loss: 0.418 | Acc: 86.416% (3429/3968)
[Train] Epoch= 41  BatchID= 40 Loss: 0.415 | Acc: 86.528% (4541/5248)
[Train] Epoch= 41  BatchID= 50 Loss: 0.419 | Acc: 86.382% (5639/6528)
[Train] Epoch= 41  BatchID= 60 Loss: 0.419 | Acc: 86.488% (6753/7808)
[Train] Epoch= 41  BatchID= 70 Loss: 0.425 | Acc: 86.268% (7840/9088)
[Test] Epoch= 41  BatchID= 0 Loss: 0.121 | Acc: 96.094% (123/128)
[Test] Epoch= 41  BatchID= 10 Loss: 0.353 | Acc: 88.991% (1253/1408)
[Test] Epoch= 41  BatchID= 20 Loss: 0.344 | Acc: 89.174% (2397/2688)
[Test] Epoch= 41  BatchID= 30 Loss: 0.405 | Acc: 86.828% (3408/3925)

Epoch: 42
[Train] Epoch= 42  BatchID= 0 Loss: 0.431 | Acc: 85.156% (109/128)
[Train] Epoch= 42  BatchID= 10 Loss: 0.397 | Acc: 86.648% (1220/1408)
[Train] Epoch= 42  BatchID= 20 Loss: 0.396 | Acc: 87.463% (2351/2688)
[Train] Epoch= 42  BatchID= 30 Loss: 0.389 | Acc: 87.273% (3463/3968)
[Train] Epoch= 42  BatchID= 40 Loss: 0.396 | Acc: 87.081% (4570/5248)
[Train] Epoch= 42  BatchID= 50 Loss: 0.399 | Acc: 87.025% (5681/6528)
[Train] Epoch= 42  BatchID= 60 Loss: 0.407 | Acc: 86.898% (6785/7808)
[Train] Epoch= 42  BatchID= 70 Loss: 0.409 | Acc: 86.917% (7899/9088)
[Test] Epoch= 42  BatchID= 0 Loss: 0.217 | Acc: 91.406% (117/128)
[Test] Epoch= 42  BatchID= 10 Loss: 0.420 | Acc: 86.080% (1212/1408)
[Test] Epoch= 42  BatchID= 20 Loss: 0.371 | Acc: 87.798% (2360/2688)
[Test] Epoch= 42  BatchID= 30 Loss: 0.416 | Acc: 86.395% (3391/3925)

Epoch: 43
[Train] Epoch= 43  BatchID= 0 Loss: 0.278 | Acc: 91.406% (117/128)
[Train] Epoch= 43  BatchID= 10 Loss: 0.345 | Acc: 89.560% (1261/1408)
[Train] Epoch= 43  BatchID= 20 Loss: 0.367 | Acc: 88.616% (2382/2688)
[Train] Epoch= 43  BatchID= 30 Loss: 0.379 | Acc: 88.155% (3498/3968)
[Train] Epoch= 43  BatchID= 40 Loss: 0.390 | Acc: 87.309% (4582/5248)
[Train] Epoch= 43  BatchID= 50 Loss: 0.393 | Acc: 87.331% (5701/6528)
[Train] Epoch= 43  BatchID= 60 Loss: 0.403 | Acc: 87.013% (6794/7808)
[Train] Epoch= 43  BatchID= 70 Loss: 0.406 | Acc: 86.906% (7898/9088)
[Test] Epoch= 43  BatchID= 0 Loss: 0.151 | Acc: 94.531% (121/128)
[Test] Epoch= 43  BatchID= 10 Loss: 0.361 | Acc: 88.423% (1245/1408)
[Test] Epoch= 43  BatchID= 20 Loss: 0.409 | Acc: 87.240% (2345/2688)
[Test] Epoch= 43  BatchID= 30 Loss: 0.405 | Acc: 86.955% (3413/3925)

Epoch: 44
[Train] Epoch= 44  BatchID= 0 Loss: 0.372 | Acc: 87.500% (112/128)
[Train] Epoch= 44  BatchID= 10 Loss: 0.377 | Acc: 87.571% (1233/1408)
[Train] Epoch= 44  BatchID= 20 Loss: 0.386 | Acc: 87.426% (2350/2688)
[Train] Epoch= 44  BatchID= 30 Loss: 0.374 | Acc: 87.651% (3478/3968)
[Train] Epoch= 44  BatchID= 40 Loss: 0.373 | Acc: 87.748% (4605/5248)
[Train] Epoch= 44  BatchID= 50 Loss: 0.375 | Acc: 87.669% (5723/6528)
[Train] Epoch= 44  BatchID= 60 Loss: 0.377 | Acc: 87.628% (6842/7808)
[Train] Epoch= 44  BatchID= 70 Loss: 0.382 | Acc: 87.423% (7945/9088)
[Test] Epoch= 44  BatchID= 0 Loss: 0.095 | Acc: 97.656% (125/128)
[Test] Epoch= 44  BatchID= 10 Loss: 0.388 | Acc: 88.139% (1241/1408)
[Test] Epoch= 44  BatchID= 20 Loss: 0.487 | Acc: 85.528% (2299/2688)
[Test] Epoch= 44  BatchID= 30 Loss: 0.458 | Acc: 86.446% (3393/3925)

Epoch: 45
[Train] Epoch= 45  BatchID= 0 Loss: 0.377 | Acc: 87.500% (112/128)
[Train] Epoch= 45  BatchID= 10 Loss: 0.402 | Acc: 87.216% (1228/1408)
[Train] Epoch= 45  BatchID= 20 Loss: 0.393 | Acc: 87.537% (2353/2688)
[Train] Epoch= 45  BatchID= 30 Loss: 0.383 | Acc: 87.500% (3472/3968)
[Train] Epoch= 45  BatchID= 40 Loss: 0.392 | Acc: 87.043% (4568/5248)
[Train] Epoch= 45  BatchID= 50 Loss: 0.391 | Acc: 87.178% (5691/6528)
[Train] Epoch= 45  BatchID= 60 Loss: 0.399 | Acc: 86.847% (6781/7808)
[Train] Epoch= 45  BatchID= 70 Loss: 0.397 | Acc: 86.950% (7902/9088)
[Test] Epoch= 45  BatchID= 0 Loss: 0.266 | Acc: 91.406% (117/128)
[Test] Epoch= 45  BatchID= 10 Loss: 0.493 | Acc: 84.730% (1193/1408)
[Test] Epoch= 45  BatchID= 20 Loss: 0.444 | Acc: 85.938% (2310/2688)
[Test] Epoch= 45  BatchID= 30 Loss: 0.457 | Acc: 85.503% (3356/3925)

Epoch: 46
[Train] Epoch= 46  BatchID= 0 Loss: 0.391 | Acc: 89.062% (114/128)
[Train] Epoch= 46  BatchID= 10 Loss: 0.350 | Acc: 89.347% (1258/1408)
[Train] Epoch= 46  BatchID= 20 Loss: 0.382 | Acc: 87.872% (2362/2688)
[Train] Epoch= 46  BatchID= 30 Loss: 0.386 | Acc: 87.676% (3479/3968)
[Train] Epoch= 46  BatchID= 40 Loss: 0.381 | Acc: 87.995% (4618/5248)
[Train] Epoch= 46  BatchID= 50 Loss: 0.387 | Acc: 87.806% (5732/6528)
[Train] Epoch= 46  BatchID= 60 Loss: 0.391 | Acc: 87.551% (6836/7808)
[Train] Epoch= 46  BatchID= 70 Loss: 0.391 | Acc: 87.445% (7947/9088)
[Test] Epoch= 46  BatchID= 0 Loss: 0.185 | Acc: 92.969% (119/128)
[Test] Epoch= 46  BatchID= 10 Loss: 0.367 | Acc: 88.068% (1240/1408)
[Test] Epoch= 46  BatchID= 20 Loss: 0.396 | Acc: 87.314% (2347/2688)
[Test] Epoch= 46  BatchID= 30 Loss: 0.424 | Acc: 86.293% (3387/3925)

Epoch: 47
[Train] Epoch= 47  BatchID= 0 Loss: 0.298 | Acc: 89.844% (115/128)
[Train] Epoch= 47  BatchID= 10 Loss: 0.358 | Acc: 88.991% (1253/1408)
[Train] Epoch= 47  BatchID= 20 Loss: 0.364 | Acc: 88.542% (2380/2688)
[Train] Epoch= 47  BatchID= 30 Loss: 0.369 | Acc: 88.281% (3503/3968)
[Train] Epoch= 47  BatchID= 40 Loss: 0.377 | Acc: 88.034% (4620/5248)
[Train] Epoch= 47  BatchID= 50 Loss: 0.386 | Acc: 87.730% (5727/6528)
[Train] Epoch= 47  BatchID= 60 Loss: 0.383 | Acc: 87.961% (6868/7808)
[Train] Epoch= 47  BatchID= 70 Loss: 0.378 | Acc: 88.050% (8002/9088)
[Test] Epoch= 47  BatchID= 0 Loss: 0.117 | Acc: 96.094% (123/128)
[Test] Epoch= 47  BatchID= 10 Loss: 0.374 | Acc: 88.139% (1241/1408)
[Test] Epoch= 47  BatchID= 20 Loss: 0.386 | Acc: 88.132% (2369/2688)
[Test] Epoch= 47  BatchID= 30 Loss: 0.387 | Acc: 87.898% (3450/3925)
Saving..
Best accuracy:  87.89808917197452

Epoch: 48
[Train] Epoch= 48  BatchID= 0 Loss: 0.480 | Acc: 87.500% (112/128)
[Train] Epoch= 48  BatchID= 10 Loss: 0.379 | Acc: 88.423% (1245/1408)
[Train] Epoch= 48  BatchID= 20 Loss: 0.363 | Acc: 88.579% (2381/2688)
[Train] Epoch= 48  BatchID= 30 Loss: 0.381 | Acc: 87.954% (3490/3968)
[Train] Epoch= 48  BatchID= 40 Loss: 0.379 | Acc: 88.034% (4620/5248)
[Train] Epoch= 48  BatchID= 50 Loss: 0.377 | Acc: 88.281% (5763/6528)
[Train] Epoch= 48  BatchID= 60 Loss: 0.377 | Acc: 88.230% (6889/7808)
[Train] Epoch= 48  BatchID= 70 Loss: 0.374 | Acc: 88.237% (8019/9088)
[Test] Epoch= 48  BatchID= 0 Loss: 0.055 | Acc: 98.438% (126/128)
[Test] Epoch= 48  BatchID= 10 Loss: 0.276 | Acc: 91.761% (1292/1408)
[Test] Epoch= 48  BatchID= 20 Loss: 0.308 | Acc: 90.365% (2429/2688)
[Test] Epoch= 48  BatchID= 30 Loss: 0.380 | Acc: 88.204% (3462/3925)
Saving..
Best accuracy:  88.20382165605096

Epoch: 49
[Train] Epoch= 49  BatchID= 0 Loss: 0.396 | Acc: 86.719% (111/128)
[Train] Epoch= 49  BatchID= 10 Loss: 0.367 | Acc: 88.068% (1240/1408)
[Train] Epoch= 49  BatchID= 20 Loss: 0.377 | Acc: 87.574% (2354/2688)
[Train] Epoch= 49  BatchID= 30 Loss: 0.379 | Acc: 87.954% (3490/3968)
[Train] Epoch= 49  BatchID= 40 Loss: 0.383 | Acc: 87.691% (4602/5248)
[Train] Epoch= 49  BatchID= 50 Loss: 0.386 | Acc: 87.561% (5716/6528)
[Train] Epoch= 49  BatchID= 60 Loss: 0.381 | Acc: 87.705% (6848/7808)
[Train] Epoch= 49  BatchID= 70 Loss: 0.378 | Acc: 87.896% (7988/9088)
[Test] Epoch= 49  BatchID= 0 Loss: 0.131 | Acc: 96.094% (123/128)
[Test] Epoch= 49  BatchID= 10 Loss: 0.353 | Acc: 88.494% (1246/1408)
[Test] Epoch= 49  BatchID= 20 Loss: 0.331 | Acc: 89.174% (2397/2688)
[Test] Epoch= 49  BatchID= 30 Loss: 0.342 | Acc: 89.096% (3497/3925)
Saving..
Best accuracy:  89.09554140127389

Epoch: 50
[Train] Epoch= 50  BatchID= 0 Loss: 0.342 | Acc: 88.281% (113/128)
[Train] Epoch= 50  BatchID= 10 Loss: 0.327 | Acc: 89.702% (1263/1408)
[Train] Epoch= 50  BatchID= 20 Loss: 0.325 | Acc: 89.695% (2411/2688)
[Train] Epoch= 50  BatchID= 30 Loss: 0.328 | Acc: 89.592% (3555/3968)
[Train] Epoch= 50  BatchID= 40 Loss: 0.339 | Acc: 88.967% (4669/5248)
[Train] Epoch= 50  BatchID= 50 Loss: 0.344 | Acc: 88.894% (5803/6528)
[Train] Epoch= 50  BatchID= 60 Loss: 0.355 | Acc: 88.589% (6917/7808)
[Train] Epoch= 50  BatchID= 70 Loss: 0.351 | Acc: 88.765% (8067/9088)
[Test] Epoch= 50  BatchID= 0 Loss: 0.088 | Acc: 96.875% (124/128)
[Test] Epoch= 50  BatchID= 10 Loss: 0.265 | Acc: 91.477% (1288/1408)
[Test] Epoch= 50  BatchID= 20 Loss: 0.320 | Acc: 90.253% (2426/2688)
[Test] Epoch= 50  BatchID= 30 Loss: 0.342 | Acc: 89.299% (3505/3925)
Saving..
Best accuracy:  89.29936305732484

Epoch: 51
[Train] Epoch= 51  BatchID= 0 Loss: 0.331 | Acc: 85.156% (109/128)
[Train] Epoch= 51  BatchID= 10 Loss: 0.344 | Acc: 88.494% (1246/1408)
[Train] Epoch= 51  BatchID= 20 Loss: 0.342 | Acc: 88.765% (2386/2688)
[Train] Epoch= 51  BatchID= 30 Loss: 0.340 | Acc: 88.684% (3519/3968)
[Train] Epoch= 51  BatchID= 40 Loss: 0.348 | Acc: 88.396% (4639/5248)
[Train] Epoch= 51  BatchID= 50 Loss: 0.349 | Acc: 88.373% (5769/6528)
[Train] Epoch= 51  BatchID= 60 Loss: 0.355 | Acc: 88.397% (6902/7808)
[Train] Epoch= 51  BatchID= 70 Loss: 0.357 | Acc: 88.501% (8043/9088)
[Test] Epoch= 51  BatchID= 0 Loss: 0.272 | Acc: 92.188% (118/128)
[Test] Epoch= 51  BatchID= 10 Loss: 0.388 | Acc: 87.287% (1229/1408)
[Test] Epoch= 51  BatchID= 20 Loss: 0.366 | Acc: 88.579% (2381/2688)
[Test] Epoch= 51  BatchID= 30 Loss: 0.431 | Acc: 86.828% (3408/3925)

Epoch: 52
[Train] Epoch= 52  BatchID= 0 Loss: 0.351 | Acc: 89.062% (114/128)
[Train] Epoch= 52  BatchID= 10 Loss: 0.347 | Acc: 89.276% (1257/1408)
[Train] Epoch= 52  BatchID= 20 Loss: 0.326 | Acc: 90.216% (2425/2688)
[Train] Epoch= 52  BatchID= 30 Loss: 0.339 | Acc: 89.466% (3550/3968)
[Train] Epoch= 52  BatchID= 40 Loss: 0.339 | Acc: 89.463% (4695/5248)
[Train] Epoch= 52  BatchID= 50 Loss: 0.343 | Acc: 89.384% (5835/6528)
[Train] Epoch= 52  BatchID= 60 Loss: 0.343 | Acc: 89.267% (6970/7808)
[Train] Epoch= 52  BatchID= 70 Loss: 0.341 | Acc: 89.162% (8103/9088)
[Test] Epoch= 52  BatchID= 0 Loss: 0.169 | Acc: 95.312% (122/128)
[Test] Epoch= 52  BatchID= 10 Loss: 0.401 | Acc: 87.358% (1230/1408)
[Test] Epoch= 52  BatchID= 20 Loss: 0.400 | Acc: 87.351% (2348/2688)
[Test] Epoch= 52  BatchID= 30 Loss: 0.379 | Acc: 88.153% (3460/3925)

Epoch: 53
[Train] Epoch= 53  BatchID= 0 Loss: 0.229 | Acc: 92.969% (119/128)
[Train] Epoch= 53  BatchID= 10 Loss: 0.349 | Acc: 88.281% (1243/1408)
[Train] Epoch= 53  BatchID= 20 Loss: 0.342 | Acc: 88.653% (2383/2688)
[Train] Epoch= 53  BatchID= 30 Loss: 0.329 | Acc: 89.365% (3546/3968)
[Train] Epoch= 53  BatchID= 40 Loss: 0.336 | Acc: 89.177% (4680/5248)
[Train] Epoch= 53  BatchID= 50 Loss: 0.334 | Acc: 89.354% (5833/6528)
[Train] Epoch= 53  BatchID= 60 Loss: 0.339 | Acc: 89.203% (6965/7808)
[Train] Epoch= 53  BatchID= 70 Loss: 0.338 | Acc: 89.062% (8094/9088)
[Test] Epoch= 53  BatchID= 0 Loss: 0.103 | Acc: 96.875% (124/128)
[Test] Epoch= 53  BatchID= 10 Loss: 0.327 | Acc: 89.631% (1262/1408)
[Test] Epoch= 53  BatchID= 20 Loss: 0.362 | Acc: 88.802% (2387/2688)
[Test] Epoch= 53  BatchID= 30 Loss: 0.368 | Acc: 88.662% (3480/3925)

Epoch: 54
[Train] Epoch= 54  BatchID= 0 Loss: 0.315 | Acc: 87.500% (112/128)
[Train] Epoch= 54  BatchID= 10 Loss: 0.326 | Acc: 89.347% (1258/1408)
[Train] Epoch= 54  BatchID= 20 Loss: 0.335 | Acc: 89.286% (2400/2688)
[Train] Epoch= 54  BatchID= 30 Loss: 0.329 | Acc: 89.516% (3552/3968)
[Train] Epoch= 54  BatchID= 40 Loss: 0.325 | Acc: 89.539% (4699/5248)
[Train] Epoch= 54  BatchID= 50 Loss: 0.331 | Acc: 89.568% (5847/6528)
[Train] Epoch= 54  BatchID= 60 Loss: 0.331 | Acc: 89.485% (6987/7808)
[Train] Epoch= 54  BatchID= 70 Loss: 0.334 | Acc: 89.426% (8127/9088)
[Test] Epoch= 54  BatchID= 0 Loss: 0.262 | Acc: 92.969% (119/128)
[Test] Epoch= 54  BatchID= 10 Loss: 0.390 | Acc: 88.565% (1247/1408)
[Test] Epoch= 54  BatchID= 20 Loss: 0.379 | Acc: 88.207% (2371/2688)
[Test] Epoch= 54  BatchID= 30 Loss: 0.378 | Acc: 88.331% (3467/3925)

Epoch: 55
[Train] Epoch= 55  BatchID= 0 Loss: 0.252 | Acc: 92.969% (119/128)
[Train] Epoch= 55  BatchID= 10 Loss: 0.309 | Acc: 89.986% (1267/1408)
[Train] Epoch= 55  BatchID= 20 Loss: 0.301 | Acc: 90.365% (2429/2688)
[Train] Epoch= 55  BatchID= 30 Loss: 0.315 | Acc: 89.869% (3566/3968)
[Train] Epoch= 55  BatchID= 40 Loss: 0.311 | Acc: 89.977% (4722/5248)
[Train] Epoch= 55  BatchID= 50 Loss: 0.312 | Acc: 89.982% (5874/6528)
[Train] Epoch= 55  BatchID= 60 Loss: 0.315 | Acc: 89.869% (7017/7808)
[Train] Epoch= 55  BatchID= 70 Loss: 0.320 | Acc: 89.701% (8152/9088)
[Test] Epoch= 55  BatchID= 0 Loss: 0.132 | Acc: 96.094% (123/128)
[Test] Epoch= 55  BatchID= 10 Loss: 0.301 | Acc: 90.270% (1271/1408)
[Test] Epoch= 55  BatchID= 20 Loss: 0.399 | Acc: 87.128% (2342/2688)
[Test] Epoch= 55  BatchID= 30 Loss: 0.393 | Acc: 87.134% (3420/3925)

Epoch: 56
[Train] Epoch= 56  BatchID= 0 Loss: 0.406 | Acc: 87.500% (112/128)
[Train] Epoch= 56  BatchID= 10 Loss: 0.297 | Acc: 90.696% (1277/1408)
[Train] Epoch= 56  BatchID= 20 Loss: 0.288 | Acc: 90.551% (2434/2688)
[Train] Epoch= 56  BatchID= 30 Loss: 0.292 | Acc: 90.499% (3591/3968)
[Train] Epoch= 56  BatchID= 40 Loss: 0.294 | Acc: 90.111% (4729/5248)
[Train] Epoch= 56  BatchID= 50 Loss: 0.299 | Acc: 90.028% (5877/6528)
[Train] Epoch= 56  BatchID= 60 Loss: 0.301 | Acc: 89.959% (7024/7808)
[Train] Epoch= 56  BatchID= 70 Loss: 0.308 | Acc: 89.833% (8164/9088)
[Test] Epoch= 56  BatchID= 0 Loss: 0.085 | Acc: 96.875% (124/128)
[Test] Epoch= 56  BatchID= 10 Loss: 0.300 | Acc: 90.057% (1268/1408)
[Test] Epoch= 56  BatchID= 20 Loss: 0.298 | Acc: 90.030% (2420/2688)
[Test] Epoch= 56  BatchID= 30 Loss: 0.327 | Acc: 89.350% (3507/3925)
Saving..
Best accuracy:  89.35031847133757

Epoch: 57
[Train] Epoch= 57  BatchID= 0 Loss: 0.354 | Acc: 88.281% (113/128)
[Train] Epoch= 57  BatchID= 10 Loss: 0.330 | Acc: 89.702% (1263/1408)
[Train] Epoch= 57  BatchID= 20 Loss: 0.303 | Acc: 90.513% (2433/2688)
[Train] Epoch= 57  BatchID= 30 Loss: 0.296 | Acc: 90.524% (3592/3968)
[Train] Epoch= 57  BatchID= 40 Loss: 0.295 | Acc: 90.606% (4755/5248)
[Train] Epoch= 57  BatchID= 50 Loss: 0.307 | Acc: 90.273% (5893/6528)
[Train] Epoch= 57  BatchID= 60 Loss: 0.308 | Acc: 90.266% (7048/7808)
[Train] Epoch= 57  BatchID= 70 Loss: 0.313 | Acc: 90.064% (8185/9088)
[Test] Epoch= 57  BatchID= 0 Loss: 0.147 | Acc: 93.750% (120/128)
[Test] Epoch= 57  BatchID= 10 Loss: 0.433 | Acc: 85.938% (1210/1408)
[Test] Epoch= 57  BatchID= 20 Loss: 0.377 | Acc: 88.132% (2369/2688)
[Test] Epoch= 57  BatchID= 30 Loss: 0.348 | Acc: 88.917% (3490/3925)

Epoch: 58
[Train] Epoch= 58  BatchID= 0 Loss: 0.253 | Acc: 89.844% (115/128)
[Train] Epoch= 58  BatchID= 10 Loss: 0.282 | Acc: 90.554% (1275/1408)
[Train] Epoch= 58  BatchID= 20 Loss: 0.287 | Acc: 90.997% (2446/2688)
[Train] Epoch= 58  BatchID= 30 Loss: 0.301 | Acc: 90.499% (3591/3968)
[Train] Epoch= 58  BatchID= 40 Loss: 0.297 | Acc: 90.434% (4746/5248)
[Train] Epoch= 58  BatchID= 50 Loss: 0.298 | Acc: 90.640% (5917/6528)
[Train] Epoch= 58  BatchID= 60 Loss: 0.300 | Acc: 90.548% (7070/7808)
[Train] Epoch= 58  BatchID= 70 Loss: 0.300 | Acc: 90.471% (8222/9088)
[Test] Epoch= 58  BatchID= 0 Loss: 0.267 | Acc: 92.188% (118/128)
[Test] Epoch= 58  BatchID= 10 Loss: 0.330 | Acc: 90.554% (1275/1408)
[Test] Epoch= 58  BatchID= 20 Loss: 0.336 | Acc: 90.290% (2427/2688)
[Test] Epoch= 58  BatchID= 30 Loss: 0.357 | Acc: 89.401% (3509/3925)
Saving..
Best accuracy:  89.40127388535032

Epoch: 59
[Train] Epoch= 59  BatchID= 0 Loss: 0.291 | Acc: 92.188% (118/128)
[Train] Epoch= 59  BatchID= 10 Loss: 0.273 | Acc: 91.335% (1286/1408)
[Train] Epoch= 59  BatchID= 20 Loss: 0.291 | Acc: 90.848% (2442/2688)
[Train] Epoch= 59  BatchID= 30 Loss: 0.312 | Acc: 90.146% (3577/3968)
[Train] Epoch= 59  BatchID= 40 Loss: 0.312 | Acc: 90.130% (4730/5248)
[Train] Epoch= 59  BatchID= 50 Loss: 0.313 | Acc: 90.165% (5886/6528)
[Train] Epoch= 59  BatchID= 60 Loss: 0.312 | Acc: 90.215% (7044/7808)
[Train] Epoch= 59  BatchID= 70 Loss: 0.315 | Acc: 90.240% (8201/9088)
[Test] Epoch= 59  BatchID= 0 Loss: 0.138 | Acc: 95.312% (122/128)
[Test] Epoch= 59  BatchID= 10 Loss: 0.342 | Acc: 88.991% (1253/1408)
[Test] Epoch= 59  BatchID= 20 Loss: 0.325 | Acc: 89.807% (2414/2688)
[Test] Epoch= 59  BatchID= 30 Loss: 0.346 | Acc: 88.994% (3493/3925)

Epoch: 60
[Train] Epoch= 60  BatchID= 0 Loss: 0.250 | Acc: 91.406% (117/128)
[Train] Epoch= 60  BatchID= 10 Loss: 0.311 | Acc: 90.412% (1273/1408)
[Train] Epoch= 60  BatchID= 20 Loss: 0.301 | Acc: 90.662% (2437/2688)
[Train] Epoch= 60  BatchID= 30 Loss: 0.293 | Acc: 90.978% (3610/3968)
[Train] Epoch= 60  BatchID= 40 Loss: 0.292 | Acc: 91.006% (4776/5248)
[Train] Epoch= 60  BatchID= 50 Loss: 0.291 | Acc: 90.947% (5937/6528)
[Train] Epoch= 60  BatchID= 60 Loss: 0.287 | Acc: 91.137% (7116/7808)
[Train] Epoch= 60  BatchID= 70 Loss: 0.288 | Acc: 91.054% (8275/9088)
[Test] Epoch= 60  BatchID= 0 Loss: 0.096 | Acc: 96.094% (123/128)
[Test] Epoch= 60  BatchID= 10 Loss: 0.310 | Acc: 89.773% (1264/1408)
[Test] Epoch= 60  BatchID= 20 Loss: 0.288 | Acc: 90.551% (2434/2688)
[Test] Epoch= 60  BatchID= 30 Loss: 0.316 | Acc: 89.809% (3525/3925)
Saving..
Best accuracy:  89.80891719745223

Epoch: 61
[Train] Epoch= 61  BatchID= 0 Loss: 0.299 | Acc: 91.406% (117/128)
[Train] Epoch= 61  BatchID= 10 Loss: 0.283 | Acc: 90.980% (1281/1408)
[Train] Epoch= 61  BatchID= 20 Loss: 0.278 | Acc: 90.960% (2445/2688)
[Train] Epoch= 61  BatchID= 30 Loss: 0.285 | Acc: 90.902% (3607/3968)
[Train] Epoch= 61  BatchID= 40 Loss: 0.280 | Acc: 91.235% (4788/5248)
[Train] Epoch= 61  BatchID= 50 Loss: 0.282 | Acc: 91.238% (5956/6528)
[Train] Epoch= 61  BatchID= 60 Loss: 0.284 | Acc: 91.112% (7114/7808)
[Train] Epoch= 61  BatchID= 70 Loss: 0.287 | Acc: 91.032% (8273/9088)
[Test] Epoch= 61  BatchID= 0 Loss: 0.113 | Acc: 96.875% (124/128)
[Test] Epoch= 61  BatchID= 10 Loss: 0.301 | Acc: 90.696% (1277/1408)
[Test] Epoch= 61  BatchID= 20 Loss: 0.313 | Acc: 90.327% (2428/2688)
[Test] Epoch= 61  BatchID= 30 Loss: 0.352 | Acc: 89.197% (3501/3925)

Epoch: 62
[Train] Epoch= 62  BatchID= 0 Loss: 0.234 | Acc: 91.406% (117/128)
[Train] Epoch= 62  BatchID= 10 Loss: 0.248 | Acc: 92.330% (1300/1408)
[Train] Epoch= 62  BatchID= 20 Loss: 0.255 | Acc: 92.039% (2474/2688)
[Train] Epoch= 62  BatchID= 30 Loss: 0.263 | Acc: 91.482% (3630/3968)
[Train] Epoch= 62  BatchID= 40 Loss: 0.261 | Acc: 91.654% (4810/5248)
[Train] Epoch= 62  BatchID= 50 Loss: 0.259 | Acc: 91.774% (5991/6528)
[Train] Epoch= 62  BatchID= 60 Loss: 0.260 | Acc: 91.778% (7166/7808)
[Train] Epoch= 62  BatchID= 70 Loss: 0.265 | Acc: 91.670% (8331/9088)
[Test] Epoch= 62  BatchID= 0 Loss: 0.116 | Acc: 96.094% (123/128)
[Test] Epoch= 62  BatchID= 10 Loss: 0.266 | Acc: 91.761% (1292/1408)
[Test] Epoch= 62  BatchID= 20 Loss: 0.305 | Acc: 90.551% (2434/2688)
[Test] Epoch= 62  BatchID= 30 Loss: 0.327 | Acc: 89.834% (3526/3925)
Saving..
Best accuracy:  89.8343949044586

Epoch: 63
[Train] Epoch= 63  BatchID= 0 Loss: 0.259 | Acc: 92.969% (119/128)
[Train] Epoch= 63  BatchID= 10 Loss: 0.287 | Acc: 91.406% (1287/1408)
[Train] Epoch= 63  BatchID= 20 Loss: 0.279 | Acc: 91.146% (2450/2688)
[Train] Epoch= 63  BatchID= 30 Loss: 0.279 | Acc: 91.104% (3615/3968)
[Train] Epoch= 63  BatchID= 40 Loss: 0.270 | Acc: 91.425% (4798/5248)
[Train] Epoch= 63  BatchID= 50 Loss: 0.273 | Acc: 91.238% (5956/6528)
[Train] Epoch= 63  BatchID= 60 Loss: 0.278 | Acc: 91.060% (7110/7808)
[Train] Epoch= 63  BatchID= 70 Loss: 0.279 | Acc: 90.999% (8270/9088)
[Test] Epoch= 63  BatchID= 0 Loss: 0.129 | Acc: 96.094% (123/128)
[Test] Epoch= 63  BatchID= 10 Loss: 0.328 | Acc: 89.986% (1267/1408)
[Test] Epoch= 63  BatchID= 20 Loss: 0.335 | Acc: 89.509% (2406/2688)
[Test] Epoch= 63  BatchID= 30 Loss: 0.319 | Acc: 89.758% (3523/3925)

Epoch: 64
[Train] Epoch= 64  BatchID= 0 Loss: 0.160 | Acc: 94.531% (121/128)
[Train] Epoch= 64  BatchID= 10 Loss: 0.277 | Acc: 91.761% (1292/1408)
[Train] Epoch= 64  BatchID= 20 Loss: 0.277 | Acc: 91.778% (2467/2688)
[Train] Epoch= 64  BatchID= 30 Loss: 0.267 | Acc: 91.860% (3645/3968)
[Train] Epoch= 64  BatchID= 40 Loss: 0.262 | Acc: 91.883% (4822/5248)
[Train] Epoch= 64  BatchID= 50 Loss: 0.260 | Acc: 91.942% (6002/6528)
[Train] Epoch= 64  BatchID= 60 Loss: 0.264 | Acc: 91.714% (7161/7808)
[Train] Epoch= 64  BatchID= 70 Loss: 0.264 | Acc: 91.747% (8338/9088)
[Test] Epoch= 64  BatchID= 0 Loss: 0.136 | Acc: 94.531% (121/128)
[Test] Epoch= 64  BatchID= 10 Loss: 0.268 | Acc: 91.122% (1283/1408)
[Test] Epoch= 64  BatchID= 20 Loss: 0.305 | Acc: 90.476% (2432/2688)
[Test] Epoch= 64  BatchID= 30 Loss: 0.335 | Acc: 89.325% (3506/3925)

Epoch: 65
[Train] Epoch= 65  BatchID= 0 Loss: 0.235 | Acc: 90.625% (116/128)
[Train] Epoch= 65  BatchID= 10 Loss: 0.282 | Acc: 90.696% (1277/1408)
[Train] Epoch= 65  BatchID= 20 Loss: 0.263 | Acc: 91.667% (2464/2688)
[Train] Epoch= 65  BatchID= 30 Loss: 0.267 | Acc: 91.910% (3647/3968)
[Train] Epoch= 65  BatchID= 40 Loss: 0.257 | Acc: 92.111% (4834/5248)
[Train] Epoch= 65  BatchID= 50 Loss: 0.258 | Acc: 91.973% (6004/6528)
[Train] Epoch= 65  BatchID= 60 Loss: 0.261 | Acc: 91.957% (7180/7808)
[Train] Epoch= 65  BatchID= 70 Loss: 0.259 | Acc: 92.033% (8364/9088)
[Test] Epoch= 65  BatchID= 0 Loss: 0.087 | Acc: 96.875% (124/128)
[Test] Epoch= 65  BatchID= 10 Loss: 0.278 | Acc: 91.122% (1283/1408)
[Test] Epoch= 65  BatchID= 20 Loss: 0.302 | Acc: 90.476% (2432/2688)
[Test] Epoch= 65  BatchID= 30 Loss: 0.320 | Acc: 89.529% (3514/3925)

Epoch: 66
[Train] Epoch= 66  BatchID= 0 Loss: 0.241 | Acc: 92.188% (118/128)
[Train] Epoch= 66  BatchID= 10 Loss: 0.229 | Acc: 93.182% (1312/1408)
[Train] Epoch= 66  BatchID= 20 Loss: 0.224 | Acc: 93.192% (2505/2688)
[Train] Epoch= 66  BatchID= 30 Loss: 0.229 | Acc: 92.868% (3685/3968)
[Train] Epoch= 66  BatchID= 40 Loss: 0.235 | Acc: 92.797% (4870/5248)
[Train] Epoch= 66  BatchID= 50 Loss: 0.241 | Acc: 92.540% (6041/6528)
[Train] Epoch= 66  BatchID= 60 Loss: 0.245 | Acc: 92.354% (7211/7808)
[Train] Epoch= 66  BatchID= 70 Loss: 0.243 | Acc: 92.320% (8390/9088)
[Test] Epoch= 66  BatchID= 0 Loss: 0.098 | Acc: 96.094% (123/128)
[Test] Epoch= 66  BatchID= 10 Loss: 0.253 | Acc: 91.406% (1287/1408)
[Test] Epoch= 66  BatchID= 20 Loss: 0.294 | Acc: 90.662% (2437/2688)
[Test] Epoch= 66  BatchID= 30 Loss: 0.313 | Acc: 90.038% (3534/3925)
Saving..
Best accuracy:  90.03821656050955

Epoch: 67
[Train] Epoch= 67  BatchID= 0 Loss: 0.366 | Acc: 89.062% (114/128)
[Train] Epoch= 67  BatchID= 10 Loss: 0.214 | Acc: 93.111% (1311/1408)
[Train] Epoch= 67  BatchID= 20 Loss: 0.244 | Acc: 92.039% (2474/2688)
[Train] Epoch= 67  BatchID= 30 Loss: 0.247 | Acc: 91.935% (3648/3968)
[Train] Epoch= 67  BatchID= 40 Loss: 0.246 | Acc: 91.978% (4827/5248)
[Train] Epoch= 67  BatchID= 50 Loss: 0.255 | Acc: 91.835% (5995/6528)
[Train] Epoch= 67  BatchID= 60 Loss: 0.253 | Acc: 91.816% (7169/7808)
[Train] Epoch= 67  BatchID= 70 Loss: 0.253 | Acc: 91.824% (8345/9088)
[Test] Epoch= 67  BatchID= 0 Loss: 0.073 | Acc: 96.875% (124/128)
[Test] Epoch= 67  BatchID= 10 Loss: 0.291 | Acc: 91.193% (1284/1408)
[Test] Epoch= 67  BatchID= 20 Loss: 0.299 | Acc: 90.774% (2440/2688)
[Test] Epoch= 67  BatchID= 30 Loss: 0.311 | Acc: 90.318% (3545/3925)
Saving..
Best accuracy:  90.31847133757962

Epoch: 68
[Train] Epoch= 68  BatchID= 0 Loss: 0.286 | Acc: 89.844% (115/128)
[Train] Epoch= 68  BatchID= 10 Loss: 0.234 | Acc: 92.685% (1305/1408)
[Train] Epoch= 68  BatchID= 20 Loss: 0.234 | Acc: 92.671% (2491/2688)
[Train] Epoch= 68  BatchID= 30 Loss: 0.235 | Acc: 92.868% (3685/3968)
[Train] Epoch= 68  BatchID= 40 Loss: 0.235 | Acc: 92.721% (4866/5248)
[Train] Epoch= 68  BatchID= 50 Loss: 0.235 | Acc: 92.678% (6050/6528)
[Train] Epoch= 68  BatchID= 60 Loss: 0.238 | Acc: 92.610% (7231/7808)
[Train] Epoch= 68  BatchID= 70 Loss: 0.240 | Acc: 92.518% (8408/9088)
[Test] Epoch= 68  BatchID= 0 Loss: 0.091 | Acc: 97.656% (125/128)
[Test] Epoch= 68  BatchID= 10 Loss: 0.343 | Acc: 89.702% (1263/1408)
[Test] Epoch= 68  BatchID= 20 Loss: 0.320 | Acc: 90.402% (2430/2688)
[Test] Epoch= 68  BatchID= 30 Loss: 0.331 | Acc: 89.682% (3520/3925)

Epoch: 69
[Train] Epoch= 69  BatchID= 0 Loss: 0.227 | Acc: 91.406% (117/128)
[Train] Epoch= 69  BatchID= 10 Loss: 0.263 | Acc: 91.832% (1293/1408)
[Train] Epoch= 69  BatchID= 20 Loss: 0.251 | Acc: 91.964% (2472/2688)
[Train] Epoch= 69  BatchID= 30 Loss: 0.253 | Acc: 91.910% (3647/3968)
[Train] Epoch= 69  BatchID= 40 Loss: 0.244 | Acc: 92.207% (4839/5248)
[Train] Epoch= 69  BatchID= 50 Loss: 0.234 | Acc: 92.540% (6041/6528)
[Train] Epoch= 69  BatchID= 60 Loss: 0.235 | Acc: 92.380% (7213/7808)
[Train] Epoch= 69  BatchID= 70 Loss: 0.234 | Acc: 92.320% (8390/9088)
[Test] Epoch= 69  BatchID= 0 Loss: 0.095 | Acc: 97.656% (125/128)
[Test] Epoch= 69  BatchID= 10 Loss: 0.285 | Acc: 91.477% (1288/1408)
[Test] Epoch= 69  BatchID= 20 Loss: 0.286 | Acc: 91.443% (2458/2688)
[Test] Epoch= 69  BatchID= 30 Loss: 0.300 | Acc: 90.955% (3570/3925)
Saving..
Best accuracy:  90.95541401273886

Epoch: 70
[Train] Epoch= 70  BatchID= 0 Loss: 0.210 | Acc: 92.969% (119/128)
[Train] Epoch= 70  BatchID= 10 Loss: 0.261 | Acc: 91.264% (1285/1408)
[Train] Epoch= 70  BatchID= 20 Loss: 0.235 | Acc: 92.336% (2482/2688)
[Train] Epoch= 70  BatchID= 30 Loss: 0.240 | Acc: 92.061% (3653/3968)
[Train] Epoch= 70  BatchID= 40 Loss: 0.239 | Acc: 92.321% (4845/5248)
[Train] Epoch= 70  BatchID= 50 Loss: 0.233 | Acc: 92.678% (6050/6528)
[Train] Epoch= 70  BatchID= 60 Loss: 0.236 | Acc: 92.649% (7234/7808)
[Train] Epoch= 70  BatchID= 70 Loss: 0.236 | Acc: 92.661% (8421/9088)
[Test] Epoch= 70  BatchID= 0 Loss: 0.139 | Acc: 96.875% (124/128)
[Test] Epoch= 70  BatchID= 10 Loss: 0.324 | Acc: 89.418% (1259/1408)
[Test] Epoch= 70  BatchID= 20 Loss: 0.299 | Acc: 90.402% (2430/2688)
[Test] Epoch= 70  BatchID= 30 Loss: 0.299 | Acc: 90.497% (3552/3925)

Epoch: 71
[Train] Epoch= 71  BatchID= 0 Loss: 0.144 | Acc: 97.656% (125/128)
[Train] Epoch= 71  BatchID= 10 Loss: 0.199 | Acc: 94.105% (1325/1408)
[Train] Epoch= 71  BatchID= 20 Loss: 0.227 | Acc: 93.155% (2504/2688)
[Train] Epoch= 71  BatchID= 30 Loss: 0.217 | Acc: 93.322% (3703/3968)
[Train] Epoch= 71  BatchID= 40 Loss: 0.221 | Acc: 93.255% (4894/5248)
[Train] Epoch= 71  BatchID= 50 Loss: 0.226 | Acc: 92.892% (6064/6528)
[Train] Epoch= 71  BatchID= 60 Loss: 0.232 | Acc: 92.725% (7240/7808)
[Train] Epoch= 71  BatchID= 70 Loss: 0.228 | Acc: 92.881% (8441/9088)
[Test] Epoch= 71  BatchID= 0 Loss: 0.074 | Acc: 96.875% (124/128)
[Test] Epoch= 71  BatchID= 10 Loss: 0.307 | Acc: 90.625% (1276/1408)
[Test] Epoch= 71  BatchID= 20 Loss: 0.315 | Acc: 90.513% (2433/2688)
[Test] Epoch= 71  BatchID= 30 Loss: 0.314 | Acc: 90.420% (3549/3925)

Epoch: 72
[Train] Epoch= 72  BatchID= 0 Loss: 0.226 | Acc: 90.625% (116/128)
[Train] Epoch= 72  BatchID= 10 Loss: 0.213 | Acc: 93.111% (1311/1408)
[Train] Epoch= 72  BatchID= 20 Loss: 0.215 | Acc: 93.229% (2506/2688)
[Train] Epoch= 72  BatchID= 30 Loss: 0.215 | Acc: 93.246% (3700/3968)
[Train] Epoch= 72  BatchID= 40 Loss: 0.214 | Acc: 93.369% (4900/5248)
[Train] Epoch= 72  BatchID= 50 Loss: 0.217 | Acc: 93.336% (6093/6528)
[Train] Epoch= 72  BatchID= 60 Loss: 0.218 | Acc: 93.263% (7282/7808)
[Train] Epoch= 72  BatchID= 70 Loss: 0.217 | Acc: 93.321% (8481/9088)
[Test] Epoch= 72  BatchID= 0 Loss: 0.079 | Acc: 96.875% (124/128)
[Test] Epoch= 72  BatchID= 10 Loss: 0.271 | Acc: 91.477% (1288/1408)
[Test] Epoch= 72  BatchID= 20 Loss: 0.275 | Acc: 91.220% (2452/2688)
[Test] Epoch= 72  BatchID= 30 Loss: 0.303 | Acc: 90.420% (3549/3925)

Epoch: 73
[Train] Epoch= 73  BatchID= 0 Loss: 0.187 | Acc: 92.188% (118/128)
[Train] Epoch= 73  BatchID= 10 Loss: 0.187 | Acc: 93.963% (1323/1408)
[Train] Epoch= 73  BatchID= 20 Loss: 0.191 | Acc: 94.048% (2528/2688)
[Train] Epoch= 73  BatchID= 30 Loss: 0.202 | Acc: 93.674% (3717/3968)
[Train] Epoch= 73  BatchID= 40 Loss: 0.205 | Acc: 93.636% (4914/5248)
[Train] Epoch= 73  BatchID= 50 Loss: 0.205 | Acc: 93.735% (6119/6528)
[Train] Epoch= 73  BatchID= 60 Loss: 0.208 | Acc: 93.622% (7310/7808)
[Train] Epoch= 73  BatchID= 70 Loss: 0.207 | Acc: 93.662% (8512/9088)
[Test] Epoch= 73  BatchID= 0 Loss: 0.141 | Acc: 94.531% (121/128)
[Test] Epoch= 73  BatchID= 10 Loss: 0.275 | Acc: 91.548% (1289/1408)
[Test] Epoch= 73  BatchID= 20 Loss: 0.279 | Acc: 91.332% (2455/2688)
[Test] Epoch= 73  BatchID= 30 Loss: 0.294 | Acc: 90.675% (3559/3925)

Epoch: 74
[Train] Epoch= 74  BatchID= 0 Loss: 0.180 | Acc: 94.531% (121/128)
[Train] Epoch= 74  BatchID= 10 Loss: 0.198 | Acc: 94.247% (1327/1408)
[Train] Epoch= 74  BatchID= 20 Loss: 0.204 | Acc: 94.085% (2529/2688)
[Train] Epoch= 74  BatchID= 30 Loss: 0.193 | Acc: 94.204% (3738/3968)
[Train] Epoch= 74  BatchID= 40 Loss: 0.197 | Acc: 93.902% (4928/5248)
[Train] Epoch= 74  BatchID= 50 Loss: 0.200 | Acc: 93.888% (6129/6528)
[Train] Epoch= 74  BatchID= 60 Loss: 0.195 | Acc: 94.070% (7345/7808)
[Train] Epoch= 74  BatchID= 70 Loss: 0.198 | Acc: 93.937% (8537/9088)
[Test] Epoch= 74  BatchID= 0 Loss: 0.083 | Acc: 97.656% (125/128)
[Test] Epoch= 74  BatchID= 10 Loss: 0.280 | Acc: 91.335% (1286/1408)
[Test] Epoch= 74  BatchID= 20 Loss: 0.287 | Acc: 91.109% (2449/2688)
[Test] Epoch= 74  BatchID= 30 Loss: 0.300 | Acc: 90.726% (3561/3925)

Epoch: 75
[Train] Epoch= 75  BatchID= 0 Loss: 0.173 | Acc: 93.750% (120/128)
[Train] Epoch= 75  BatchID= 10 Loss: 0.217 | Acc: 92.685% (1305/1408)
[Train] Epoch= 75  BatchID= 20 Loss: 0.204 | Acc: 93.341% (2509/2688)
[Train] Epoch= 75  BatchID= 30 Loss: 0.206 | Acc: 93.448% (3708/3968)
[Train] Epoch= 75  BatchID= 40 Loss: 0.206 | Acc: 93.216% (4892/5248)
[Train] Epoch= 75  BatchID= 50 Loss: 0.204 | Acc: 93.306% (6091/6528)
[Train] Epoch= 75  BatchID= 60 Loss: 0.203 | Acc: 93.353% (7289/7808)
[Train] Epoch= 75  BatchID= 70 Loss: 0.204 | Acc: 93.343% (8483/9088)
[Test] Epoch= 75  BatchID= 0 Loss: 0.096 | Acc: 96.094% (123/128)
[Test] Epoch= 75  BatchID= 10 Loss: 0.280 | Acc: 91.193% (1284/1408)
[Test] Epoch= 75  BatchID= 20 Loss: 0.258 | Acc: 91.369% (2456/2688)
[Test] Epoch= 75  BatchID= 30 Loss: 0.290 | Acc: 90.726% (3561/3925)

Epoch: 76
[Train] Epoch= 76  BatchID= 0 Loss: 0.128 | Acc: 95.312% (122/128)
[Train] Epoch= 76  BatchID= 10 Loss: 0.195 | Acc: 93.821% (1321/1408)
[Train] Epoch= 76  BatchID= 20 Loss: 0.196 | Acc: 93.638% (2517/2688)
[Train] Epoch= 76  BatchID= 30 Loss: 0.202 | Acc: 93.523% (3711/3968)
[Train] Epoch= 76  BatchID= 40 Loss: 0.206 | Acc: 93.293% (4896/5248)
[Train] Epoch= 76  BatchID= 50 Loss: 0.215 | Acc: 93.015% (6072/6528)
[Train] Epoch= 76  BatchID= 60 Loss: 0.217 | Acc: 93.110% (7270/7808)
[Train] Epoch= 76  BatchID= 70 Loss: 0.217 | Acc: 93.112% (8462/9088)
[Test] Epoch= 76  BatchID= 0 Loss: 0.084 | Acc: 96.875% (124/128)
[Test] Epoch= 76  BatchID= 10 Loss: 0.270 | Acc: 91.619% (1290/1408)
[Test] Epoch= 76  BatchID= 20 Loss: 0.268 | Acc: 91.406% (2457/2688)
[Test] Epoch= 76  BatchID= 30 Loss: 0.290 | Acc: 90.879% (3567/3925)

Epoch: 77
[Train] Epoch= 77  BatchID= 0 Loss: 0.174 | Acc: 92.969% (119/128)
[Train] Epoch= 77  BatchID= 10 Loss: 0.160 | Acc: 94.673% (1333/1408)
[Train] Epoch= 77  BatchID= 20 Loss: 0.174 | Acc: 94.531% (2541/2688)
[Train] Epoch= 77  BatchID= 30 Loss: 0.178 | Acc: 94.506% (3750/3968)
[Train] Epoch= 77  BatchID= 40 Loss: 0.185 | Acc: 94.284% (4948/5248)
[Train] Epoch= 77  BatchID= 50 Loss: 0.187 | Acc: 94.148% (6146/6528)
[Train] Epoch= 77  BatchID= 60 Loss: 0.185 | Acc: 94.249% (7359/7808)
[Train] Epoch= 77  BatchID= 70 Loss: 0.186 | Acc: 94.245% (8565/9088)
[Test] Epoch= 77  BatchID= 0 Loss: 0.061 | Acc: 97.656% (125/128)
[Test] Epoch= 77  BatchID= 10 Loss: 0.262 | Acc: 92.472% (1302/1408)
[Test] Epoch= 77  BatchID= 20 Loss: 0.267 | Acc: 91.927% (2471/2688)
[Test] Epoch= 77  BatchID= 30 Loss: 0.285 | Acc: 91.363% (3586/3925)
Saving..
Best accuracy:  91.36305732484077

Epoch: 78
[Train] Epoch= 78  BatchID= 0 Loss: 0.233 | Acc: 94.531% (121/128)
[Train] Epoch= 78  BatchID= 10 Loss: 0.175 | Acc: 94.815% (1335/1408)
[Train] Epoch= 78  BatchID= 20 Loss: 0.177 | Acc: 94.420% (2538/2688)
[Train] Epoch= 78  BatchID= 30 Loss: 0.186 | Acc: 94.304% (3742/3968)
[Train] Epoch= 78  BatchID= 40 Loss: 0.181 | Acc: 94.512% (4960/5248)
[Train] Epoch= 78  BatchID= 50 Loss: 0.185 | Acc: 94.439% (6165/6528)
[Train] Epoch= 78  BatchID= 60 Loss: 0.187 | Acc: 94.314% (7364/7808)
[Train] Epoch= 78  BatchID= 70 Loss: 0.189 | Acc: 94.256% (8566/9088)
[Test] Epoch= 78  BatchID= 0 Loss: 0.059 | Acc: 97.656% (125/128)
[Test] Epoch= 78  BatchID= 10 Loss: 0.258 | Acc: 92.543% (1303/1408)
[Test] Epoch= 78  BatchID= 20 Loss: 0.255 | Acc: 92.039% (2474/2688)
[Test] Epoch= 78  BatchID= 30 Loss: 0.293 | Acc: 90.955% (3570/3925)

Epoch: 79
[Train] Epoch= 79  BatchID= 0 Loss: 0.176 | Acc: 95.312% (122/128)
[Train] Epoch= 79  BatchID= 10 Loss: 0.186 | Acc: 94.247% (1327/1408)
[Train] Epoch= 79  BatchID= 20 Loss: 0.185 | Acc: 94.494% (2540/2688)
[Train] Epoch= 79  BatchID= 30 Loss: 0.191 | Acc: 94.103% (3734/3968)
[Train] Epoch= 79  BatchID= 40 Loss: 0.185 | Acc: 94.112% (4939/5248)
[Train] Epoch= 79  BatchID= 50 Loss: 0.184 | Acc: 94.056% (6140/6528)
[Train] Epoch= 79  BatchID= 60 Loss: 0.183 | Acc: 94.185% (7354/7808)
[Train] Epoch= 79  BatchID= 70 Loss: 0.180 | Acc: 94.355% (8575/9088)
[Test] Epoch= 79  BatchID= 0 Loss: 0.081 | Acc: 96.875% (124/128)
[Test] Epoch= 79  BatchID= 10 Loss: 0.274 | Acc: 91.761% (1292/1408)
[Test] Epoch= 79  BatchID= 20 Loss: 0.260 | Acc: 91.704% (2465/2688)
[Test] Epoch= 79  BatchID= 30 Loss: 0.291 | Acc: 91.006% (3572/3925)

Epoch: 80
[Train] Epoch= 80  BatchID= 0 Loss: 0.232 | Acc: 92.969% (119/128)
[Train] Epoch= 80  BatchID= 10 Loss: 0.222 | Acc: 93.040% (1310/1408)
[Train] Epoch= 80  BatchID= 20 Loss: 0.194 | Acc: 94.159% (2531/2688)
[Train] Epoch= 80  BatchID= 30 Loss: 0.203 | Acc: 93.775% (3721/3968)
[Train] Epoch= 80  BatchID= 40 Loss: 0.204 | Acc: 93.712% (4918/5248)
[Train] Epoch= 80  BatchID= 50 Loss: 0.201 | Acc: 93.689% (6116/6528)
[Train] Epoch= 80  BatchID= 60 Loss: 0.199 | Acc: 93.712% (7317/7808)
[Train] Epoch= 80  BatchID= 70 Loss: 0.201 | Acc: 93.618% (8508/9088)
[Test] Epoch= 80  BatchID= 0 Loss: 0.086 | Acc: 97.656% (125/128)
[Test] Epoch= 80  BatchID= 10 Loss: 0.250 | Acc: 92.188% (1298/1408)
[Test] Epoch= 80  BatchID= 20 Loss: 0.265 | Acc: 91.555% (2461/2688)
[Test] Epoch= 80  BatchID= 30 Loss: 0.297 | Acc: 90.803% (3564/3925)

Epoch: 81
[Train] Epoch= 81  BatchID= 0 Loss: 0.212 | Acc: 96.094% (123/128)
[Train] Epoch= 81  BatchID= 10 Loss: 0.193 | Acc: 94.673% (1333/1408)
[Train] Epoch= 81  BatchID= 20 Loss: 0.187 | Acc: 94.903% (2551/2688)
[Train] Epoch= 81  BatchID= 30 Loss: 0.189 | Acc: 94.657% (3756/3968)
[Train] Epoch= 81  BatchID= 40 Loss: 0.190 | Acc: 94.531% (4961/5248)
[Train] Epoch= 81  BatchID= 50 Loss: 0.192 | Acc: 94.409% (6163/6528)
[Train] Epoch= 81  BatchID= 60 Loss: 0.188 | Acc: 94.531% (7381/7808)
[Train] Epoch= 81  BatchID= 70 Loss: 0.185 | Acc: 94.553% (8593/9088)
[Test] Epoch= 81  BatchID= 0 Loss: 0.091 | Acc: 96.875% (124/128)
[Test] Epoch= 81  BatchID= 10 Loss: 0.260 | Acc: 92.116% (1297/1408)
[Test] Epoch= 81  BatchID= 20 Loss: 0.254 | Acc: 91.853% (2469/2688)
[Test] Epoch= 81  BatchID= 30 Loss: 0.292 | Acc: 91.032% (3573/3925)

Epoch: 82
[Train] Epoch= 82  BatchID= 0 Loss: 0.222 | Acc: 91.406% (117/128)
[Train] Epoch= 82  BatchID= 10 Loss: 0.168 | Acc: 95.170% (1340/1408)
[Train] Epoch= 82  BatchID= 20 Loss: 0.165 | Acc: 95.238% (2560/2688)
[Train] Epoch= 82  BatchID= 30 Loss: 0.165 | Acc: 95.136% (3775/3968)
[Train] Epoch= 82  BatchID= 40 Loss: 0.166 | Acc: 94.950% (4983/5248)
[Train] Epoch= 82  BatchID= 50 Loss: 0.169 | Acc: 94.838% (6191/6528)
[Train] Epoch= 82  BatchID= 60 Loss: 0.177 | Acc: 94.544% (7382/7808)
[Train] Epoch= 82  BatchID= 70 Loss: 0.180 | Acc: 94.498% (8588/9088)
[Test] Epoch= 82  BatchID= 0 Loss: 0.086 | Acc: 96.875% (124/128)
[Test] Epoch= 82  BatchID= 10 Loss: 0.271 | Acc: 91.832% (1293/1408)
[Test] Epoch= 82  BatchID= 20 Loss: 0.260 | Acc: 91.518% (2460/2688)
[Test] Epoch= 82  BatchID= 30 Loss: 0.283 | Acc: 90.955% (3570/3925)

Epoch: 83
[Train] Epoch= 83  BatchID= 0 Loss: 0.122 | Acc: 95.312% (122/128)
[Train] Epoch= 83  BatchID= 10 Loss: 0.214 | Acc: 93.466% (1316/1408)
[Train] Epoch= 83  BatchID= 20 Loss: 0.195 | Acc: 93.713% (2519/2688)
[Train] Epoch= 83  BatchID= 30 Loss: 0.193 | Acc: 93.876% (3725/3968)
[Train] Epoch= 83  BatchID= 40 Loss: 0.190 | Acc: 94.055% (4936/5248)
[Train] Epoch= 83  BatchID= 50 Loss: 0.193 | Acc: 93.964% (6134/6528)
[Train] Epoch= 83  BatchID= 60 Loss: 0.187 | Acc: 94.147% (7351/7808)
[Train] Epoch= 83  BatchID= 70 Loss: 0.183 | Acc: 94.267% (8567/9088)
[Test] Epoch= 83  BatchID= 0 Loss: 0.086 | Acc: 96.094% (123/128)
[Test] Epoch= 83  BatchID= 10 Loss: 0.273 | Acc: 91.335% (1286/1408)
[Test] Epoch= 83  BatchID= 20 Loss: 0.256 | Acc: 91.741% (2466/2688)
[Test] Epoch= 83  BatchID= 30 Loss: 0.287 | Acc: 91.108% (3576/3925)

Epoch: 84
[Train] Epoch= 84  BatchID= 0 Loss: 0.277 | Acc: 92.188% (118/128)
[Train] Epoch= 84  BatchID= 10 Loss: 0.195 | Acc: 94.602% (1332/1408)
[Train] Epoch= 84  BatchID= 20 Loss: 0.183 | Acc: 94.568% (2542/2688)
[Train] Epoch= 84  BatchID= 30 Loss: 0.178 | Acc: 94.531% (3751/3968)
[Train] Epoch= 84  BatchID= 40 Loss: 0.180 | Acc: 94.455% (4957/5248)
[Train] Epoch= 84  BatchID= 50 Loss: 0.176 | Acc: 94.593% (6175/6528)
[Train] Epoch= 84  BatchID= 60 Loss: 0.174 | Acc: 94.608% (7387/7808)
[Train] Epoch= 84  BatchID= 70 Loss: 0.175 | Acc: 94.586% (8596/9088)
[Test] Epoch= 84  BatchID= 0 Loss: 0.083 | Acc: 97.656% (125/128)
[Test] Epoch= 84  BatchID= 10 Loss: 0.275 | Acc: 91.264% (1285/1408)
[Test] Epoch= 84  BatchID= 20 Loss: 0.267 | Acc: 91.146% (2450/2688)
[Test] Epoch= 84  BatchID= 30 Loss: 0.291 | Acc: 90.828% (3565/3925)

Epoch: 85
[Train] Epoch= 85  BatchID= 0 Loss: 0.180 | Acc: 94.531% (121/128)
[Train] Epoch= 85  BatchID= 10 Loss: 0.177 | Acc: 94.034% (1324/1408)
[Train] Epoch= 85  BatchID= 20 Loss: 0.186 | Acc: 94.085% (2529/2688)
[Train] Epoch= 85  BatchID= 30 Loss: 0.190 | Acc: 93.851% (3724/3968)
[Train] Epoch= 85  BatchID= 40 Loss: 0.191 | Acc: 93.826% (4924/5248)
[Train] Epoch= 85  BatchID= 50 Loss: 0.188 | Acc: 93.949% (6133/6528)
[Train] Epoch= 85  BatchID= 60 Loss: 0.186 | Acc: 94.070% (7345/7808)
[Train] Epoch= 85  BatchID= 70 Loss: 0.180 | Acc: 94.256% (8566/9088)
[Test] Epoch= 85  BatchID= 0 Loss: 0.081 | Acc: 96.094% (123/128)
[Test] Epoch= 85  BatchID= 10 Loss: 0.272 | Acc: 91.122% (1283/1408)
[Test] Epoch= 85  BatchID= 20 Loss: 0.267 | Acc: 91.257% (2453/2688)
[Test] Epoch= 85  BatchID= 30 Loss: 0.293 | Acc: 90.650% (3558/3925)

Epoch: 86
[Train] Epoch= 86  BatchID= 0 Loss: 0.165 | Acc: 95.312% (122/128)
[Train] Epoch= 86  BatchID= 10 Loss: 0.171 | Acc: 94.034% (1324/1408)
[Train] Epoch= 86  BatchID= 20 Loss: 0.172 | Acc: 94.308% (2535/2688)
[Train] Epoch= 86  BatchID= 30 Loss: 0.181 | Acc: 94.052% (3732/3968)
[Train] Epoch= 86  BatchID= 40 Loss: 0.186 | Acc: 94.017% (4934/5248)
[Train] Epoch= 86  BatchID= 50 Loss: 0.181 | Acc: 94.225% (6151/6528)
[Train] Epoch= 86  BatchID= 60 Loss: 0.181 | Acc: 94.288% (7362/7808)
[Train] Epoch= 86  BatchID= 70 Loss: 0.180 | Acc: 94.443% (8583/9088)
[Test] Epoch= 86  BatchID= 0 Loss: 0.086 | Acc: 96.094% (123/128)
[Test] Epoch= 86  BatchID= 10 Loss: 0.264 | Acc: 91.477% (1288/1408)
[Test] Epoch= 86  BatchID= 20 Loss: 0.256 | Acc: 91.369% (2456/2688)
[Test] Epoch= 86  BatchID= 30 Loss: 0.282 | Acc: 90.955% (3570/3925)

Epoch: 87
[Train] Epoch= 87  BatchID= 0 Loss: 0.213 | Acc: 92.969% (119/128)
[Train] Epoch= 87  BatchID= 10 Loss: 0.150 | Acc: 95.881% (1350/1408)
[Train] Epoch= 87  BatchID= 20 Loss: 0.150 | Acc: 95.647% (2571/2688)
[Train] Epoch= 87  BatchID= 30 Loss: 0.163 | Acc: 95.136% (3775/3968)
[Train] Epoch= 87  BatchID= 40 Loss: 0.159 | Acc: 95.293% (5001/5248)
[Train] Epoch= 87  BatchID= 50 Loss: 0.161 | Acc: 95.113% (6209/6528)
[Train] Epoch= 87  BatchID= 60 Loss: 0.163 | Acc: 94.967% (7415/7808)
[Train] Epoch= 87  BatchID= 70 Loss: 0.167 | Acc: 94.817% (8617/9088)
[Test] Epoch= 87  BatchID= 0 Loss: 0.098 | Acc: 96.094% (123/128)
[Test] Epoch= 87  BatchID= 10 Loss: 0.261 | Acc: 91.903% (1294/1408)
[Test] Epoch= 87  BatchID= 20 Loss: 0.257 | Acc: 91.778% (2467/2688)
[Test] Epoch= 87  BatchID= 30 Loss: 0.283 | Acc: 91.134% (3577/3925)

Epoch: 88
[Train] Epoch= 88  BatchID= 0 Loss: 0.245 | Acc: 92.969% (119/128)
[Train] Epoch= 88  BatchID= 10 Loss: 0.162 | Acc: 95.099% (1339/1408)
[Train] Epoch= 88  BatchID= 20 Loss: 0.161 | Acc: 95.052% (2555/2688)
[Train] Epoch= 88  BatchID= 30 Loss: 0.164 | Acc: 94.859% (3764/3968)
[Train] Epoch= 88  BatchID= 40 Loss: 0.165 | Acc: 95.027% (4987/5248)
[Train] Epoch= 88  BatchID= 50 Loss: 0.169 | Acc: 94.899% (6195/6528)
[Train] Epoch= 88  BatchID= 60 Loss: 0.171 | Acc: 94.749% (7398/7808)
[Train] Epoch= 88  BatchID= 70 Loss: 0.175 | Acc: 94.696% (8606/9088)
[Test] Epoch= 88  BatchID= 0 Loss: 0.089 | Acc: 96.875% (124/128)
[Test] Epoch= 88  BatchID= 10 Loss: 0.256 | Acc: 92.330% (1300/1408)
[Test] Epoch= 88  BatchID= 20 Loss: 0.256 | Acc: 91.815% (2468/2688)
[Test] Epoch= 88  BatchID= 30 Loss: 0.282 | Acc: 91.287% (3583/3925)

Epoch: 89
[Train] Epoch= 89  BatchID= 0 Loss: 0.238 | Acc: 89.844% (115/128)
[Train] Epoch= 89  BatchID= 10 Loss: 0.178 | Acc: 94.389% (1329/1408)
[Train] Epoch= 89  BatchID= 20 Loss: 0.175 | Acc: 94.568% (2542/2688)
[Train] Epoch= 89  BatchID= 30 Loss: 0.182 | Acc: 94.481% (3749/3968)
[Train] Epoch= 89  BatchID= 40 Loss: 0.182 | Acc: 94.531% (4961/5248)
[Train] Epoch= 89  BatchID= 50 Loss: 0.177 | Acc: 94.669% (6180/6528)
[Train] Epoch= 89  BatchID= 60 Loss: 0.176 | Acc: 94.672% (7392/7808)
[Train] Epoch= 89  BatchID= 70 Loss: 0.171 | Acc: 94.905% (8625/9088)
[Test] Epoch= 89  BatchID= 0 Loss: 0.082 | Acc: 96.094% (123/128)
[Test] Epoch= 89  BatchID= 10 Loss: 0.272 | Acc: 91.477% (1288/1408)
[Test] Epoch= 89  BatchID= 20 Loss: 0.256 | Acc: 91.667% (2464/2688)
[Test] Epoch= 89  BatchID= 30 Loss: 0.282 | Acc: 91.159% (3578/3925)
