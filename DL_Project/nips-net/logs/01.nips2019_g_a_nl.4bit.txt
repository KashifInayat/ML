==> Preparing data..
==> Building model..
DataParallel(
  (module): ResNet(
    (conv1): InputConv2dLSQ(
      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): LinearLSQ(
      in_features=512, out_features=10, bias=True
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=True)
    )
  )
)
==> Initializing from checkpoint..

Epoch: 0
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
[Train] Epoch= 0  BatchID= 0 Loss: 0.269 | Acc: 91.406% (117/128)
[Train] Epoch= 0  BatchID= 10 Loss: 0.763 | Acc: 78.409% (1104/1408)
[Train] Epoch= 0  BatchID= 20 Loss: 0.767 | Acc: 76.972% (2069/2688)
[Train] Epoch= 0  BatchID= 30 Loss: 0.746 | Acc: 77.092% (3059/3968)
[Train] Epoch= 0  BatchID= 40 Loss: 0.709 | Acc: 78.258% (4107/5248)
[Train] Epoch= 0  BatchID= 50 Loss: 0.674 | Acc: 79.151% (5167/6528)
[Train] Epoch= 0  BatchID= 60 Loss: 0.663 | Acc: 79.470% (6205/7808)
[Train] Epoch= 0  BatchID= 70 Loss: 0.649 | Acc: 79.864% (7258/9088)
[Test] Epoch= 0  BatchID= 0 Loss: 0.225 | Acc: 93.750% (120/128)
[Test] Epoch= 0  BatchID= 10 Loss: 0.473 | Acc: 84.375% (1188/1408)
[Test] Epoch= 0  BatchID= 20 Loss: 0.462 | Acc: 85.231% (2291/2688)
[Test] Epoch= 0  BatchID= 30 Loss: 0.598 | Acc: 81.070% (3182/3925)
Saving..
Best accuracy:  81.07006369426752

Epoch: 1
[Train] Epoch= 1  BatchID= 0 Loss: 0.455 | Acc: 89.062% (114/128)
[Train] Epoch= 1  BatchID= 10 Loss: 0.550 | Acc: 81.818% (1152/1408)
[Train] Epoch= 1  BatchID= 20 Loss: 0.572 | Acc: 81.138% (2181/2688)
[Train] Epoch= 1  BatchID= 30 Loss: 0.587 | Acc: 80.469% (3193/3968)
[Train] Epoch= 1  BatchID= 40 Loss: 0.584 | Acc: 80.469% (4223/5248)
[Train] Epoch= 1  BatchID= 50 Loss: 0.581 | Acc: 80.729% (5270/6528)
[Train] Epoch= 1  BatchID= 60 Loss: 0.584 | Acc: 80.712% (6302/7808)
[Train] Epoch= 1  BatchID= 70 Loss: 0.588 | Acc: 80.513% (7317/9088)
[Test] Epoch= 1  BatchID= 0 Loss: 0.108 | Acc: 96.875% (124/128)
[Test] Epoch= 1  BatchID= 10 Loss: 0.812 | Acc: 74.219% (1045/1408)
[Test] Epoch= 1  BatchID= 20 Loss: 0.764 | Acc: 74.926% (2014/2688)
[Test] Epoch= 1  BatchID= 30 Loss: 0.699 | Acc: 77.783% (3053/3925)

Epoch: 2
[Train] Epoch= 2  BatchID= 0 Loss: 0.670 | Acc: 77.344% (99/128)
[Train] Epoch= 2  BatchID= 10 Loss: 0.556 | Acc: 81.534% (1148/1408)
[Train] Epoch= 2  BatchID= 20 Loss: 0.569 | Acc: 80.804% (2172/2688)
[Train] Epoch= 2  BatchID= 30 Loss: 0.564 | Acc: 81.149% (3220/3968)
[Train] Epoch= 2  BatchID= 40 Loss: 0.558 | Acc: 81.269% (4265/5248)
[Train] Epoch= 2  BatchID= 50 Loss: 0.560 | Acc: 81.403% (5314/6528)
[Train] Epoch= 2  BatchID= 60 Loss: 0.568 | Acc: 81.276% (6346/7808)
[Train] Epoch= 2  BatchID= 70 Loss: 0.574 | Acc: 81.173% (7377/9088)
[Test] Epoch= 2  BatchID= 0 Loss: 0.253 | Acc: 92.969% (119/128)
[Test] Epoch= 2  BatchID= 10 Loss: 0.450 | Acc: 84.943% (1196/1408)
[Test] Epoch= 2  BatchID= 20 Loss: 0.481 | Acc: 84.412% (2269/2688)
[Test] Epoch= 2  BatchID= 30 Loss: 0.511 | Acc: 83.694% (3285/3925)
Saving..
Best accuracy:  83.69426751592357

Epoch: 3
[Train] Epoch= 3  BatchID= 0 Loss: 0.564 | Acc: 85.156% (109/128)
[Train] Epoch= 3  BatchID= 10 Loss: 0.553 | Acc: 81.676% (1150/1408)
[Train] Epoch= 3  BatchID= 20 Loss: 0.570 | Acc: 81.287% (2185/2688)
[Train] Epoch= 3  BatchID= 30 Loss: 0.573 | Acc: 81.578% (3237/3968)
[Train] Epoch= 3  BatchID= 40 Loss: 0.577 | Acc: 81.498% (4277/5248)
[Train] Epoch= 3  BatchID= 50 Loss: 0.588 | Acc: 81.036% (5290/6528)
[Train] Epoch= 3  BatchID= 60 Loss: 0.589 | Acc: 81.019% (6326/7808)
[Train] Epoch= 3  BatchID= 70 Loss: 0.590 | Acc: 81.063% (7367/9088)
[Test] Epoch= 3  BatchID= 0 Loss: 0.074 | Acc: 98.438% (126/128)
[Test] Epoch= 3  BatchID= 10 Loss: 0.643 | Acc: 79.261% (1116/1408)
[Test] Epoch= 3  BatchID= 20 Loss: 0.697 | Acc: 77.604% (2086/2688)
[Test] Epoch= 3  BatchID= 30 Loss: 0.644 | Acc: 79.287% (3112/3925)

Epoch: 4
[Train] Epoch= 4  BatchID= 0 Loss: 0.658 | Acc: 79.688% (102/128)
[Train] Epoch= 4  BatchID= 10 Loss: 0.622 | Acc: 80.043% (1127/1408)
[Train] Epoch= 4  BatchID= 20 Loss: 0.597 | Acc: 80.469% (2163/2688)
[Train] Epoch= 4  BatchID= 30 Loss: 0.591 | Acc: 80.393% (3190/3968)
[Train] Epoch= 4  BatchID= 40 Loss: 0.586 | Acc: 80.488% (4224/5248)
[Train] Epoch= 4  BatchID= 50 Loss: 0.595 | Acc: 80.377% (5247/6528)
[Train] Epoch= 4  BatchID= 60 Loss: 0.599 | Acc: 80.302% (6270/7808)
[Train] Epoch= 4  BatchID= 70 Loss: 0.595 | Acc: 80.524% (7318/9088)
[Test] Epoch= 4  BatchID= 0 Loss: 0.368 | Acc: 88.281% (113/128)
[Test] Epoch= 4  BatchID= 10 Loss: 0.570 | Acc: 81.250% (1144/1408)
[Test] Epoch= 4  BatchID= 20 Loss: 0.540 | Acc: 82.403% (2215/2688)
[Test] Epoch= 4  BatchID= 30 Loss: 0.542 | Acc: 82.497% (3238/3925)

Epoch: 5
[Train] Epoch= 5  BatchID= 0 Loss: 0.402 | Acc: 89.062% (114/128)
[Train] Epoch= 5  BatchID= 10 Loss: 0.556 | Acc: 81.818% (1152/1408)
[Train] Epoch= 5  BatchID= 20 Loss: 0.573 | Acc: 81.659% (2195/2688)
[Train] Epoch= 5  BatchID= 30 Loss: 0.575 | Acc: 81.426% (3231/3968)
[Train] Epoch= 5  BatchID= 40 Loss: 0.588 | Acc: 81.326% (4268/5248)
[Train] Epoch= 5  BatchID= 50 Loss: 0.580 | Acc: 81.373% (5312/6528)
[Train] Epoch= 5  BatchID= 60 Loss: 0.587 | Acc: 81.301% (6348/7808)
[Train] Epoch= 5  BatchID= 70 Loss: 0.597 | Acc: 81.019% (7363/9088)
[Test] Epoch= 5  BatchID= 0 Loss: 0.265 | Acc: 92.188% (118/128)
[Test] Epoch= 5  BatchID= 10 Loss: 0.399 | Acc: 87.074% (1226/1408)
[Test] Epoch= 5  BatchID= 20 Loss: 0.649 | Acc: 79.688% (2142/2688)
[Test] Epoch= 5  BatchID= 30 Loss: 0.681 | Acc: 78.675% (3088/3925)

Epoch: 6
[Train] Epoch= 6  BatchID= 0 Loss: 0.697 | Acc: 75.000% (96/128)
[Train] Epoch= 6  BatchID= 10 Loss: 0.647 | Acc: 78.764% (1109/1408)
[Train] Epoch= 6  BatchID= 20 Loss: 0.617 | Acc: 79.762% (2144/2688)
[Train] Epoch= 6  BatchID= 30 Loss: 0.607 | Acc: 80.166% (3181/3968)
[Train] Epoch= 6  BatchID= 40 Loss: 0.600 | Acc: 80.583% (4229/5248)
[Train] Epoch= 6  BatchID= 50 Loss: 0.607 | Acc: 80.438% (5251/6528)
[Train] Epoch= 6  BatchID= 60 Loss: 0.608 | Acc: 80.238% (6265/7808)
[Train] Epoch= 6  BatchID= 70 Loss: 0.607 | Acc: 80.194% (7288/9088)
[Test] Epoch= 6  BatchID= 0 Loss: 0.496 | Acc: 87.500% (112/128)
[Test] Epoch= 6  BatchID= 10 Loss: 0.564 | Acc: 82.173% (1157/1408)
[Test] Epoch= 6  BatchID= 20 Loss: 0.591 | Acc: 81.808% (2199/2688)
[Test] Epoch= 6  BatchID= 30 Loss: 0.637 | Acc: 80.178% (3147/3925)

Epoch: 7
[Train] Epoch= 7  BatchID= 0 Loss: 0.507 | Acc: 85.156% (109/128)
[Train] Epoch= 7  BatchID= 10 Loss: 0.554 | Acc: 81.676% (1150/1408)
[Train] Epoch= 7  BatchID= 20 Loss: 0.566 | Acc: 81.362% (2187/2688)
[Train] Epoch= 7  BatchID= 30 Loss: 0.574 | Acc: 81.628% (3239/3968)
[Train] Epoch= 7  BatchID= 40 Loss: 0.592 | Acc: 80.983% (4250/5248)
[Train] Epoch= 7  BatchID= 50 Loss: 0.600 | Acc: 80.714% (5269/6528)
[Train] Epoch= 7  BatchID= 60 Loss: 0.604 | Acc: 80.520% (6287/7808)
[Train] Epoch= 7  BatchID= 70 Loss: 0.611 | Acc: 80.249% (7293/9088)
[Test] Epoch= 7  BatchID= 0 Loss: 0.210 | Acc: 95.312% (122/128)
[Test] Epoch= 7  BatchID= 10 Loss: 0.435 | Acc: 85.511% (1204/1408)
[Test] Epoch= 7  BatchID= 20 Loss: 0.459 | Acc: 85.119% (2288/2688)
[Test] Epoch= 7  BatchID= 30 Loss: 0.516 | Acc: 83.745% (3287/3925)
Saving..
Best accuracy:  83.7452229299363

Epoch: 8
[Train] Epoch= 8  BatchID= 0 Loss: 0.688 | Acc: 78.906% (101/128)
[Train] Epoch= 8  BatchID= 10 Loss: 0.584 | Acc: 80.611% (1135/1408)
[Train] Epoch= 8  BatchID= 20 Loss: 0.606 | Acc: 80.469% (2163/2688)
[Train] Epoch= 8  BatchID= 30 Loss: 0.622 | Acc: 79.814% (3167/3968)
[Train] Epoch= 8  BatchID= 40 Loss: 0.630 | Acc: 79.592% (4177/5248)
[Train] Epoch= 8  BatchID= 50 Loss: 0.622 | Acc: 80.132% (5231/6528)
[Train] Epoch= 8  BatchID= 60 Loss: 0.626 | Acc: 79.944% (6242/7808)
[Train] Epoch= 8  BatchID= 70 Loss: 0.623 | Acc: 80.106% (7280/9088)
[Test] Epoch= 8  BatchID= 0 Loss: 0.435 | Acc: 85.156% (109/128)
[Test] Epoch= 8  BatchID= 10 Loss: 0.637 | Acc: 79.830% (1124/1408)
[Test] Epoch= 8  BatchID= 20 Loss: 0.646 | Acc: 79.241% (2130/2688)
[Test] Epoch= 8  BatchID= 30 Loss: 0.654 | Acc: 79.210% (3109/3925)

Epoch: 9
[Train] Epoch= 9  BatchID= 0 Loss: 0.552 | Acc: 81.250% (104/128)
[Train] Epoch= 9  BatchID= 10 Loss: 0.570 | Acc: 81.463% (1147/1408)
[Train] Epoch= 9  BatchID= 20 Loss: 0.587 | Acc: 81.324% (2186/2688)
[Train] Epoch= 9  BatchID= 30 Loss: 0.586 | Acc: 81.225% (3223/3968)
[Train] Epoch= 9  BatchID= 40 Loss: 0.591 | Acc: 81.193% (4261/5248)
[Train] Epoch= 9  BatchID= 50 Loss: 0.606 | Acc: 80.285% (5241/6528)
[Train] Epoch= 9  BatchID= 60 Loss: 0.620 | Acc: 79.918% (6240/7808)
[Train] Epoch= 9  BatchID= 70 Loss: 0.626 | Acc: 79.621% (7236/9088)
[Test] Epoch= 9  BatchID= 0 Loss: 0.358 | Acc: 90.625% (116/128)
[Test] Epoch= 9  BatchID= 10 Loss: 0.452 | Acc: 85.227% (1200/1408)
[Test] Epoch= 9  BatchID= 20 Loss: 0.592 | Acc: 80.469% (2163/2688)
[Test] Epoch= 9  BatchID= 30 Loss: 0.648 | Acc: 78.752% (3091/3925)

Epoch: 10
[Train] Epoch= 10  BatchID= 0 Loss: 0.808 | Acc: 74.219% (95/128)
[Train] Epoch= 10  BatchID= 10 Loss: 0.614 | Acc: 81.250% (1144/1408)
[Train] Epoch= 10  BatchID= 20 Loss: 0.631 | Acc: 80.171% (2155/2688)
[Train] Epoch= 10  BatchID= 30 Loss: 0.618 | Acc: 80.242% (3184/3968)
[Train] Epoch= 10  BatchID= 40 Loss: 0.618 | Acc: 80.183% (4208/5248)
[Train] Epoch= 10  BatchID= 50 Loss: 0.622 | Acc: 79.994% (5222/6528)
[Train] Epoch= 10  BatchID= 60 Loss: 0.613 | Acc: 80.174% (6260/7808)
[Train] Epoch= 10  BatchID= 70 Loss: 0.614 | Acc: 80.139% (7283/9088)
[Test] Epoch= 10  BatchID= 0 Loss: 0.383 | Acc: 88.281% (113/128)
[Test] Epoch= 10  BatchID= 10 Loss: 0.693 | Acc: 76.989% (1084/1408)
[Test] Epoch= 10  BatchID= 20 Loss: 0.796 | Acc: 74.628% (2006/2688)
[Test] Epoch= 10  BatchID= 30 Loss: 0.692 | Acc: 77.809% (3054/3925)

Epoch: 11
[Train] Epoch= 11  BatchID= 0 Loss: 0.482 | Acc: 85.938% (110/128)
[Train] Epoch= 11  BatchID= 10 Loss: 0.543 | Acc: 82.386% (1160/1408)
[Train] Epoch= 11  BatchID= 20 Loss: 0.564 | Acc: 81.659% (2195/2688)
[Train] Epoch= 11  BatchID= 30 Loss: 0.583 | Acc: 81.099% (3218/3968)
[Train] Epoch= 11  BatchID= 40 Loss: 0.584 | Acc: 81.193% (4261/5248)
[Train] Epoch= 11  BatchID= 50 Loss: 0.577 | Acc: 81.189% (5300/6528)
[Train] Epoch= 11  BatchID= 60 Loss: 0.588 | Acc: 80.853% (6313/7808)
[Train] Epoch= 11  BatchID= 70 Loss: 0.593 | Acc: 80.667% (7331/9088)
[Test] Epoch= 11  BatchID= 0 Loss: 0.112 | Acc: 96.094% (123/128)
[Test] Epoch= 11  BatchID= 10 Loss: 0.507 | Acc: 83.949% (1182/1408)
[Test] Epoch= 11  BatchID= 20 Loss: 0.450 | Acc: 85.342% (2294/2688)
[Test] Epoch= 11  BatchID= 30 Loss: 0.520 | Acc: 83.261% (3268/3925)

Epoch: 12
[Train] Epoch= 12  BatchID= 0 Loss: 0.534 | Acc: 81.250% (104/128)
[Train] Epoch= 12  BatchID= 10 Loss: 0.535 | Acc: 82.884% (1167/1408)
[Train] Epoch= 12  BatchID= 20 Loss: 0.549 | Acc: 82.552% (2219/2688)
[Train] Epoch= 12  BatchID= 30 Loss: 0.560 | Acc: 81.855% (3248/3968)
[Train] Epoch= 12  BatchID= 40 Loss: 0.573 | Acc: 81.326% (4268/5248)
[Train] Epoch= 12  BatchID= 50 Loss: 0.580 | Acc: 81.449% (5317/6528)
[Train] Epoch= 12  BatchID= 60 Loss: 0.580 | Acc: 81.570% (6369/7808)
[Train] Epoch= 12  BatchID= 70 Loss: 0.578 | Acc: 81.492% (7406/9088)
[Test] Epoch= 12  BatchID= 0 Loss: 0.188 | Acc: 93.750% (120/128)
[Test] Epoch= 12  BatchID= 10 Loss: 0.494 | Acc: 83.665% (1178/1408)
[Test] Epoch= 12  BatchID= 20 Loss: 0.486 | Acc: 84.449% (2270/2688)
[Test] Epoch= 12  BatchID= 30 Loss: 0.630 | Acc: 79.465% (3119/3925)

Epoch: 13
[Train] Epoch= 13  BatchID= 0 Loss: 0.619 | Acc: 79.688% (102/128)
[Train] Epoch= 13  BatchID= 10 Loss: 0.533 | Acc: 83.239% (1172/1408)
[Train] Epoch= 13  BatchID= 20 Loss: 0.554 | Acc: 82.068% (2206/2688)
[Train] Epoch= 13  BatchID= 30 Loss: 0.567 | Acc: 81.855% (3248/3968)
[Train] Epoch= 13  BatchID= 40 Loss: 0.568 | Acc: 81.631% (4284/5248)
[Train] Epoch= 13  BatchID= 50 Loss: 0.579 | Acc: 81.204% (5301/6528)
[Train] Epoch= 13  BatchID= 60 Loss: 0.578 | Acc: 81.148% (6336/7808)
[Train] Epoch= 13  BatchID= 70 Loss: 0.587 | Acc: 80.843% (7347/9088)
[Test] Epoch= 13  BatchID= 0 Loss: 0.270 | Acc: 91.406% (117/128)
[Test] Epoch= 13  BatchID= 10 Loss: 0.549 | Acc: 81.960% (1154/1408)
[Test] Epoch= 13  BatchID= 20 Loss: 0.613 | Acc: 79.762% (2144/2688)
[Test] Epoch= 13  BatchID= 30 Loss: 0.604 | Acc: 80.306% (3152/3925)

Epoch: 14
[Train] Epoch= 14  BatchID= 0 Loss: 0.476 | Acc: 83.594% (107/128)
[Train] Epoch= 14  BatchID= 10 Loss: 0.573 | Acc: 80.469% (1133/1408)
[Train] Epoch= 14  BatchID= 20 Loss: 0.559 | Acc: 80.915% (2175/2688)
[Train] Epoch= 14  BatchID= 30 Loss: 0.544 | Acc: 81.653% (3240/3968)
[Train] Epoch= 14  BatchID= 40 Loss: 0.555 | Acc: 81.421% (4273/5248)
[Train] Epoch= 14  BatchID= 50 Loss: 0.566 | Acc: 81.327% (5309/6528)
[Train] Epoch= 14  BatchID= 60 Loss: 0.570 | Acc: 81.352% (6352/7808)
[Train] Epoch= 14  BatchID= 70 Loss: 0.574 | Acc: 81.107% (7371/9088)
[Test] Epoch= 14  BatchID= 0 Loss: 0.111 | Acc: 96.875% (124/128)
[Test] Epoch= 14  BatchID= 10 Loss: 0.480 | Acc: 84.659% (1192/1408)
[Test] Epoch= 14  BatchID= 20 Loss: 0.487 | Acc: 85.119% (2288/2688)
[Test] Epoch= 14  BatchID= 30 Loss: 0.555 | Acc: 82.395% (3234/3925)

Epoch: 15
[Train] Epoch= 15  BatchID= 0 Loss: 0.569 | Acc: 78.906% (101/128)
[Train] Epoch= 15  BatchID= 10 Loss: 0.534 | Acc: 82.599% (1163/1408)
[Train] Epoch= 15  BatchID= 20 Loss: 0.555 | Acc: 82.217% (2210/2688)
[Train] Epoch= 15  BatchID= 30 Loss: 0.563 | Acc: 81.930% (3251/3968)
[Train] Epoch= 15  BatchID= 40 Loss: 0.566 | Acc: 81.803% (4293/5248)
[Train] Epoch= 15  BatchID= 50 Loss: 0.570 | Acc: 81.679% (5332/6528)
[Train] Epoch= 15  BatchID= 60 Loss: 0.570 | Acc: 81.468% (6361/7808)
[Train] Epoch= 15  BatchID= 70 Loss: 0.572 | Acc: 81.415% (7399/9088)
[Test] Epoch= 15  BatchID= 0 Loss: 0.229 | Acc: 92.188% (118/128)
[Test] Epoch= 15  BatchID= 10 Loss: 0.663 | Acc: 77.983% (1098/1408)
[Test] Epoch= 15  BatchID= 20 Loss: 0.546 | Acc: 82.292% (2212/2688)
[Test] Epoch= 15  BatchID= 30 Loss: 0.520 | Acc: 83.643% (3283/3925)

Epoch: 16
[Train] Epoch= 16  BatchID= 0 Loss: 0.664 | Acc: 78.906% (101/128)
[Train] Epoch= 16  BatchID= 10 Loss: 0.604 | Acc: 80.398% (1132/1408)
[Train] Epoch= 16  BatchID= 20 Loss: 0.588 | Acc: 80.841% (2173/2688)
[Train] Epoch= 16  BatchID= 30 Loss: 0.586 | Acc: 80.595% (3198/3968)
[Train] Epoch= 16  BatchID= 40 Loss: 0.594 | Acc: 80.202% (4209/5248)
[Train] Epoch= 16  BatchID= 50 Loss: 0.598 | Acc: 80.331% (5244/6528)
[Train] Epoch= 16  BatchID= 60 Loss: 0.595 | Acc: 80.277% (6268/7808)
[Train] Epoch= 16  BatchID= 70 Loss: 0.592 | Acc: 80.381% (7305/9088)
[Test] Epoch= 16  BatchID= 0 Loss: 0.218 | Acc: 92.969% (119/128)
[Test] Epoch= 16  BatchID= 10 Loss: 0.386 | Acc: 86.151% (1213/1408)
[Test] Epoch= 16  BatchID= 20 Loss: 0.427 | Acc: 85.379% (2295/2688)
[Test] Epoch= 16  BatchID= 30 Loss: 0.501 | Acc: 83.287% (3269/3925)

Epoch: 17
[Train] Epoch= 17  BatchID= 0 Loss: 0.533 | Acc: 84.375% (108/128)
[Train] Epoch= 17  BatchID= 10 Loss: 0.532 | Acc: 82.599% (1163/1408)
[Train] Epoch= 17  BatchID= 20 Loss: 0.532 | Acc: 82.961% (2230/2688)
[Train] Epoch= 17  BatchID= 30 Loss: 0.540 | Acc: 82.535% (3275/3968)
[Train] Epoch= 17  BatchID= 40 Loss: 0.545 | Acc: 82.146% (4311/5248)
[Train] Epoch= 17  BatchID= 50 Loss: 0.551 | Acc: 82.230% (5368/6528)
[Train] Epoch= 17  BatchID= 60 Loss: 0.550 | Acc: 82.172% (6416/7808)
[Train] Epoch= 17  BatchID= 70 Loss: 0.553 | Acc: 82.086% (7460/9088)
[Test] Epoch= 17  BatchID= 0 Loss: 0.237 | Acc: 94.531% (121/128)
[Test] Epoch= 17  BatchID= 10 Loss: 0.637 | Acc: 80.469% (1133/1408)
[Test] Epoch= 17  BatchID= 20 Loss: 0.625 | Acc: 81.138% (2181/2688)
[Test] Epoch= 17  BatchID= 30 Loss: 0.700 | Acc: 79.210% (3109/3925)

Epoch: 18
[Train] Epoch= 18  BatchID= 0 Loss: 0.673 | Acc: 73.438% (94/128)
[Train] Epoch= 18  BatchID= 10 Loss: 0.556 | Acc: 80.753% (1137/1408)
[Train] Epoch= 18  BatchID= 20 Loss: 0.552 | Acc: 81.510% (2191/2688)
[Train] Epoch= 18  BatchID= 30 Loss: 0.548 | Acc: 82.107% (3258/3968)
[Train] Epoch= 18  BatchID= 40 Loss: 0.555 | Acc: 81.955% (4301/5248)
[Train] Epoch= 18  BatchID= 50 Loss: 0.558 | Acc: 81.756% (5337/6528)
[Train] Epoch= 18  BatchID= 60 Loss: 0.561 | Acc: 81.775% (6385/7808)
[Train] Epoch= 18  BatchID= 70 Loss: 0.563 | Acc: 81.690% (7424/9088)
[Test] Epoch= 18  BatchID= 0 Loss: 0.116 | Acc: 94.531% (121/128)
[Test] Epoch= 18  BatchID= 10 Loss: 0.517 | Acc: 82.315% (1159/1408)
[Test] Epoch= 18  BatchID= 20 Loss: 0.465 | Acc: 84.598% (2274/2688)
[Test] Epoch= 18  BatchID= 30 Loss: 0.518 | Acc: 83.490% (3277/3925)

Epoch: 19
[Train] Epoch= 19  BatchID= 0 Loss: 0.487 | Acc: 83.594% (107/128)
[Train] Epoch= 19  BatchID= 10 Loss: 0.534 | Acc: 83.026% (1169/1408)
[Train] Epoch= 19  BatchID= 20 Loss: 0.541 | Acc: 83.073% (2233/2688)
[Train] Epoch= 19  BatchID= 30 Loss: 0.544 | Acc: 82.939% (3291/3968)
[Train] Epoch= 19  BatchID= 40 Loss: 0.557 | Acc: 82.336% (4321/5248)
[Train] Epoch= 19  BatchID= 50 Loss: 0.563 | Acc: 82.123% (5361/6528)
[Train] Epoch= 19  BatchID= 60 Loss: 0.563 | Acc: 82.082% (6409/7808)
[Train] Epoch= 19  BatchID= 70 Loss: 0.562 | Acc: 82.020% (7454/9088)
[Test] Epoch= 19  BatchID= 0 Loss: 0.135 | Acc: 95.312% (122/128)
[Test] Epoch= 19  BatchID= 10 Loss: 0.628 | Acc: 80.114% (1128/1408)
[Test] Epoch= 19  BatchID= 20 Loss: 0.557 | Acc: 83.110% (2234/2688)
[Test] Epoch= 19  BatchID= 30 Loss: 0.528 | Acc: 83.873% (3292/3925)
Saving..
Best accuracy:  83.87261146496816

Epoch: 20
[Train] Epoch= 20  BatchID= 0 Loss: 0.551 | Acc: 82.812% (106/128)
[Train] Epoch= 20  BatchID= 10 Loss: 0.527 | Acc: 82.812% (1166/1408)
[Train] Epoch= 20  BatchID= 20 Loss: 0.527 | Acc: 83.371% (2241/2688)
[Train] Epoch= 20  BatchID= 30 Loss: 0.531 | Acc: 82.939% (3291/3968)
[Train] Epoch= 20  BatchID= 40 Loss: 0.546 | Acc: 82.355% (4322/5248)
[Train] Epoch= 20  BatchID= 50 Loss: 0.544 | Acc: 82.093% (5359/6528)
[Train] Epoch= 20  BatchID= 60 Loss: 0.544 | Acc: 82.147% (6414/7808)
[Train] Epoch= 20  BatchID= 70 Loss: 0.544 | Acc: 82.174% (7468/9088)
[Test] Epoch= 20  BatchID= 0 Loss: 0.396 | Acc: 89.844% (115/128)
[Test] Epoch= 20  BatchID= 10 Loss: 0.605 | Acc: 80.114% (1128/1408)
[Test] Epoch= 20  BatchID= 20 Loss: 0.723 | Acc: 76.749% (2063/2688)
[Test] Epoch= 20  BatchID= 30 Loss: 0.683 | Acc: 78.217% (3070/3925)

Epoch: 21
[Train] Epoch= 21  BatchID= 0 Loss: 0.630 | Acc: 80.469% (103/128)
[Train] Epoch= 21  BatchID= 10 Loss: 0.544 | Acc: 82.670% (1164/1408)
[Train] Epoch= 21  BatchID= 20 Loss: 0.529 | Acc: 83.110% (2234/2688)
[Train] Epoch= 21  BatchID= 30 Loss: 0.524 | Acc: 82.989% (3293/3968)
[Train] Epoch= 21  BatchID= 40 Loss: 0.524 | Acc: 82.755% (4343/5248)
[Train] Epoch= 21  BatchID= 50 Loss: 0.529 | Acc: 82.751% (5402/6528)
[Train] Epoch= 21  BatchID= 60 Loss: 0.534 | Acc: 82.531% (6444/7808)
[Train] Epoch= 21  BatchID= 70 Loss: 0.539 | Acc: 82.361% (7485/9088)
[Test] Epoch= 21  BatchID= 0 Loss: 0.107 | Acc: 96.094% (123/128)
[Test] Epoch= 21  BatchID= 10 Loss: 0.447 | Acc: 85.369% (1202/1408)
[Test] Epoch= 21  BatchID= 20 Loss: 0.496 | Acc: 83.966% (2257/2688)
[Test] Epoch= 21  BatchID= 30 Loss: 0.526 | Acc: 83.185% (3265/3925)

Epoch: 22
[Train] Epoch= 22  BatchID= 0 Loss: 0.557 | Acc: 85.938% (110/128)
[Train] Epoch= 22  BatchID= 10 Loss: 0.538 | Acc: 82.386% (1160/1408)
[Train] Epoch= 22  BatchID= 20 Loss: 0.535 | Acc: 82.552% (2219/2688)
[Train] Epoch= 22  BatchID= 30 Loss: 0.532 | Acc: 82.863% (3288/3968)
[Train] Epoch= 22  BatchID= 40 Loss: 0.532 | Acc: 82.965% (4354/5248)
[Train] Epoch= 22  BatchID= 50 Loss: 0.542 | Acc: 82.675% (5397/6528)
[Train] Epoch= 22  BatchID= 60 Loss: 0.537 | Acc: 82.723% (6459/7808)
[Train] Epoch= 22  BatchID= 70 Loss: 0.533 | Acc: 82.956% (7539/9088)
[Test] Epoch= 22  BatchID= 0 Loss: 0.159 | Acc: 96.094% (123/128)
[Test] Epoch= 22  BatchID= 10 Loss: 0.336 | Acc: 89.915% (1266/1408)
[Test] Epoch= 22  BatchID= 20 Loss: 0.425 | Acc: 86.458% (2324/2688)
[Test] Epoch= 22  BatchID= 30 Loss: 0.551 | Acc: 82.675% (3245/3925)

Epoch: 23
[Train] Epoch= 23  BatchID= 0 Loss: 0.555 | Acc: 81.250% (104/128)
[Train] Epoch= 23  BatchID= 10 Loss: 0.546 | Acc: 82.670% (1164/1408)
[Train] Epoch= 23  BatchID= 20 Loss: 0.543 | Acc: 81.845% (2200/2688)
[Train] Epoch= 23  BatchID= 30 Loss: 0.533 | Acc: 82.132% (3259/3968)
[Train] Epoch= 23  BatchID= 40 Loss: 0.542 | Acc: 81.860% (4296/5248)
[Train] Epoch= 23  BatchID= 50 Loss: 0.546 | Acc: 81.939% (5349/6528)
[Train] Epoch= 23  BatchID= 60 Loss: 0.553 | Acc: 81.775% (6385/7808)
[Train] Epoch= 23  BatchID= 70 Loss: 0.557 | Acc: 81.712% (7426/9088)
[Test] Epoch= 23  BatchID= 0 Loss: 0.152 | Acc: 95.312% (122/128)
[Test] Epoch= 23  BatchID= 10 Loss: 0.456 | Acc: 85.369% (1202/1408)
[Test] Epoch= 23  BatchID= 20 Loss: 0.500 | Acc: 84.226% (2264/2688)
[Test] Epoch= 23  BatchID= 30 Loss: 0.580 | Acc: 82.395% (3234/3925)

Epoch: 24
[Train] Epoch= 24  BatchID= 0 Loss: 0.480 | Acc: 85.938% (110/128)
[Train] Epoch= 24  BatchID= 10 Loss: 0.508 | Acc: 83.949% (1182/1408)
[Train] Epoch= 24  BatchID= 20 Loss: 0.514 | Acc: 84.263% (2265/2688)
[Train] Epoch= 24  BatchID= 30 Loss: 0.504 | Acc: 84.425% (3350/3968)
[Train] Epoch= 24  BatchID= 40 Loss: 0.499 | Acc: 84.299% (4424/5248)
[Train] Epoch= 24  BatchID= 50 Loss: 0.512 | Acc: 83.885% (5476/6528)
[Train] Epoch= 24  BatchID= 60 Loss: 0.516 | Acc: 83.863% (6548/7808)
[Train] Epoch= 24  BatchID= 70 Loss: 0.515 | Acc: 83.759% (7612/9088)
[Test] Epoch= 24  BatchID= 0 Loss: 0.167 | Acc: 96.094% (123/128)
[Test] Epoch= 24  BatchID= 10 Loss: 0.343 | Acc: 88.494% (1246/1408)
[Test] Epoch= 24  BatchID= 20 Loss: 0.400 | Acc: 87.500% (2352/2688)
[Test] Epoch= 24  BatchID= 30 Loss: 0.443 | Acc: 85.478% (3355/3925)
Saving..
Best accuracy:  85.47770700636943

Epoch: 25
[Train] Epoch= 25  BatchID= 0 Loss: 0.437 | Acc: 84.375% (108/128)
[Train] Epoch= 25  BatchID= 10 Loss: 0.483 | Acc: 84.233% (1186/1408)
[Train] Epoch= 25  BatchID= 20 Loss: 0.498 | Acc: 83.445% (2243/2688)
[Train] Epoch= 25  BatchID= 30 Loss: 0.498 | Acc: 83.745% (3323/3968)
[Train] Epoch= 25  BatchID= 40 Loss: 0.507 | Acc: 83.441% (4379/5248)
[Train] Epoch= 25  BatchID= 50 Loss: 0.510 | Acc: 83.349% (5441/6528)
[Train] Epoch= 25  BatchID= 60 Loss: 0.513 | Acc: 83.210% (6497/7808)
[Train] Epoch= 25  BatchID= 70 Loss: 0.516 | Acc: 83.176% (7559/9088)
[Test] Epoch= 25  BatchID= 0 Loss: 0.385 | Acc: 90.625% (116/128)
[Test] Epoch= 25  BatchID= 10 Loss: 0.520 | Acc: 83.026% (1169/1408)
[Test] Epoch= 25  BatchID= 20 Loss: 0.545 | Acc: 81.696% (2196/2688)
[Test] Epoch= 25  BatchID= 30 Loss: 0.555 | Acc: 82.140% (3224/3925)

Epoch: 26
[Train] Epoch= 26  BatchID= 0 Loss: 0.489 | Acc: 86.719% (111/128)
[Train] Epoch= 26  BatchID= 10 Loss: 0.500 | Acc: 84.872% (1195/1408)
[Train] Epoch= 26  BatchID= 20 Loss: 0.506 | Acc: 84.263% (2265/2688)
[Train] Epoch= 26  BatchID= 30 Loss: 0.504 | Acc: 84.148% (3339/3968)
[Train] Epoch= 26  BatchID= 40 Loss: 0.500 | Acc: 84.108% (4414/5248)
[Train] Epoch= 26  BatchID= 50 Loss: 0.495 | Acc: 84.314% (5504/6528)
[Train] Epoch= 26  BatchID= 60 Loss: 0.500 | Acc: 84.055% (6563/7808)
[Train] Epoch= 26  BatchID= 70 Loss: 0.495 | Acc: 84.122% (7645/9088)
[Test] Epoch= 26  BatchID= 0 Loss: 0.138 | Acc: 95.312% (122/128)
[Test] Epoch= 26  BatchID= 10 Loss: 0.387 | Acc: 87.571% (1233/1408)
[Test] Epoch= 26  BatchID= 20 Loss: 0.410 | Acc: 87.054% (2340/2688)
[Test] Epoch= 26  BatchID= 30 Loss: 0.445 | Acc: 86.089% (3379/3925)
Saving..
Best accuracy:  86.08917197452229

Epoch: 27
[Train] Epoch= 27  BatchID= 0 Loss: 0.468 | Acc: 85.156% (109/128)
[Train] Epoch= 27  BatchID= 10 Loss: 0.477 | Acc: 84.020% (1183/1408)
[Train] Epoch= 27  BatchID= 20 Loss: 0.473 | Acc: 84.449% (2270/2688)
[Train] Epoch= 27  BatchID= 30 Loss: 0.487 | Acc: 83.921% (3330/3968)
[Train] Epoch= 27  BatchID= 40 Loss: 0.494 | Acc: 83.956% (4406/5248)
[Train] Epoch= 27  BatchID= 50 Loss: 0.488 | Acc: 84.314% (5504/6528)
[Train] Epoch= 27  BatchID= 60 Loss: 0.494 | Acc: 84.068% (6564/7808)
[Train] Epoch= 27  BatchID= 70 Loss: 0.504 | Acc: 83.737% (7610/9088)
[Test] Epoch= 27  BatchID= 0 Loss: 0.149 | Acc: 95.312% (122/128)
[Test] Epoch= 27  BatchID= 10 Loss: 0.336 | Acc: 89.631% (1262/1408)
[Test] Epoch= 27  BatchID= 20 Loss: 0.386 | Acc: 88.132% (2369/2688)
[Test] Epoch= 27  BatchID= 30 Loss: 0.516 | Acc: 83.771% (3288/3925)

Epoch: 28
[Train] Epoch= 28  BatchID= 0 Loss: 0.614 | Acc: 78.125% (100/128)
[Train] Epoch= 28  BatchID= 10 Loss: 0.490 | Acc: 84.375% (1188/1408)
[Train] Epoch= 28  BatchID= 20 Loss: 0.498 | Acc: 84.040% (2259/2688)
[Train] Epoch= 28  BatchID= 30 Loss: 0.504 | Acc: 83.846% (3327/3968)
[Train] Epoch= 28  BatchID= 40 Loss: 0.498 | Acc: 83.956% (4406/5248)
[Train] Epoch= 28  BatchID= 50 Loss: 0.503 | Acc: 83.655% (5461/6528)
[Train] Epoch= 28  BatchID= 60 Loss: 0.510 | Acc: 83.402% (6512/7808)
[Train] Epoch= 28  BatchID= 70 Loss: 0.514 | Acc: 83.341% (7574/9088)
[Test] Epoch= 28  BatchID= 0 Loss: 0.064 | Acc: 98.438% (126/128)
[Test] Epoch= 28  BatchID= 10 Loss: 0.285 | Acc: 90.483% (1274/1408)
[Test] Epoch= 28  BatchID= 20 Loss: 0.401 | Acc: 86.830% (2334/2688)
[Test] Epoch= 28  BatchID= 30 Loss: 0.494 | Acc: 83.847% (3291/3925)

Epoch: 29
[Train] Epoch= 29  BatchID= 0 Loss: 0.475 | Acc: 86.719% (111/128)
[Train] Epoch= 29  BatchID= 10 Loss: 0.450 | Acc: 86.080% (1212/1408)
[Train] Epoch= 29  BatchID= 20 Loss: 0.456 | Acc: 85.491% (2298/2688)
[Train] Epoch= 29  BatchID= 30 Loss: 0.472 | Acc: 84.929% (3370/3968)
[Train] Epoch= 29  BatchID= 40 Loss: 0.477 | Acc: 84.546% (4437/5248)
[Train] Epoch= 29  BatchID= 50 Loss: 0.483 | Acc: 84.528% (5518/6528)
[Train] Epoch= 29  BatchID= 60 Loss: 0.485 | Acc: 84.362% (6587/7808)
[Train] Epoch= 29  BatchID= 70 Loss: 0.486 | Acc: 84.397% (7670/9088)
[Test] Epoch= 29  BatchID= 0 Loss: 0.273 | Acc: 89.844% (115/128)
[Test] Epoch= 29  BatchID= 10 Loss: 0.442 | Acc: 84.943% (1196/1408)
[Test] Epoch= 29  BatchID= 20 Loss: 0.478 | Acc: 84.152% (2262/2688)
[Test] Epoch= 29  BatchID= 30 Loss: 0.569 | Acc: 81.732% (3208/3925)

Epoch: 30
[Train] Epoch= 30  BatchID= 0 Loss: 0.436 | Acc: 85.938% (110/128)
[Train] Epoch= 30  BatchID= 10 Loss: 0.477 | Acc: 83.736% (1179/1408)
[Train] Epoch= 30  BatchID= 20 Loss: 0.474 | Acc: 84.152% (2262/2688)
[Train] Epoch= 30  BatchID= 30 Loss: 0.484 | Acc: 83.896% (3329/3968)
[Train] Epoch= 30  BatchID= 40 Loss: 0.490 | Acc: 83.670% (4391/5248)
[Train] Epoch= 30  BatchID= 50 Loss: 0.487 | Acc: 84.038% (5486/6528)
[Train] Epoch= 30  BatchID= 60 Loss: 0.489 | Acc: 83.940% (6554/7808)
[Train] Epoch= 30  BatchID= 70 Loss: 0.492 | Acc: 83.957% (7630/9088)
[Test] Epoch= 30  BatchID= 0 Loss: 0.170 | Acc: 95.312% (122/128)
[Test] Epoch= 30  BatchID= 10 Loss: 0.509 | Acc: 83.878% (1181/1408)
[Test] Epoch= 30  BatchID= 20 Loss: 0.468 | Acc: 84.859% (2281/2688)
[Test] Epoch= 30  BatchID= 30 Loss: 0.599 | Acc: 81.325% (3192/3925)

Epoch: 31
[Train] Epoch= 31  BatchID= 0 Loss: 0.386 | Acc: 86.719% (111/128)
[Train] Epoch= 31  BatchID= 10 Loss: 0.468 | Acc: 84.446% (1189/1408)
[Train] Epoch= 31  BatchID= 20 Loss: 0.476 | Acc: 84.710% (2277/2688)
[Train] Epoch= 31  BatchID= 30 Loss: 0.462 | Acc: 85.106% (3377/3968)
[Train] Epoch= 31  BatchID= 40 Loss: 0.472 | Acc: 84.680% (4444/5248)
[Train] Epoch= 31  BatchID= 50 Loss: 0.476 | Acc: 84.513% (5517/6528)
[Train] Epoch= 31  BatchID= 60 Loss: 0.487 | Acc: 84.285% (6581/7808)
[Train] Epoch= 31  BatchID= 70 Loss: 0.495 | Acc: 84.023% (7636/9088)
[Test] Epoch= 31  BatchID= 0 Loss: 0.300 | Acc: 91.406% (117/128)
[Test] Epoch= 31  BatchID= 10 Loss: 0.348 | Acc: 88.565% (1247/1408)
[Test] Epoch= 31  BatchID= 20 Loss: 0.402 | Acc: 87.128% (2342/2688)
[Test] Epoch= 31  BatchID= 30 Loss: 0.434 | Acc: 85.936% (3373/3925)

Epoch: 32
[Train] Epoch= 32  BatchID= 0 Loss: 0.459 | Acc: 83.594% (107/128)
[Train] Epoch= 32  BatchID= 10 Loss: 0.485 | Acc: 84.304% (1187/1408)
[Train] Epoch= 32  BatchID= 20 Loss: 0.478 | Acc: 85.156% (2289/2688)
[Train] Epoch= 32  BatchID= 30 Loss: 0.484 | Acc: 85.106% (3377/3968)
[Train] Epoch= 32  BatchID= 40 Loss: 0.485 | Acc: 85.004% (4461/5248)
[Train] Epoch= 32  BatchID= 50 Loss: 0.481 | Acc: 85.064% (5553/6528)
[Train] Epoch= 32  BatchID= 60 Loss: 0.474 | Acc: 85.246% (6656/7808)
[Train] Epoch= 32  BatchID= 70 Loss: 0.478 | Acc: 85.024% (7727/9088)
[Test] Epoch= 32  BatchID= 0 Loss: 0.202 | Acc: 95.312% (122/128)
[Test] Epoch= 32  BatchID= 10 Loss: 0.449 | Acc: 85.866% (1209/1408)
[Test] Epoch= 32  BatchID= 20 Loss: 0.405 | Acc: 87.091% (2341/2688)
[Test] Epoch= 32  BatchID= 30 Loss: 0.420 | Acc: 86.573% (3398/3925)
Saving..
Best accuracy:  86.5732484076433

Epoch: 33
[Train] Epoch= 33  BatchID= 0 Loss: 0.520 | Acc: 82.812% (106/128)
[Train] Epoch= 33  BatchID= 10 Loss: 0.486 | Acc: 84.375% (1188/1408)
[Train] Epoch= 33  BatchID= 20 Loss: 0.481 | Acc: 84.561% (2273/2688)
[Train] Epoch= 33  BatchID= 30 Loss: 0.478 | Acc: 84.803% (3365/3968)
[Train] Epoch= 33  BatchID= 40 Loss: 0.469 | Acc: 85.175% (4470/5248)
[Train] Epoch= 33  BatchID= 50 Loss: 0.468 | Acc: 85.187% (5561/6528)
[Train] Epoch= 33  BatchID= 60 Loss: 0.465 | Acc: 85.336% (6663/7808)
[Train] Epoch= 33  BatchID= 70 Loss: 0.468 | Acc: 85.244% (7747/9088)
[Test] Epoch= 33  BatchID= 0 Loss: 0.104 | Acc: 96.094% (123/128)
[Test] Epoch= 33  BatchID= 10 Loss: 0.378 | Acc: 87.500% (1232/1408)
[Test] Epoch= 33  BatchID= 20 Loss: 0.386 | Acc: 87.612% (2355/2688)
[Test] Epoch= 33  BatchID= 30 Loss: 0.479 | Acc: 85.172% (3343/3925)

Epoch: 34
[Train] Epoch= 34  BatchID= 0 Loss: 0.593 | Acc: 81.250% (104/128)
[Train] Epoch= 34  BatchID= 10 Loss: 0.459 | Acc: 85.440% (1203/1408)
[Train] Epoch= 34  BatchID= 20 Loss: 0.470 | Acc: 84.821% (2280/2688)
[Train] Epoch= 34  BatchID= 30 Loss: 0.483 | Acc: 84.249% (3343/3968)
[Train] Epoch= 34  BatchID= 40 Loss: 0.478 | Acc: 84.413% (4430/5248)
[Train] Epoch= 34  BatchID= 50 Loss: 0.473 | Acc: 84.697% (5529/6528)
[Train] Epoch= 34  BatchID= 60 Loss: 0.468 | Acc: 84.900% (6629/7808)
[Train] Epoch= 34  BatchID= 70 Loss: 0.469 | Acc: 84.925% (7718/9088)
[Test] Epoch= 34  BatchID= 0 Loss: 0.092 | Acc: 97.656% (125/128)
[Test] Epoch= 34  BatchID= 10 Loss: 0.414 | Acc: 86.648% (1220/1408)
[Test] Epoch= 34  BatchID= 20 Loss: 0.442 | Acc: 85.454% (2297/2688)
[Test] Epoch= 34  BatchID= 30 Loss: 0.489 | Acc: 84.051% (3299/3925)

Epoch: 35
[Train] Epoch= 35  BatchID= 0 Loss: 0.509 | Acc: 87.500% (112/128)
[Train] Epoch= 35  BatchID= 10 Loss: 0.469 | Acc: 84.872% (1195/1408)
[Train] Epoch= 35  BatchID= 20 Loss: 0.450 | Acc: 85.045% (2286/2688)
[Train] Epoch= 35  BatchID= 30 Loss: 0.456 | Acc: 85.081% (3376/3968)
[Train] Epoch= 35  BatchID= 40 Loss: 0.456 | Acc: 84.947% (4458/5248)
[Train] Epoch= 35  BatchID= 50 Loss: 0.457 | Acc: 84.972% (5547/6528)
[Train] Epoch= 35  BatchID= 60 Loss: 0.458 | Acc: 84.887% (6628/7808)
[Train] Epoch= 35  BatchID= 70 Loss: 0.466 | Acc: 84.716% (7699/9088)
[Test] Epoch= 35  BatchID= 0 Loss: 0.113 | Acc: 96.875% (124/128)
[Test] Epoch= 35  BatchID= 10 Loss: 0.430 | Acc: 85.014% (1197/1408)
[Test] Epoch= 35  BatchID= 20 Loss: 0.482 | Acc: 83.854% (2254/2688)
[Test] Epoch= 35  BatchID= 30 Loss: 0.490 | Acc: 84.051% (3299/3925)

Epoch: 36
[Train] Epoch= 36  BatchID= 0 Loss: 0.428 | Acc: 86.719% (111/128)
[Train] Epoch= 36  BatchID= 10 Loss: 0.465 | Acc: 85.298% (1201/1408)
[Train] Epoch= 36  BatchID= 20 Loss: 0.454 | Acc: 85.305% (2293/2688)
[Train] Epoch= 36  BatchID= 30 Loss: 0.464 | Acc: 84.854% (3367/3968)
[Train] Epoch= 36  BatchID= 40 Loss: 0.460 | Acc: 84.985% (4460/5248)
[Train] Epoch= 36  BatchID= 50 Loss: 0.462 | Acc: 85.126% (5557/6528)
[Train] Epoch= 36  BatchID= 60 Loss: 0.452 | Acc: 85.438% (6671/7808)
[Train] Epoch= 36  BatchID= 70 Loss: 0.454 | Acc: 85.365% (7758/9088)
[Test] Epoch= 36  BatchID= 0 Loss: 0.115 | Acc: 97.656% (125/128)
[Test] Epoch= 36  BatchID= 10 Loss: 0.313 | Acc: 89.773% (1264/1408)
[Test] Epoch= 36  BatchID= 20 Loss: 0.376 | Acc: 87.835% (2361/2688)
[Test] Epoch= 36  BatchID= 30 Loss: 0.405 | Acc: 87.261% (3425/3925)
Saving..
Best accuracy:  87.26114649681529

Epoch: 37
[Train] Epoch= 37  BatchID= 0 Loss: 0.360 | Acc: 87.500% (112/128)
[Train] Epoch= 37  BatchID= 10 Loss: 0.418 | Acc: 86.506% (1218/1408)
[Train] Epoch= 37  BatchID= 20 Loss: 0.414 | Acc: 86.756% (2332/2688)
[Train] Epoch= 37  BatchID= 30 Loss: 0.426 | Acc: 86.517% (3433/3968)
[Train] Epoch= 37  BatchID= 40 Loss: 0.441 | Acc: 85.995% (4513/5248)
[Train] Epoch= 37  BatchID= 50 Loss: 0.435 | Acc: 86.366% (5638/6528)
[Train] Epoch= 37  BatchID= 60 Loss: 0.447 | Acc: 85.809% (6700/7808)
[Train] Epoch= 37  BatchID= 70 Loss: 0.446 | Acc: 85.728% (7791/9088)
[Test] Epoch= 37  BatchID= 0 Loss: 0.074 | Acc: 97.656% (125/128)
[Test] Epoch= 37  BatchID= 10 Loss: 0.369 | Acc: 88.849% (1251/1408)
[Test] Epoch= 37  BatchID= 20 Loss: 0.414 | Acc: 86.979% (2338/2688)
[Test] Epoch= 37  BatchID= 30 Loss: 0.432 | Acc: 86.369% (3390/3925)

Epoch: 38
[Train] Epoch= 38  BatchID= 0 Loss: 0.391 | Acc: 88.281% (113/128)
[Train] Epoch= 38  BatchID= 10 Loss: 0.445 | Acc: 85.866% (1209/1408)
[Train] Epoch= 38  BatchID= 20 Loss: 0.447 | Acc: 85.789% (2306/2688)
[Train] Epoch= 38  BatchID= 30 Loss: 0.441 | Acc: 85.938% (3410/3968)
[Train] Epoch= 38  BatchID= 40 Loss: 0.445 | Acc: 85.880% (4507/5248)
[Train] Epoch= 38  BatchID= 50 Loss: 0.450 | Acc: 85.631% (5590/6528)
[Train] Epoch= 38  BatchID= 60 Loss: 0.446 | Acc: 85.745% (6695/7808)
[Train] Epoch= 38  BatchID= 70 Loss: 0.450 | Acc: 85.662% (7785/9088)
[Test] Epoch= 38  BatchID= 0 Loss: 0.113 | Acc: 96.875% (124/128)
[Test] Epoch= 38  BatchID= 10 Loss: 0.410 | Acc: 86.932% (1224/1408)
[Test] Epoch= 38  BatchID= 20 Loss: 0.370 | Acc: 88.281% (2373/2688)
[Test] Epoch= 38  BatchID= 30 Loss: 0.372 | Acc: 88.306% (3466/3925)
Saving..
Best accuracy:  88.30573248407643

Epoch: 39
[Train] Epoch= 39  BatchID= 0 Loss: 0.338 | Acc: 85.938% (110/128)
[Train] Epoch= 39  BatchID= 10 Loss: 0.400 | Acc: 87.145% (1227/1408)
[Train] Epoch= 39  BatchID= 20 Loss: 0.410 | Acc: 86.719% (2331/2688)
[Train] Epoch= 39  BatchID= 30 Loss: 0.425 | Acc: 86.265% (3423/3968)
[Train] Epoch= 39  BatchID= 40 Loss: 0.418 | Acc: 86.547% (4542/5248)
[Train] Epoch= 39  BatchID= 50 Loss: 0.420 | Acc: 86.581% (5652/6528)
[Train] Epoch= 39  BatchID= 60 Loss: 0.431 | Acc: 86.283% (6737/7808)
[Train] Epoch= 39  BatchID= 70 Loss: 0.433 | Acc: 86.235% (7837/9088)
[Test] Epoch= 39  BatchID= 0 Loss: 0.150 | Acc: 96.875% (124/128)
[Test] Epoch= 39  BatchID= 10 Loss: 0.442 | Acc: 85.653% (1206/1408)
[Test] Epoch= 39  BatchID= 20 Loss: 0.416 | Acc: 86.161% (2316/2688)
[Test] Epoch= 39  BatchID= 30 Loss: 0.450 | Acc: 85.758% (3366/3925)

Epoch: 40
[Train] Epoch= 40  BatchID= 0 Loss: 0.491 | Acc: 87.500% (112/128)
[Train] Epoch= 40  BatchID= 10 Loss: 0.409 | Acc: 86.719% (1221/1408)
[Train] Epoch= 40  BatchID= 20 Loss: 0.416 | Acc: 86.496% (2325/2688)
[Train] Epoch= 40  BatchID= 30 Loss: 0.417 | Acc: 86.316% (3425/3968)
[Train] Epoch= 40  BatchID= 40 Loss: 0.418 | Acc: 86.071% (4517/5248)
[Train] Epoch= 40  BatchID= 50 Loss: 0.418 | Acc: 85.983% (5613/6528)
[Train] Epoch= 40  BatchID= 60 Loss: 0.427 | Acc: 85.822% (6701/7808)
[Train] Epoch= 40  BatchID= 70 Loss: 0.427 | Acc: 85.960% (7812/9088)
[Test] Epoch= 40  BatchID= 0 Loss: 0.381 | Acc: 89.062% (114/128)
[Test] Epoch= 40  BatchID= 10 Loss: 0.559 | Acc: 81.534% (1148/1408)
[Test] Epoch= 40  BatchID= 20 Loss: 0.529 | Acc: 82.515% (2218/2688)
[Test] Epoch= 40  BatchID= 30 Loss: 0.479 | Acc: 84.561% (3319/3925)

Epoch: 41
[Train] Epoch= 41  BatchID= 0 Loss: 0.438 | Acc: 82.812% (106/128)
[Train] Epoch= 41  BatchID= 10 Loss: 0.387 | Acc: 87.003% (1225/1408)
[Train] Epoch= 41  BatchID= 20 Loss: 0.401 | Acc: 87.091% (2341/2688)
[Train] Epoch= 41  BatchID= 30 Loss: 0.401 | Acc: 87.021% (3453/3968)
[Train] Epoch= 41  BatchID= 40 Loss: 0.402 | Acc: 86.757% (4553/5248)
[Train] Epoch= 41  BatchID= 50 Loss: 0.409 | Acc: 86.397% (5640/6528)
[Train] Epoch= 41  BatchID= 60 Loss: 0.409 | Acc: 86.514% (6755/7808)
[Train] Epoch= 41  BatchID= 70 Loss: 0.414 | Acc: 86.466% (7858/9088)
[Test] Epoch= 41  BatchID= 0 Loss: 0.292 | Acc: 92.188% (118/128)
[Test] Epoch= 41  BatchID= 10 Loss: 0.519 | Acc: 83.736% (1179/1408)
[Test] Epoch= 41  BatchID= 20 Loss: 0.454 | Acc: 85.751% (2305/2688)
[Test] Epoch= 41  BatchID= 30 Loss: 0.419 | Acc: 86.828% (3408/3925)

Epoch: 42
[Train] Epoch= 42  BatchID= 0 Loss: 0.444 | Acc: 85.938% (110/128)
[Train] Epoch= 42  BatchID= 10 Loss: 0.407 | Acc: 86.790% (1222/1408)
[Train] Epoch= 42  BatchID= 20 Loss: 0.376 | Acc: 87.909% (2363/2688)
[Train] Epoch= 42  BatchID= 30 Loss: 0.388 | Acc: 87.676% (3479/3968)
[Train] Epoch= 42  BatchID= 40 Loss: 0.393 | Acc: 87.633% (4599/5248)
[Train] Epoch= 42  BatchID= 50 Loss: 0.397 | Acc: 87.500% (5712/6528)
[Train] Epoch= 42  BatchID= 60 Loss: 0.404 | Acc: 87.244% (6812/7808)
[Train] Epoch= 42  BatchID= 70 Loss: 0.401 | Acc: 87.225% (7927/9088)
[Test] Epoch= 42  BatchID= 0 Loss: 0.174 | Acc: 95.312% (122/128)
[Test] Epoch= 42  BatchID= 10 Loss: 0.338 | Acc: 88.991% (1253/1408)
[Test] Epoch= 42  BatchID= 20 Loss: 0.357 | Acc: 88.504% (2379/2688)
[Test] Epoch= 42  BatchID= 30 Loss: 0.400 | Acc: 86.930% (3412/3925)

Epoch: 43
[Train] Epoch= 43  BatchID= 0 Loss: 0.328 | Acc: 91.406% (117/128)
[Train] Epoch= 43  BatchID= 10 Loss: 0.351 | Acc: 88.565% (1247/1408)
[Train] Epoch= 43  BatchID= 20 Loss: 0.375 | Acc: 87.798% (2360/2688)
[Train] Epoch= 43  BatchID= 30 Loss: 0.389 | Acc: 87.374% (3467/3968)
[Train] Epoch= 43  BatchID= 40 Loss: 0.387 | Acc: 87.519% (4593/5248)
[Train] Epoch= 43  BatchID= 50 Loss: 0.391 | Acc: 87.377% (5704/6528)
[Train] Epoch= 43  BatchID= 60 Loss: 0.393 | Acc: 87.346% (6820/7808)
[Train] Epoch= 43  BatchID= 70 Loss: 0.401 | Acc: 87.203% (7925/9088)
[Test] Epoch= 43  BatchID= 0 Loss: 0.068 | Acc: 97.656% (125/128)
[Test] Epoch= 43  BatchID= 10 Loss: 0.707 | Acc: 77.060% (1085/1408)
[Test] Epoch= 43  BatchID= 20 Loss: 0.567 | Acc: 82.143% (2208/2688)
[Test] Epoch= 43  BatchID= 30 Loss: 0.542 | Acc: 82.624% (3243/3925)

Epoch: 44
[Train] Epoch= 44  BatchID= 0 Loss: 0.281 | Acc: 92.188% (118/128)
[Train] Epoch= 44  BatchID= 10 Loss: 0.398 | Acc: 86.577% (1219/1408)
[Train] Epoch= 44  BatchID= 20 Loss: 0.386 | Acc: 87.686% (2357/2688)
[Train] Epoch= 44  BatchID= 30 Loss: 0.410 | Acc: 86.895% (3448/3968)
[Train] Epoch= 44  BatchID= 40 Loss: 0.422 | Acc: 86.757% (4553/5248)
[Train] Epoch= 44  BatchID= 50 Loss: 0.420 | Acc: 86.688% (5659/6528)
[Train] Epoch= 44  BatchID= 60 Loss: 0.420 | Acc: 86.770% (6775/7808)
[Train] Epoch= 44  BatchID= 70 Loss: 0.426 | Acc: 86.521% (7863/9088)
[Test] Epoch= 44  BatchID= 0 Loss: 0.288 | Acc: 91.406% (117/128)
[Test] Epoch= 44  BatchID= 10 Loss: 0.505 | Acc: 83.310% (1173/1408)
[Test] Epoch= 44  BatchID= 20 Loss: 0.441 | Acc: 85.491% (2298/2688)
[Test] Epoch= 44  BatchID= 30 Loss: 0.408 | Acc: 86.701% (3403/3925)

Epoch: 45
[Train] Epoch= 45  BatchID= 0 Loss: 0.464 | Acc: 85.938% (110/128)
[Train] Epoch= 45  BatchID= 10 Loss: 0.370 | Acc: 87.784% (1236/1408)
[Train] Epoch= 45  BatchID= 20 Loss: 0.373 | Acc: 88.393% (2376/2688)
[Train] Epoch= 45  BatchID= 30 Loss: 0.379 | Acc: 87.979% (3491/3968)
[Train] Epoch= 45  BatchID= 40 Loss: 0.391 | Acc: 87.767% (4606/5248)
[Train] Epoch= 45  BatchID= 50 Loss: 0.397 | Acc: 87.255% (5696/6528)
[Train] Epoch= 45  BatchID= 60 Loss: 0.398 | Acc: 87.129% (6803/7808)
[Train] Epoch= 45  BatchID= 70 Loss: 0.402 | Acc: 87.104% (7916/9088)
[Test] Epoch= 45  BatchID= 0 Loss: 0.196 | Acc: 95.312% (122/128)
[Test] Epoch= 45  BatchID= 10 Loss: 0.394 | Acc: 87.926% (1238/1408)
[Test] Epoch= 45  BatchID= 20 Loss: 0.417 | Acc: 87.054% (2340/2688)
[Test] Epoch= 45  BatchID= 30 Loss: 0.410 | Acc: 87.083% (3418/3925)

Epoch: 46
[Train] Epoch= 46  BatchID= 0 Loss: 0.406 | Acc: 85.156% (109/128)
[Train] Epoch= 46  BatchID= 10 Loss: 0.360 | Acc: 89.062% (1254/1408)
[Train] Epoch= 46  BatchID= 20 Loss: 0.368 | Acc: 88.579% (2381/2688)
[Train] Epoch= 46  BatchID= 30 Loss: 0.380 | Acc: 88.432% (3509/3968)
[Train] Epoch= 46  BatchID= 40 Loss: 0.376 | Acc: 88.624% (4651/5248)
[Train] Epoch= 46  BatchID= 50 Loss: 0.374 | Acc: 88.588% (5783/6528)
[Train] Epoch= 46  BatchID= 60 Loss: 0.376 | Acc: 88.345% (6898/7808)
[Train] Epoch= 46  BatchID= 70 Loss: 0.379 | Acc: 88.105% (8007/9088)
[Test] Epoch= 46  BatchID= 0 Loss: 0.232 | Acc: 94.531% (121/128)
[Test] Epoch= 46  BatchID= 10 Loss: 0.394 | Acc: 86.719% (1221/1408)
[Test] Epoch= 46  BatchID= 20 Loss: 0.372 | Acc: 87.946% (2364/2688)
[Test] Epoch= 46  BatchID= 30 Loss: 0.363 | Acc: 88.204% (3462/3925)

Epoch: 47
[Train] Epoch= 47  BatchID= 0 Loss: 0.385 | Acc: 92.188% (118/128)
[Train] Epoch= 47  BatchID= 10 Loss: 0.358 | Acc: 88.920% (1252/1408)
[Train] Epoch= 47  BatchID= 20 Loss: 0.354 | Acc: 88.653% (2383/2688)
[Train] Epoch= 47  BatchID= 30 Loss: 0.355 | Acc: 88.609% (3516/3968)
[Train] Epoch= 47  BatchID= 40 Loss: 0.368 | Acc: 88.167% (4627/5248)
[Train] Epoch= 47  BatchID= 50 Loss: 0.375 | Acc: 88.082% (5750/6528)
[Train] Epoch= 47  BatchID= 60 Loss: 0.383 | Acc: 87.705% (6848/7808)
[Train] Epoch= 47  BatchID= 70 Loss: 0.381 | Acc: 87.852% (7984/9088)
[Test] Epoch= 47  BatchID= 0 Loss: 0.204 | Acc: 92.969% (119/128)
[Test] Epoch= 47  BatchID= 10 Loss: 0.384 | Acc: 88.352% (1244/1408)
[Test] Epoch= 47  BatchID= 20 Loss: 0.370 | Acc: 88.170% (2370/2688)
[Test] Epoch= 47  BatchID= 30 Loss: 0.391 | Acc: 87.338% (3428/3925)

Epoch: 48
[Train] Epoch= 48  BatchID= 0 Loss: 0.403 | Acc: 88.281% (113/128)
[Train] Epoch= 48  BatchID= 10 Loss: 0.373 | Acc: 88.707% (1249/1408)
[Train] Epoch= 48  BatchID= 20 Loss: 0.373 | Acc: 88.542% (2380/2688)
[Train] Epoch= 48  BatchID= 30 Loss: 0.369 | Acc: 88.357% (3506/3968)
[Train] Epoch= 48  BatchID= 40 Loss: 0.369 | Acc: 88.434% (4641/5248)
[Train] Epoch= 48  BatchID= 50 Loss: 0.371 | Acc: 88.143% (5754/6528)
[Train] Epoch= 48  BatchID= 60 Loss: 0.376 | Acc: 88.153% (6883/7808)
[Train] Epoch= 48  BatchID= 70 Loss: 0.375 | Acc: 88.292% (8024/9088)
[Test] Epoch= 48  BatchID= 0 Loss: 0.120 | Acc: 96.094% (123/128)
[Test] Epoch= 48  BatchID= 10 Loss: 0.272 | Acc: 92.116% (1297/1408)
[Test] Epoch= 48  BatchID= 20 Loss: 0.313 | Acc: 90.513% (2433/2688)
[Test] Epoch= 48  BatchID= 30 Loss: 0.392 | Acc: 87.541% (3436/3925)

Epoch: 49
[Train] Epoch= 49  BatchID= 0 Loss: 0.306 | Acc: 92.188% (118/128)
[Train] Epoch= 49  BatchID= 10 Loss: 0.352 | Acc: 87.926% (1238/1408)
[Train] Epoch= 49  BatchID= 20 Loss: 0.362 | Acc: 88.132% (2369/2688)
[Train] Epoch= 49  BatchID= 30 Loss: 0.363 | Acc: 88.080% (3495/3968)
[Train] Epoch= 49  BatchID= 40 Loss: 0.370 | Acc: 88.034% (4620/5248)
[Train] Epoch= 49  BatchID= 50 Loss: 0.374 | Acc: 88.021% (5746/6528)
[Train] Epoch= 49  BatchID= 60 Loss: 0.373 | Acc: 88.089% (6878/7808)
[Train] Epoch= 49  BatchID= 70 Loss: 0.380 | Acc: 87.907% (7989/9088)
[Test] Epoch= 49  BatchID= 0 Loss: 0.073 | Acc: 97.656% (125/128)
[Test] Epoch= 49  BatchID= 10 Loss: 0.325 | Acc: 89.773% (1264/1408)
[Test] Epoch= 49  BatchID= 20 Loss: 0.381 | Acc: 88.318% (2374/2688)
[Test] Epoch= 49  BatchID= 30 Loss: 0.434 | Acc: 86.420% (3392/3925)

Epoch: 50
[Train] Epoch= 50  BatchID= 0 Loss: 0.386 | Acc: 87.500% (112/128)
[Train] Epoch= 50  BatchID= 10 Loss: 0.321 | Acc: 89.844% (1265/1408)
[Train] Epoch= 50  BatchID= 20 Loss: 0.337 | Acc: 89.137% (2396/2688)
[Train] Epoch= 50  BatchID= 30 Loss: 0.344 | Acc: 89.138% (3537/3968)
[Train] Epoch= 50  BatchID= 40 Loss: 0.342 | Acc: 89.082% (4675/5248)
[Train] Epoch= 50  BatchID= 50 Loss: 0.341 | Acc: 89.108% (5817/6528)
[Train] Epoch= 50  BatchID= 60 Loss: 0.347 | Acc: 88.794% (6933/7808)
[Train] Epoch= 50  BatchID= 70 Loss: 0.353 | Acc: 88.710% (8062/9088)
[Test] Epoch= 50  BatchID= 0 Loss: 0.264 | Acc: 91.406% (117/128)
[Test] Epoch= 50  BatchID= 10 Loss: 0.363 | Acc: 88.210% (1242/1408)
[Test] Epoch= 50  BatchID= 20 Loss: 0.375 | Acc: 87.649% (2356/2688)
[Test] Epoch= 50  BatchID= 30 Loss: 0.371 | Acc: 87.618% (3439/3925)

Epoch: 51
[Train] Epoch= 51  BatchID= 0 Loss: 0.330 | Acc: 91.406% (117/128)
[Train] Epoch= 51  BatchID= 10 Loss: 0.338 | Acc: 89.631% (1262/1408)
[Train] Epoch= 51  BatchID= 20 Loss: 0.343 | Acc: 88.802% (2387/2688)
[Train] Epoch= 51  BatchID= 30 Loss: 0.357 | Acc: 88.558% (3514/3968)
[Train] Epoch= 51  BatchID= 40 Loss: 0.358 | Acc: 88.529% (4646/5248)
[Train] Epoch= 51  BatchID= 50 Loss: 0.357 | Acc: 88.343% (5767/6528)
[Train] Epoch= 51  BatchID= 60 Loss: 0.360 | Acc: 88.358% (6899/7808)
[Train] Epoch= 51  BatchID= 70 Loss: 0.360 | Acc: 88.512% (8044/9088)
[Test] Epoch= 51  BatchID= 0 Loss: 0.151 | Acc: 96.875% (124/128)
[Test] Epoch= 51  BatchID= 10 Loss: 0.325 | Acc: 89.631% (1262/1408)
[Test] Epoch= 51  BatchID= 20 Loss: 0.344 | Acc: 89.472% (2405/2688)
[Test] Epoch= 51  BatchID= 30 Loss: 0.365 | Acc: 88.764% (3484/3925)
Saving..
Best accuracy:  88.76433121019109

Epoch: 52
[Train] Epoch= 52  BatchID= 0 Loss: 0.404 | Acc: 89.844% (115/128)
[Train] Epoch= 52  BatchID= 10 Loss: 0.344 | Acc: 89.347% (1258/1408)
[Train] Epoch= 52  BatchID= 20 Loss: 0.350 | Acc: 88.951% (2391/2688)
[Train] Epoch= 52  BatchID= 30 Loss: 0.362 | Acc: 88.684% (3519/3968)
[Train] Epoch= 52  BatchID= 40 Loss: 0.361 | Acc: 88.548% (4647/5248)
[Train] Epoch= 52  BatchID= 50 Loss: 0.358 | Acc: 88.710% (5791/6528)
[Train] Epoch= 52  BatchID= 60 Loss: 0.356 | Acc: 88.819% (6935/7808)
[Train] Epoch= 52  BatchID= 70 Loss: 0.353 | Acc: 88.985% (8087/9088)
[Test] Epoch= 52  BatchID= 0 Loss: 0.181 | Acc: 96.094% (123/128)
[Test] Epoch= 52  BatchID= 10 Loss: 0.405 | Acc: 88.281% (1243/1408)
[Test] Epoch= 52  BatchID= 20 Loss: 0.353 | Acc: 89.100% (2395/2688)
[Test] Epoch= 52  BatchID= 30 Loss: 0.392 | Acc: 87.796% (3446/3925)

Epoch: 53
[Train] Epoch= 53  BatchID= 0 Loss: 0.323 | Acc: 88.281% (113/128)
[Train] Epoch= 53  BatchID= 10 Loss: 0.289 | Acc: 91.051% (1282/1408)
[Train] Epoch= 53  BatchID= 20 Loss: 0.311 | Acc: 90.216% (2425/2688)
[Train] Epoch= 53  BatchID= 30 Loss: 0.323 | Acc: 89.642% (3557/3968)
[Train] Epoch= 53  BatchID= 40 Loss: 0.328 | Acc: 89.482% (4696/5248)
[Train] Epoch= 53  BatchID= 50 Loss: 0.330 | Acc: 89.583% (5848/6528)
[Train] Epoch= 53  BatchID= 60 Loss: 0.334 | Acc: 89.255% (6969/7808)
[Train] Epoch= 53  BatchID= 70 Loss: 0.339 | Acc: 89.107% (8098/9088)
[Test] Epoch= 53  BatchID= 0 Loss: 0.137 | Acc: 96.094% (123/128)
[Test] Epoch= 53  BatchID= 10 Loss: 0.369 | Acc: 88.920% (1252/1408)
[Test] Epoch= 53  BatchID= 20 Loss: 0.375 | Acc: 88.393% (2376/2688)
[Test] Epoch= 53  BatchID= 30 Loss: 0.393 | Acc: 87.516% (3435/3925)

Epoch: 54
[Train] Epoch= 54  BatchID= 0 Loss: 0.372 | Acc: 89.062% (114/128)
[Train] Epoch= 54  BatchID= 10 Loss: 0.335 | Acc: 89.560% (1261/1408)
[Train] Epoch= 54  BatchID= 20 Loss: 0.333 | Acc: 89.546% (2407/2688)
[Train] Epoch= 54  BatchID= 30 Loss: 0.334 | Acc: 89.415% (3548/3968)
[Train] Epoch= 54  BatchID= 40 Loss: 0.337 | Acc: 89.329% (4688/5248)
[Train] Epoch= 54  BatchID= 50 Loss: 0.336 | Acc: 89.400% (5836/6528)
[Train] Epoch= 54  BatchID= 60 Loss: 0.337 | Acc: 89.280% (6971/7808)
[Train] Epoch= 54  BatchID= 70 Loss: 0.338 | Acc: 89.184% (8105/9088)
[Test] Epoch= 54  BatchID= 0 Loss: 0.128 | Acc: 95.312% (122/128)
[Test] Epoch= 54  BatchID= 10 Loss: 0.280 | Acc: 90.767% (1278/1408)
[Test] Epoch= 54  BatchID= 20 Loss: 0.289 | Acc: 90.699% (2438/2688)
[Test] Epoch= 54  BatchID= 30 Loss: 0.324 | Acc: 89.682% (3520/3925)
Saving..
Best accuracy:  89.68152866242038

Epoch: 55
[Train] Epoch= 55  BatchID= 0 Loss: 0.372 | Acc: 89.062% (114/128)
[Train] Epoch= 55  BatchID= 10 Loss: 0.310 | Acc: 90.625% (1276/1408)
[Train] Epoch= 55  BatchID= 20 Loss: 0.310 | Acc: 90.253% (2426/2688)
[Train] Epoch= 55  BatchID= 30 Loss: 0.325 | Acc: 89.743% (3561/3968)
[Train] Epoch= 55  BatchID= 40 Loss: 0.317 | Acc: 90.034% (4725/5248)
[Train] Epoch= 55  BatchID= 50 Loss: 0.326 | Acc: 89.721% (5857/6528)
[Train] Epoch= 55  BatchID= 60 Loss: 0.326 | Acc: 89.703% (7004/7808)
[Train] Epoch= 55  BatchID= 70 Loss: 0.332 | Acc: 89.448% (8129/9088)
[Test] Epoch= 55  BatchID= 0 Loss: 0.093 | Acc: 97.656% (125/128)
[Test] Epoch= 55  BatchID= 10 Loss: 0.286 | Acc: 90.341% (1272/1408)
[Test] Epoch= 55  BatchID= 20 Loss: 0.305 | Acc: 89.993% (2419/2688)
[Test] Epoch= 55  BatchID= 30 Loss: 0.336 | Acc: 88.994% (3493/3925)

Epoch: 56
[Train] Epoch= 56  BatchID= 0 Loss: 0.393 | Acc: 88.281% (113/128)
[Train] Epoch= 56  BatchID= 10 Loss: 0.357 | Acc: 88.210% (1242/1408)
[Train] Epoch= 56  BatchID= 20 Loss: 0.330 | Acc: 89.583% (2408/2688)
[Train] Epoch= 56  BatchID= 30 Loss: 0.329 | Acc: 89.718% (3560/3968)
[Train] Epoch= 56  BatchID= 40 Loss: 0.323 | Acc: 89.825% (4714/5248)
[Train] Epoch= 56  BatchID= 50 Loss: 0.324 | Acc: 89.767% (5860/6528)
[Train] Epoch= 56  BatchID= 60 Loss: 0.327 | Acc: 89.652% (7000/7808)
[Train] Epoch= 56  BatchID= 70 Loss: 0.323 | Acc: 89.756% (8157/9088)
[Test] Epoch= 56  BatchID= 0 Loss: 0.154 | Acc: 94.531% (121/128)
[Test] Epoch= 56  BatchID= 10 Loss: 0.291 | Acc: 90.554% (1275/1408)
[Test] Epoch= 56  BatchID= 20 Loss: 0.281 | Acc: 91.109% (2449/2688)
[Test] Epoch= 56  BatchID= 30 Loss: 0.339 | Acc: 89.172% (3500/3925)

Epoch: 57
[Train] Epoch= 57  BatchID= 0 Loss: 0.375 | Acc: 89.844% (115/128)
[Train] Epoch= 57  BatchID= 10 Loss: 0.274 | Acc: 91.264% (1285/1408)
[Train] Epoch= 57  BatchID= 20 Loss: 0.295 | Acc: 90.365% (2429/2688)
[Train] Epoch= 57  BatchID= 30 Loss: 0.293 | Acc: 90.499% (3591/3968)
[Train] Epoch= 57  BatchID= 40 Loss: 0.296 | Acc: 90.720% (4761/5248)
[Train] Epoch= 57  BatchID= 50 Loss: 0.302 | Acc: 90.533% (5910/6528)
[Train] Epoch= 57  BatchID= 60 Loss: 0.310 | Acc: 90.266% (7048/7808)
[Train] Epoch= 57  BatchID= 70 Loss: 0.312 | Acc: 90.207% (8198/9088)
[Test] Epoch= 57  BatchID= 0 Loss: 0.136 | Acc: 95.312% (122/128)
[Test] Epoch= 57  BatchID= 10 Loss: 0.333 | Acc: 89.631% (1262/1408)
[Test] Epoch= 57  BatchID= 20 Loss: 0.301 | Acc: 90.588% (2435/2688)
[Test] Epoch= 57  BatchID= 30 Loss: 0.359 | Acc: 88.713% (3482/3925)

Epoch: 58
[Train] Epoch= 58  BatchID= 0 Loss: 0.374 | Acc: 88.281% (113/128)
[Train] Epoch= 58  BatchID= 10 Loss: 0.328 | Acc: 89.347% (1258/1408)
[Train] Epoch= 58  BatchID= 20 Loss: 0.310 | Acc: 89.509% (2406/2688)
[Train] Epoch= 58  BatchID= 30 Loss: 0.300 | Acc: 89.894% (3567/3968)
[Train] Epoch= 58  BatchID= 40 Loss: 0.303 | Acc: 89.920% (4719/5248)
[Train] Epoch= 58  BatchID= 50 Loss: 0.301 | Acc: 90.012% (5876/6528)
[Train] Epoch= 58  BatchID= 60 Loss: 0.304 | Acc: 89.933% (7022/7808)
[Train] Epoch= 58  BatchID= 70 Loss: 0.307 | Acc: 89.921% (8172/9088)
[Test] Epoch= 58  BatchID= 0 Loss: 0.125 | Acc: 96.875% (124/128)
[Test] Epoch= 58  BatchID= 10 Loss: 0.358 | Acc: 89.205% (1256/1408)
[Test] Epoch= 58  BatchID= 20 Loss: 0.356 | Acc: 88.802% (2387/2688)
[Test] Epoch= 58  BatchID= 30 Loss: 0.359 | Acc: 88.637% (3479/3925)

Epoch: 59
[Train] Epoch= 59  BatchID= 0 Loss: 0.186 | Acc: 94.531% (121/128)
[Train] Epoch= 59  BatchID= 10 Loss: 0.288 | Acc: 91.051% (1282/1408)
[Train] Epoch= 59  BatchID= 20 Loss: 0.307 | Acc: 90.513% (2433/2688)
[Train] Epoch= 59  BatchID= 30 Loss: 0.311 | Acc: 90.398% (3587/3968)
[Train] Epoch= 59  BatchID= 40 Loss: 0.312 | Acc: 90.244% (4736/5248)
[Train] Epoch= 59  BatchID= 50 Loss: 0.311 | Acc: 90.257% (5892/6528)
[Train] Epoch= 59  BatchID= 60 Loss: 0.307 | Acc: 90.433% (7061/7808)
[Train] Epoch= 59  BatchID= 70 Loss: 0.310 | Acc: 90.284% (8205/9088)
[Test] Epoch= 59  BatchID= 0 Loss: 0.159 | Acc: 94.531% (121/128)
[Test] Epoch= 59  BatchID= 10 Loss: 0.294 | Acc: 89.986% (1267/1408)
[Test] Epoch= 59  BatchID= 20 Loss: 0.300 | Acc: 90.179% (2424/2688)
[Test] Epoch= 59  BatchID= 30 Loss: 0.358 | Acc: 88.331% (3467/3925)

Epoch: 60
[Train] Epoch= 60  BatchID= 0 Loss: 0.209 | Acc: 93.750% (120/128)
[Train] Epoch= 60  BatchID= 10 Loss: 0.294 | Acc: 91.264% (1285/1408)
[Train] Epoch= 60  BatchID= 20 Loss: 0.283 | Acc: 91.295% (2454/2688)
[Train] Epoch= 60  BatchID= 30 Loss: 0.291 | Acc: 90.726% (3600/3968)
[Train] Epoch= 60  BatchID= 40 Loss: 0.293 | Acc: 90.530% (4751/5248)
[Train] Epoch= 60  BatchID= 50 Loss: 0.290 | Acc: 90.548% (5911/6528)
[Train] Epoch= 60  BatchID= 60 Loss: 0.288 | Acc: 90.817% (7091/7808)
[Train] Epoch= 60  BatchID= 70 Loss: 0.294 | Acc: 90.691% (8242/9088)
[Test] Epoch= 60  BatchID= 0 Loss: 0.134 | Acc: 96.094% (123/128)
[Test] Epoch= 60  BatchID= 10 Loss: 0.324 | Acc: 90.128% (1269/1408)
[Test] Epoch= 60  BatchID= 20 Loss: 0.312 | Acc: 90.104% (2422/2688)
[Test] Epoch= 60  BatchID= 30 Loss: 0.320 | Acc: 89.809% (3525/3925)
Saving..
Best accuracy:  89.80891719745223

Epoch: 61
[Train] Epoch= 61  BatchID= 0 Loss: 0.199 | Acc: 92.969% (119/128)
[Train] Epoch= 61  BatchID= 10 Loss: 0.288 | Acc: 90.696% (1277/1408)
[Train] Epoch= 61  BatchID= 20 Loss: 0.295 | Acc: 90.588% (2435/2688)
[Train] Epoch= 61  BatchID= 30 Loss: 0.293 | Acc: 90.600% (3595/3968)
[Train] Epoch= 61  BatchID= 40 Loss: 0.293 | Acc: 90.549% (4752/5248)
[Train] Epoch= 61  BatchID= 50 Loss: 0.292 | Acc: 90.748% (5924/6528)
[Train] Epoch= 61  BatchID= 60 Loss: 0.294 | Acc: 90.740% (7085/7808)
[Train] Epoch= 61  BatchID= 70 Loss: 0.288 | Acc: 90.966% (8267/9088)
[Test] Epoch= 61  BatchID= 0 Loss: 0.242 | Acc: 94.531% (121/128)
[Test] Epoch= 61  BatchID= 10 Loss: 0.354 | Acc: 89.276% (1257/1408)
[Test] Epoch= 61  BatchID= 20 Loss: 0.355 | Acc: 89.137% (2396/2688)
[Test] Epoch= 61  BatchID= 30 Loss: 0.369 | Acc: 88.611% (3478/3925)

Epoch: 62
[Train] Epoch= 62  BatchID= 0 Loss: 0.228 | Acc: 90.625% (116/128)
[Train] Epoch= 62  BatchID= 10 Loss: 0.270 | Acc: 91.477% (1288/1408)
[Train] Epoch= 62  BatchID= 20 Loss: 0.278 | Acc: 91.034% (2447/2688)
[Train] Epoch= 62  BatchID= 30 Loss: 0.277 | Acc: 91.280% (3622/3968)
[Train] Epoch= 62  BatchID= 40 Loss: 0.280 | Acc: 91.082% (4780/5248)
[Train] Epoch= 62  BatchID= 50 Loss: 0.279 | Acc: 91.085% (5946/6528)
[Train] Epoch= 62  BatchID= 60 Loss: 0.279 | Acc: 91.150% (7117/7808)
[Train] Epoch= 62  BatchID= 70 Loss: 0.279 | Acc: 91.186% (8287/9088)
[Test] Epoch= 62  BatchID= 0 Loss: 0.159 | Acc: 95.312% (122/128)
[Test] Epoch= 62  BatchID= 10 Loss: 0.308 | Acc: 90.341% (1272/1408)
[Test] Epoch= 62  BatchID= 20 Loss: 0.269 | Acc: 91.667% (2464/2688)
[Test] Epoch= 62  BatchID= 30 Loss: 0.318 | Acc: 90.038% (3534/3925)
Saving..
Best accuracy:  90.03821656050955

Epoch: 63
[Train] Epoch= 63  BatchID= 0 Loss: 0.252 | Acc: 94.531% (121/128)
[Train] Epoch= 63  BatchID= 10 Loss: 0.261 | Acc: 91.903% (1294/1408)
[Train] Epoch= 63  BatchID= 20 Loss: 0.263 | Acc: 91.629% (2463/2688)
[Train] Epoch= 63  BatchID= 30 Loss: 0.260 | Acc: 91.759% (3641/3968)
[Train] Epoch= 63  BatchID= 40 Loss: 0.261 | Acc: 91.825% (4819/5248)
[Train] Epoch= 63  BatchID= 50 Loss: 0.264 | Acc: 91.789% (5992/6528)
[Train] Epoch= 63  BatchID= 60 Loss: 0.266 | Acc: 91.752% (7164/7808)
[Train] Epoch= 63  BatchID= 70 Loss: 0.265 | Acc: 91.714% (8335/9088)
[Test] Epoch= 63  BatchID= 0 Loss: 0.119 | Acc: 96.875% (124/128)
[Test] Epoch= 63  BatchID= 10 Loss: 0.312 | Acc: 90.412% (1273/1408)
[Test] Epoch= 63  BatchID= 20 Loss: 0.289 | Acc: 90.551% (2434/2688)
[Test] Epoch= 63  BatchID= 30 Loss: 0.299 | Acc: 90.446% (3550/3925)
Saving..
Best accuracy:  90.44585987261146

Epoch: 64
[Train] Epoch= 64  BatchID= 0 Loss: 0.316 | Acc: 89.844% (115/128)
[Train] Epoch= 64  BatchID= 10 Loss: 0.282 | Acc: 90.838% (1279/1408)
[Train] Epoch= 64  BatchID= 20 Loss: 0.277 | Acc: 91.220% (2452/2688)
[Train] Epoch= 64  BatchID= 30 Loss: 0.269 | Acc: 91.331% (3624/3968)
[Train] Epoch= 64  BatchID= 40 Loss: 0.267 | Acc: 91.425% (4798/5248)
[Train] Epoch= 64  BatchID= 50 Loss: 0.267 | Acc: 91.376% (5965/6528)
[Train] Epoch= 64  BatchID= 60 Loss: 0.266 | Acc: 91.432% (7139/7808)
[Train] Epoch= 64  BatchID= 70 Loss: 0.268 | Acc: 91.395% (8306/9088)
[Test] Epoch= 64  BatchID= 0 Loss: 0.156 | Acc: 95.312% (122/128)
[Test] Epoch= 64  BatchID= 10 Loss: 0.291 | Acc: 91.548% (1289/1408)
[Test] Epoch= 64  BatchID= 20 Loss: 0.312 | Acc: 90.811% (2441/2688)
[Test] Epoch= 64  BatchID= 30 Loss: 0.325 | Acc: 89.936% (3530/3925)

Epoch: 65
[Train] Epoch= 65  BatchID= 0 Loss: 0.204 | Acc: 94.531% (121/128)
[Train] Epoch= 65  BatchID= 10 Loss: 0.258 | Acc: 91.832% (1293/1408)
[Train] Epoch= 65  BatchID= 20 Loss: 0.278 | Acc: 91.146% (2450/2688)
[Train] Epoch= 65  BatchID= 30 Loss: 0.267 | Acc: 91.507% (3631/3968)
[Train] Epoch= 65  BatchID= 40 Loss: 0.267 | Acc: 91.673% (4811/5248)
[Train] Epoch= 65  BatchID= 50 Loss: 0.267 | Acc: 91.651% (5983/6528)
[Train] Epoch= 65  BatchID= 60 Loss: 0.263 | Acc: 91.739% (7163/7808)
[Train] Epoch= 65  BatchID= 70 Loss: 0.261 | Acc: 91.736% (8337/9088)
[Test] Epoch= 65  BatchID= 0 Loss: 0.105 | Acc: 96.094% (123/128)
[Test] Epoch= 65  BatchID= 10 Loss: 0.265 | Acc: 91.690% (1291/1408)
[Test] Epoch= 65  BatchID= 20 Loss: 0.263 | Acc: 91.815% (2468/2688)
[Test] Epoch= 65  BatchID= 30 Loss: 0.298 | Acc: 90.573% (3555/3925)
Saving..
Best accuracy:  90.5732484076433

Epoch: 66
[Train] Epoch= 66  BatchID= 0 Loss: 0.187 | Acc: 92.969% (119/128)
[Train] Epoch= 66  BatchID= 10 Loss: 0.218 | Acc: 93.324% (1314/1408)
[Train] Epoch= 66  BatchID= 20 Loss: 0.241 | Acc: 92.448% (2485/2688)
[Train] Epoch= 66  BatchID= 30 Loss: 0.253 | Acc: 92.137% (3656/3968)
[Train] Epoch= 66  BatchID= 40 Loss: 0.257 | Acc: 91.921% (4824/5248)
[Train] Epoch= 66  BatchID= 50 Loss: 0.256 | Acc: 92.096% (6012/6528)
[Train] Epoch= 66  BatchID= 60 Loss: 0.261 | Acc: 91.880% (7174/7808)
[Train] Epoch= 66  BatchID= 70 Loss: 0.259 | Acc: 91.912% (8353/9088)
[Test] Epoch= 66  BatchID= 0 Loss: 0.058 | Acc: 97.656% (125/128)
[Test] Epoch= 66  BatchID= 10 Loss: 0.278 | Acc: 91.548% (1289/1408)
[Test] Epoch= 66  BatchID= 20 Loss: 0.274 | Acc: 91.667% (2464/2688)
[Test] Epoch= 66  BatchID= 30 Loss: 0.319 | Acc: 90.369% (3547/3925)

Epoch: 67
[Train] Epoch= 67  BatchID= 0 Loss: 0.323 | Acc: 87.500% (112/128)
[Train] Epoch= 67  BatchID= 10 Loss: 0.224 | Acc: 92.827% (1307/1408)
[Train] Epoch= 67  BatchID= 20 Loss: 0.233 | Acc: 92.708% (2492/2688)
[Train] Epoch= 67  BatchID= 30 Loss: 0.234 | Acc: 92.666% (3677/3968)
[Train] Epoch= 67  BatchID= 40 Loss: 0.234 | Acc: 92.759% (4868/5248)
[Train] Epoch= 67  BatchID= 50 Loss: 0.234 | Acc: 92.816% (6059/6528)
[Train] Epoch= 67  BatchID= 60 Loss: 0.239 | Acc: 92.674% (7236/7808)
[Train] Epoch= 67  BatchID= 70 Loss: 0.242 | Acc: 92.430% (8400/9088)
[Test] Epoch= 67  BatchID= 0 Loss: 0.107 | Acc: 96.875% (124/128)
[Test] Epoch= 67  BatchID= 10 Loss: 0.281 | Acc: 90.980% (1281/1408)
[Test] Epoch= 67  BatchID= 20 Loss: 0.282 | Acc: 90.960% (2445/2688)
[Test] Epoch= 67  BatchID= 30 Loss: 0.296 | Acc: 90.573% (3555/3925)

Epoch: 68
[Train] Epoch= 68  BatchID= 0 Loss: 0.247 | Acc: 93.750% (120/128)
[Train] Epoch= 68  BatchID= 10 Loss: 0.224 | Acc: 93.324% (1314/1408)
[Train] Epoch= 68  BatchID= 20 Loss: 0.236 | Acc: 92.746% (2493/2688)
[Train] Epoch= 68  BatchID= 30 Loss: 0.228 | Acc: 93.070% (3693/3968)
[Train] Epoch= 68  BatchID= 40 Loss: 0.230 | Acc: 93.007% (4881/5248)
[Train] Epoch= 68  BatchID= 50 Loss: 0.229 | Acc: 93.061% (6075/6528)
[Train] Epoch= 68  BatchID= 60 Loss: 0.226 | Acc: 93.020% (7263/7808)
[Train] Epoch= 68  BatchID= 70 Loss: 0.229 | Acc: 92.793% (8433/9088)
[Test] Epoch= 68  BatchID= 0 Loss: 0.145 | Acc: 96.094% (123/128)
[Test] Epoch= 68  BatchID= 10 Loss: 0.270 | Acc: 91.477% (1288/1408)
[Test] Epoch= 68  BatchID= 20 Loss: 0.261 | Acc: 91.815% (2468/2688)
[Test] Epoch= 68  BatchID= 30 Loss: 0.303 | Acc: 90.675% (3559/3925)
Saving..
Best accuracy:  90.67515923566879

Epoch: 69
[Train] Epoch= 69  BatchID= 0 Loss: 0.233 | Acc: 94.531% (121/128)
[Train] Epoch= 69  BatchID= 10 Loss: 0.235 | Acc: 93.324% (1314/1408)
[Train] Epoch= 69  BatchID= 20 Loss: 0.250 | Acc: 92.448% (2485/2688)
[Train] Epoch= 69  BatchID= 30 Loss: 0.241 | Acc: 92.591% (3674/3968)
[Train] Epoch= 69  BatchID= 40 Loss: 0.234 | Acc: 92.816% (4871/5248)
[Train] Epoch= 69  BatchID= 50 Loss: 0.230 | Acc: 92.892% (6064/6528)
[Train] Epoch= 69  BatchID= 60 Loss: 0.234 | Acc: 92.789% (7245/7808)
[Train] Epoch= 69  BatchID= 70 Loss: 0.236 | Acc: 92.782% (8432/9088)
[Test] Epoch= 69  BatchID= 0 Loss: 0.113 | Acc: 96.094% (123/128)
[Test] Epoch= 69  BatchID= 10 Loss: 0.264 | Acc: 91.832% (1293/1408)
[Test] Epoch= 69  BatchID= 20 Loss: 0.260 | Acc: 91.853% (2469/2688)
[Test] Epoch= 69  BatchID= 30 Loss: 0.306 | Acc: 90.497% (3552/3925)

Epoch: 70
[Train] Epoch= 70  BatchID= 0 Loss: 0.169 | Acc: 94.531% (121/128)
[Train] Epoch= 70  BatchID= 10 Loss: 0.217 | Acc: 93.253% (1313/1408)
[Train] Epoch= 70  BatchID= 20 Loss: 0.226 | Acc: 92.932% (2498/2688)
[Train] Epoch= 70  BatchID= 30 Loss: 0.223 | Acc: 92.792% (3682/3968)
[Train] Epoch= 70  BatchID= 40 Loss: 0.222 | Acc: 92.835% (4872/5248)
[Train] Epoch= 70  BatchID= 50 Loss: 0.222 | Acc: 92.877% (6063/6528)
[Train] Epoch= 70  BatchID= 60 Loss: 0.224 | Acc: 92.853% (7250/7808)
[Train] Epoch= 70  BatchID= 70 Loss: 0.225 | Acc: 92.870% (8440/9088)
[Test] Epoch= 70  BatchID= 0 Loss: 0.132 | Acc: 96.094% (123/128)
[Test] Epoch= 70  BatchID= 10 Loss: 0.269 | Acc: 91.335% (1286/1408)
[Test] Epoch= 70  BatchID= 20 Loss: 0.265 | Acc: 91.741% (2466/2688)
[Test] Epoch= 70  BatchID= 30 Loss: 0.293 | Acc: 90.955% (3570/3925)
Saving..
Best accuracy:  90.95541401273886

Epoch: 71
[Train] Epoch= 71  BatchID= 0 Loss: 0.168 | Acc: 92.188% (118/128)
[Train] Epoch= 71  BatchID= 10 Loss: 0.231 | Acc: 92.827% (1307/1408)
[Train] Epoch= 71  BatchID= 20 Loss: 0.213 | Acc: 93.527% (2514/2688)
[Train] Epoch= 71  BatchID= 30 Loss: 0.220 | Acc: 93.019% (3691/3968)
[Train] Epoch= 71  BatchID= 40 Loss: 0.220 | Acc: 92.873% (4874/5248)
[Train] Epoch= 71  BatchID= 50 Loss: 0.219 | Acc: 92.862% (6062/6528)
[Train] Epoch= 71  BatchID= 60 Loss: 0.218 | Acc: 92.956% (7258/7808)
[Train] Epoch= 71  BatchID= 70 Loss: 0.219 | Acc: 92.969% (8449/9088)
[Test] Epoch= 71  BatchID= 0 Loss: 0.121 | Acc: 96.094% (123/128)
[Test] Epoch= 71  BatchID= 10 Loss: 0.264 | Acc: 91.051% (1282/1408)
[Test] Epoch= 71  BatchID= 20 Loss: 0.280 | Acc: 90.997% (2446/2688)
[Test] Epoch= 71  BatchID= 30 Loss: 0.318 | Acc: 90.166% (3539/3925)

Epoch: 72
[Train] Epoch= 72  BatchID= 0 Loss: 0.234 | Acc: 92.188% (118/128)
[Train] Epoch= 72  BatchID= 10 Loss: 0.227 | Acc: 92.969% (1309/1408)
[Train] Epoch= 72  BatchID= 20 Loss: 0.215 | Acc: 93.043% (2501/2688)
[Train] Epoch= 72  BatchID= 30 Loss: 0.214 | Acc: 93.170% (3697/3968)
[Train] Epoch= 72  BatchID= 40 Loss: 0.225 | Acc: 92.664% (4863/5248)
[Train] Epoch= 72  BatchID= 50 Loss: 0.227 | Acc: 92.754% (6055/6528)
[Train] Epoch= 72  BatchID= 60 Loss: 0.230 | Acc: 92.597% (7230/7808)
[Train] Epoch= 72  BatchID= 70 Loss: 0.229 | Acc: 92.683% (8423/9088)
[Test] Epoch= 72  BatchID= 0 Loss: 0.099 | Acc: 96.875% (124/128)
[Test] Epoch= 72  BatchID= 10 Loss: 0.240 | Acc: 91.619% (1290/1408)
[Test] Epoch= 72  BatchID= 20 Loss: 0.244 | Acc: 91.778% (2467/2688)
[Test] Epoch= 72  BatchID= 30 Loss: 0.276 | Acc: 91.057% (3574/3925)
Saving..
Best accuracy:  91.05732484076434

Epoch: 73
[Train] Epoch= 73  BatchID= 0 Loss: 0.131 | Acc: 94.531% (121/128)
[Train] Epoch= 73  BatchID= 10 Loss: 0.207 | Acc: 92.543% (1303/1408)
[Train] Epoch= 73  BatchID= 20 Loss: 0.190 | Acc: 93.266% (2507/2688)
[Train] Epoch= 73  BatchID= 30 Loss: 0.198 | Acc: 93.120% (3695/3968)
[Train] Epoch= 73  BatchID= 40 Loss: 0.210 | Acc: 93.026% (4882/5248)
[Train] Epoch= 73  BatchID= 50 Loss: 0.213 | Acc: 92.938% (6067/6528)
[Train] Epoch= 73  BatchID= 60 Loss: 0.213 | Acc: 93.007% (7262/7808)
[Train] Epoch= 73  BatchID= 70 Loss: 0.210 | Acc: 93.167% (8467/9088)
[Test] Epoch= 73  BatchID= 0 Loss: 0.111 | Acc: 96.094% (123/128)
[Test] Epoch= 73  BatchID= 10 Loss: 0.254 | Acc: 92.116% (1297/1408)
[Test] Epoch= 73  BatchID= 20 Loss: 0.268 | Acc: 91.741% (2466/2688)
[Test] Epoch= 73  BatchID= 30 Loss: 0.277 | Acc: 91.389% (3587/3925)
Saving..
Best accuracy:  91.38853503184713

Epoch: 74
[Train] Epoch= 74  BatchID= 0 Loss: 0.142 | Acc: 96.875% (124/128)
[Train] Epoch= 74  BatchID= 10 Loss: 0.234 | Acc: 92.685% (1305/1408)
[Train] Epoch= 74  BatchID= 20 Loss: 0.224 | Acc: 93.266% (2507/2688)
[Train] Epoch= 74  BatchID= 30 Loss: 0.219 | Acc: 93.397% (3706/3968)
[Train] Epoch= 74  BatchID= 40 Loss: 0.220 | Acc: 93.312% (4897/5248)
[Train] Epoch= 74  BatchID= 50 Loss: 0.212 | Acc: 93.505% (6104/6528)
[Train] Epoch= 74  BatchID= 60 Loss: 0.215 | Acc: 93.494% (7300/7808)
[Train] Epoch= 74  BatchID= 70 Loss: 0.213 | Acc: 93.563% (8503/9088)
[Test] Epoch= 74  BatchID= 0 Loss: 0.057 | Acc: 96.875% (124/128)
[Test] Epoch= 74  BatchID= 10 Loss: 0.230 | Acc: 91.832% (1293/1408)
[Test] Epoch= 74  BatchID= 20 Loss: 0.252 | Acc: 91.555% (2461/2688)
[Test] Epoch= 74  BatchID= 30 Loss: 0.284 | Acc: 90.854% (3566/3925)

Epoch: 75
[Train] Epoch= 75  BatchID= 0 Loss: 0.234 | Acc: 91.406% (117/128)
[Train] Epoch= 75  BatchID= 10 Loss: 0.214 | Acc: 92.969% (1309/1408)
[Train] Epoch= 75  BatchID= 20 Loss: 0.212 | Acc: 93.341% (2509/2688)
[Train] Epoch= 75  BatchID= 30 Loss: 0.210 | Acc: 93.523% (3711/3968)
[Train] Epoch= 75  BatchID= 40 Loss: 0.211 | Acc: 93.540% (4909/5248)
[Train] Epoch= 75  BatchID= 50 Loss: 0.209 | Acc: 93.581% (6109/6528)
[Train] Epoch= 75  BatchID= 60 Loss: 0.204 | Acc: 93.686% (7315/7808)
[Train] Epoch= 75  BatchID= 70 Loss: 0.202 | Acc: 93.761% (8521/9088)
[Test] Epoch= 75  BatchID= 0 Loss: 0.077 | Acc: 96.875% (124/128)
[Test] Epoch= 75  BatchID= 10 Loss: 0.248 | Acc: 92.045% (1296/1408)
[Test] Epoch= 75  BatchID= 20 Loss: 0.241 | Acc: 92.001% (2473/2688)
[Test] Epoch= 75  BatchID= 30 Loss: 0.283 | Acc: 90.701% (3560/3925)

Epoch: 76
[Train] Epoch= 76  BatchID= 0 Loss: 0.157 | Acc: 96.094% (123/128)
[Train] Epoch= 76  BatchID= 10 Loss: 0.180 | Acc: 94.176% (1326/1408)
[Train] Epoch= 76  BatchID= 20 Loss: 0.187 | Acc: 94.196% (2532/2688)
[Train] Epoch= 76  BatchID= 30 Loss: 0.189 | Acc: 94.178% (3737/3968)
[Train] Epoch= 76  BatchID= 40 Loss: 0.194 | Acc: 94.036% (4935/5248)
[Train] Epoch= 76  BatchID= 50 Loss: 0.190 | Acc: 94.102% (6143/6528)
[Train] Epoch= 76  BatchID= 60 Loss: 0.190 | Acc: 94.109% (7348/7808)
[Train] Epoch= 76  BatchID= 70 Loss: 0.195 | Acc: 93.948% (8538/9088)
[Test] Epoch= 76  BatchID= 0 Loss: 0.086 | Acc: 96.875% (124/128)
[Test] Epoch= 76  BatchID= 10 Loss: 0.255 | Acc: 91.406% (1287/1408)
[Test] Epoch= 76  BatchID= 20 Loss: 0.255 | Acc: 91.964% (2472/2688)
[Test] Epoch= 76  BatchID= 30 Loss: 0.283 | Acc: 91.185% (3579/3925)

Epoch: 77
[Train] Epoch= 77  BatchID= 0 Loss: 0.251 | Acc: 92.969% (119/128)
[Train] Epoch= 77  BatchID= 10 Loss: 0.178 | Acc: 94.531% (1331/1408)
[Train] Epoch= 77  BatchID= 20 Loss: 0.187 | Acc: 94.271% (2534/2688)
[Train] Epoch= 77  BatchID= 30 Loss: 0.187 | Acc: 94.229% (3739/3968)
[Train] Epoch= 77  BatchID= 40 Loss: 0.189 | Acc: 94.188% (4943/5248)
[Train] Epoch= 77  BatchID= 50 Loss: 0.187 | Acc: 94.164% (6147/6528)
[Train] Epoch= 77  BatchID= 60 Loss: 0.189 | Acc: 94.083% (7346/7808)
[Train] Epoch= 77  BatchID= 70 Loss: 0.193 | Acc: 93.992% (8542/9088)
[Test] Epoch= 77  BatchID= 0 Loss: 0.111 | Acc: 96.875% (124/128)
[Test] Epoch= 77  BatchID= 10 Loss: 0.244 | Acc: 91.548% (1289/1408)
[Test] Epoch= 77  BatchID= 20 Loss: 0.251 | Acc: 91.443% (2458/2688)
[Test] Epoch= 77  BatchID= 30 Loss: 0.276 | Acc: 90.930% (3569/3925)

Epoch: 78
[Train] Epoch= 78  BatchID= 0 Loss: 0.169 | Acc: 92.969% (119/128)
[Train] Epoch= 78  BatchID= 10 Loss: 0.178 | Acc: 94.176% (1326/1408)
[Train] Epoch= 78  BatchID= 20 Loss: 0.177 | Acc: 94.234% (2533/2688)
[Train] Epoch= 78  BatchID= 30 Loss: 0.191 | Acc: 94.153% (3736/3968)
[Train] Epoch= 78  BatchID= 40 Loss: 0.186 | Acc: 94.226% (4945/5248)
[Train] Epoch= 78  BatchID= 50 Loss: 0.189 | Acc: 94.087% (6142/6528)
[Train] Epoch= 78  BatchID= 60 Loss: 0.194 | Acc: 94.006% (7340/7808)
[Train] Epoch= 78  BatchID= 70 Loss: 0.194 | Acc: 93.948% (8538/9088)
[Test] Epoch= 78  BatchID= 0 Loss: 0.083 | Acc: 96.875% (124/128)
[Test] Epoch= 78  BatchID= 10 Loss: 0.245 | Acc: 92.188% (1298/1408)
[Test] Epoch= 78  BatchID= 20 Loss: 0.258 | Acc: 91.741% (2466/2688)
[Test] Epoch= 78  BatchID= 30 Loss: 0.278 | Acc: 91.236% (3581/3925)

Epoch: 79
[Train] Epoch= 79  BatchID= 0 Loss: 0.221 | Acc: 93.750% (120/128)
[Train] Epoch= 79  BatchID= 10 Loss: 0.205 | Acc: 93.750% (1320/1408)
[Train] Epoch= 79  BatchID= 20 Loss: 0.199 | Acc: 93.787% (2521/2688)
[Train] Epoch= 79  BatchID= 30 Loss: 0.197 | Acc: 94.027% (3731/3968)
[Train] Epoch= 79  BatchID= 40 Loss: 0.198 | Acc: 93.941% (4930/5248)
[Train] Epoch= 79  BatchID= 50 Loss: 0.194 | Acc: 93.888% (6129/6528)
[Train] Epoch= 79  BatchID= 60 Loss: 0.194 | Acc: 93.865% (7329/7808)
[Train] Epoch= 79  BatchID= 70 Loss: 0.196 | Acc: 93.838% (8528/9088)
[Test] Epoch= 79  BatchID= 0 Loss: 0.087 | Acc: 96.875% (124/128)
[Test] Epoch= 79  BatchID= 10 Loss: 0.242 | Acc: 91.619% (1290/1408)
[Test] Epoch= 79  BatchID= 20 Loss: 0.250 | Acc: 91.853% (2469/2688)
[Test] Epoch= 79  BatchID= 30 Loss: 0.283 | Acc: 90.955% (3570/3925)

Epoch: 80
[Train] Epoch= 80  BatchID= 0 Loss: 0.120 | Acc: 97.656% (125/128)
[Train] Epoch= 80  BatchID= 10 Loss: 0.182 | Acc: 95.241% (1341/1408)
[Train] Epoch= 80  BatchID= 20 Loss: 0.184 | Acc: 94.903% (2551/2688)
[Train] Epoch= 80  BatchID= 30 Loss: 0.194 | Acc: 94.355% (3744/3968)
[Train] Epoch= 80  BatchID= 40 Loss: 0.193 | Acc: 94.360% (4952/5248)
[Train] Epoch= 80  BatchID= 50 Loss: 0.193 | Acc: 94.347% (6159/6528)
[Train] Epoch= 80  BatchID= 60 Loss: 0.193 | Acc: 94.378% (7369/7808)
[Train] Epoch= 80  BatchID= 70 Loss: 0.191 | Acc: 94.322% (8572/9088)
[Test] Epoch= 80  BatchID= 0 Loss: 0.085 | Acc: 96.875% (124/128)
[Test] Epoch= 80  BatchID= 10 Loss: 0.262 | Acc: 91.477% (1288/1408)
[Test] Epoch= 80  BatchID= 20 Loss: 0.250 | Acc: 91.927% (2471/2688)
[Test] Epoch= 80  BatchID= 30 Loss: 0.286 | Acc: 90.981% (3571/3925)

Epoch: 81
[Train] Epoch= 81  BatchID= 0 Loss: 0.148 | Acc: 94.531% (121/128)
[Train] Epoch= 81  BatchID= 10 Loss: 0.184 | Acc: 93.821% (1321/1408)
[Train] Epoch= 81  BatchID= 20 Loss: 0.183 | Acc: 94.345% (2536/2688)
[Train] Epoch= 81  BatchID= 30 Loss: 0.190 | Acc: 94.128% (3735/3968)
[Train] Epoch= 81  BatchID= 40 Loss: 0.187 | Acc: 94.150% (4941/5248)
[Train] Epoch= 81  BatchID= 50 Loss: 0.186 | Acc: 94.301% (6156/6528)
[Train] Epoch= 81  BatchID= 60 Loss: 0.184 | Acc: 94.288% (7362/7808)
[Train] Epoch= 81  BatchID= 70 Loss: 0.190 | Acc: 94.069% (8549/9088)
[Test] Epoch= 81  BatchID= 0 Loss: 0.078 | Acc: 96.875% (124/128)
[Test] Epoch= 81  BatchID= 10 Loss: 0.246 | Acc: 91.974% (1295/1408)
[Test] Epoch= 81  BatchID= 20 Loss: 0.244 | Acc: 92.299% (2481/2688)
[Test] Epoch= 81  BatchID= 30 Loss: 0.276 | Acc: 91.236% (3581/3925)

Epoch: 82
[Train] Epoch= 82  BatchID= 0 Loss: 0.137 | Acc: 96.094% (123/128)
[Train] Epoch= 82  BatchID= 10 Loss: 0.168 | Acc: 95.241% (1341/1408)
[Train] Epoch= 82  BatchID= 20 Loss: 0.186 | Acc: 94.420% (2538/2688)
[Train] Epoch= 82  BatchID= 30 Loss: 0.191 | Acc: 94.330% (3743/3968)
[Train] Epoch= 82  BatchID= 40 Loss: 0.192 | Acc: 94.245% (4946/5248)
[Train] Epoch= 82  BatchID= 50 Loss: 0.193 | Acc: 94.240% (6152/6528)
[Train] Epoch= 82  BatchID= 60 Loss: 0.193 | Acc: 94.147% (7351/7808)
[Train] Epoch= 82  BatchID= 70 Loss: 0.193 | Acc: 94.102% (8552/9088)
[Test] Epoch= 82  BatchID= 0 Loss: 0.082 | Acc: 96.875% (124/128)
[Test] Epoch= 82  BatchID= 10 Loss: 0.249 | Acc: 91.903% (1294/1408)
[Test] Epoch= 82  BatchID= 20 Loss: 0.249 | Acc: 92.113% (2476/2688)
[Test] Epoch= 82  BatchID= 30 Loss: 0.269 | Acc: 91.465% (3590/3925)
Saving..
Best accuracy:  91.46496815286625

Epoch: 83
[Train] Epoch= 83  BatchID= 0 Loss: 0.236 | Acc: 92.969% (119/128)
[Train] Epoch= 83  BatchID= 10 Loss: 0.183 | Acc: 93.963% (1323/1408)
[Train] Epoch= 83  BatchID= 20 Loss: 0.190 | Acc: 93.936% (2525/2688)
[Train] Epoch= 83  BatchID= 30 Loss: 0.181 | Acc: 94.456% (3748/3968)
[Train] Epoch= 83  BatchID= 40 Loss: 0.174 | Acc: 94.512% (4960/5248)
[Train] Epoch= 83  BatchID= 50 Loss: 0.180 | Acc: 94.256% (6153/6528)
[Train] Epoch= 83  BatchID= 60 Loss: 0.182 | Acc: 94.211% (7356/7808)
[Train] Epoch= 83  BatchID= 70 Loss: 0.178 | Acc: 94.366% (8576/9088)
[Test] Epoch= 83  BatchID= 0 Loss: 0.082 | Acc: 96.875% (124/128)
[Test] Epoch= 83  BatchID= 10 Loss: 0.252 | Acc: 91.619% (1290/1408)
[Test] Epoch= 83  BatchID= 20 Loss: 0.248 | Acc: 91.964% (2472/2688)
[Test] Epoch= 83  BatchID= 30 Loss: 0.273 | Acc: 91.159% (3578/3925)

Epoch: 84
[Train] Epoch= 84  BatchID= 0 Loss: 0.233 | Acc: 94.531% (121/128)
[Train] Epoch= 84  BatchID= 10 Loss: 0.172 | Acc: 94.957% (1337/1408)
[Train] Epoch= 84  BatchID= 20 Loss: 0.159 | Acc: 95.350% (2563/2688)
[Train] Epoch= 84  BatchID= 30 Loss: 0.165 | Acc: 95.035% (3771/3968)
[Train] Epoch= 84  BatchID= 40 Loss: 0.167 | Acc: 94.855% (4978/5248)
[Train] Epoch= 84  BatchID= 50 Loss: 0.167 | Acc: 94.945% (6198/6528)
[Train] Epoch= 84  BatchID= 60 Loss: 0.171 | Acc: 94.800% (7402/7808)
[Train] Epoch= 84  BatchID= 70 Loss: 0.174 | Acc: 94.729% (8609/9088)
[Test] Epoch= 84  BatchID= 0 Loss: 0.079 | Acc: 96.875% (124/128)
[Test] Epoch= 84  BatchID= 10 Loss: 0.245 | Acc: 91.974% (1295/1408)
[Test] Epoch= 84  BatchID= 20 Loss: 0.242 | Acc: 91.964% (2472/2688)
[Test] Epoch= 84  BatchID= 30 Loss: 0.271 | Acc: 91.185% (3579/3925)

Epoch: 85
[Train] Epoch= 85  BatchID= 0 Loss: 0.206 | Acc: 92.188% (118/128)
[Train] Epoch= 85  BatchID= 10 Loss: 0.175 | Acc: 94.460% (1330/1408)
[Train] Epoch= 85  BatchID= 20 Loss: 0.188 | Acc: 94.271% (2534/2688)
[Train] Epoch= 85  BatchID= 30 Loss: 0.185 | Acc: 94.380% (3745/3968)
[Train] Epoch= 85  BatchID= 40 Loss: 0.180 | Acc: 94.379% (4953/5248)
[Train] Epoch= 85  BatchID= 50 Loss: 0.179 | Acc: 94.577% (6174/6528)
[Train] Epoch= 85  BatchID= 60 Loss: 0.183 | Acc: 94.506% (7379/7808)
[Train] Epoch= 85  BatchID= 70 Loss: 0.183 | Acc: 94.443% (8583/9088)
[Test] Epoch= 85  BatchID= 0 Loss: 0.076 | Acc: 96.875% (124/128)
[Test] Epoch= 85  BatchID= 10 Loss: 0.248 | Acc: 91.619% (1290/1408)
[Test] Epoch= 85  BatchID= 20 Loss: 0.237 | Acc: 92.188% (2478/2688)
[Test] Epoch= 85  BatchID= 30 Loss: 0.275 | Acc: 91.134% (3577/3925)

Epoch: 86
[Train] Epoch= 86  BatchID= 0 Loss: 0.214 | Acc: 92.969% (119/128)
[Train] Epoch= 86  BatchID= 10 Loss: 0.168 | Acc: 94.744% (1334/1408)
[Train] Epoch= 86  BatchID= 20 Loss: 0.174 | Acc: 94.643% (2544/2688)
[Train] Epoch= 86  BatchID= 30 Loss: 0.180 | Acc: 94.229% (3739/3968)
[Train] Epoch= 86  BatchID= 40 Loss: 0.177 | Acc: 94.398% (4954/5248)
[Train] Epoch= 86  BatchID= 50 Loss: 0.179 | Acc: 94.271% (6154/6528)
[Train] Epoch= 86  BatchID= 60 Loss: 0.180 | Acc: 94.224% (7357/7808)
[Train] Epoch= 86  BatchID= 70 Loss: 0.177 | Acc: 94.410% (8580/9088)
[Test] Epoch= 86  BatchID= 0 Loss: 0.080 | Acc: 96.875% (124/128)
[Test] Epoch= 86  BatchID= 10 Loss: 0.247 | Acc: 91.761% (1292/1408)
[Test] Epoch= 86  BatchID= 20 Loss: 0.247 | Acc: 91.853% (2469/2688)
[Test] Epoch= 86  BatchID= 30 Loss: 0.273 | Acc: 91.159% (3578/3925)

Epoch: 87
[Train] Epoch= 87  BatchID= 0 Loss: 0.150 | Acc: 93.750% (120/128)
[Train] Epoch= 87  BatchID= 10 Loss: 0.161 | Acc: 95.170% (1340/1408)
[Train] Epoch= 87  BatchID= 20 Loss: 0.169 | Acc: 94.754% (2547/2688)
[Train] Epoch= 87  BatchID= 30 Loss: 0.169 | Acc: 94.783% (3761/3968)
[Train] Epoch= 87  BatchID= 40 Loss: 0.169 | Acc: 94.798% (4975/5248)
[Train] Epoch= 87  BatchID= 50 Loss: 0.177 | Acc: 94.593% (6175/6528)
[Train] Epoch= 87  BatchID= 60 Loss: 0.178 | Acc: 94.506% (7379/7808)
[Train] Epoch= 87  BatchID= 70 Loss: 0.179 | Acc: 94.487% (8587/9088)
[Test] Epoch= 87  BatchID= 0 Loss: 0.054 | Acc: 96.875% (124/128)
[Test] Epoch= 87  BatchID= 10 Loss: 0.236 | Acc: 91.903% (1294/1408)
[Test] Epoch= 87  BatchID= 20 Loss: 0.243 | Acc: 91.927% (2471/2688)
[Test] Epoch= 87  BatchID= 30 Loss: 0.268 | Acc: 91.312% (3584/3925)

Epoch: 88
[Train] Epoch= 88  BatchID= 0 Loss: 0.123 | Acc: 96.094% (123/128)
[Train] Epoch= 88  BatchID= 10 Loss: 0.155 | Acc: 94.744% (1334/1408)
[Train] Epoch= 88  BatchID= 20 Loss: 0.154 | Acc: 95.089% (2556/2688)
[Train] Epoch= 88  BatchID= 30 Loss: 0.163 | Acc: 94.859% (3764/3968)
[Train] Epoch= 88  BatchID= 40 Loss: 0.164 | Acc: 94.970% (4984/5248)
[Train] Epoch= 88  BatchID= 50 Loss: 0.166 | Acc: 94.807% (6189/6528)
[Train] Epoch= 88  BatchID= 60 Loss: 0.164 | Acc: 94.890% (7409/7808)
[Train] Epoch= 88  BatchID= 70 Loss: 0.167 | Acc: 94.795% (8615/9088)
[Test] Epoch= 88  BatchID= 0 Loss: 0.068 | Acc: 96.875% (124/128)
[Test] Epoch= 88  BatchID= 10 Loss: 0.240 | Acc: 91.903% (1294/1408)
[Test] Epoch= 88  BatchID= 20 Loss: 0.240 | Acc: 92.113% (2476/2688)
[Test] Epoch= 88  BatchID= 30 Loss: 0.269 | Acc: 91.338% (3585/3925)

Epoch: 89
[Train] Epoch= 89  BatchID= 0 Loss: 0.194 | Acc: 93.750% (120/128)
[Train] Epoch= 89  BatchID= 10 Loss: 0.161 | Acc: 94.886% (1336/1408)
[Train] Epoch= 89  BatchID= 20 Loss: 0.173 | Acc: 94.457% (2539/2688)
[Train] Epoch= 89  BatchID= 30 Loss: 0.173 | Acc: 94.355% (3744/3968)
[Train] Epoch= 89  BatchID= 40 Loss: 0.172 | Acc: 94.607% (4965/5248)
[Train] Epoch= 89  BatchID= 50 Loss: 0.174 | Acc: 94.562% (6173/6528)
[Train] Epoch= 89  BatchID= 60 Loss: 0.169 | Acc: 94.762% (7399/7808)
[Train] Epoch= 89  BatchID= 70 Loss: 0.169 | Acc: 94.806% (8616/9088)
[Test] Epoch= 89  BatchID= 0 Loss: 0.082 | Acc: 96.875% (124/128)
[Test] Epoch= 89  BatchID= 10 Loss: 0.247 | Acc: 92.116% (1297/1408)
[Test] Epoch= 89  BatchID= 20 Loss: 0.240 | Acc: 92.374% (2483/2688)
[Test] Epoch= 89  BatchID= 30 Loss: 0.267 | Acc: 91.618% (3596/3925)
Saving..
Best accuracy:  91.61783439490446
