==> Preparing data..
==> Building model..
DataParallel(
  (module): ResNet(
    (conv1): InputConv2dLSQ(
      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): LinearLSQ(
      in_features=512, out_features=10, bias=True
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=True)
    )
  )
)
==> Initializing from checkpoint..

Epoch: 0
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
[Train] Epoch= 0  BatchID= 0 Loss: 0.230 | Acc: 92.188% (118/128)
[Train] Epoch= 0  BatchID= 10 Loss: 2.699 | Acc: 17.543% (247/1408)
[Train] Epoch= 0  BatchID= 20 Loss: 2.522 | Acc: 14.211% (382/2688)
[Train] Epoch= 0  BatchID= 30 Loss: 2.457 | Acc: 13.281% (527/3968)
[Train] Epoch= 0  BatchID= 40 Loss: 2.421 | Acc: 12.576% (660/5248)
[Train] Epoch= 0  BatchID= 50 Loss: 2.399 | Acc: 11.918% (778/6528)
[Train] Epoch= 0  BatchID= 60 Loss: 2.384 | Acc: 11.475% (896/7808)
[Train] Epoch= 0  BatchID= 70 Loss: 2.373 | Acc: 11.279% (1025/9088)
[Test] Epoch= 0  BatchID= 0 Loss: 2.292 | Acc: 0.000% (0/128)
[Test] Epoch= 0  BatchID= 10 Loss: 2.291 | Acc: 0.000% (0/1408)
[Test] Epoch= 0  BatchID= 20 Loss: 2.309 | Acc: 0.000% (0/2688)
[Test] Epoch= 0  BatchID= 30 Loss: 2.304 | Acc: 10.166% (399/3925)
Saving..
Best accuracy:  10.165605095541402

Epoch: 1
[Train] Epoch= 1  BatchID= 0 Loss: 2.301 | Acc: 13.281% (17/128)
[Train] Epoch= 1  BatchID= 10 Loss: 2.303 | Acc: 9.872% (139/1408)
[Train] Epoch= 1  BatchID= 20 Loss: 2.303 | Acc: 9.970% (268/2688)
[Train] Epoch= 1  BatchID= 30 Loss: 2.302 | Acc: 10.282% (408/3968)
[Train] Epoch= 1  BatchID= 40 Loss: 2.303 | Acc: 10.118% (531/5248)
[Train] Epoch= 1  BatchID= 50 Loss: 2.303 | Acc: 10.417% (680/6528)
[Train] Epoch= 1  BatchID= 60 Loss: 2.303 | Acc: 9.977% (779/7808)
[Train] Epoch= 1  BatchID= 70 Loss: 2.303 | Acc: 10.145% (922/9088)
[Test] Epoch= 1  BatchID= 0 Loss: 2.312 | Acc: 0.000% (0/128)
[Test] Epoch= 1  BatchID= 10 Loss: 2.302 | Acc: 25.355% (357/1408)
[Test] Epoch= 1  BatchID= 20 Loss: 2.304 | Acc: 13.281% (357/2688)
[Test] Epoch= 1  BatchID= 30 Loss: 2.304 | Acc: 9.096% (357/3925)

Epoch: 2
[Train] Epoch= 2  BatchID= 0 Loss: 2.305 | Acc: 6.250% (8/128)
[Train] Epoch= 2  BatchID= 10 Loss: 2.302 | Acc: 9.162% (129/1408)
[Train] Epoch= 2  BatchID= 20 Loss: 2.302 | Acc: 9.487% (255/2688)
[Train] Epoch= 2  BatchID= 30 Loss: 2.302 | Acc: 9.778% (388/3968)
[Train] Epoch= 2  BatchID= 40 Loss: 2.303 | Acc: 9.775% (513/5248)
[Train] Epoch= 2  BatchID= 50 Loss: 2.303 | Acc: 10.172% (664/6528)
[Train] Epoch= 2  BatchID= 60 Loss: 2.303 | Acc: 10.195% (796/7808)
[Train] Epoch= 2  BatchID= 70 Loss: 2.303 | Acc: 10.255% (932/9088)
[Test] Epoch= 2  BatchID= 0 Loss: 2.289 | Acc: 0.000% (0/128)
[Test] Epoch= 2  BatchID= 10 Loss: 2.295 | Acc: 25.355% (357/1408)
[Test] Epoch= 2  BatchID= 20 Loss: 2.306 | Acc: 13.281% (357/2688)
[Test] Epoch= 2  BatchID= 30 Loss: 2.304 | Acc: 9.096% (357/3925)

Epoch: 3
[Train] Epoch= 3  BatchID= 0 Loss: 2.300 | Acc: 12.500% (16/128)
[Train] Epoch= 3  BatchID= 10 Loss: 2.301 | Acc: 10.724% (151/1408)
[Train] Epoch= 3  BatchID= 20 Loss: 2.302 | Acc: 10.305% (277/2688)
[Train] Epoch= 3  BatchID= 30 Loss: 2.302 | Acc: 10.081% (400/3968)
[Train] Epoch= 3  BatchID= 40 Loss: 2.302 | Acc: 10.271% (539/5248)
[Train] Epoch= 3  BatchID= 50 Loss: 2.302 | Acc: 10.263% (670/6528)
[Train] Epoch= 3  BatchID= 60 Loss: 2.302 | Acc: 10.015% (782/7808)
[Train] Epoch= 3  BatchID= 70 Loss: 2.303 | Acc: 10.035% (912/9088)
[Test] Epoch= 3  BatchID= 0 Loss: 2.298 | Acc: 0.000% (0/128)
[Test] Epoch= 3  BatchID= 10 Loss: 2.316 | Acc: 25.355% (357/1408)
[Test] Epoch= 3  BatchID= 20 Loss: 2.305 | Acc: 13.281% (357/2688)
[Test] Epoch= 3  BatchID= 30 Loss: 2.304 | Acc: 9.096% (357/3925)

Epoch: 4
[Train] Epoch= 4  BatchID= 0 Loss: 2.297 | Acc: 15.625% (20/128)
[Train] Epoch= 4  BatchID= 10 Loss: 2.301 | Acc: 11.222% (158/1408)
[Train] Epoch= 4  BatchID= 20 Loss: 2.302 | Acc: 10.975% (295/2688)
[Train] Epoch= 4  BatchID= 30 Loss: 2.302 | Acc: 10.635% (422/3968)
[Train] Epoch= 4  BatchID= 40 Loss: 2.303 | Acc: 10.404% (546/5248)
[Train] Epoch= 4  BatchID= 50 Loss: 2.302 | Acc: 10.631% (694/6528)
[Train] Epoch= 4  BatchID= 60 Loss: 2.302 | Acc: 10.669% (833/7808)
[Train] Epoch= 4  BatchID= 70 Loss: 2.303 | Acc: 10.585% (962/9088)
[Test] Epoch= 4  BatchID= 0 Loss: 2.297 | Acc: 0.000% (0/128)
[Test] Epoch= 4  BatchID= 10 Loss: 2.299 | Acc: 25.355% (357/1408)
[Test] Epoch= 4  BatchID= 20 Loss: 2.301 | Acc: 13.281% (357/2688)
[Test] Epoch= 4  BatchID= 30 Loss: 2.303 | Acc: 9.096% (357/3925)

Epoch: 5
[Train] Epoch= 5  BatchID= 0 Loss: 2.306 | Acc: 7.031% (9/128)
[Train] Epoch= 5  BatchID= 10 Loss: 2.304 | Acc: 8.736% (123/1408)
[Train] Epoch= 5  BatchID= 20 Loss: 2.304 | Acc: 9.189% (247/2688)
[Train] Epoch= 5  BatchID= 30 Loss: 2.303 | Acc: 9.299% (369/3968)
[Train] Epoch= 5  BatchID= 40 Loss: 2.303 | Acc: 9.527% (500/5248)
[Train] Epoch= 5  BatchID= 50 Loss: 2.303 | Acc: 9.559% (624/6528)
[Train] Epoch= 5  BatchID= 60 Loss: 2.303 | Acc: 9.772% (763/7808)
[Train] Epoch= 5  BatchID= 70 Loss: 2.303 | Acc: 9.837% (894/9088)
[Test] Epoch= 5  BatchID= 0 Loss: 2.243 | Acc: 0.000% (0/128)
[Test] Epoch= 5  BatchID= 10 Loss: 2.297 | Acc: 25.355% (357/1408)
[Test] Epoch= 5  BatchID= 20 Loss: 2.310 | Acc: 13.281% (357/2688)
[Test] Epoch= 5  BatchID= 30 Loss: 2.304 | Acc: 9.096% (357/3925)

Epoch: 6
[Train] Epoch= 6  BatchID= 0 Loss: 2.300 | Acc: 4.688% (6/128)
[Train] Epoch= 6  BatchID= 10 Loss: 2.302 | Acc: 10.156% (143/1408)
[Train] Epoch= 6  BatchID= 20 Loss: 2.303 | Acc: 9.784% (263/2688)
[Train] Epoch= 6  BatchID= 30 Loss: 2.303 | Acc: 9.980% (396/3968)
[Train] Epoch= 6  BatchID= 40 Loss: 2.303 | Acc: 10.252% (538/5248)
[Train] Epoch= 6  BatchID= 50 Loss: 2.303 | Acc: 10.202% (666/6528)
[Train] Epoch= 6  BatchID= 60 Loss: 2.303 | Acc: 10.105% (789/7808)
[Train] Epoch= 6  BatchID= 70 Loss: 2.303 | Acc: 10.090% (917/9088)
[Test] Epoch= 6  BatchID= 0 Loss: 2.294 | Acc: 0.000% (0/128)
[Test] Epoch= 6  BatchID= 10 Loss: 2.305 | Acc: 0.000% (0/1408)
[Test] Epoch= 6  BatchID= 20 Loss: 2.302 | Acc: 13.393% (360/2688)
[Test] Epoch= 6  BatchID= 30 Loss: 2.303 | Acc: 9.911% (389/3925)

Epoch: 7
[Train] Epoch= 7  BatchID= 0 Loss: 2.300 | Acc: 10.156% (13/128)
[Train] Epoch= 7  BatchID= 10 Loss: 2.303 | Acc: 10.085% (142/1408)
[Train] Epoch= 7  BatchID= 20 Loss: 2.302 | Acc: 10.268% (276/2688)
[Train] Epoch= 7  BatchID= 30 Loss: 2.303 | Acc: 10.156% (403/3968)
[Train] Epoch= 7  BatchID= 40 Loss: 2.303 | Acc: 10.137% (532/5248)
[Train] Epoch= 7  BatchID= 50 Loss: 2.303 | Acc: 9.988% (652/6528)
[Train] Epoch= 7  BatchID= 60 Loss: 2.303 | Acc: 9.887% (772/7808)
[Train] Epoch= 7  BatchID= 70 Loss: 2.303 | Acc: 9.947% (904/9088)
[Test] Epoch= 7  BatchID= 0 Loss: 2.273 | Acc: 0.000% (0/128)
[Test] Epoch= 7  BatchID= 10 Loss: 2.310 | Acc: 0.000% (0/1408)
[Test] Epoch= 7  BatchID= 20 Loss: 2.307 | Acc: 0.000% (0/2688)
[Test] Epoch= 7  BatchID= 30 Loss: 2.303 | Acc: 9.936% (390/3925)

Epoch: 8
[Train] Epoch= 8  BatchID= 0 Loss: 2.301 | Acc: 17.188% (22/128)
[Train] Epoch= 8  BatchID= 10 Loss: 2.302 | Acc: 11.435% (161/1408)
[Train] Epoch= 8  BatchID= 20 Loss: 2.303 | Acc: 10.342% (278/2688)
[Train] Epoch= 8  BatchID= 30 Loss: 2.303 | Acc: 10.333% (410/3968)
[Train] Epoch= 8  BatchID= 40 Loss: 2.303 | Acc: 10.118% (531/5248)
[Train] Epoch= 8  BatchID= 50 Loss: 2.303 | Acc: 9.972% (651/6528)
[Train] Epoch= 8  BatchID= 60 Loss: 2.303 | Acc: 10.156% (793/7808)
[Train] Epoch= 8  BatchID= 70 Loss: 2.303 | Acc: 10.123% (920/9088)
[Test] Epoch= 8  BatchID= 0 Loss: 2.265 | Acc: 0.000% (0/128)
[Test] Epoch= 8  BatchID= 10 Loss: 2.292 | Acc: 25.355% (357/1408)
[Test] Epoch= 8  BatchID= 20 Loss: 2.306 | Acc: 13.281% (357/2688)
[Test] Epoch= 8  BatchID= 30 Loss: 2.304 | Acc: 9.096% (357/3925)

Epoch: 9
[Train] Epoch= 9  BatchID= 0 Loss: 2.305 | Acc: 10.156% (13/128)
[Train] Epoch= 9  BatchID= 10 Loss: 2.304 | Acc: 8.523% (120/1408)
[Train] Epoch= 9  BatchID= 20 Loss: 2.303 | Acc: 8.557% (230/2688)
[Train] Epoch= 9  BatchID= 30 Loss: 2.303 | Acc: 8.997% (357/3968)
[Train] Epoch= 9  BatchID= 40 Loss: 2.303 | Acc: 9.013% (473/5248)
[Train] Epoch= 9  BatchID= 50 Loss: 2.303 | Acc: 9.559% (624/6528)
[Train] Epoch= 9  BatchID= 60 Loss: 2.303 | Acc: 9.452% (738/7808)
[Train] Epoch= 9  BatchID= 70 Loss: 2.303 | Acc: 9.683% (880/9088)
[Test] Epoch= 9  BatchID= 0 Loss: 2.275 | Acc: 0.000% (0/128)
[Test] Epoch= 9  BatchID= 10 Loss: 2.298 | Acc: 25.355% (357/1408)
[Test] Epoch= 9  BatchID= 20 Loss: 2.300 | Acc: 13.281% (357/2688)
[Test] Epoch= 9  BatchID= 30 Loss: 2.304 | Acc: 9.096% (357/3925)

Epoch: 10
[Train] Epoch= 10  BatchID= 0 Loss: 2.299 | Acc: 10.156% (13/128)
[Train] Epoch= 10  BatchID= 10 Loss: 2.303 | Acc: 10.014% (141/1408)
[Train] Epoch= 10  BatchID= 20 Loss: 2.303 | Acc: 9.784% (263/2688)
[Train] Epoch= 10  BatchID= 30 Loss: 2.302 | Acc: 9.652% (383/3968)
[Train] Epoch= 10  BatchID= 40 Loss: 2.303 | Acc: 9.642% (506/5248)
[Train] Epoch= 10  BatchID= 50 Loss: 2.303 | Acc: 9.896% (646/6528)
[Train] Epoch= 10  BatchID= 60 Loss: nan | Acc: 9.977% (779/7808)
[Train] Epoch= 10  BatchID= 70 Loss: nan | Acc: 10.079% (916/9088)
[Test] Epoch= 10  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 10  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 10  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 10  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 11
[Train] Epoch= 11  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 11  BatchID= 10 Loss: nan | Acc: 10.085% (142/1408)
[Train] Epoch= 11  BatchID= 20 Loss: nan | Acc: 10.491% (282/2688)
[Train] Epoch= 11  BatchID= 30 Loss: nan | Acc: 10.333% (410/3968)
[Train] Epoch= 11  BatchID= 40 Loss: nan | Acc: 10.404% (546/5248)
[Train] Epoch= 11  BatchID= 50 Loss: nan | Acc: 10.202% (666/6528)
[Train] Epoch= 11  BatchID= 60 Loss: nan | Acc: 10.284% (803/7808)
[Train] Epoch= 11  BatchID= 70 Loss: nan | Acc: 10.255% (932/9088)
[Test] Epoch= 11  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 11  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 11  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 11  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 12
[Train] Epoch= 12  BatchID= 0 Loss: nan | Acc: 6.250% (8/128)
[Train] Epoch= 12  BatchID= 10 Loss: nan | Acc: 9.943% (140/1408)
[Train] Epoch= 12  BatchID= 20 Loss: nan | Acc: 10.119% (272/2688)
[Train] Epoch= 12  BatchID= 30 Loss: nan | Acc: 10.081% (400/3968)
[Train] Epoch= 12  BatchID= 40 Loss: nan | Acc: 10.290% (540/5248)
[Train] Epoch= 12  BatchID= 50 Loss: nan | Acc: 10.309% (673/6528)
[Train] Epoch= 12  BatchID= 60 Loss: nan | Acc: 10.297% (804/7808)
[Train] Epoch= 12  BatchID= 70 Loss: nan | Acc: 10.200% (927/9088)
[Test] Epoch= 12  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 12  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 12  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 12  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 13
[Train] Epoch= 13  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 13  BatchID= 10 Loss: nan | Acc: 9.872% (139/1408)
[Train] Epoch= 13  BatchID= 20 Loss: nan | Acc: 9.896% (266/2688)
[Train] Epoch= 13  BatchID= 30 Loss: nan | Acc: 9.929% (394/3968)
[Train] Epoch= 13  BatchID= 40 Loss: nan | Acc: 9.947% (522/5248)
[Train] Epoch= 13  BatchID= 50 Loss: nan | Acc: 9.896% (646/6528)
[Train] Epoch= 13  BatchID= 60 Loss: nan | Acc: 9.939% (776/7808)
[Train] Epoch= 13  BatchID= 70 Loss: nan | Acc: 10.200% (927/9088)
[Test] Epoch= 13  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 13  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 13  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 13  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 14
[Train] Epoch= 14  BatchID= 0 Loss: nan | Acc: 7.812% (10/128)
[Train] Epoch= 14  BatchID= 10 Loss: nan | Acc: 10.582% (149/1408)
[Train] Epoch= 14  BatchID= 20 Loss: nan | Acc: 10.119% (272/2688)
[Train] Epoch= 14  BatchID= 30 Loss: nan | Acc: 9.955% (395/3968)
[Train] Epoch= 14  BatchID= 40 Loss: nan | Acc: 9.813% (515/5248)
[Train] Epoch= 14  BatchID= 50 Loss: nan | Acc: 9.804% (640/6528)
[Train] Epoch= 14  BatchID= 60 Loss: nan | Acc: 10.079% (787/7808)
[Train] Epoch= 14  BatchID= 70 Loss: nan | Acc: 10.013% (910/9088)
[Test] Epoch= 14  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 14  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 14  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 14  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 15
[Train] Epoch= 15  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 15  BatchID= 10 Loss: nan | Acc: 10.724% (151/1408)
[Train] Epoch= 15  BatchID= 20 Loss: nan | Acc: 10.082% (271/2688)
[Train] Epoch= 15  BatchID= 30 Loss: nan | Acc: 10.207% (405/3968)
[Train] Epoch= 15  BatchID= 40 Loss: nan | Acc: 10.404% (546/5248)
[Train] Epoch= 15  BatchID= 50 Loss: nan | Acc: 10.463% (683/6528)
[Train] Epoch= 15  BatchID= 60 Loss: nan | Acc: 10.246% (800/7808)
[Train] Epoch= 15  BatchID= 70 Loss: nan | Acc: 10.145% (922/9088)
[Test] Epoch= 15  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 15  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 15  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 15  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 16
[Train] Epoch= 16  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 16  BatchID= 10 Loss: nan | Acc: 10.653% (150/1408)
[Train] Epoch= 16  BatchID= 20 Loss: nan | Acc: 10.119% (272/2688)
[Train] Epoch= 16  BatchID= 30 Loss: nan | Acc: 9.980% (396/3968)
[Train] Epoch= 16  BatchID= 40 Loss: nan | Acc: 10.175% (534/5248)
[Train] Epoch= 16  BatchID= 50 Loss: nan | Acc: 9.988% (652/6528)
[Train] Epoch= 16  BatchID= 60 Loss: nan | Acc: 10.220% (798/7808)
[Train] Epoch= 16  BatchID= 70 Loss: nan | Acc: 10.145% (922/9088)
[Test] Epoch= 16  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 16  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 16  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 16  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 17
[Train] Epoch= 17  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 17  BatchID= 10 Loss: nan | Acc: 10.866% (153/1408)
[Train] Epoch= 17  BatchID= 20 Loss: nan | Acc: 10.603% (285/2688)
[Train] Epoch= 17  BatchID= 30 Loss: nan | Acc: 10.635% (422/3968)
[Train] Epoch= 17  BatchID= 40 Loss: nan | Acc: 10.537% (553/5248)
[Train] Epoch= 17  BatchID= 50 Loss: nan | Acc: 10.371% (677/6528)
[Train] Epoch= 17  BatchID= 60 Loss: nan | Acc: 10.246% (800/7808)
[Train] Epoch= 17  BatchID= 70 Loss: nan | Acc: 10.299% (936/9088)
[Test] Epoch= 17  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 17  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 17  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 17  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 18
[Train] Epoch= 18  BatchID= 0 Loss: nan | Acc: 7.812% (10/128)
[Train] Epoch= 18  BatchID= 10 Loss: nan | Acc: 9.943% (140/1408)
[Train] Epoch= 18  BatchID= 20 Loss: nan | Acc: 10.193% (274/2688)
[Train] Epoch= 18  BatchID= 30 Loss: nan | Acc: 10.106% (401/3968)
[Train] Epoch= 18  BatchID= 40 Loss: nan | Acc: 9.623% (505/5248)
[Train] Epoch= 18  BatchID= 50 Loss: nan | Acc: 9.773% (638/6528)
[Train] Epoch= 18  BatchID= 60 Loss: nan | Acc: 9.900% (773/7808)
[Train] Epoch= 18  BatchID= 70 Loss: nan | Acc: 10.057% (914/9088)
[Test] Epoch= 18  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 18  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 18  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 18  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 19
[Train] Epoch= 19  BatchID= 0 Loss: nan | Acc: 5.469% (7/128)
[Train] Epoch= 19  BatchID= 10 Loss: nan | Acc: 10.724% (151/1408)
[Train] Epoch= 19  BatchID= 20 Loss: nan | Acc: 10.417% (280/2688)
[Train] Epoch= 19  BatchID= 30 Loss: nan | Acc: 10.156% (403/3968)
[Train] Epoch= 19  BatchID= 40 Loss: nan | Acc: 10.080% (529/5248)
[Train] Epoch= 19  BatchID= 50 Loss: nan | Acc: 10.049% (656/6528)
[Train] Epoch= 19  BatchID= 60 Loss: nan | Acc: 10.041% (784/7808)
[Train] Epoch= 19  BatchID= 70 Loss: nan | Acc: 10.145% (922/9088)
[Test] Epoch= 19  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 19  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 19  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 19  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 20
[Train] Epoch= 20  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 20  BatchID= 10 Loss: nan | Acc: 9.162% (129/1408)
[Train] Epoch= 20  BatchID= 20 Loss: nan | Acc: 9.896% (266/2688)
[Train] Epoch= 20  BatchID= 30 Loss: nan | Acc: 9.929% (394/3968)
[Train] Epoch= 20  BatchID= 40 Loss: nan | Acc: 9.947% (522/5248)
[Train] Epoch= 20  BatchID= 50 Loss: nan | Acc: 10.049% (656/6528)
[Train] Epoch= 20  BatchID= 60 Loss: nan | Acc: 9.913% (774/7808)
[Train] Epoch= 20  BatchID= 70 Loss: nan | Acc: 10.134% (921/9088)
[Test] Epoch= 20  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 20  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 20  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 20  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 21
[Train] Epoch= 21  BatchID= 0 Loss: nan | Acc: 7.812% (10/128)
[Train] Epoch= 21  BatchID= 10 Loss: nan | Acc: 9.872% (139/1408)
[Train] Epoch= 21  BatchID= 20 Loss: nan | Acc: 9.896% (266/2688)
[Train] Epoch= 21  BatchID= 30 Loss: nan | Acc: 9.980% (396/3968)
[Train] Epoch= 21  BatchID= 40 Loss: nan | Acc: 10.175% (534/5248)
[Train] Epoch= 21  BatchID= 50 Loss: nan | Acc: 10.248% (669/6528)
[Train] Epoch= 21  BatchID= 60 Loss: nan | Acc: 10.182% (795/7808)
[Train] Epoch= 21  BatchID= 70 Loss: nan | Acc: 10.145% (922/9088)
[Test] Epoch= 21  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 21  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 21  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 21  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 22
[Train] Epoch= 22  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 22  BatchID= 10 Loss: nan | Acc: 10.156% (143/1408)
[Train] Epoch= 22  BatchID= 20 Loss: nan | Acc: 10.007% (269/2688)
[Train] Epoch= 22  BatchID= 30 Loss: nan | Acc: 9.980% (396/3968)
[Train] Epoch= 22  BatchID= 40 Loss: nan | Acc: 10.156% (533/5248)
[Train] Epoch= 22  BatchID= 50 Loss: nan | Acc: 10.294% (672/6528)
[Train] Epoch= 22  BatchID= 60 Loss: nan | Acc: 10.195% (796/7808)
[Train] Epoch= 22  BatchID= 70 Loss: nan | Acc: 10.101% (918/9088)
[Test] Epoch= 22  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 22  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 22  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 22  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 23
[Train] Epoch= 23  BatchID= 0 Loss: nan | Acc: 14.062% (18/128)
[Train] Epoch= 23  BatchID= 10 Loss: nan | Acc: 10.014% (141/1408)
[Train] Epoch= 23  BatchID= 20 Loss: nan | Acc: 9.859% (265/2688)
[Train] Epoch= 23  BatchID= 30 Loss: nan | Acc: 9.829% (390/3968)
[Train] Epoch= 23  BatchID= 40 Loss: nan | Acc: 9.756% (512/5248)
[Train] Epoch= 23  BatchID= 50 Loss: nan | Acc: 10.064% (657/6528)
[Train] Epoch= 23  BatchID= 60 Loss: nan | Acc: 9.990% (780/7808)
[Train] Epoch= 23  BatchID= 70 Loss: nan | Acc: 10.101% (918/9088)
[Test] Epoch= 23  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 23  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 23  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 23  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 24
[Train] Epoch= 24  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 24  BatchID= 10 Loss: nan | Acc: 9.943% (140/1408)
[Train] Epoch= 24  BatchID= 20 Loss: nan | Acc: 10.417% (280/2688)
[Train] Epoch= 24  BatchID= 30 Loss: nan | Acc: 10.559% (419/3968)
[Train] Epoch= 24  BatchID= 40 Loss: nan | Acc: 10.766% (565/5248)
[Train] Epoch= 24  BatchID= 50 Loss: nan | Acc: 10.509% (686/6528)
[Train] Epoch= 24  BatchID= 60 Loss: nan | Acc: 10.323% (806/7808)
[Train] Epoch= 24  BatchID= 70 Loss: nan | Acc: 10.222% (929/9088)
[Test] Epoch= 24  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 24  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 24  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 24  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 25
[Train] Epoch= 25  BatchID= 0 Loss: nan | Acc: 10.938% (14/128)
[Train] Epoch= 25  BatchID= 10 Loss: nan | Acc: 10.440% (147/1408)
[Train] Epoch= 25  BatchID= 20 Loss: nan | Acc: 10.305% (277/2688)
[Train] Epoch= 25  BatchID= 30 Loss: nan | Acc: 10.433% (414/3968)
[Train] Epoch= 25  BatchID= 40 Loss: nan | Acc: 10.232% (537/5248)
[Train] Epoch= 25  BatchID= 50 Loss: nan | Acc: 10.279% (671/6528)
[Train] Epoch= 25  BatchID= 60 Loss: nan | Acc: 10.284% (803/7808)
[Train] Epoch= 25  BatchID= 70 Loss: nan | Acc: 10.057% (914/9088)
[Test] Epoch= 25  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 25  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 25  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 25  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 26
[Train] Epoch= 26  BatchID= 0 Loss: nan | Acc: 12.500% (16/128)
[Train] Epoch= 26  BatchID= 10 Loss: nan | Acc: 9.020% (127/1408)
[Train] Epoch= 26  BatchID= 20 Loss: nan | Acc: 10.231% (275/2688)
[Train] Epoch= 26  BatchID= 30 Loss: nan | Acc: 10.307% (409/3968)
[Train] Epoch= 26  BatchID= 40 Loss: nan | Acc: 10.213% (536/5248)
[Train] Epoch= 26  BatchID= 50 Loss: nan | Acc: 10.187% (665/6528)
[Train] Epoch= 26  BatchID= 60 Loss: nan | Acc: 10.092% (788/7808)
[Train] Epoch= 26  BatchID= 70 Loss: nan | Acc: 10.211% (928/9088)
[Test] Epoch= 26  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 26  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 26  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 26  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 27
[Train] Epoch= 27  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 27  BatchID= 10 Loss: nan | Acc: 8.736% (123/1408)
[Train] Epoch= 27  BatchID= 20 Loss: nan | Acc: 10.119% (272/2688)
[Train] Epoch= 27  BatchID= 30 Loss: nan | Acc: 10.207% (405/3968)
[Train] Epoch= 27  BatchID= 40 Loss: nan | Acc: 10.099% (530/5248)
[Train] Epoch= 27  BatchID= 50 Loss: nan | Acc: 10.218% (667/6528)
[Train] Epoch= 27  BatchID= 60 Loss: nan | Acc: 10.387% (811/7808)
[Train] Epoch= 27  BatchID= 70 Loss: nan | Acc: 10.211% (928/9088)
[Test] Epoch= 27  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 27  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 27  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 27  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 28
[Train] Epoch= 28  BatchID= 0 Loss: nan | Acc: 14.844% (19/128)
[Train] Epoch= 28  BatchID= 10 Loss: nan | Acc: 9.446% (133/1408)
[Train] Epoch= 28  BatchID= 20 Loss: nan | Acc: 10.379% (279/2688)
[Train] Epoch= 28  BatchID= 30 Loss: nan | Acc: 10.307% (409/3968)
[Train] Epoch= 28  BatchID= 40 Loss: nan | Acc: 10.328% (542/5248)
[Train] Epoch= 28  BatchID= 50 Loss: nan | Acc: 10.340% (675/6528)
[Train] Epoch= 28  BatchID= 60 Loss: nan | Acc: 10.374% (810/7808)
[Train] Epoch= 28  BatchID= 70 Loss: nan | Acc: 10.233% (930/9088)
[Test] Epoch= 28  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 28  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 28  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 28  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 29
[Train] Epoch= 29  BatchID= 0 Loss: nan | Acc: 10.938% (14/128)
[Train] Epoch= 29  BatchID= 10 Loss: nan | Acc: 10.369% (146/1408)
[Train] Epoch= 29  BatchID= 20 Loss: nan | Acc: 10.938% (294/2688)
[Train] Epoch= 29  BatchID= 30 Loss: nan | Acc: 10.660% (423/3968)
[Train] Epoch= 29  BatchID= 40 Loss: nan | Acc: 10.423% (547/5248)
[Train] Epoch= 29  BatchID= 50 Loss: nan | Acc: 10.493% (685/6528)
[Train] Epoch= 29  BatchID= 60 Loss: nan | Acc: 10.502% (820/7808)
[Train] Epoch= 29  BatchID= 70 Loss: nan | Acc: 10.233% (930/9088)
[Test] Epoch= 29  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 29  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 29  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 29  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 30
[Train] Epoch= 30  BatchID= 0 Loss: nan | Acc: 7.812% (10/128)
[Train] Epoch= 30  BatchID= 10 Loss: nan | Acc: 9.730% (137/1408)
[Train] Epoch= 30  BatchID= 20 Loss: nan | Acc: 10.268% (276/2688)
[Train] Epoch= 30  BatchID= 30 Loss: nan | Acc: 9.929% (394/3968)
[Train] Epoch= 30  BatchID= 40 Loss: nan | Acc: 10.118% (531/5248)
[Train] Epoch= 30  BatchID= 50 Loss: nan | Acc: 10.034% (655/6528)
[Train] Epoch= 30  BatchID= 60 Loss: nan | Acc: 10.207% (797/7808)
[Train] Epoch= 30  BatchID= 70 Loss: nan | Acc: 10.167% (924/9088)
[Test] Epoch= 30  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 30  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 30  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 30  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 31
[Train] Epoch= 31  BatchID= 0 Loss: nan | Acc: 12.500% (16/128)
[Train] Epoch= 31  BatchID= 10 Loss: nan | Acc: 11.080% (156/1408)
[Train] Epoch= 31  BatchID= 20 Loss: nan | Acc: 10.975% (295/2688)
[Train] Epoch= 31  BatchID= 30 Loss: nan | Acc: 10.484% (416/3968)
[Train] Epoch= 31  BatchID= 40 Loss: nan | Acc: 10.232% (537/5248)
[Train] Epoch= 31  BatchID= 50 Loss: nan | Acc: 10.018% (654/6528)
[Train] Epoch= 31  BatchID= 60 Loss: nan | Acc: 10.272% (802/7808)
[Train] Epoch= 31  BatchID= 70 Loss: nan | Acc: 10.101% (918/9088)
[Test] Epoch= 31  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 31  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 31  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 31  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 32
[Train] Epoch= 32  BatchID= 0 Loss: nan | Acc: 10.938% (14/128)
[Train] Epoch= 32  BatchID= 10 Loss: nan | Acc: 10.795% (152/1408)
[Train] Epoch= 32  BatchID= 20 Loss: nan | Acc: 10.677% (287/2688)
[Train] Epoch= 32  BatchID= 30 Loss: nan | Acc: 10.610% (421/3968)
[Train] Epoch= 32  BatchID= 40 Loss: nan | Acc: 10.480% (550/5248)
[Train] Epoch= 32  BatchID= 50 Loss: nan | Acc: 10.279% (671/6528)
[Train] Epoch= 32  BatchID= 60 Loss: nan | Acc: 10.195% (796/7808)
[Train] Epoch= 32  BatchID= 70 Loss: nan | Acc: 10.189% (926/9088)
[Test] Epoch= 32  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 32  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 32  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 32  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 33
[Train] Epoch= 33  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 33  BatchID= 10 Loss: nan | Acc: 8.807% (124/1408)
[Train] Epoch= 33  BatchID= 20 Loss: nan | Acc: 9.784% (263/2688)
[Train] Epoch= 33  BatchID= 30 Loss: nan | Acc: 9.879% (392/3968)
[Train] Epoch= 33  BatchID= 40 Loss: nan | Acc: 10.175% (534/5248)
[Train] Epoch= 33  BatchID= 50 Loss: nan | Acc: 10.263% (670/6528)
[Train] Epoch= 33  BatchID= 60 Loss: nan | Acc: 10.169% (794/7808)
[Train] Epoch= 33  BatchID= 70 Loss: nan | Acc: 10.123% (920/9088)
[Test] Epoch= 33  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 33  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 33  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 33  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 34
[Train] Epoch= 34  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 34  BatchID= 10 Loss: nan | Acc: 8.878% (125/1408)
[Train] Epoch= 34  BatchID= 20 Loss: nan | Acc: 9.673% (260/2688)
[Train] Epoch= 34  BatchID= 30 Loss: nan | Acc: 9.476% (376/3968)
[Train] Epoch= 34  BatchID= 40 Loss: nan | Acc: 9.661% (507/5248)
[Train] Epoch= 34  BatchID= 50 Loss: nan | Acc: 9.850% (643/6528)
[Train] Epoch= 34  BatchID= 60 Loss: nan | Acc: 10.015% (782/7808)
[Train] Epoch= 34  BatchID= 70 Loss: nan | Acc: 10.123% (920/9088)
[Test] Epoch= 34  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 34  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 34  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 34  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 35
[Train] Epoch= 35  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 35  BatchID= 10 Loss: nan | Acc: 10.227% (144/1408)
[Train] Epoch= 35  BatchID= 20 Loss: nan | Acc: 10.379% (279/2688)
[Train] Epoch= 35  BatchID= 30 Loss: nan | Acc: 10.610% (421/3968)
[Train] Epoch= 35  BatchID= 40 Loss: nan | Acc: 10.328% (542/5248)
[Train] Epoch= 35  BatchID= 50 Loss: nan | Acc: 10.386% (678/6528)
[Train] Epoch= 35  BatchID= 60 Loss: nan | Acc: 10.400% (812/7808)
[Train] Epoch= 35  BatchID= 70 Loss: nan | Acc: 10.156% (923/9088)
[Test] Epoch= 35  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 35  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 35  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 35  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 36
[Train] Epoch= 36  BatchID= 0 Loss: nan | Acc: 10.938% (14/128)
[Train] Epoch= 36  BatchID= 10 Loss: nan | Acc: 11.151% (157/1408)
[Train] Epoch= 36  BatchID= 20 Loss: nan | Acc: 10.565% (284/2688)
[Train] Epoch= 36  BatchID= 30 Loss: nan | Acc: 10.585% (420/3968)
[Train] Epoch= 36  BatchID= 40 Loss: nan | Acc: 10.366% (544/5248)
[Train] Epoch= 36  BatchID= 50 Loss: nan | Acc: 10.432% (681/6528)
[Train] Epoch= 36  BatchID= 60 Loss: nan | Acc: 10.438% (815/7808)
[Train] Epoch= 36  BatchID= 70 Loss: nan | Acc: 10.244% (931/9088)
[Test] Epoch= 36  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 36  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 36  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 36  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 37
[Train] Epoch= 37  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 37  BatchID= 10 Loss: nan | Acc: 10.014% (141/1408)
[Train] Epoch= 37  BatchID= 20 Loss: nan | Acc: 10.379% (279/2688)
[Train] Epoch= 37  BatchID= 30 Loss: nan | Acc: 10.181% (404/3968)
[Train] Epoch= 37  BatchID= 40 Loss: nan | Acc: 10.004% (525/5248)
[Train] Epoch= 37  BatchID= 50 Loss: nan | Acc: 10.018% (654/6528)
[Train] Epoch= 37  BatchID= 60 Loss: nan | Acc: 10.195% (796/7808)
[Train] Epoch= 37  BatchID= 70 Loss: nan | Acc: 10.145% (922/9088)
[Test] Epoch= 37  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 37  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 37  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 37  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 38
[Train] Epoch= 38  BatchID= 0 Loss: nan | Acc: 6.250% (8/128)
[Train] Epoch= 38  BatchID= 10 Loss: nan | Acc: 10.014% (141/1408)
[Train] Epoch= 38  BatchID= 20 Loss: nan | Acc: 10.119% (272/2688)
[Train] Epoch= 38  BatchID= 30 Loss: nan | Acc: 10.106% (401/3968)
[Train] Epoch= 38  BatchID= 40 Loss: nan | Acc: 10.118% (531/5248)
[Train] Epoch= 38  BatchID= 50 Loss: nan | Acc: 10.095% (659/6528)
[Train] Epoch= 38  BatchID= 60 Loss: nan | Acc: 10.169% (794/7808)
[Train] Epoch= 38  BatchID= 70 Loss: nan | Acc: 10.068% (915/9088)
[Test] Epoch= 38  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 38  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 38  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 38  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 39
[Train] Epoch= 39  BatchID= 0 Loss: nan | Acc: 12.500% (16/128)
[Train] Epoch= 39  BatchID= 10 Loss: nan | Acc: 9.588% (135/1408)
[Train] Epoch= 39  BatchID= 20 Loss: nan | Acc: 9.710% (261/2688)
[Train] Epoch= 39  BatchID= 30 Loss: nan | Acc: 9.778% (388/3968)
[Train] Epoch= 39  BatchID= 40 Loss: nan | Acc: 10.232% (537/5248)
[Train] Epoch= 39  BatchID= 50 Loss: nan | Acc: 10.233% (668/6528)
[Train] Epoch= 39  BatchID= 60 Loss: nan | Acc: 10.272% (802/7808)
[Train] Epoch= 39  BatchID= 70 Loss: nan | Acc: 10.189% (926/9088)
[Test] Epoch= 39  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 39  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 39  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 39  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 40
[Train] Epoch= 40  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 40  BatchID= 10 Loss: nan | Acc: 10.227% (144/1408)
[Train] Epoch= 40  BatchID= 20 Loss: nan | Acc: 9.784% (263/2688)
[Train] Epoch= 40  BatchID= 30 Loss: nan | Acc: 10.282% (408/3968)
[Train] Epoch= 40  BatchID= 40 Loss: nan | Acc: 10.232% (537/5248)
[Train] Epoch= 40  BatchID= 50 Loss: nan | Acc: 10.294% (672/6528)
[Train] Epoch= 40  BatchID= 60 Loss: nan | Acc: 10.169% (794/7808)
[Train] Epoch= 40  BatchID= 70 Loss: nan | Acc: 10.189% (926/9088)
[Test] Epoch= 40  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 40  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 40  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 40  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 41
[Train] Epoch= 41  BatchID= 0 Loss: nan | Acc: 14.062% (18/128)
[Train] Epoch= 41  BatchID= 10 Loss: nan | Acc: 10.440% (147/1408)
[Train] Epoch= 41  BatchID= 20 Loss: nan | Acc: 10.454% (281/2688)
[Train] Epoch= 41  BatchID= 30 Loss: nan | Acc: 10.509% (417/3968)
[Train] Epoch= 41  BatchID= 40 Loss: nan | Acc: 9.889% (519/5248)
[Train] Epoch= 41  BatchID= 50 Loss: nan | Acc: 10.080% (658/6528)
[Train] Epoch= 41  BatchID= 60 Loss: nan | Acc: 9.913% (774/7808)
[Train] Epoch= 41  BatchID= 70 Loss: nan | Acc: 10.101% (918/9088)
[Test] Epoch= 41  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 41  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 41  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 41  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 42
[Train] Epoch= 42  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 42  BatchID= 10 Loss: nan | Acc: 10.085% (142/1408)
[Train] Epoch= 42  BatchID= 20 Loss: nan | Acc: 10.528% (283/2688)
[Train] Epoch= 42  BatchID= 30 Loss: nan | Acc: 10.433% (414/3968)
[Train] Epoch= 42  BatchID= 40 Loss: nan | Acc: 10.575% (555/5248)
[Train] Epoch= 42  BatchID= 50 Loss: nan | Acc: 10.478% (684/6528)
[Train] Epoch= 42  BatchID= 60 Loss: nan | Acc: 10.323% (806/7808)
[Train] Epoch= 42  BatchID= 70 Loss: nan | Acc: 10.255% (932/9088)
[Test] Epoch= 42  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 42  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 42  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 42  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 43
[Train] Epoch= 43  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 43  BatchID= 10 Loss: nan | Acc: 11.009% (155/1408)
[Train] Epoch= 43  BatchID= 20 Loss: nan | Acc: 10.119% (272/2688)
[Train] Epoch= 43  BatchID= 30 Loss: nan | Acc: 10.408% (413/3968)
[Train] Epoch= 43  BatchID= 40 Loss: nan | Acc: 10.137% (532/5248)
[Train] Epoch= 43  BatchID= 50 Loss: nan | Acc: 10.248% (669/6528)
[Train] Epoch= 43  BatchID= 60 Loss: nan | Acc: 10.182% (795/7808)
[Train] Epoch= 43  BatchID= 70 Loss: nan | Acc: 10.156% (923/9088)
[Test] Epoch= 43  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 43  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 43  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 43  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 44
[Train] Epoch= 44  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 44  BatchID= 10 Loss: nan | Acc: 8.665% (122/1408)
[Train] Epoch= 44  BatchID= 20 Loss: nan | Acc: 9.449% (254/2688)
[Train] Epoch= 44  BatchID= 30 Loss: nan | Acc: 9.753% (387/3968)
[Train] Epoch= 44  BatchID= 40 Loss: nan | Acc: 9.680% (508/5248)
[Train] Epoch= 44  BatchID= 50 Loss: nan | Acc: 9.743% (636/6528)
[Train] Epoch= 44  BatchID= 60 Loss: nan | Acc: 9.990% (780/7808)
[Train] Epoch= 44  BatchID= 70 Loss: nan | Acc: 10.068% (915/9088)
[Test] Epoch= 44  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 44  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 44  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 44  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 45
[Train] Epoch= 45  BatchID= 0 Loss: nan | Acc: 14.844% (19/128)
[Train] Epoch= 45  BatchID= 10 Loss: nan | Acc: 12.074% (170/1408)
[Train] Epoch= 45  BatchID= 20 Loss: nan | Acc: 10.119% (272/2688)
[Train] Epoch= 45  BatchID= 30 Loss: nan | Acc: 10.005% (397/3968)
[Train] Epoch= 45  BatchID= 40 Loss: nan | Acc: 9.928% (521/5248)
[Train] Epoch= 45  BatchID= 50 Loss: nan | Acc: 10.034% (655/6528)
[Train] Epoch= 45  BatchID= 60 Loss: nan | Acc: 10.067% (786/7808)
[Train] Epoch= 45  BatchID= 70 Loss: nan | Acc: 10.013% (910/9088)
[Test] Epoch= 45  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 45  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 45  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 45  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 46
[Train] Epoch= 46  BatchID= 0 Loss: nan | Acc: 9.375% (12/128)
[Train] Epoch= 46  BatchID= 10 Loss: nan | Acc: 10.298% (145/1408)
[Train] Epoch= 46  BatchID= 20 Loss: nan | Acc: 10.565% (284/2688)
[Train] Epoch= 46  BatchID= 30 Loss: nan | Acc: 9.854% (391/3968)
[Train] Epoch= 46  BatchID= 40 Loss: nan | Acc: 9.870% (518/5248)
[Train] Epoch= 46  BatchID= 50 Loss: nan | Acc: 10.126% (661/6528)
[Train] Epoch= 46  BatchID= 60 Loss: nan | Acc: 10.131% (791/7808)
[Train] Epoch= 46  BatchID= 70 Loss: nan | Acc: 10.211% (928/9088)
[Test] Epoch= 46  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 46  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 46  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 46  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 47
[Train] Epoch= 47  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 47  BatchID= 10 Loss: nan | Acc: 9.659% (136/1408)
[Train] Epoch= 47  BatchID= 20 Loss: nan | Acc: 9.784% (263/2688)
[Train] Epoch= 47  BatchID= 30 Loss: nan | Acc: 10.081% (400/3968)
[Train] Epoch= 47  BatchID= 40 Loss: nan | Acc: 10.156% (533/5248)
[Train] Epoch= 47  BatchID= 50 Loss: nan | Acc: 10.141% (662/6528)
[Train] Epoch= 47  BatchID= 60 Loss: nan | Acc: 10.207% (797/7808)
[Train] Epoch= 47  BatchID= 70 Loss: nan | Acc: 10.211% (928/9088)
[Test] Epoch= 47  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 47  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 47  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 47  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 48
[Train] Epoch= 48  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 48  BatchID= 10 Loss: nan | Acc: 9.233% (130/1408)
[Train] Epoch= 48  BatchID= 20 Loss: nan | Acc: 9.859% (265/2688)
[Train] Epoch= 48  BatchID= 30 Loss: nan | Acc: 10.307% (409/3968)
[Train] Epoch= 48  BatchID= 40 Loss: nan | Acc: 10.252% (538/5248)
[Train] Epoch= 48  BatchID= 50 Loss: nan | Acc: 9.957% (650/6528)
[Train] Epoch= 48  BatchID= 60 Loss: nan | Acc: 10.169% (794/7808)
[Train] Epoch= 48  BatchID= 70 Loss: nan | Acc: 10.299% (936/9088)
[Test] Epoch= 48  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 48  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 48  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 48  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 49
[Train] Epoch= 49  BatchID= 0 Loss: nan | Acc: 14.062% (18/128)
[Train] Epoch= 49  BatchID= 10 Loss: nan | Acc: 12.500% (176/1408)
[Train] Epoch= 49  BatchID= 20 Loss: nan | Acc: 11.272% (303/2688)
[Train] Epoch= 49  BatchID= 30 Loss: nan | Acc: 10.711% (425/3968)
[Train] Epoch= 49  BatchID= 40 Loss: nan | Acc: 10.309% (541/5248)
[Train] Epoch= 49  BatchID= 50 Loss: nan | Acc: 10.202% (666/6528)
[Train] Epoch= 49  BatchID= 60 Loss: nan | Acc: 10.131% (791/7808)
[Train] Epoch= 49  BatchID= 70 Loss: nan | Acc: 10.200% (927/9088)
[Test] Epoch= 49  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 49  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 49  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 49  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 50
[Train] Epoch= 50  BatchID= 0 Loss: nan | Acc: 9.375% (12/128)
[Train] Epoch= 50  BatchID= 10 Loss: nan | Acc: 9.233% (130/1408)
[Train] Epoch= 50  BatchID= 20 Loss: nan | Acc: 9.561% (257/2688)
[Train] Epoch= 50  BatchID= 30 Loss: nan | Acc: 10.055% (399/3968)
[Train] Epoch= 50  BatchID= 40 Loss: nan | Acc: 10.690% (561/5248)
[Train] Epoch= 50  BatchID= 50 Loss: nan | Acc: 10.233% (668/6528)
[Train] Epoch= 50  BatchID= 60 Loss: nan | Acc: 10.220% (798/7808)
[Train] Epoch= 50  BatchID= 70 Loss: nan | Acc: 10.200% (927/9088)
[Test] Epoch= 50  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 50  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 50  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 50  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 51
[Train] Epoch= 51  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 51  BatchID= 10 Loss: nan | Acc: 10.653% (150/1408)
[Train] Epoch= 51  BatchID= 20 Loss: nan | Acc: 10.268% (276/2688)
[Train] Epoch= 51  BatchID= 30 Loss: nan | Acc: 9.803% (389/3968)
[Train] Epoch= 51  BatchID= 40 Loss: nan | Acc: 10.004% (525/5248)
[Train] Epoch= 51  BatchID= 50 Loss: nan | Acc: 10.064% (657/6528)
[Train] Epoch= 51  BatchID= 60 Loss: nan | Acc: 10.220% (798/7808)
[Train] Epoch= 51  BatchID= 70 Loss: nan | Acc: 10.277% (934/9088)
[Test] Epoch= 51  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 51  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 51  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 51  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 52
[Train] Epoch= 52  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 52  BatchID= 10 Loss: nan | Acc: 9.801% (138/1408)
[Train] Epoch= 52  BatchID= 20 Loss: nan | Acc: 9.896% (266/2688)
[Train] Epoch= 52  BatchID= 30 Loss: nan | Acc: 9.753% (387/3968)
[Train] Epoch= 52  BatchID= 40 Loss: nan | Acc: 10.042% (527/5248)
[Train] Epoch= 52  BatchID= 50 Loss: nan | Acc: 10.064% (657/6528)
[Train] Epoch= 52  BatchID= 60 Loss: nan | Acc: 10.233% (799/7808)
[Train] Epoch= 52  BatchID= 70 Loss: nan | Acc: 10.244% (931/9088)
[Test] Epoch= 52  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 52  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 52  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 52  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 53
[Train] Epoch= 53  BatchID= 0 Loss: nan | Acc: 10.938% (14/128)
[Train] Epoch= 53  BatchID= 10 Loss: nan | Acc: 9.659% (136/1408)
[Train] Epoch= 53  BatchID= 20 Loss: nan | Acc: 9.784% (263/2688)
[Train] Epoch= 53  BatchID= 30 Loss: nan | Acc: 9.980% (396/3968)
[Train] Epoch= 53  BatchID= 40 Loss: nan | Acc: 10.271% (539/5248)
[Train] Epoch= 53  BatchID= 50 Loss: nan | Acc: 10.172% (664/6528)
[Train] Epoch= 53  BatchID= 60 Loss: nan | Acc: 10.131% (791/7808)
[Train] Epoch= 53  BatchID= 70 Loss: nan | Acc: 10.134% (921/9088)
[Test] Epoch= 53  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 53  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 53  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 53  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 54
[Train] Epoch= 54  BatchID= 0 Loss: nan | Acc: 7.031% (9/128)
[Train] Epoch= 54  BatchID= 10 Loss: nan | Acc: 9.801% (138/1408)
[Train] Epoch= 54  BatchID= 20 Loss: nan | Acc: 9.821% (264/2688)
[Train] Epoch= 54  BatchID= 30 Loss: nan | Acc: 10.156% (403/3968)
[Train] Epoch= 54  BatchID= 40 Loss: nan | Acc: 9.966% (523/5248)
[Train] Epoch= 54  BatchID= 50 Loss: nan | Acc: 10.003% (653/6528)
[Train] Epoch= 54  BatchID= 60 Loss: nan | Acc: 10.041% (784/7808)
[Train] Epoch= 54  BatchID= 70 Loss: nan | Acc: 10.189% (926/9088)
[Test] Epoch= 54  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 54  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 54  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 54  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 55
[Train] Epoch= 55  BatchID= 0 Loss: nan | Acc: 13.281% (17/128)
[Train] Epoch= 55  BatchID= 10 Loss: nan | Acc: 10.795% (152/1408)
[Train] Epoch= 55  BatchID= 20 Loss: nan | Acc: 10.119% (272/2688)
[Train] Epoch= 55  BatchID= 30 Loss: nan | Acc: 10.081% (400/3968)
[Train] Epoch= 55  BatchID= 40 Loss: nan | Acc: 10.118% (531/5248)
[Train] Epoch= 55  BatchID= 50 Loss: nan | Acc: 9.972% (651/6528)
[Train] Epoch= 55  BatchID= 60 Loss: nan | Acc: 10.259% (801/7808)
[Train] Epoch= 55  BatchID= 70 Loss: nan | Acc: 10.090% (917/9088)
[Test] Epoch= 55  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 55  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 55  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 55  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 56
[Train] Epoch= 56  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 56  BatchID= 10 Loss: nan | Acc: 9.943% (140/1408)
[Train] Epoch= 56  BatchID= 20 Loss: nan | Acc: 10.156% (273/2688)
[Train] Epoch= 56  BatchID= 30 Loss: nan | Acc: 10.509% (417/3968)
[Train] Epoch= 56  BatchID= 40 Loss: nan | Acc: 10.404% (546/5248)
[Train] Epoch= 56  BatchID= 50 Loss: nan | Acc: 10.447% (682/6528)
[Train] Epoch= 56  BatchID= 60 Loss: nan | Acc: 10.502% (820/7808)
[Train] Epoch= 56  BatchID= 70 Loss: nan | Acc: 10.244% (931/9088)
[Test] Epoch= 56  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 56  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 56  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 56  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 57
[Train] Epoch= 57  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 57  BatchID= 10 Loss: nan | Acc: 11.222% (158/1408)
[Train] Epoch= 57  BatchID= 20 Loss: nan | Acc: 11.124% (299/2688)
[Train] Epoch= 57  BatchID= 30 Loss: nan | Acc: 9.955% (395/3968)
[Train] Epoch= 57  BatchID= 40 Loss: nan | Acc: 10.080% (529/5248)
[Train] Epoch= 57  BatchID= 50 Loss: nan | Acc: 10.294% (672/6528)
[Train] Epoch= 57  BatchID= 60 Loss: nan | Acc: 10.207% (797/7808)
[Train] Epoch= 57  BatchID= 70 Loss: nan | Acc: 10.145% (922/9088)
[Test] Epoch= 57  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 57  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 57  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 57  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 58
[Train] Epoch= 58  BatchID= 0 Loss: nan | Acc: 7.812% (10/128)
[Train] Epoch= 58  BatchID= 10 Loss: nan | Acc: 10.227% (144/1408)
[Train] Epoch= 58  BatchID= 20 Loss: nan | Acc: 9.747% (262/2688)
[Train] Epoch= 58  BatchID= 30 Loss: nan | Acc: 10.106% (401/3968)
[Train] Epoch= 58  BatchID= 40 Loss: nan | Acc: 9.870% (518/5248)
[Train] Epoch= 58  BatchID= 50 Loss: nan | Acc: 10.064% (657/6528)
[Train] Epoch= 58  BatchID= 60 Loss: nan | Acc: 9.990% (780/7808)
[Train] Epoch= 58  BatchID= 70 Loss: nan | Acc: 10.233% (930/9088)
[Test] Epoch= 58  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 58  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 58  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 58  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 59
[Train] Epoch= 59  BatchID= 0 Loss: nan | Acc: 14.844% (19/128)
[Train] Epoch= 59  BatchID= 10 Loss: nan | Acc: 10.582% (149/1408)
[Train] Epoch= 59  BatchID= 20 Loss: nan | Acc: 10.751% (289/2688)
[Train] Epoch= 59  BatchID= 30 Loss: nan | Acc: 10.862% (431/3968)
[Train] Epoch= 59  BatchID= 40 Loss: nan | Acc: 10.309% (541/5248)
[Train] Epoch= 59  BatchID= 50 Loss: nan | Acc: 10.080% (658/6528)
[Train] Epoch= 59  BatchID= 60 Loss: nan | Acc: 10.079% (787/7808)
[Train] Epoch= 59  BatchID= 70 Loss: nan | Acc: 10.200% (927/9088)
[Test] Epoch= 59  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 59  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 59  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 59  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 60
[Train] Epoch= 60  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 60  BatchID= 10 Loss: nan | Acc: 9.801% (138/1408)
[Train] Epoch= 60  BatchID= 20 Loss: nan | Acc: 9.933% (267/2688)
[Train] Epoch= 60  BatchID= 30 Loss: nan | Acc: 10.459% (415/3968)
[Train] Epoch= 60  BatchID= 40 Loss: nan | Acc: 10.366% (544/5248)
[Train] Epoch= 60  BatchID= 50 Loss: nan | Acc: 10.524% (687/6528)
[Train] Epoch= 60  BatchID= 60 Loss: nan | Acc: 10.233% (799/7808)
[Train] Epoch= 60  BatchID= 70 Loss: nan | Acc: 10.134% (921/9088)
[Test] Epoch= 60  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 60  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 60  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 60  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 61
[Train] Epoch= 61  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 61  BatchID= 10 Loss: nan | Acc: 10.511% (148/1408)
[Train] Epoch= 61  BatchID= 20 Loss: nan | Acc: 10.231% (275/2688)
[Train] Epoch= 61  BatchID= 30 Loss: nan | Acc: 9.803% (389/3968)
[Train] Epoch= 61  BatchID= 40 Loss: nan | Acc: 9.832% (516/5248)
[Train] Epoch= 61  BatchID= 50 Loss: nan | Acc: 9.773% (638/6528)
[Train] Epoch= 61  BatchID= 60 Loss: nan | Acc: 10.054% (785/7808)
[Train] Epoch= 61  BatchID= 70 Loss: nan | Acc: 10.156% (923/9088)
[Test] Epoch= 61  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 61  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 61  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 61  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 62
[Train] Epoch= 62  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 62  BatchID= 10 Loss: nan | Acc: 11.577% (163/1408)
[Train] Epoch= 62  BatchID= 20 Loss: nan | Acc: 11.012% (296/2688)
[Train] Epoch= 62  BatchID= 30 Loss: nan | Acc: 10.963% (435/3968)
[Train] Epoch= 62  BatchID= 40 Loss: nan | Acc: 10.766% (565/5248)
[Train] Epoch= 62  BatchID= 50 Loss: nan | Acc: 10.355% (676/6528)
[Train] Epoch= 62  BatchID= 60 Loss: nan | Acc: 10.118% (790/7808)
[Train] Epoch= 62  BatchID= 70 Loss: nan | Acc: 10.090% (917/9088)
[Test] Epoch= 62  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 62  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 62  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 62  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 63
[Train] Epoch= 63  BatchID= 0 Loss: nan | Acc: 6.250% (8/128)
[Train] Epoch= 63  BatchID= 10 Loss: nan | Acc: 10.653% (150/1408)
[Train] Epoch= 63  BatchID= 20 Loss: nan | Acc: 10.938% (294/2688)
[Train] Epoch= 63  BatchID= 30 Loss: nan | Acc: 10.433% (414/3968)
[Train] Epoch= 63  BatchID= 40 Loss: nan | Acc: 9.947% (522/5248)
[Train] Epoch= 63  BatchID= 50 Loss: nan | Acc: 10.202% (666/6528)
[Train] Epoch= 63  BatchID= 60 Loss: nan | Acc: 10.156% (793/7808)
[Train] Epoch= 63  BatchID= 70 Loss: nan | Acc: 10.134% (921/9088)
[Test] Epoch= 63  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 63  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 63  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 63  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 64
[Train] Epoch= 64  BatchID= 0 Loss: nan | Acc: 13.281% (17/128)
[Train] Epoch= 64  BatchID= 10 Loss: nan | Acc: 8.878% (125/1408)
[Train] Epoch= 64  BatchID= 20 Loss: nan | Acc: 8.854% (238/2688)
[Train] Epoch= 64  BatchID= 30 Loss: nan | Acc: 9.703% (385/3968)
[Train] Epoch= 64  BatchID= 40 Loss: nan | Acc: 10.252% (538/5248)
[Train] Epoch= 64  BatchID= 50 Loss: nan | Acc: 10.432% (681/6528)
[Train] Epoch= 64  BatchID= 60 Loss: nan | Acc: 10.207% (797/7808)
[Train] Epoch= 64  BatchID= 70 Loss: nan | Acc: 10.123% (920/9088)
[Test] Epoch= 64  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 64  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 64  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 64  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 65
[Train] Epoch= 65  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 65  BatchID= 10 Loss: nan | Acc: 10.014% (141/1408)
[Train] Epoch= 65  BatchID= 20 Loss: nan | Acc: 10.268% (276/2688)
[Train] Epoch= 65  BatchID= 30 Loss: nan | Acc: 10.156% (403/3968)
[Train] Epoch= 65  BatchID= 40 Loss: nan | Acc: 10.232% (537/5248)
[Train] Epoch= 65  BatchID= 50 Loss: nan | Acc: 10.156% (663/6528)
[Train] Epoch= 65  BatchID= 60 Loss: nan | Acc: 10.131% (791/7808)
[Train] Epoch= 65  BatchID= 70 Loss: nan | Acc: 10.112% (919/9088)
[Test] Epoch= 65  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 65  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 65  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 65  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 66
[Train] Epoch= 66  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 66  BatchID= 10 Loss: nan | Acc: 9.446% (133/1408)
[Train] Epoch= 66  BatchID= 20 Loss: nan | Acc: 9.710% (261/2688)
[Train] Epoch= 66  BatchID= 30 Loss: nan | Acc: 9.929% (394/3968)
[Train] Epoch= 66  BatchID= 40 Loss: nan | Acc: 9.756% (512/5248)
[Train] Epoch= 66  BatchID= 50 Loss: nan | Acc: 10.034% (655/6528)
[Train] Epoch= 66  BatchID= 60 Loss: nan | Acc: 9.862% (770/7808)
[Train] Epoch= 66  BatchID= 70 Loss: nan | Acc: 10.244% (931/9088)
[Test] Epoch= 66  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 66  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 66  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 66  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 67
[Train] Epoch= 67  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 67  BatchID= 10 Loss: nan | Acc: 10.440% (147/1408)
[Train] Epoch= 67  BatchID= 20 Loss: nan | Acc: 10.975% (295/2688)
[Train] Epoch= 67  BatchID= 30 Loss: nan | Acc: 10.534% (418/3968)
[Train] Epoch= 67  BatchID= 40 Loss: nan | Acc: 10.480% (550/5248)
[Train] Epoch= 67  BatchID= 50 Loss: nan | Acc: 10.447% (682/6528)
[Train] Epoch= 67  BatchID= 60 Loss: nan | Acc: 10.425% (814/7808)
[Train] Epoch= 67  BatchID= 70 Loss: nan | Acc: 10.211% (928/9088)
[Test] Epoch= 67  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 67  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 67  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 67  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 68
[Train] Epoch= 68  BatchID= 0 Loss: nan | Acc: 4.688% (6/128)
[Train] Epoch= 68  BatchID= 10 Loss: nan | Acc: 9.872% (139/1408)
[Train] Epoch= 68  BatchID= 20 Loss: nan | Acc: 10.007% (269/2688)
[Train] Epoch= 68  BatchID= 30 Loss: nan | Acc: 9.955% (395/3968)
[Train] Epoch= 68  BatchID= 40 Loss: nan | Acc: 10.137% (532/5248)
[Train] Epoch= 68  BatchID= 50 Loss: nan | Acc: 10.218% (667/6528)
[Train] Epoch= 68  BatchID= 60 Loss: nan | Acc: 10.156% (793/7808)
[Train] Epoch= 68  BatchID= 70 Loss: nan | Acc: 10.211% (928/9088)
[Test] Epoch= 68  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 68  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 68  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 68  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 69
[Train] Epoch= 69  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 69  BatchID= 10 Loss: nan | Acc: 10.085% (142/1408)
[Train] Epoch= 69  BatchID= 20 Loss: nan | Acc: 9.747% (262/2688)
[Train] Epoch= 69  BatchID= 30 Loss: nan | Acc: 10.055% (399/3968)
[Train] Epoch= 69  BatchID= 40 Loss: nan | Acc: 10.137% (532/5248)
[Train] Epoch= 69  BatchID= 50 Loss: nan | Acc: 10.003% (653/6528)
[Train] Epoch= 69  BatchID= 60 Loss: nan | Acc: 10.028% (783/7808)
[Train] Epoch= 69  BatchID= 70 Loss: nan | Acc: 10.255% (932/9088)
[Test] Epoch= 69  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 69  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 69  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 69  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 70
[Train] Epoch= 70  BatchID= 0 Loss: nan | Acc: 12.500% (16/128)
[Train] Epoch= 70  BatchID= 10 Loss: nan | Acc: 9.588% (135/1408)
[Train] Epoch= 70  BatchID= 20 Loss: nan | Acc: 9.710% (261/2688)
[Train] Epoch= 70  BatchID= 30 Loss: nan | Acc: 9.274% (368/3968)
[Train] Epoch= 70  BatchID= 40 Loss: nan | Acc: 9.813% (515/5248)
[Train] Epoch= 70  BatchID= 50 Loss: nan | Acc: 10.218% (667/6528)
[Train] Epoch= 70  BatchID= 60 Loss: nan | Acc: 10.182% (795/7808)
[Train] Epoch= 70  BatchID= 70 Loss: nan | Acc: 10.178% (925/9088)
[Test] Epoch= 70  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 70  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 70  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 70  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 71
[Train] Epoch= 71  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 71  BatchID= 10 Loss: nan | Acc: 11.080% (156/1408)
[Train] Epoch= 71  BatchID= 20 Loss: nan | Acc: 10.677% (287/2688)
[Train] Epoch= 71  BatchID= 30 Loss: nan | Acc: 10.358% (411/3968)
[Train] Epoch= 71  BatchID= 40 Loss: nan | Acc: 10.442% (548/5248)
[Train] Epoch= 71  BatchID= 50 Loss: nan | Acc: 10.447% (682/6528)
[Train] Epoch= 71  BatchID= 60 Loss: nan | Acc: 10.310% (805/7808)
[Train] Epoch= 71  BatchID= 70 Loss: nan | Acc: 10.244% (931/9088)
[Test] Epoch= 71  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 71  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 71  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 71  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 72
[Train] Epoch= 72  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 72  BatchID= 10 Loss: nan | Acc: 10.227% (144/1408)
[Train] Epoch= 72  BatchID= 20 Loss: nan | Acc: 10.379% (279/2688)
[Train] Epoch= 72  BatchID= 30 Loss: nan | Acc: 10.333% (410/3968)
[Train] Epoch= 72  BatchID= 40 Loss: nan | Acc: 10.271% (539/5248)
[Train] Epoch= 72  BatchID= 50 Loss: nan | Acc: 10.218% (667/6528)
[Train] Epoch= 72  BatchID= 60 Loss: nan | Acc: 10.182% (795/7808)
[Train] Epoch= 72  BatchID= 70 Loss: nan | Acc: 10.200% (927/9088)
[Test] Epoch= 72  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 72  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 72  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 72  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 73
[Train] Epoch= 73  BatchID= 0 Loss: nan | Acc: 12.500% (16/128)
[Train] Epoch= 73  BatchID= 10 Loss: nan | Acc: 9.872% (139/1408)
[Train] Epoch= 73  BatchID= 20 Loss: nan | Acc: 10.119% (272/2688)
[Train] Epoch= 73  BatchID= 30 Loss: nan | Acc: 10.761% (427/3968)
[Train] Epoch= 73  BatchID= 40 Loss: nan | Acc: 10.442% (548/5248)
[Train] Epoch= 73  BatchID= 50 Loss: nan | Acc: 10.432% (681/6528)
[Train] Epoch= 73  BatchID= 60 Loss: nan | Acc: 10.464% (817/7808)
[Train] Epoch= 73  BatchID= 70 Loss: nan | Acc: 10.277% (934/9088)
[Test] Epoch= 73  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 73  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 73  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 73  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 74
[Train] Epoch= 74  BatchID= 0 Loss: nan | Acc: 13.281% (17/128)
[Train] Epoch= 74  BatchID= 10 Loss: nan | Acc: 10.227% (144/1408)
[Train] Epoch= 74  BatchID= 20 Loss: nan | Acc: 10.417% (280/2688)
[Train] Epoch= 74  BatchID= 30 Loss: nan | Acc: 10.131% (402/3968)
[Train] Epoch= 74  BatchID= 40 Loss: nan | Acc: 10.194% (535/5248)
[Train] Epoch= 74  BatchID= 50 Loss: nan | Acc: 10.233% (668/6528)
[Train] Epoch= 74  BatchID= 60 Loss: nan | Acc: 9.810% (766/7808)
[Train] Epoch= 74  BatchID= 70 Loss: nan | Acc: 10.079% (916/9088)
[Test] Epoch= 74  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 74  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 74  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 74  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 75
[Train] Epoch= 75  BatchID= 0 Loss: nan | Acc: 9.375% (12/128)
[Train] Epoch= 75  BatchID= 10 Loss: nan | Acc: 10.582% (149/1408)
[Train] Epoch= 75  BatchID= 20 Loss: nan | Acc: 10.603% (285/2688)
[Train] Epoch= 75  BatchID= 30 Loss: nan | Acc: 10.484% (416/3968)
[Train] Epoch= 75  BatchID= 40 Loss: nan | Acc: 10.423% (547/5248)
[Train] Epoch= 75  BatchID= 50 Loss: nan | Acc: 10.401% (679/6528)
[Train] Epoch= 75  BatchID= 60 Loss: nan | Acc: 10.131% (791/7808)
[Train] Epoch= 75  BatchID= 70 Loss: nan | Acc: 10.079% (916/9088)
[Test] Epoch= 75  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 75  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 75  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 75  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 76
[Train] Epoch= 76  BatchID= 0 Loss: nan | Acc: 17.969% (23/128)
[Train] Epoch= 76  BatchID= 10 Loss: nan | Acc: 11.293% (159/1408)
[Train] Epoch= 76  BatchID= 20 Loss: nan | Acc: 10.565% (284/2688)
[Train] Epoch= 76  BatchID= 30 Loss: nan | Acc: 10.307% (409/3968)
[Train] Epoch= 76  BatchID= 40 Loss: nan | Acc: 9.775% (513/5248)
[Train] Epoch= 76  BatchID= 50 Loss: nan | Acc: 10.049% (656/6528)
[Train] Epoch= 76  BatchID= 60 Loss: nan | Acc: 10.207% (797/7808)
[Train] Epoch= 76  BatchID= 70 Loss: nan | Acc: 10.178% (925/9088)
[Test] Epoch= 76  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 76  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 76  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 76  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 77
[Train] Epoch= 77  BatchID= 0 Loss: nan | Acc: 7.812% (10/128)
[Train] Epoch= 77  BatchID= 10 Loss: nan | Acc: 10.085% (142/1408)
[Train] Epoch= 77  BatchID= 20 Loss: nan | Acc: 10.640% (286/2688)
[Train] Epoch= 77  BatchID= 30 Loss: nan | Acc: 10.333% (410/3968)
[Train] Epoch= 77  BatchID= 40 Loss: nan | Acc: 10.404% (546/5248)
[Train] Epoch= 77  BatchID= 50 Loss: nan | Acc: 10.187% (665/6528)
[Train] Epoch= 77  BatchID= 60 Loss: nan | Acc: 10.297% (804/7808)
[Train] Epoch= 77  BatchID= 70 Loss: nan | Acc: 10.189% (926/9088)
[Test] Epoch= 77  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 77  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 77  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 77  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 78
[Train] Epoch= 78  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 78  BatchID= 10 Loss: nan | Acc: 10.511% (148/1408)
[Train] Epoch= 78  BatchID= 20 Loss: nan | Acc: 10.268% (276/2688)
[Train] Epoch= 78  BatchID= 30 Loss: nan | Acc: 10.081% (400/3968)
[Train] Epoch= 78  BatchID= 40 Loss: nan | Acc: 10.080% (529/5248)
[Train] Epoch= 78  BatchID= 50 Loss: nan | Acc: 10.018% (654/6528)
[Train] Epoch= 78  BatchID= 60 Loss: nan | Acc: 10.207% (797/7808)
[Train] Epoch= 78  BatchID= 70 Loss: nan | Acc: 10.123% (920/9088)
[Test] Epoch= 78  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 78  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 78  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 78  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 79
[Train] Epoch= 79  BatchID= 0 Loss: nan | Acc: 9.375% (12/128)
[Train] Epoch= 79  BatchID= 10 Loss: nan | Acc: 11.648% (164/1408)
[Train] Epoch= 79  BatchID= 20 Loss: nan | Acc: 10.826% (291/2688)
[Train] Epoch= 79  BatchID= 30 Loss: nan | Acc: 10.509% (417/3968)
[Train] Epoch= 79  BatchID= 40 Loss: nan | Acc: 10.194% (535/5248)
[Train] Epoch= 79  BatchID= 50 Loss: nan | Acc: 10.371% (677/6528)
[Train] Epoch= 79  BatchID= 60 Loss: nan | Acc: 10.323% (806/7808)
[Train] Epoch= 79  BatchID= 70 Loss: nan | Acc: 10.167% (924/9088)
[Test] Epoch= 79  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 79  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 79  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 79  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 80
[Train] Epoch= 80  BatchID= 0 Loss: nan | Acc: 13.281% (17/128)
[Train] Epoch= 80  BatchID= 10 Loss: nan | Acc: 10.653% (150/1408)
[Train] Epoch= 80  BatchID= 20 Loss: nan | Acc: 10.491% (282/2688)
[Train] Epoch= 80  BatchID= 30 Loss: nan | Acc: 10.761% (427/3968)
[Train] Epoch= 80  BatchID= 40 Loss: nan | Acc: 10.347% (543/5248)
[Train] Epoch= 80  BatchID= 50 Loss: nan | Acc: 10.524% (687/6528)
[Train] Epoch= 80  BatchID= 60 Loss: nan | Acc: 10.169% (794/7808)
[Train] Epoch= 80  BatchID= 70 Loss: nan | Acc: 10.200% (927/9088)
[Test] Epoch= 80  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 80  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 80  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 80  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 81
[Train] Epoch= 81  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 81  BatchID= 10 Loss: nan | Acc: 10.156% (143/1408)
[Train] Epoch= 81  BatchID= 20 Loss: nan | Acc: 9.859% (265/2688)
[Train] Epoch= 81  BatchID= 30 Loss: nan | Acc: 10.106% (401/3968)
[Train] Epoch= 81  BatchID= 40 Loss: nan | Acc: 9.794% (514/5248)
[Train] Epoch= 81  BatchID= 50 Loss: nan | Acc: 9.835% (642/6528)
[Train] Epoch= 81  BatchID= 60 Loss: nan | Acc: 10.041% (784/7808)
[Train] Epoch= 81  BatchID= 70 Loss: nan | Acc: 10.090% (917/9088)
[Test] Epoch= 81  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 81  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 81  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 81  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 82
[Train] Epoch= 82  BatchID= 0 Loss: nan | Acc: 6.250% (8/128)
[Train] Epoch= 82  BatchID= 10 Loss: nan | Acc: 8.878% (125/1408)
[Train] Epoch= 82  BatchID= 20 Loss: nan | Acc: 9.933% (267/2688)
[Train] Epoch= 82  BatchID= 30 Loss: nan | Acc: 9.476% (376/3968)
[Train] Epoch= 82  BatchID= 40 Loss: nan | Acc: 9.680% (508/5248)
[Train] Epoch= 82  BatchID= 50 Loss: nan | Acc: 10.034% (655/6528)
[Train] Epoch= 82  BatchID= 60 Loss: nan | Acc: 9.990% (780/7808)
[Train] Epoch= 82  BatchID= 70 Loss: nan | Acc: 10.167% (924/9088)
[Test] Epoch= 82  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 82  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 82  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 82  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 83
[Train] Epoch= 83  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 83  BatchID= 10 Loss: nan | Acc: 11.222% (158/1408)
[Train] Epoch= 83  BatchID= 20 Loss: nan | Acc: 10.938% (294/2688)
[Train] Epoch= 83  BatchID= 30 Loss: nan | Acc: 10.358% (411/3968)
[Train] Epoch= 83  BatchID= 40 Loss: nan | Acc: 10.118% (531/5248)
[Train] Epoch= 83  BatchID= 50 Loss: nan | Acc: 9.911% (647/6528)
[Train] Epoch= 83  BatchID= 60 Loss: nan | Acc: 9.990% (780/7808)
[Train] Epoch= 83  BatchID= 70 Loss: nan | Acc: 10.123% (920/9088)
[Test] Epoch= 83  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 83  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 83  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 83  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 84
[Train] Epoch= 84  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 84  BatchID= 10 Loss: nan | Acc: 10.369% (146/1408)
[Train] Epoch= 84  BatchID= 20 Loss: nan | Acc: 9.784% (263/2688)
[Train] Epoch= 84  BatchID= 30 Loss: nan | Acc: 9.929% (394/3968)
[Train] Epoch= 84  BatchID= 40 Loss: nan | Acc: 10.004% (525/5248)
[Train] Epoch= 84  BatchID= 50 Loss: nan | Acc: 10.233% (668/6528)
[Train] Epoch= 84  BatchID= 60 Loss: nan | Acc: 10.169% (794/7808)
[Train] Epoch= 84  BatchID= 70 Loss: nan | Acc: 10.024% (911/9088)
[Test] Epoch= 84  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 84  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 84  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 84  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 85
[Train] Epoch= 85  BatchID= 0 Loss: nan | Acc: 12.500% (16/128)
[Train] Epoch= 85  BatchID= 10 Loss: nan | Acc: 11.293% (159/1408)
[Train] Epoch= 85  BatchID= 20 Loss: nan | Acc: 11.012% (296/2688)
[Train] Epoch= 85  BatchID= 30 Loss: nan | Acc: 10.685% (424/3968)
[Train] Epoch= 85  BatchID= 40 Loss: nan | Acc: 10.671% (560/5248)
[Train] Epoch= 85  BatchID= 50 Loss: nan | Acc: 10.539% (688/6528)
[Train] Epoch= 85  BatchID= 60 Loss: nan | Acc: 10.297% (804/7808)
[Train] Epoch= 85  BatchID= 70 Loss: nan | Acc: 10.255% (932/9088)
[Test] Epoch= 85  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 85  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 85  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 85  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 86
[Train] Epoch= 86  BatchID= 0 Loss: nan | Acc: 13.281% (17/128)
[Train] Epoch= 86  BatchID= 10 Loss: nan | Acc: 11.364% (160/1408)
[Train] Epoch= 86  BatchID= 20 Loss: nan | Acc: 10.863% (292/2688)
[Train] Epoch= 86  BatchID= 30 Loss: nan | Acc: 10.761% (427/3968)
[Train] Epoch= 86  BatchID= 40 Loss: nan | Acc: 10.366% (544/5248)
[Train] Epoch= 86  BatchID= 50 Loss: nan | Acc: 10.218% (667/6528)
[Train] Epoch= 86  BatchID= 60 Loss: nan | Acc: 10.336% (807/7808)
[Train] Epoch= 86  BatchID= 70 Loss: nan | Acc: 10.145% (922/9088)
[Test] Epoch= 86  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 86  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 86  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 86  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 87
[Train] Epoch= 87  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 87  BatchID= 10 Loss: nan | Acc: 10.582% (149/1408)
[Train] Epoch= 87  BatchID= 20 Loss: nan | Acc: 10.156% (273/2688)
[Train] Epoch= 87  BatchID= 30 Loss: nan | Acc: 10.408% (413/3968)
[Train] Epoch= 87  BatchID= 40 Loss: nan | Acc: 9.832% (516/5248)
[Train] Epoch= 87  BatchID= 50 Loss: nan | Acc: 9.697% (633/6528)
[Train] Epoch= 87  BatchID= 60 Loss: nan | Acc: 9.874% (771/7808)
[Train] Epoch= 87  BatchID= 70 Loss: nan | Acc: 10.101% (918/9088)
[Test] Epoch= 87  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 87  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 87  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 87  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 88
[Train] Epoch= 88  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 88  BatchID= 10 Loss: nan | Acc: 10.014% (141/1408)
[Train] Epoch= 88  BatchID= 20 Loss: nan | Acc: 10.454% (281/2688)
[Train] Epoch= 88  BatchID= 30 Loss: nan | Acc: 10.660% (423/3968)
[Train] Epoch= 88  BatchID= 40 Loss: nan | Acc: 10.366% (544/5248)
[Train] Epoch= 88  BatchID= 50 Loss: nan | Acc: 10.279% (671/6528)
[Train] Epoch= 88  BatchID= 60 Loss: nan | Acc: 10.387% (811/7808)
[Train] Epoch= 88  BatchID= 70 Loss: nan | Acc: 10.222% (929/9088)
[Test] Epoch= 88  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 88  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 88  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 88  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 89
[Train] Epoch= 89  BatchID= 0 Loss: nan | Acc: 14.844% (19/128)
[Train] Epoch= 89  BatchID= 10 Loss: nan | Acc: 10.440% (147/1408)
[Train] Epoch= 89  BatchID= 20 Loss: nan | Acc: 9.635% (259/2688)
[Train] Epoch= 89  BatchID= 30 Loss: nan | Acc: 9.501% (377/3968)
[Train] Epoch= 89  BatchID= 40 Loss: nan | Acc: 9.928% (521/5248)
[Train] Epoch= 89  BatchID= 50 Loss: nan | Acc: 10.095% (659/6528)
[Train] Epoch= 89  BatchID= 60 Loss: nan | Acc: 10.054% (785/7808)
[Train] Epoch= 89  BatchID= 70 Loss: nan | Acc: 10.046% (913/9088)
[Test] Epoch= 89  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 89  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 89  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 89  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)
