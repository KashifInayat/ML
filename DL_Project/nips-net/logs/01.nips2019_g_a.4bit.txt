==> Preparing data..
==> Building model..
DataParallel(
  (module): ResNet(
    (conv1): InputConv2dLSQ(
      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): LinearLSQ(
      in_features=512, out_features=10, bias=True
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=True)
    )
  )
)
==> Initializing from checkpoint..
==> Preparing data..
==> Building model..
DataParallel(
  (module): ResNet(
    (conv1): InputConv2dLSQ(
      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=False)
    )
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2dLSQ(
          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2dLSQ(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (quan_w): LSQQuantizer (bit=4, is_activation=False)
            (quan_a): LSQQuantizer (bit=4, is_activation=True)
          )
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2dLSQ(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w): LSQQuantizer (bit=4, is_activation=False)
          (quan_a): LSQQuantizer (bit=4, is_activation=True)
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): LinearLSQ(
      in_features=512, out_features=10, bias=True
      (quan_w): LSQQuantizer (bit=8, is_activation=False)
      (quan_a): LSQQuantizer (bit=8, is_activation=True)
    )
  )
)
==> Initializing from checkpoint..

Epoch: 0
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
Initializing step-size value ...
[Train] Epoch= 0  BatchID= 0 Loss: 0.158 | Acc: 96.094% (123/128)
[Train] Epoch= 0  BatchID= 10 Loss: 3.056 | Acc: 27.415% (386/1408)
[Train] Epoch= 0  BatchID= 20 Loss: 2.530 | Acc: 29.464% (792/2688)
[Train] Epoch= 0  BatchID= 30 Loss: 2.287 | Acc: 32.208% (1278/3968)
[Train] Epoch= 0  BatchID= 40 Loss: 2.139 | Acc: 34.280% (1799/5248)
[Train] Epoch= 0  BatchID= 50 Loss: 2.053 | Acc: 35.692% (2330/6528)
[Train] Epoch= 0  BatchID= 60 Loss: 1.982 | Acc: 37.334% (2915/7808)
[Train] Epoch= 0  BatchID= 70 Loss: 1.923 | Acc: 38.875% (3533/9088)
[Test] Epoch= 0  BatchID= 0 Loss: 2.823 | Acc: 10.938% (14/128)
[Test] Epoch= 0  BatchID= 10 Loss: 2.253 | Acc: 32.955% (464/1408)
[Test] Epoch= 0  BatchID= 20 Loss: 2.342 | Acc: 28.832% (775/2688)
[Test] Epoch= 0  BatchID= 30 Loss: 2.461 | Acc: 28.000% (1099/3925)
Saving..
Best accuracy:  28.0

Epoch: 1
[Train] Epoch= 1  BatchID= 0 Loss: 1.414 | Acc: 55.469% (71/128)
[Train] Epoch= 1  BatchID= 10 Loss: 1.509 | Acc: 49.503% (697/1408)
[Train] Epoch= 1  BatchID= 20 Loss: 1.469 | Acc: 51.004% (1371/2688)
[Train] Epoch= 1  BatchID= 30 Loss: 1.490 | Acc: 50.605% (2008/3968)
[Train] Epoch= 1  BatchID= 40 Loss: 1.480 | Acc: 50.762% (2664/5248)
[Train] Epoch= 1  BatchID= 50 Loss: 1.470 | Acc: 51.026% (3331/6528)
[Train] Epoch= 1  BatchID= 60 Loss: 1.461 | Acc: 51.460% (4018/7808)
[Train] Epoch= 1  BatchID= 70 Loss: 1.455 | Acc: 51.629% (4692/9088)
[Test] Epoch= 1  BatchID= 0 Loss: 1.100 | Acc: 87.500% (112/128)
[Test] Epoch= 1  BatchID= 10 Loss: 1.584 | Acc: 46.662% (657/1408)
[Test] Epoch= 1  BatchID= 20 Loss: 1.945 | Acc: 35.119% (944/2688)
[Test] Epoch= 1  BatchID= 30 Loss: 1.999 | Acc: 31.134% (1222/3925)
Saving..
Best accuracy:  31.133757961783438

Epoch: 2
[Train] Epoch= 2  BatchID= 0 Loss: 1.501 | Acc: 47.656% (61/128)
[Train] Epoch= 2  BatchID= 10 Loss: 1.438 | Acc: 52.273% (736/1408)
[Train] Epoch= 2  BatchID= 20 Loss: 1.426 | Acc: 52.381% (1408/2688)
[Train] Epoch= 2  BatchID= 30 Loss: 1.432 | Acc: 51.638% (2049/3968)
[Train] Epoch= 2  BatchID= 40 Loss: 1.426 | Acc: 51.848% (2721/5248)
[Train] Epoch= 2  BatchID= 50 Loss: 1.417 | Acc: 52.359% (3418/6528)
[Train] Epoch= 2  BatchID= 60 Loss: 1.423 | Acc: 52.664% (4112/7808)
[Train] Epoch= 2  BatchID= 70 Loss: 1.424 | Acc: 52.586% (4779/9088)
[Test] Epoch= 2  BatchID= 0 Loss: 0.366 | Acc: 92.188% (118/128)
[Test] Epoch= 2  BatchID= 10 Loss: 2.369 | Acc: 39.631% (558/1408)
[Test] Epoch= 2  BatchID= 20 Loss: 3.233 | Acc: 23.772% (639/2688)
[Test] Epoch= 2  BatchID= 30 Loss: 3.251 | Acc: 20.357% (799/3925)

Epoch: 3
[Train] Epoch= 3  BatchID= 0 Loss: 1.459 | Acc: 53.906% (69/128)
[Train] Epoch= 3  BatchID= 10 Loss: 1.439 | Acc: 54.901% (773/1408)
[Train] Epoch= 3  BatchID= 20 Loss: 1.424 | Acc: 55.097% (1481/2688)
[Train] Epoch= 3  BatchID= 30 Loss: 1.396 | Acc: 55.292% (2194/3968)
[Train] Epoch= 3  BatchID= 40 Loss: 1.397 | Acc: 54.707% (2871/5248)
[Train] Epoch= 3  BatchID= 50 Loss: 1.394 | Acc: 54.534% (3560/6528)
[Train] Epoch= 3  BatchID= 60 Loss: 1.399 | Acc: 54.367% (4245/7808)
[Train] Epoch= 3  BatchID= 70 Loss: 1.390 | Acc: 54.599% (4962/9088)
[Test] Epoch= 3  BatchID= 0 Loss: 4.918 | Acc: 7.812% (10/128)
[Test] Epoch= 3  BatchID= 10 Loss: 4.984 | Acc: 8.026% (113/1408)
[Test] Epoch= 3  BatchID= 20 Loss: 4.883 | Acc: 18.638% (501/2688)
[Test] Epoch= 3  BatchID= 30 Loss: 5.813 | Acc: 14.013% (550/3925)

Epoch: 4
[Train] Epoch= 4  BatchID= 0 Loss: 1.246 | Acc: 64.062% (82/128)
[Train] Epoch= 4  BatchID= 10 Loss: 1.322 | Acc: 56.463% (795/1408)
[Train] Epoch= 4  BatchID= 20 Loss: 1.371 | Acc: 55.022% (1479/2688)
[Train] Epoch= 4  BatchID= 30 Loss: 1.372 | Acc: 55.116% (2187/3968)
[Train] Epoch= 4  BatchID= 40 Loss: 1.384 | Acc: 54.916% (2882/5248)
[Train] Epoch= 4  BatchID= 50 Loss: 1.380 | Acc: 55.316% (3611/6528)
[Train] Epoch= 4  BatchID= 60 Loss: 1.379 | Acc: 55.392% (4325/7808)
[Train] Epoch= 4  BatchID= 70 Loss: 1.383 | Acc: 55.029% (5001/9088)
[Test] Epoch= 4  BatchID= 0 Loss: 2.449 | Acc: 9.375% (12/128)
[Test] Epoch= 4  BatchID= 10 Loss: 2.361 | Acc: 16.690% (235/1408)
[Test] Epoch= 4  BatchID= 20 Loss: 2.076 | Acc: 30.283% (814/2688)
[Test] Epoch= 4  BatchID= 30 Loss: 2.037 | Acc: 32.713% (1284/3925)
Saving..
Best accuracy:  32.71337579617835

Epoch: 5
[Train] Epoch= 5  BatchID= 0 Loss: 1.384 | Acc: 56.250% (72/128)
[Train] Epoch= 5  BatchID= 10 Loss: 1.327 | Acc: 56.179% (791/1408)
[Train] Epoch= 5  BatchID= 20 Loss: 1.321 | Acc: 56.473% (1518/2688)
[Train] Epoch= 5  BatchID= 30 Loss: 1.334 | Acc: 56.099% (2226/3968)
[Train] Epoch= 5  BatchID= 40 Loss: 1.326 | Acc: 56.326% (2956/5248)
[Train] Epoch= 5  BatchID= 50 Loss: 1.323 | Acc: 56.403% (3682/6528)
[Train] Epoch= 5  BatchID= 60 Loss: 1.319 | Acc: 56.378% (4402/7808)
[Train] Epoch= 5  BatchID= 70 Loss: 1.306 | Acc: 56.833% (5165/9088)
[Test] Epoch= 5  BatchID= 0 Loss: 1.547 | Acc: 54.688% (70/128)
[Test] Epoch= 5  BatchID= 10 Loss: 2.047 | Acc: 30.185% (425/1408)
[Test] Epoch= 5  BatchID= 20 Loss: 2.054 | Acc: 34.226% (920/2688)
[Test] Epoch= 5  BatchID= 30 Loss: 1.684 | Acc: 46.318% (1818/3925)
Saving..
Best accuracy:  46.318471337579616

Epoch: 6
[Train] Epoch= 6  BatchID= 0 Loss: 1.329 | Acc: 55.469% (71/128)
[Train] Epoch= 6  BatchID= 10 Loss: 1.278 | Acc: 58.239% (820/1408)
[Train] Epoch= 6  BatchID= 20 Loss: 1.258 | Acc: 58.780% (1580/2688)
[Train] Epoch= 6  BatchID= 30 Loss: 1.264 | Acc: 58.241% (2311/3968)
[Train] Epoch= 6  BatchID= 40 Loss: 1.272 | Acc: 58.213% (3055/5248)
[Train] Epoch= 6  BatchID= 50 Loss: 1.272 | Acc: 58.073% (3791/6528)
[Train] Epoch= 6  BatchID= 60 Loss: 1.273 | Acc: 57.979% (4527/7808)
[Train] Epoch= 6  BatchID= 70 Loss: 1.272 | Acc: 57.901% (5262/9088)
[Test] Epoch= 6  BatchID= 0 Loss: 2.440 | Acc: 23.438% (30/128)
[Test] Epoch= 6  BatchID= 10 Loss: 2.292 | Acc: 29.688% (418/1408)
[Test] Epoch= 6  BatchID= 20 Loss: 1.916 | Acc: 37.314% (1003/2688)
[Test] Epoch= 6  BatchID= 30 Loss: 1.827 | Acc: 41.427% (1626/3925)

Epoch: 7
[Train] Epoch= 7  BatchID= 0 Loss: 1.423 | Acc: 55.469% (71/128)
[Train] Epoch= 7  BatchID= 10 Loss: 1.269 | Acc: 58.665% (826/1408)
[Train] Epoch= 7  BatchID= 20 Loss: 1.399 | Acc: 53.609% (1441/2688)
[Train] Epoch= 7  BatchID= 30 Loss: 1.701 | Acc: 40.071% (1590/3968)
[Train] Epoch= 7  BatchID= 40 Loss: 1.856 | Acc: 32.832% (1723/5248)
[Train] Epoch= 7  BatchID= 50 Loss: 1.946 | Acc: 28.462% (1858/6528)
[Train] Epoch= 7  BatchID= 60 Loss: 2.007 | Acc: 25.256% (1972/7808)
[Train] Epoch= 7  BatchID= 70 Loss: 2.050 | Acc: 23.074% (2097/9088)
[Test] Epoch= 7  BatchID= 0 Loss: 0.473 | Acc: 100.000% (128/128)
[Test] Epoch= 7  BatchID= 10 Loss: 2.299 | Acc: 27.486% (387/1408)
[Test] Epoch= 7  BatchID= 20 Loss: 2.794 | Acc: 14.397% (387/2688)
[Test] Epoch= 7  BatchID= 30 Loss: 3.305 | Acc: 9.860% (387/3925)

Epoch: 8
[Train] Epoch= 8  BatchID= 0 Loss: 2.292 | Acc: 10.156% (13/128)
[Train] Epoch= 8  BatchID= 10 Loss: 2.311 | Acc: 10.298% (145/1408)
[Train] Epoch= 8  BatchID= 20 Loss: 2.309 | Acc: 10.975% (295/2688)
[Train] Epoch= 8  BatchID= 30 Loss: 2.309 | Acc: 10.938% (434/3968)
[Train] Epoch= 8  BatchID= 40 Loss: 2.308 | Acc: 11.014% (578/5248)
[Train] Epoch= 8  BatchID= 50 Loss: 2.308 | Acc: 10.876% (710/6528)
[Train] Epoch= 8  BatchID= 60 Loss: 2.308 | Acc: 10.733% (838/7808)
[Train] Epoch= 8  BatchID= 70 Loss: 2.307 | Acc: 10.607% (964/9088)
[Test] Epoch= 8  BatchID= 0 Loss: 2.437 | Acc: 0.000% (0/128)
[Test] Epoch= 8  BatchID= 10 Loss: 2.378 | Acc: 0.000% (0/1408)
[Test] Epoch= 8  BatchID= 20 Loss: 2.310 | Acc: 15.216% (409/2688)
[Test] Epoch= 8  BatchID= 30 Loss: 2.308 | Acc: 10.420% (409/3925)

Epoch: 9
[Train] Epoch= 9  BatchID= 0 Loss: 2.297 | Acc: 14.844% (19/128)
[Train] Epoch= 9  BatchID= 10 Loss: 2.303 | Acc: 10.298% (145/1408)
[Train] Epoch= 9  BatchID= 20 Loss: 2.305 | Acc: 10.379% (279/2688)
[Train] Epoch= 9  BatchID= 30 Loss: 2.306 | Acc: 10.307% (409/3968)
[Train] Epoch= 9  BatchID= 40 Loss: 2.306 | Acc: 10.137% (532/5248)
[Train] Epoch= 9  BatchID= 50 Loss: 2.306 | Acc: 9.773% (638/6528)
[Train] Epoch= 9  BatchID= 60 Loss: 2.306 | Acc: 9.682% (756/7808)
[Train] Epoch= 9  BatchID= 70 Loss: 2.306 | Acc: 9.826% (893/9088)
[Test] Epoch= 9  BatchID= 0 Loss: 2.311 | Acc: 0.000% (0/128)
[Test] Epoch= 9  BatchID= 10 Loss: 2.296 | Acc: 0.000% (0/1408)
[Test] Epoch= 9  BatchID= 20 Loss: 2.331 | Acc: 0.000% (0/2688)
[Test] Epoch= 9  BatchID= 30 Loss: 2.304 | Acc: 10.166% (399/3925)

Epoch: 10
[Train] Epoch= 10  BatchID= 0 Loss: 2.303 | Acc: 14.844% (19/128)
[Train] Epoch= 10  BatchID= 10 Loss: 2.302 | Acc: 11.932% (168/1408)
[Train] Epoch= 10  BatchID= 20 Loss: 2.303 | Acc: 11.310% (304/2688)
[Train] Epoch= 10  BatchID= 30 Loss: 2.303 | Acc: 10.660% (423/3968)
[Train] Epoch= 10  BatchID= 40 Loss: 2.305 | Acc: 10.309% (541/5248)
[Train] Epoch= 10  BatchID= 50 Loss: 2.305 | Acc: 10.126% (661/6528)
[Train] Epoch= 10  BatchID= 60 Loss: 2.305 | Acc: 9.887% (772/7808)
[Train] Epoch= 10  BatchID= 70 Loss: 2.305 | Acc: 9.925% (902/9088)
[Test] Epoch= 10  BatchID= 0 Loss: 2.118 | Acc: 100.000% (128/128)
[Test] Epoch= 10  BatchID= 10 Loss: 2.275 | Acc: 27.486% (387/1408)
[Test] Epoch= 10  BatchID= 20 Loss: 2.277 | Acc: 14.397% (387/2688)
[Test] Epoch= 10  BatchID= 30 Loss: 2.308 | Acc: 9.860% (387/3925)

Epoch: 11
[Train] Epoch= 11  BatchID= 0 Loss: 2.297 | Acc: 10.938% (14/128)
[Train] Epoch= 11  BatchID= 10 Loss: 2.303 | Acc: 10.369% (146/1408)
[Train] Epoch= 11  BatchID= 20 Loss: 2.302 | Acc: 11.347% (305/2688)
[Train] Epoch= 11  BatchID= 30 Loss: 2.304 | Acc: 10.635% (422/3968)
[Train] Epoch= 11  BatchID= 40 Loss: 2.304 | Acc: 10.518% (552/5248)
[Train] Epoch= 11  BatchID= 50 Loss: 2.303 | Acc: 10.371% (677/6528)
[Train] Epoch= 11  BatchID= 60 Loss: 2.304 | Acc: 10.220% (798/7808)
[Train] Epoch= 11  BatchID= 70 Loss: 2.304 | Acc: 10.233% (930/9088)
[Test] Epoch= 11  BatchID= 0 Loss: 2.360 | Acc: 0.000% (0/128)
[Test] Epoch= 11  BatchID= 10 Loss: 2.291 | Acc: 0.000% (0/1408)
[Test] Epoch= 11  BatchID= 20 Loss: 2.348 | Acc: 13.393% (360/2688)
[Test] Epoch= 11  BatchID= 30 Loss: 2.328 | Acc: 9.911% (389/3925)

Epoch: 12
[Train] Epoch= 12  BatchID= 0 Loss: 2.296 | Acc: 12.500% (16/128)
[Train] Epoch= 12  BatchID= 10 Loss: nan | Acc: 10.795% (152/1408)
[Train] Epoch= 12  BatchID= 20 Loss: nan | Acc: 10.417% (280/2688)
[Train] Epoch= 12  BatchID= 30 Loss: nan | Acc: 10.433% (414/3968)
[Train] Epoch= 12  BatchID= 40 Loss: nan | Acc: 10.347% (543/5248)
[Train] Epoch= 12  BatchID= 50 Loss: nan | Acc: 9.957% (650/6528)
[Train] Epoch= 12  BatchID= 60 Loss: nan | Acc: 10.156% (793/7808)
[Train] Epoch= 12  BatchID= 70 Loss: nan | Acc: 10.156% (923/9088)
[Test] Epoch= 12  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 12  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 12  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 12  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 13
[Train] Epoch= 13  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 13  BatchID= 10 Loss: nan | Acc: 9.659% (136/1408)
[Train] Epoch= 13  BatchID= 20 Loss: nan | Acc: 9.673% (260/2688)
[Train] Epoch= 13  BatchID= 30 Loss: nan | Acc: 10.081% (400/3968)
[Train] Epoch= 13  BatchID= 40 Loss: nan | Acc: 10.080% (529/5248)
[Train] Epoch= 13  BatchID= 50 Loss: nan | Acc: 9.957% (650/6528)
[Train] Epoch= 13  BatchID= 60 Loss: nan | Acc: 10.348% (808/7808)
[Train] Epoch= 13  BatchID= 70 Loss: nan | Acc: 10.211% (928/9088)
[Test] Epoch= 13  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 13  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 13  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 13  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 14
[Train] Epoch= 14  BatchID= 0 Loss: nan | Acc: 13.281% (17/128)
[Train] Epoch= 14  BatchID= 10 Loss: nan | Acc: 11.435% (161/1408)
[Train] Epoch= 14  BatchID= 20 Loss: nan | Acc: 10.491% (282/2688)
[Train] Epoch= 14  BatchID= 30 Loss: nan | Acc: 10.307% (409/3968)
[Train] Epoch= 14  BatchID= 40 Loss: nan | Acc: 10.118% (531/5248)
[Train] Epoch= 14  BatchID= 50 Loss: nan | Acc: 9.926% (648/6528)
[Train] Epoch= 14  BatchID= 60 Loss: nan | Acc: 10.054% (785/7808)
[Train] Epoch= 14  BatchID= 70 Loss: nan | Acc: 10.101% (918/9088)
[Test] Epoch= 14  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 14  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 14  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 14  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 15
[Train] Epoch= 15  BatchID= 0 Loss: nan | Acc: 7.031% (9/128)
[Train] Epoch= 15  BatchID= 10 Loss: nan | Acc: 9.943% (140/1408)
[Train] Epoch= 15  BatchID= 20 Loss: nan | Acc: 9.859% (265/2688)
[Train] Epoch= 15  BatchID= 30 Loss: nan | Acc: 10.685% (424/3968)
[Train] Epoch= 15  BatchID= 40 Loss: nan | Acc: 10.309% (541/5248)
[Train] Epoch= 15  BatchID= 50 Loss: nan | Acc: 10.263% (670/6528)
[Train] Epoch= 15  BatchID= 60 Loss: nan | Acc: 10.336% (807/7808)
[Train] Epoch= 15  BatchID= 70 Loss: nan | Acc: 10.101% (918/9088)
[Test] Epoch= 15  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 15  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 15  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 15  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 16
[Train] Epoch= 16  BatchID= 0 Loss: nan | Acc: 13.281% (17/128)
[Train] Epoch= 16  BatchID= 10 Loss: nan | Acc: 11.080% (156/1408)
[Train] Epoch= 16  BatchID= 20 Loss: nan | Acc: 10.417% (280/2688)
[Train] Epoch= 16  BatchID= 30 Loss: nan | Acc: 10.484% (416/3968)
[Train] Epoch= 16  BatchID= 40 Loss: nan | Acc: 10.366% (544/5248)
[Train] Epoch= 16  BatchID= 50 Loss: nan | Acc: 10.340% (675/6528)
[Train] Epoch= 16  BatchID= 60 Loss: nan | Acc: 10.118% (790/7808)
[Train] Epoch= 16  BatchID= 70 Loss: nan | Acc: 10.167% (924/9088)
[Test] Epoch= 16  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 16  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 16  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 16  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 17
[Train] Epoch= 17  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 17  BatchID= 10 Loss: nan | Acc: 10.582% (149/1408)
[Train] Epoch= 17  BatchID= 20 Loss: nan | Acc: 10.305% (277/2688)
[Train] Epoch= 17  BatchID= 30 Loss: nan | Acc: 10.081% (400/3968)
[Train] Epoch= 17  BatchID= 40 Loss: nan | Acc: 10.213% (536/5248)
[Train] Epoch= 17  BatchID= 50 Loss: nan | Acc: 10.309% (673/6528)
[Train] Epoch= 17  BatchID= 60 Loss: nan | Acc: 10.272% (802/7808)
[Train] Epoch= 17  BatchID= 70 Loss: nan | Acc: 10.189% (926/9088)
[Test] Epoch= 17  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 17  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 17  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 17  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 18
[Train] Epoch= 18  BatchID= 0 Loss: nan | Acc: 9.375% (12/128)
[Train] Epoch= 18  BatchID= 10 Loss: nan | Acc: 10.795% (152/1408)
[Train] Epoch= 18  BatchID= 20 Loss: nan | Acc: 10.640% (286/2688)
[Train] Epoch= 18  BatchID= 30 Loss: nan | Acc: 10.055% (399/3968)
[Train] Epoch= 18  BatchID= 40 Loss: nan | Acc: 9.909% (520/5248)
[Train] Epoch= 18  BatchID= 50 Loss: nan | Acc: 10.049% (656/6528)
[Train] Epoch= 18  BatchID= 60 Loss: nan | Acc: 10.054% (785/7808)
[Train] Epoch= 18  BatchID= 70 Loss: nan | Acc: 10.101% (918/9088)
[Test] Epoch= 18  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 18  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 18  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 18  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 19
[Train] Epoch= 19  BatchID= 0 Loss: nan | Acc: 7.812% (10/128)
[Train] Epoch= 19  BatchID= 10 Loss: nan | Acc: 11.080% (156/1408)
[Train] Epoch= 19  BatchID= 20 Loss: nan | Acc: 10.156% (273/2688)
[Train] Epoch= 19  BatchID= 30 Loss: nan | Acc: 10.408% (413/3968)
[Train] Epoch= 19  BatchID= 40 Loss: nan | Acc: 10.137% (532/5248)
[Train] Epoch= 19  BatchID= 50 Loss: nan | Acc: 10.233% (668/6528)
[Train] Epoch= 19  BatchID= 60 Loss: nan | Acc: 10.182% (795/7808)
[Train] Epoch= 19  BatchID= 70 Loss: nan | Acc: 10.112% (919/9088)
[Test] Epoch= 19  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 19  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 19  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 19  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 20
[Train] Epoch= 20  BatchID= 0 Loss: nan | Acc: 7.812% (10/128)
[Train] Epoch= 20  BatchID= 10 Loss: nan | Acc: 9.091% (128/1408)
[Train] Epoch= 20  BatchID= 20 Loss: nan | Acc: 9.635% (259/2688)
[Train] Epoch= 20  BatchID= 30 Loss: nan | Acc: 9.955% (395/3968)
[Train] Epoch= 20  BatchID= 40 Loss: nan | Acc: 10.023% (526/5248)
[Train] Epoch= 20  BatchID= 50 Loss: nan | Acc: 10.110% (660/6528)
[Train] Epoch= 20  BatchID= 60 Loss: nan | Acc: 10.246% (800/7808)
[Train] Epoch= 20  BatchID= 70 Loss: nan | Acc: 10.211% (928/9088)
[Test] Epoch= 20  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 20  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 20  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 20  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 21
[Train] Epoch= 21  BatchID= 0 Loss: nan | Acc: 7.812% (10/128)
[Train] Epoch= 21  BatchID= 10 Loss: nan | Acc: 10.582% (149/1408)
[Train] Epoch= 21  BatchID= 20 Loss: nan | Acc: 10.454% (281/2688)
[Train] Epoch= 21  BatchID= 30 Loss: nan | Acc: 10.333% (410/3968)
[Train] Epoch= 21  BatchID= 40 Loss: nan | Acc: 10.118% (531/5248)
[Train] Epoch= 21  BatchID= 50 Loss: nan | Acc: 10.095% (659/6528)
[Train] Epoch= 21  BatchID= 60 Loss: nan | Acc: 10.092% (788/7808)
[Train] Epoch= 21  BatchID= 70 Loss: nan | Acc: 10.090% (917/9088)
[Test] Epoch= 21  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 21  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 21  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 21  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 22
[Train] Epoch= 22  BatchID= 0 Loss: nan | Acc: 9.375% (12/128)
[Train] Epoch= 22  BatchID= 10 Loss: nan | Acc: 10.156% (143/1408)
[Train] Epoch= 22  BatchID= 20 Loss: nan | Acc: 9.338% (251/2688)
[Train] Epoch= 22  BatchID= 30 Loss: nan | Acc: 9.703% (385/3968)
[Train] Epoch= 22  BatchID= 40 Loss: nan | Acc: 9.623% (505/5248)
[Train] Epoch= 22  BatchID= 50 Loss: nan | Acc: 9.942% (649/6528)
[Train] Epoch= 22  BatchID= 60 Loss: nan | Acc: 10.079% (787/7808)
[Train] Epoch= 22  BatchID= 70 Loss: nan | Acc: 10.156% (923/9088)
[Test] Epoch= 22  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 22  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 22  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 22  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 23
[Train] Epoch= 23  BatchID= 0 Loss: nan | Acc: 6.250% (8/128)
[Train] Epoch= 23  BatchID= 10 Loss: nan | Acc: 9.801% (138/1408)
[Train] Epoch= 23  BatchID= 20 Loss: nan | Acc: 9.412% (253/2688)
[Train] Epoch= 23  BatchID= 30 Loss: nan | Acc: 9.551% (379/3968)
[Train] Epoch= 23  BatchID= 40 Loss: nan | Acc: 9.832% (516/5248)
[Train] Epoch= 23  BatchID= 50 Loss: nan | Acc: 9.850% (643/6528)
[Train] Epoch= 23  BatchID= 60 Loss: nan | Acc: 10.015% (782/7808)
[Train] Epoch= 23  BatchID= 70 Loss: nan | Acc: 10.101% (918/9088)
[Test] Epoch= 23  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 23  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 23  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 23  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 24
[Train] Epoch= 24  BatchID= 0 Loss: nan | Acc: 6.250% (8/128)
[Train] Epoch= 24  BatchID= 10 Loss: nan | Acc: 10.156% (143/1408)
[Train] Epoch= 24  BatchID= 20 Loss: nan | Acc: 10.528% (283/2688)
[Train] Epoch= 24  BatchID= 30 Loss: nan | Acc: 10.433% (414/3968)
[Train] Epoch= 24  BatchID= 40 Loss: nan | Acc: 10.366% (544/5248)
[Train] Epoch= 24  BatchID= 50 Loss: nan | Acc: 10.156% (663/6528)
[Train] Epoch= 24  BatchID= 60 Loss: nan | Acc: 10.118% (790/7808)
[Train] Epoch= 24  BatchID= 70 Loss: nan | Acc: 10.200% (927/9088)
[Test] Epoch= 24  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 24  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 24  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 24  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 25
[Train] Epoch= 25  BatchID= 0 Loss: nan | Acc: 5.469% (7/128)
[Train] Epoch= 25  BatchID= 10 Loss: nan | Acc: 9.020% (127/1408)
[Train] Epoch= 25  BatchID= 20 Loss: nan | Acc: 9.710% (261/2688)
[Train] Epoch= 25  BatchID= 30 Loss: nan | Acc: 9.249% (367/3968)
[Train] Epoch= 25  BatchID= 40 Loss: nan | Acc: 9.680% (508/5248)
[Train] Epoch= 25  BatchID= 50 Loss: nan | Acc: 9.957% (650/6528)
[Train] Epoch= 25  BatchID= 60 Loss: nan | Acc: 10.220% (798/7808)
[Train] Epoch= 25  BatchID= 70 Loss: nan | Acc: 10.079% (916/9088)
[Test] Epoch= 25  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 25  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 25  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 25  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 26
[Train] Epoch= 26  BatchID= 0 Loss: nan | Acc: 9.375% (12/128)
[Train] Epoch= 26  BatchID= 10 Loss: nan | Acc: 9.162% (129/1408)
[Train] Epoch= 26  BatchID= 20 Loss: nan | Acc: 9.933% (267/2688)
[Train] Epoch= 26  BatchID= 30 Loss: nan | Acc: 10.055% (399/3968)
[Train] Epoch= 26  BatchID= 40 Loss: nan | Acc: 10.366% (544/5248)
[Train] Epoch= 26  BatchID= 50 Loss: nan | Acc: 10.294% (672/6528)
[Train] Epoch= 26  BatchID= 60 Loss: nan | Acc: 10.195% (796/7808)
[Train] Epoch= 26  BatchID= 70 Loss: nan | Acc: 10.255% (932/9088)
[Test] Epoch= 26  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 26  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 26  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 26  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 27
[Train] Epoch= 27  BatchID= 0 Loss: nan | Acc: 6.250% (8/128)
[Train] Epoch= 27  BatchID= 10 Loss: nan | Acc: 9.162% (129/1408)
[Train] Epoch= 27  BatchID= 20 Loss: nan | Acc: 10.268% (276/2688)
[Train] Epoch= 27  BatchID= 30 Loss: nan | Acc: 10.181% (404/3968)
[Train] Epoch= 27  BatchID= 40 Loss: nan | Acc: 10.099% (530/5248)
[Train] Epoch= 27  BatchID= 50 Loss: nan | Acc: 10.034% (655/6528)
[Train] Epoch= 27  BatchID= 60 Loss: nan | Acc: 10.233% (799/7808)
[Train] Epoch= 27  BatchID= 70 Loss: nan | Acc: 10.255% (932/9088)
[Test] Epoch= 27  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 27  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 27  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 27  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 28
[Train] Epoch= 28  BatchID= 0 Loss: nan | Acc: 12.500% (16/128)
[Train] Epoch= 28  BatchID= 10 Loss: nan | Acc: 10.653% (150/1408)
[Train] Epoch= 28  BatchID= 20 Loss: nan | Acc: 10.900% (293/2688)
[Train] Epoch= 28  BatchID= 30 Loss: nan | Acc: 10.232% (406/3968)
[Train] Epoch= 28  BatchID= 40 Loss: nan | Acc: 10.290% (540/5248)
[Train] Epoch= 28  BatchID= 50 Loss: nan | Acc: 10.218% (667/6528)
[Train] Epoch= 28  BatchID= 60 Loss: nan | Acc: 10.195% (796/7808)
[Train] Epoch= 28  BatchID= 70 Loss: nan | Acc: 10.178% (925/9088)
[Test] Epoch= 28  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 28  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 28  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 28  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 29
[Train] Epoch= 29  BatchID= 0 Loss: nan | Acc: 14.844% (19/128)
[Train] Epoch= 29  BatchID= 10 Loss: nan | Acc: 9.943% (140/1408)
[Train] Epoch= 29  BatchID= 20 Loss: nan | Acc: 10.714% (288/2688)
[Train] Epoch= 29  BatchID= 30 Loss: nan | Acc: 9.803% (389/3968)
[Train] Epoch= 29  BatchID= 40 Loss: nan | Acc: 9.966% (523/5248)
[Train] Epoch= 29  BatchID= 50 Loss: nan | Acc: 10.141% (662/6528)
[Train] Epoch= 29  BatchID= 60 Loss: nan | Acc: 10.259% (801/7808)
[Train] Epoch= 29  BatchID= 70 Loss: nan | Acc: 10.244% (931/9088)
[Test] Epoch= 29  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 29  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 29  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 29  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 30
[Train] Epoch= 30  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 30  BatchID= 10 Loss: nan | Acc: 9.659% (136/1408)
[Train] Epoch= 30  BatchID= 20 Loss: nan | Acc: 10.305% (277/2688)
[Train] Epoch= 30  BatchID= 30 Loss: nan | Acc: 9.955% (395/3968)
[Train] Epoch= 30  BatchID= 40 Loss: nan | Acc: 10.328% (542/5248)
[Train] Epoch= 30  BatchID= 50 Loss: nan | Acc: 10.279% (671/6528)
[Train] Epoch= 30  BatchID= 60 Loss: nan | Acc: 10.310% (805/7808)
[Train] Epoch= 30  BatchID= 70 Loss: nan | Acc: 10.200% (927/9088)
[Test] Epoch= 30  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 30  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 30  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 30  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 31
[Train] Epoch= 31  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 31  BatchID= 10 Loss: nan | Acc: 10.014% (141/1408)
[Train] Epoch= 31  BatchID= 20 Loss: nan | Acc: 10.007% (269/2688)
[Train] Epoch= 31  BatchID= 30 Loss: nan | Acc: 9.904% (393/3968)
[Train] Epoch= 31  BatchID= 40 Loss: nan | Acc: 10.118% (531/5248)
[Train] Epoch= 31  BatchID= 50 Loss: nan | Acc: 10.034% (655/6528)
[Train] Epoch= 31  BatchID= 60 Loss: nan | Acc: 10.092% (788/7808)
[Train] Epoch= 31  BatchID= 70 Loss: nan | Acc: 10.189% (926/9088)
[Test] Epoch= 31  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 31  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 31  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 31  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 32
[Train] Epoch= 32  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 32  BatchID= 10 Loss: nan | Acc: 10.582% (149/1408)
[Train] Epoch= 32  BatchID= 20 Loss: nan | Acc: 10.565% (284/2688)
[Train] Epoch= 32  BatchID= 30 Loss: nan | Acc: 10.257% (407/3968)
[Train] Epoch= 32  BatchID= 40 Loss: nan | Acc: 10.423% (547/5248)
[Train] Epoch= 32  BatchID= 50 Loss: nan | Acc: 10.294% (672/6528)
[Train] Epoch= 32  BatchID= 60 Loss: nan | Acc: 10.438% (815/7808)
[Train] Epoch= 32  BatchID= 70 Loss: nan | Acc: 10.200% (927/9088)
[Test] Epoch= 32  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 32  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 32  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 32  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 33
[Train] Epoch= 33  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 33  BatchID= 10 Loss: nan | Acc: 9.233% (130/1408)
[Train] Epoch= 33  BatchID= 20 Loss: nan | Acc: 9.896% (266/2688)
[Train] Epoch= 33  BatchID= 30 Loss: nan | Acc: 10.005% (397/3968)
[Train] Epoch= 33  BatchID= 40 Loss: nan | Acc: 9.928% (521/5248)
[Train] Epoch= 33  BatchID= 50 Loss: nan | Acc: 10.095% (659/6528)
[Train] Epoch= 33  BatchID= 60 Loss: nan | Acc: 10.092% (788/7808)
[Train] Epoch= 33  BatchID= 70 Loss: nan | Acc: 10.277% (934/9088)
[Test] Epoch= 33  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 33  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 33  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 33  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 34
[Train] Epoch= 34  BatchID= 0 Loss: nan | Acc: 10.938% (14/128)
[Train] Epoch= 34  BatchID= 10 Loss: nan | Acc: 10.227% (144/1408)
[Train] Epoch= 34  BatchID= 20 Loss: nan | Acc: 10.193% (274/2688)
[Train] Epoch= 34  BatchID= 30 Loss: nan | Acc: 10.156% (403/3968)
[Train] Epoch= 34  BatchID= 40 Loss: nan | Acc: 10.080% (529/5248)
[Train] Epoch= 34  BatchID= 50 Loss: nan | Acc: 9.926% (648/6528)
[Train] Epoch= 34  BatchID= 60 Loss: nan | Acc: 9.939% (776/7808)
[Train] Epoch= 34  BatchID= 70 Loss: nan | Acc: 10.145% (922/9088)
[Test] Epoch= 34  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 34  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 34  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 34  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 35
[Train] Epoch= 35  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 35  BatchID= 10 Loss: nan | Acc: 10.511% (148/1408)
[Train] Epoch= 35  BatchID= 20 Loss: nan | Acc: 9.747% (262/2688)
[Train] Epoch= 35  BatchID= 30 Loss: nan | Acc: 9.652% (383/3968)
[Train] Epoch= 35  BatchID= 40 Loss: nan | Acc: 9.623% (505/5248)
[Train] Epoch= 35  BatchID= 50 Loss: nan | Acc: 9.666% (631/6528)
[Train] Epoch= 35  BatchID= 60 Loss: nan | Acc: 10.079% (787/7808)
[Train] Epoch= 35  BatchID= 70 Loss: nan | Acc: 10.145% (922/9088)
[Test] Epoch= 35  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 35  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 35  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 35  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 36
[Train] Epoch= 36  BatchID= 0 Loss: nan | Acc: 13.281% (17/128)
[Train] Epoch= 36  BatchID= 10 Loss: nan | Acc: 10.369% (146/1408)
[Train] Epoch= 36  BatchID= 20 Loss: nan | Acc: 10.900% (293/2688)
[Train] Epoch= 36  BatchID= 30 Loss: nan | Acc: 10.711% (425/3968)
[Train] Epoch= 36  BatchID= 40 Loss: nan | Acc: 10.461% (549/5248)
[Train] Epoch= 36  BatchID= 50 Loss: nan | Acc: 10.355% (676/6528)
[Train] Epoch= 36  BatchID= 60 Loss: nan | Acc: 10.361% (809/7808)
[Train] Epoch= 36  BatchID= 70 Loss: nan | Acc: 10.200% (927/9088)
[Test] Epoch= 36  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 36  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 36  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 36  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 37
[Train] Epoch= 37  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 37  BatchID= 10 Loss: nan | Acc: 9.801% (138/1408)
[Train] Epoch= 37  BatchID= 20 Loss: nan | Acc: 10.491% (282/2688)
[Train] Epoch= 37  BatchID= 30 Loss: nan | Acc: 10.988% (436/3968)
[Train] Epoch= 37  BatchID= 40 Loss: nan | Acc: 10.823% (568/5248)
[Train] Epoch= 37  BatchID= 50 Loss: nan | Acc: 10.524% (687/6528)
[Train] Epoch= 37  BatchID= 60 Loss: nan | Acc: 10.310% (805/7808)
[Train] Epoch= 37  BatchID= 70 Loss: nan | Acc: 10.266% (933/9088)
[Test] Epoch= 37  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 37  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 37  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 37  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 38
[Train] Epoch= 38  BatchID= 0 Loss: nan | Acc: 7.031% (9/128)
[Train] Epoch= 38  BatchID= 10 Loss: nan | Acc: 10.369% (146/1408)
[Train] Epoch= 38  BatchID= 20 Loss: nan | Acc: 9.933% (267/2688)
[Train] Epoch= 38  BatchID= 30 Loss: nan | Acc: 10.358% (411/3968)
[Train] Epoch= 38  BatchID= 40 Loss: nan | Acc: 10.137% (532/5248)
[Train] Epoch= 38  BatchID= 50 Loss: nan | Acc: 10.294% (672/6528)
[Train] Epoch= 38  BatchID= 60 Loss: nan | Acc: 10.156% (793/7808)
[Train] Epoch= 38  BatchID= 70 Loss: nan | Acc: 10.178% (925/9088)
[Test] Epoch= 38  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 38  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 38  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 38  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 39
[Train] Epoch= 39  BatchID= 0 Loss: nan | Acc: 9.375% (12/128)
[Train] Epoch= 39  BatchID= 10 Loss: nan | Acc: 10.298% (145/1408)
[Train] Epoch= 39  BatchID= 20 Loss: nan | Acc: 11.235% (302/2688)
[Train] Epoch= 39  BatchID= 30 Loss: nan | Acc: 10.963% (435/3968)
[Train] Epoch= 39  BatchID= 40 Loss: nan | Acc: 10.404% (546/5248)
[Train] Epoch= 39  BatchID= 50 Loss: nan | Acc: 10.126% (661/6528)
[Train] Epoch= 39  BatchID= 60 Loss: nan | Acc: 10.118% (790/7808)
[Train] Epoch= 39  BatchID= 70 Loss: nan | Acc: 10.167% (924/9088)
[Test] Epoch= 39  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 39  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 39  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 39  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 40
[Train] Epoch= 40  BatchID= 0 Loss: nan | Acc: 5.469% (7/128)
[Train] Epoch= 40  BatchID= 10 Loss: nan | Acc: 11.151% (157/1408)
[Train] Epoch= 40  BatchID= 20 Loss: nan | Acc: 10.938% (294/2688)
[Train] Epoch= 40  BatchID= 30 Loss: nan | Acc: 10.610% (421/3968)
[Train] Epoch= 40  BatchID= 40 Loss: nan | Acc: 10.480% (550/5248)
[Train] Epoch= 40  BatchID= 50 Loss: nan | Acc: 10.355% (676/6528)
[Train] Epoch= 40  BatchID= 60 Loss: nan | Acc: 10.374% (810/7808)
[Train] Epoch= 40  BatchID= 70 Loss: nan | Acc: 10.299% (936/9088)
[Test] Epoch= 40  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 40  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 40  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 40  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 41
[Train] Epoch= 41  BatchID= 0 Loss: nan | Acc: 12.500% (16/128)
[Train] Epoch= 41  BatchID= 10 Loss: nan | Acc: 11.932% (168/1408)
[Train] Epoch= 41  BatchID= 20 Loss: nan | Acc: 10.677% (287/2688)
[Train] Epoch= 41  BatchID= 30 Loss: nan | Acc: 10.156% (403/3968)
[Train] Epoch= 41  BatchID= 40 Loss: nan | Acc: 10.252% (538/5248)
[Train] Epoch= 41  BatchID= 50 Loss: nan | Acc: 10.141% (662/6528)
[Train] Epoch= 41  BatchID= 60 Loss: nan | Acc: 10.323% (806/7808)
[Train] Epoch= 41  BatchID= 70 Loss: nan | Acc: 10.134% (921/9088)
[Test] Epoch= 41  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 41  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 41  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 41  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 42
[Train] Epoch= 42  BatchID= 0 Loss: nan | Acc: 16.406% (21/128)
[Train] Epoch= 42  BatchID= 10 Loss: nan | Acc: 10.369% (146/1408)
[Train] Epoch= 42  BatchID= 20 Loss: nan | Acc: 10.045% (270/2688)
[Train] Epoch= 42  BatchID= 30 Loss: nan | Acc: 9.904% (393/3968)
[Train] Epoch= 42  BatchID= 40 Loss: nan | Acc: 9.794% (514/5248)
[Train] Epoch= 42  BatchID= 50 Loss: nan | Acc: 9.865% (644/6528)
[Train] Epoch= 42  BatchID= 60 Loss: nan | Acc: 10.105% (789/7808)
[Train] Epoch= 42  BatchID= 70 Loss: nan | Acc: 10.101% (918/9088)
[Test] Epoch= 42  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 42  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 42  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 42  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 43
[Train] Epoch= 43  BatchID= 0 Loss: nan | Acc: 12.500% (16/128)
[Train] Epoch= 43  BatchID= 10 Loss: nan | Acc: 10.938% (154/1408)
[Train] Epoch= 43  BatchID= 20 Loss: nan | Acc: 10.640% (286/2688)
[Train] Epoch= 43  BatchID= 30 Loss: nan | Acc: 10.181% (404/3968)
[Train] Epoch= 43  BatchID= 40 Loss: nan | Acc: 10.156% (533/5248)
[Train] Epoch= 43  BatchID= 50 Loss: nan | Acc: 10.202% (666/6528)
[Train] Epoch= 43  BatchID= 60 Loss: nan | Acc: 10.131% (791/7808)
[Train] Epoch= 43  BatchID= 70 Loss: nan | Acc: 10.189% (926/9088)
[Test] Epoch= 43  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 43  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 43  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 43  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 44
[Train] Epoch= 44  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 44  BatchID= 10 Loss: nan | Acc: 9.943% (140/1408)
[Train] Epoch= 44  BatchID= 20 Loss: nan | Acc: 9.375% (252/2688)
[Train] Epoch= 44  BatchID= 30 Loss: nan | Acc: 9.551% (379/3968)
[Train] Epoch= 44  BatchID= 40 Loss: nan | Acc: 9.623% (505/5248)
[Train] Epoch= 44  BatchID= 50 Loss: nan | Acc: 10.034% (655/6528)
[Train] Epoch= 44  BatchID= 60 Loss: nan | Acc: 10.272% (802/7808)
[Train] Epoch= 44  BatchID= 70 Loss: nan | Acc: 10.233% (930/9088)
[Test] Epoch= 44  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 44  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 44  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 44  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 45
[Train] Epoch= 45  BatchID= 0 Loss: nan | Acc: 15.625% (20/128)
[Train] Epoch= 45  BatchID= 10 Loss: nan | Acc: 12.713% (179/1408)
[Train] Epoch= 45  BatchID= 20 Loss: nan | Acc: 11.086% (298/2688)
[Train] Epoch= 45  BatchID= 30 Loss: nan | Acc: 10.635% (422/3968)
[Train] Epoch= 45  BatchID= 40 Loss: nan | Acc: 10.404% (546/5248)
[Train] Epoch= 45  BatchID= 50 Loss: nan | Acc: 10.095% (659/6528)
[Train] Epoch= 45  BatchID= 60 Loss: nan | Acc: 10.195% (796/7808)
[Train] Epoch= 45  BatchID= 70 Loss: nan | Acc: 10.134% (921/9088)
[Test] Epoch= 45  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 45  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 45  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 45  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 46
[Train] Epoch= 46  BatchID= 0 Loss: nan | Acc: 7.031% (9/128)
[Train] Epoch= 46  BatchID= 10 Loss: nan | Acc: 9.801% (138/1408)
[Train] Epoch= 46  BatchID= 20 Loss: nan | Acc: 9.933% (267/2688)
[Train] Epoch= 46  BatchID= 30 Loss: nan | Acc: 10.005% (397/3968)
[Train] Epoch= 46  BatchID= 40 Loss: nan | Acc: 10.137% (532/5248)
[Train] Epoch= 46  BatchID= 50 Loss: nan | Acc: 10.110% (660/6528)
[Train] Epoch= 46  BatchID= 60 Loss: nan | Acc: 10.143% (792/7808)
[Train] Epoch= 46  BatchID= 70 Loss: nan | Acc: 10.156% (923/9088)
[Test] Epoch= 46  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 46  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 46  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 46  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 47
[Train] Epoch= 47  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 47  BatchID= 10 Loss: nan | Acc: 10.227% (144/1408)
[Train] Epoch= 47  BatchID= 20 Loss: nan | Acc: 10.082% (271/2688)
[Train] Epoch= 47  BatchID= 30 Loss: nan | Acc: 10.156% (403/3968)
[Train] Epoch= 47  BatchID= 40 Loss: nan | Acc: 10.347% (543/5248)
[Train] Epoch= 47  BatchID= 50 Loss: nan | Acc: 10.126% (661/6528)
[Train] Epoch= 47  BatchID= 60 Loss: nan | Acc: 10.054% (785/7808)
[Train] Epoch= 47  BatchID= 70 Loss: nan | Acc: 10.233% (930/9088)
[Test] Epoch= 47  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 47  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 47  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 47  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 48
[Train] Epoch= 48  BatchID= 0 Loss: nan | Acc: 7.031% (9/128)
[Train] Epoch= 48  BatchID= 10 Loss: nan | Acc: 9.304% (131/1408)
[Train] Epoch= 48  BatchID= 20 Loss: nan | Acc: 8.891% (239/2688)
[Train] Epoch= 48  BatchID= 30 Loss: nan | Acc: 9.451% (375/3968)
[Train] Epoch= 48  BatchID= 40 Loss: nan | Acc: 9.851% (517/5248)
[Train] Epoch= 48  BatchID= 50 Loss: nan | Acc: 10.095% (659/6528)
[Train] Epoch= 48  BatchID= 60 Loss: nan | Acc: 10.118% (790/7808)
[Train] Epoch= 48  BatchID= 70 Loss: nan | Acc: 10.134% (921/9088)
[Test] Epoch= 48  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 48  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 48  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 48  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 49
[Train] Epoch= 49  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 49  BatchID= 10 Loss: nan | Acc: 10.582% (149/1408)
[Train] Epoch= 49  BatchID= 20 Loss: nan | Acc: 10.826% (291/2688)
[Train] Epoch= 49  BatchID= 30 Loss: nan | Acc: 10.333% (410/3968)
[Train] Epoch= 49  BatchID= 40 Loss: nan | Acc: 10.366% (544/5248)
[Train] Epoch= 49  BatchID= 50 Loss: nan | Acc: 10.156% (663/6528)
[Train] Epoch= 49  BatchID= 60 Loss: nan | Acc: 10.233% (799/7808)
[Train] Epoch= 49  BatchID= 70 Loss: nan | Acc: 10.145% (922/9088)
[Test] Epoch= 49  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 49  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 49  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 49  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 50
[Train] Epoch= 50  BatchID= 0 Loss: nan | Acc: 13.281% (17/128)
[Train] Epoch= 50  BatchID= 10 Loss: nan | Acc: 10.938% (154/1408)
[Train] Epoch= 50  BatchID= 20 Loss: nan | Acc: 10.491% (282/2688)
[Train] Epoch= 50  BatchID= 30 Loss: nan | Acc: 10.181% (404/3968)
[Train] Epoch= 50  BatchID= 40 Loss: nan | Acc: 10.118% (531/5248)
[Train] Epoch= 50  BatchID= 50 Loss: nan | Acc: 10.064% (657/6528)
[Train] Epoch= 50  BatchID= 60 Loss: nan | Acc: 10.067% (786/7808)
[Train] Epoch= 50  BatchID= 70 Loss: nan | Acc: 10.090% (917/9088)
[Test] Epoch= 50  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 50  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 50  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 50  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 51
[Train] Epoch= 51  BatchID= 0 Loss: nan | Acc: 16.406% (21/128)
[Train] Epoch= 51  BatchID= 10 Loss: nan | Acc: 10.938% (154/1408)
[Train] Epoch= 51  BatchID= 20 Loss: nan | Acc: 10.751% (289/2688)
[Train] Epoch= 51  BatchID= 30 Loss: nan | Acc: 10.333% (410/3968)
[Train] Epoch= 51  BatchID= 40 Loss: nan | Acc: 10.175% (534/5248)
[Train] Epoch= 51  BatchID= 50 Loss: nan | Acc: 10.064% (657/6528)
[Train] Epoch= 51  BatchID= 60 Loss: nan | Acc: 10.182% (795/7808)
[Train] Epoch= 51  BatchID= 70 Loss: nan | Acc: 10.200% (927/9088)
[Test] Epoch= 51  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 51  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 51  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 51  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 52
[Train] Epoch= 52  BatchID= 0 Loss: nan | Acc: 7.812% (10/128)
[Train] Epoch= 52  BatchID= 10 Loss: nan | Acc: 9.162% (129/1408)
[Train] Epoch= 52  BatchID= 20 Loss: nan | Acc: 9.487% (255/2688)
[Train] Epoch= 52  BatchID= 30 Loss: nan | Acc: 9.602% (381/3968)
[Train] Epoch= 52  BatchID= 40 Loss: nan | Acc: 9.947% (522/5248)
[Train] Epoch= 52  BatchID= 50 Loss: nan | Acc: 10.248% (669/6528)
[Train] Epoch= 52  BatchID= 60 Loss: nan | Acc: 10.310% (805/7808)
[Train] Epoch= 52  BatchID= 70 Loss: nan | Acc: 10.277% (934/9088)
[Test] Epoch= 52  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 52  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 52  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 52  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 53
[Train] Epoch= 53  BatchID= 0 Loss: nan | Acc: 2.344% (3/128)
[Train] Epoch= 53  BatchID= 10 Loss: nan | Acc: 8.807% (124/1408)
[Train] Epoch= 53  BatchID= 20 Loss: nan | Acc: 9.598% (258/2688)
[Train] Epoch= 53  BatchID= 30 Loss: nan | Acc: 9.778% (388/3968)
[Train] Epoch= 53  BatchID= 40 Loss: nan | Acc: 10.328% (542/5248)
[Train] Epoch= 53  BatchID= 50 Loss: nan | Acc: 10.110% (660/6528)
[Train] Epoch= 53  BatchID= 60 Loss: nan | Acc: 10.169% (794/7808)
[Train] Epoch= 53  BatchID= 70 Loss: nan | Acc: 10.189% (926/9088)
[Test] Epoch= 53  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 53  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 53  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 53  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 54
[Train] Epoch= 54  BatchID= 0 Loss: nan | Acc: 13.281% (17/128)
[Train] Epoch= 54  BatchID= 10 Loss: nan | Acc: 11.222% (158/1408)
[Train] Epoch= 54  BatchID= 20 Loss: nan | Acc: 10.491% (282/2688)
[Train] Epoch= 54  BatchID= 30 Loss: nan | Acc: 10.383% (412/3968)
[Train] Epoch= 54  BatchID= 40 Loss: nan | Acc: 10.137% (532/5248)
[Train] Epoch= 54  BatchID= 50 Loss: nan | Acc: 10.126% (661/6528)
[Train] Epoch= 54  BatchID= 60 Loss: nan | Acc: 10.233% (799/7808)
[Train] Epoch= 54  BatchID= 70 Loss: nan | Acc: 10.244% (931/9088)
[Test] Epoch= 54  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 54  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 54  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 54  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 55
[Train] Epoch= 55  BatchID= 0 Loss: nan | Acc: 10.938% (14/128)
[Train] Epoch= 55  BatchID= 10 Loss: nan | Acc: 11.222% (158/1408)
[Train] Epoch= 55  BatchID= 20 Loss: nan | Acc: 10.863% (292/2688)
[Train] Epoch= 55  BatchID= 30 Loss: nan | Acc: 10.635% (422/3968)
[Train] Epoch= 55  BatchID= 40 Loss: nan | Acc: 10.728% (563/5248)
[Train] Epoch= 55  BatchID= 50 Loss: nan | Acc: 10.386% (678/6528)
[Train] Epoch= 55  BatchID= 60 Loss: nan | Acc: 10.233% (799/7808)
[Train] Epoch= 55  BatchID= 70 Loss: nan | Acc: 10.211% (928/9088)
[Test] Epoch= 55  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 55  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 55  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 55  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 56
[Train] Epoch= 56  BatchID= 0 Loss: nan | Acc: 5.469% (7/128)
[Train] Epoch= 56  BatchID= 10 Loss: nan | Acc: 10.440% (147/1408)
[Train] Epoch= 56  BatchID= 20 Loss: nan | Acc: 10.305% (277/2688)
[Train] Epoch= 56  BatchID= 30 Loss: nan | Acc: 9.980% (396/3968)
[Train] Epoch= 56  BatchID= 40 Loss: nan | Acc: 10.042% (527/5248)
[Train] Epoch= 56  BatchID= 50 Loss: nan | Acc: 10.110% (660/6528)
[Train] Epoch= 56  BatchID= 60 Loss: nan | Acc: 10.195% (796/7808)
[Train] Epoch= 56  BatchID= 70 Loss: nan | Acc: 10.057% (914/9088)
[Test] Epoch= 56  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 56  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 56  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 56  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 57
[Train] Epoch= 57  BatchID= 0 Loss: nan | Acc: 3.906% (5/128)
[Train] Epoch= 57  BatchID= 10 Loss: nan | Acc: 9.801% (138/1408)
[Train] Epoch= 57  BatchID= 20 Loss: nan | Acc: 10.045% (270/2688)
[Train] Epoch= 57  BatchID= 30 Loss: nan | Acc: 10.005% (397/3968)
[Train] Epoch= 57  BatchID= 40 Loss: nan | Acc: 10.252% (538/5248)
[Train] Epoch= 57  BatchID= 50 Loss: nan | Acc: 10.355% (676/6528)
[Train] Epoch= 57  BatchID= 60 Loss: nan | Acc: 10.246% (800/7808)
[Train] Epoch= 57  BatchID= 70 Loss: nan | Acc: 10.156% (923/9088)
[Test] Epoch= 57  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 57  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 57  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 57  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 58
[Train] Epoch= 58  BatchID= 0 Loss: nan | Acc: 7.812% (10/128)
[Train] Epoch= 58  BatchID= 10 Loss: nan | Acc: 10.085% (142/1408)
[Train] Epoch= 58  BatchID= 20 Loss: nan | Acc: 9.673% (260/2688)
[Train] Epoch= 58  BatchID= 30 Loss: nan | Acc: 10.282% (408/3968)
[Train] Epoch= 58  BatchID= 40 Loss: nan | Acc: 10.575% (555/5248)
[Train] Epoch= 58  BatchID= 50 Loss: nan | Acc: 10.463% (683/6528)
[Train] Epoch= 58  BatchID= 60 Loss: nan | Acc: 10.348% (808/7808)
[Train] Epoch= 58  BatchID= 70 Loss: nan | Acc: 10.222% (929/9088)
[Test] Epoch= 58  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 58  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 58  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 58  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 59
[Train] Epoch= 59  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 59  BatchID= 10 Loss: nan | Acc: 10.227% (144/1408)
[Train] Epoch= 59  BatchID= 20 Loss: nan | Acc: 10.231% (275/2688)
[Train] Epoch= 59  BatchID= 30 Loss: nan | Acc: 9.879% (392/3968)
[Train] Epoch= 59  BatchID= 40 Loss: nan | Acc: 10.194% (535/5248)
[Train] Epoch= 59  BatchID= 50 Loss: nan | Acc: 10.172% (664/6528)
[Train] Epoch= 59  BatchID= 60 Loss: nan | Acc: 10.079% (787/7808)
[Train] Epoch= 59  BatchID= 70 Loss: nan | Acc: 10.167% (924/9088)
[Test] Epoch= 59  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 59  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 59  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 59  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 60
[Train] Epoch= 60  BatchID= 0 Loss: nan | Acc: 7.812% (10/128)
[Train] Epoch= 60  BatchID= 10 Loss: nan | Acc: 9.233% (130/1408)
[Train] Epoch= 60  BatchID= 20 Loss: nan | Acc: 9.598% (258/2688)
[Train] Epoch= 60  BatchID= 30 Loss: nan | Acc: 10.232% (406/3968)
[Train] Epoch= 60  BatchID= 40 Loss: nan | Acc: 10.252% (538/5248)
[Train] Epoch= 60  BatchID= 50 Loss: nan | Acc: 10.110% (660/6528)
[Train] Epoch= 60  BatchID= 60 Loss: nan | Acc: 10.156% (793/7808)
[Train] Epoch= 60  BatchID= 70 Loss: nan | Acc: 10.222% (929/9088)
[Test] Epoch= 60  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 60  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 60  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 60  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 61
[Train] Epoch= 61  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 61  BatchID= 10 Loss: nan | Acc: 9.091% (128/1408)
[Train] Epoch= 61  BatchID= 20 Loss: nan | Acc: 9.784% (263/2688)
[Train] Epoch= 61  BatchID= 30 Loss: nan | Acc: 10.005% (397/3968)
[Train] Epoch= 61  BatchID= 40 Loss: nan | Acc: 10.175% (534/5248)
[Train] Epoch= 61  BatchID= 50 Loss: nan | Acc: 10.110% (660/6528)
[Train] Epoch= 61  BatchID= 60 Loss: nan | Acc: 10.182% (795/7808)
[Train] Epoch= 61  BatchID= 70 Loss: nan | Acc: 10.200% (927/9088)
[Test] Epoch= 61  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 61  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 61  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 61  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 62
[Train] Epoch= 62  BatchID= 0 Loss: nan | Acc: 12.500% (16/128)
[Train] Epoch= 62  BatchID= 10 Loss: nan | Acc: 10.511% (148/1408)
[Train] Epoch= 62  BatchID= 20 Loss: nan | Acc: 11.086% (298/2688)
[Train] Epoch= 62  BatchID= 30 Loss: nan | Acc: 10.685% (424/3968)
[Train] Epoch= 62  BatchID= 40 Loss: nan | Acc: 10.366% (544/5248)
[Train] Epoch= 62  BatchID= 50 Loss: nan | Acc: 10.386% (678/6528)
[Train] Epoch= 62  BatchID= 60 Loss: nan | Acc: 10.310% (805/7808)
[Train] Epoch= 62  BatchID= 70 Loss: nan | Acc: 10.200% (927/9088)
[Test] Epoch= 62  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 62  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 62  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 62  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 63
[Train] Epoch= 63  BatchID= 0 Loss: nan | Acc: 7.812% (10/128)
[Train] Epoch= 63  BatchID= 10 Loss: nan | Acc: 10.938% (154/1408)
[Train] Epoch= 63  BatchID= 20 Loss: nan | Acc: 10.900% (293/2688)
[Train] Epoch= 63  BatchID= 30 Loss: nan | Acc: 10.660% (423/3968)
[Train] Epoch= 63  BatchID= 40 Loss: nan | Acc: 10.213% (536/5248)
[Train] Epoch= 63  BatchID= 50 Loss: nan | Acc: 10.432% (681/6528)
[Train] Epoch= 63  BatchID= 60 Loss: nan | Acc: 10.233% (799/7808)
[Train] Epoch= 63  BatchID= 70 Loss: nan | Acc: 10.222% (929/9088)
[Test] Epoch= 63  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 63  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 63  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 63  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 64
[Train] Epoch= 64  BatchID= 0 Loss: nan | Acc: 7.812% (10/128)
[Train] Epoch= 64  BatchID= 10 Loss: nan | Acc: 10.156% (143/1408)
[Train] Epoch= 64  BatchID= 20 Loss: nan | Acc: 10.268% (276/2688)
[Train] Epoch= 64  BatchID= 30 Loss: nan | Acc: 10.081% (400/3968)
[Train] Epoch= 64  BatchID= 40 Loss: nan | Acc: 10.080% (529/5248)
[Train] Epoch= 64  BatchID= 50 Loss: nan | Acc: 10.110% (660/6528)
[Train] Epoch= 64  BatchID= 60 Loss: nan | Acc: 10.284% (803/7808)
[Train] Epoch= 64  BatchID= 70 Loss: nan | Acc: 10.233% (930/9088)
[Test] Epoch= 64  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 64  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 64  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 64  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 65
[Train] Epoch= 65  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 65  BatchID= 10 Loss: nan | Acc: 9.233% (130/1408)
[Train] Epoch= 65  BatchID= 20 Loss: nan | Acc: 9.821% (264/2688)
[Train] Epoch= 65  BatchID= 30 Loss: nan | Acc: 10.131% (402/3968)
[Train] Epoch= 65  BatchID= 40 Loss: nan | Acc: 10.061% (528/5248)
[Train] Epoch= 65  BatchID= 50 Loss: nan | Acc: 10.141% (662/6528)
[Train] Epoch= 65  BatchID= 60 Loss: nan | Acc: 10.259% (801/7808)
[Train] Epoch= 65  BatchID= 70 Loss: nan | Acc: 10.233% (930/9088)
[Test] Epoch= 65  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 65  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 65  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 65  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 66
[Train] Epoch= 66  BatchID= 0 Loss: nan | Acc: 9.375% (12/128)
[Train] Epoch= 66  BatchID= 10 Loss: nan | Acc: 9.801% (138/1408)
[Train] Epoch= 66  BatchID= 20 Loss: nan | Acc: 9.338% (251/2688)
[Train] Epoch= 66  BatchID= 30 Loss: nan | Acc: 9.501% (377/3968)
[Train] Epoch= 66  BatchID= 40 Loss: nan | Acc: 9.813% (515/5248)
[Train] Epoch= 66  BatchID= 50 Loss: nan | Acc: 9.911% (647/6528)
[Train] Epoch= 66  BatchID= 60 Loss: nan | Acc: 10.015% (782/7808)
[Train] Epoch= 66  BatchID= 70 Loss: nan | Acc: 10.200% (927/9088)
[Test] Epoch= 66  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 66  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 66  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 66  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 67
[Train] Epoch= 67  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 67  BatchID= 10 Loss: nan | Acc: 10.511% (148/1408)
[Train] Epoch= 67  BatchID= 20 Loss: nan | Acc: 9.635% (259/2688)
[Train] Epoch= 67  BatchID= 30 Loss: nan | Acc: 9.854% (391/3968)
[Train] Epoch= 67  BatchID= 40 Loss: nan | Acc: 10.099% (530/5248)
[Train] Epoch= 67  BatchID= 50 Loss: nan | Acc: 10.141% (662/6528)
[Train] Epoch= 67  BatchID= 60 Loss: nan | Acc: 10.156% (793/7808)
[Train] Epoch= 67  BatchID= 70 Loss: nan | Acc: 10.178% (925/9088)
[Test] Epoch= 67  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 67  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 67  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 67  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 68
[Train] Epoch= 68  BatchID= 0 Loss: nan | Acc: 10.938% (14/128)
[Train] Epoch= 68  BatchID= 10 Loss: nan | Acc: 9.943% (140/1408)
[Train] Epoch= 68  BatchID= 20 Loss: nan | Acc: 10.268% (276/2688)
[Train] Epoch= 68  BatchID= 30 Loss: nan | Acc: 10.207% (405/3968)
[Train] Epoch= 68  BatchID= 40 Loss: nan | Acc: 10.099% (530/5248)
[Train] Epoch= 68  BatchID= 50 Loss: nan | Acc: 10.325% (674/6528)
[Train] Epoch= 68  BatchID= 60 Loss: nan | Acc: 10.156% (793/7808)
[Train] Epoch= 68  BatchID= 70 Loss: nan | Acc: 10.167% (924/9088)
[Test] Epoch= 68  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 68  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 68  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 68  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 69
[Train] Epoch= 69  BatchID= 0 Loss: nan | Acc: 9.375% (12/128)
[Train] Epoch= 69  BatchID= 10 Loss: nan | Acc: 10.298% (145/1408)
[Train] Epoch= 69  BatchID= 20 Loss: nan | Acc: 9.710% (261/2688)
[Train] Epoch= 69  BatchID= 30 Loss: nan | Acc: 9.879% (392/3968)
[Train] Epoch= 69  BatchID= 40 Loss: nan | Acc: 9.870% (518/5248)
[Train] Epoch= 69  BatchID= 50 Loss: nan | Acc: 10.080% (658/6528)
[Train] Epoch= 69  BatchID= 60 Loss: nan | Acc: 10.131% (791/7808)
[Train] Epoch= 69  BatchID= 70 Loss: nan | Acc: 10.134% (921/9088)
[Test] Epoch= 69  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 69  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 69  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 69  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 70
[Train] Epoch= 70  BatchID= 0 Loss: nan | Acc: 7.812% (10/128)
[Train] Epoch= 70  BatchID= 10 Loss: nan | Acc: 9.375% (132/1408)
[Train] Epoch= 70  BatchID= 20 Loss: nan | Acc: 9.635% (259/2688)
[Train] Epoch= 70  BatchID= 30 Loss: nan | Acc: 10.030% (398/3968)
[Train] Epoch= 70  BatchID= 40 Loss: nan | Acc: 9.851% (517/5248)
[Train] Epoch= 70  BatchID= 50 Loss: nan | Acc: 10.049% (656/6528)
[Train] Epoch= 70  BatchID= 60 Loss: nan | Acc: 10.054% (785/7808)
[Train] Epoch= 70  BatchID= 70 Loss: nan | Acc: 10.189% (926/9088)
[Test] Epoch= 70  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 70  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 70  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 70  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 71
[Train] Epoch= 71  BatchID= 0 Loss: nan | Acc: 9.375% (12/128)
[Train] Epoch= 71  BatchID= 10 Loss: nan | Acc: 10.795% (152/1408)
[Train] Epoch= 71  BatchID= 20 Loss: nan | Acc: 10.789% (290/2688)
[Train] Epoch= 71  BatchID= 30 Loss: nan | Acc: 10.786% (428/3968)
[Train] Epoch= 71  BatchID= 40 Loss: nan | Acc: 10.595% (556/5248)
[Train] Epoch= 71  BatchID= 50 Loss: nan | Acc: 10.325% (674/6528)
[Train] Epoch= 71  BatchID= 60 Loss: nan | Acc: 10.156% (793/7808)
[Train] Epoch= 71  BatchID= 70 Loss: nan | Acc: 10.244% (931/9088)
[Test] Epoch= 71  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 71  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 71  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 71  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 72
[Train] Epoch= 72  BatchID= 0 Loss: nan | Acc: 9.375% (12/128)
[Train] Epoch= 72  BatchID= 10 Loss: nan | Acc: 10.014% (141/1408)
[Train] Epoch= 72  BatchID= 20 Loss: nan | Acc: 10.379% (279/2688)
[Train] Epoch= 72  BatchID= 30 Loss: nan | Acc: 10.005% (397/3968)
[Train] Epoch= 72  BatchID= 40 Loss: nan | Acc: 10.099% (530/5248)
[Train] Epoch= 72  BatchID= 50 Loss: nan | Acc: 10.034% (655/6528)
[Train] Epoch= 72  BatchID= 60 Loss: nan | Acc: 10.156% (793/7808)
[Train] Epoch= 72  BatchID= 70 Loss: nan | Acc: 10.178% (925/9088)
[Test] Epoch= 72  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 72  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 72  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 72  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 73
[Train] Epoch= 73  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 73  BatchID= 10 Loss: nan | Acc: 11.009% (155/1408)
[Train] Epoch= 73  BatchID= 20 Loss: nan | Acc: 10.603% (285/2688)
[Train] Epoch= 73  BatchID= 30 Loss: nan | Acc: 10.005% (397/3968)
[Train] Epoch= 73  BatchID= 40 Loss: nan | Acc: 10.042% (527/5248)
[Train] Epoch= 73  BatchID= 50 Loss: nan | Acc: 9.972% (651/6528)
[Train] Epoch= 73  BatchID= 60 Loss: nan | Acc: 10.067% (786/7808)
[Train] Epoch= 73  BatchID= 70 Loss: nan | Acc: 10.112% (919/9088)
[Test] Epoch= 73  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 73  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 73  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 73  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 74
[Train] Epoch= 74  BatchID= 0 Loss: nan | Acc: 9.375% (12/128)
[Train] Epoch= 74  BatchID= 10 Loss: nan | Acc: 10.156% (143/1408)
[Train] Epoch= 74  BatchID= 20 Loss: nan | Acc: 9.821% (264/2688)
[Train] Epoch= 74  BatchID= 30 Loss: nan | Acc: 10.181% (404/3968)
[Train] Epoch= 74  BatchID= 40 Loss: nan | Acc: 10.347% (543/5248)
[Train] Epoch= 74  BatchID= 50 Loss: nan | Acc: 10.187% (665/6528)
[Train] Epoch= 74  BatchID= 60 Loss: nan | Acc: 10.220% (798/7808)
[Train] Epoch= 74  BatchID= 70 Loss: nan | Acc: 10.189% (926/9088)
[Test] Epoch= 74  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 74  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 74  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 74  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 75
[Train] Epoch= 75  BatchID= 0 Loss: nan | Acc: 8.594% (11/128)
[Train] Epoch= 75  BatchID= 10 Loss: nan | Acc: 10.227% (144/1408)
[Train] Epoch= 75  BatchID= 20 Loss: nan | Acc: 9.449% (254/2688)
[Train] Epoch= 75  BatchID= 30 Loss: nan | Acc: 9.627% (382/3968)
[Train] Epoch= 75  BatchID= 40 Loss: nan | Acc: 9.832% (516/5248)
[Train] Epoch= 75  BatchID= 50 Loss: nan | Acc: 9.789% (639/6528)
[Train] Epoch= 75  BatchID= 60 Loss: nan | Acc: 9.939% (776/7808)
[Train] Epoch= 75  BatchID= 70 Loss: nan | Acc: 10.046% (913/9088)
[Test] Epoch= 75  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 75  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 75  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 75  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 76
[Train] Epoch= 76  BatchID= 0 Loss: nan | Acc: 7.812% (10/128)
[Train] Epoch= 76  BatchID= 10 Loss: nan | Acc: 9.091% (128/1408)
[Train] Epoch= 76  BatchID= 20 Loss: nan | Acc: 10.193% (274/2688)
[Train] Epoch= 76  BatchID= 30 Loss: nan | Acc: 10.484% (416/3968)
[Train] Epoch= 76  BatchID= 40 Loss: nan | Acc: 10.556% (554/5248)
[Train] Epoch= 76  BatchID= 50 Loss: nan | Acc: 10.555% (689/6528)
[Train] Epoch= 76  BatchID= 60 Loss: nan | Acc: 10.284% (803/7808)
[Train] Epoch= 76  BatchID= 70 Loss: nan | Acc: 10.189% (926/9088)
[Test] Epoch= 76  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 76  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 76  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 76  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 77
[Train] Epoch= 77  BatchID= 0 Loss: nan | Acc: 7.031% (9/128)
[Train] Epoch= 77  BatchID= 10 Loss: nan | Acc: 9.233% (130/1408)
[Train] Epoch= 77  BatchID= 20 Loss: nan | Acc: 10.045% (270/2688)
[Train] Epoch= 77  BatchID= 30 Loss: nan | Acc: 9.829% (390/3968)
[Train] Epoch= 77  BatchID= 40 Loss: nan | Acc: 9.947% (522/5248)
[Train] Epoch= 77  BatchID= 50 Loss: nan | Acc: 10.110% (660/6528)
[Train] Epoch= 77  BatchID= 60 Loss: nan | Acc: 10.156% (793/7808)
[Train] Epoch= 77  BatchID= 70 Loss: nan | Acc: 10.255% (932/9088)
[Test] Epoch= 77  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 77  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 77  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 77  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 78
[Train] Epoch= 78  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 78  BatchID= 10 Loss: nan | Acc: 10.724% (151/1408)
[Train] Epoch= 78  BatchID= 20 Loss: nan | Acc: 10.305% (277/2688)
[Train] Epoch= 78  BatchID= 30 Loss: nan | Acc: 10.862% (431/3968)
[Train] Epoch= 78  BatchID= 40 Loss: nan | Acc: 10.537% (553/5248)
[Train] Epoch= 78  BatchID= 50 Loss: nan | Acc: 10.355% (676/6528)
[Train] Epoch= 78  BatchID= 60 Loss: nan | Acc: 10.284% (803/7808)
[Train] Epoch= 78  BatchID= 70 Loss: nan | Acc: 10.145% (922/9088)
[Test] Epoch= 78  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 78  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 78  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 78  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 79
[Train] Epoch= 79  BatchID= 0 Loss: nan | Acc: 12.500% (16/128)
[Train] Epoch= 79  BatchID= 10 Loss: nan | Acc: 11.151% (157/1408)
[Train] Epoch= 79  BatchID= 20 Loss: nan | Acc: 10.938% (294/2688)
[Train] Epoch= 79  BatchID= 30 Loss: nan | Acc: 10.736% (426/3968)
[Train] Epoch= 79  BatchID= 40 Loss: nan | Acc: 10.957% (575/5248)
[Train] Epoch= 79  BatchID= 50 Loss: nan | Acc: 10.386% (678/6528)
[Train] Epoch= 79  BatchID= 60 Loss: nan | Acc: 10.387% (811/7808)
[Train] Epoch= 79  BatchID= 70 Loss: nan | Acc: 10.189% (926/9088)
[Test] Epoch= 79  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 79  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 79  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 79  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 80
[Train] Epoch= 80  BatchID= 0 Loss: nan | Acc: 9.375% (12/128)
[Train] Epoch= 80  BatchID= 10 Loss: nan | Acc: 9.943% (140/1408)
[Train] Epoch= 80  BatchID= 20 Loss: nan | Acc: 9.635% (259/2688)
[Train] Epoch= 80  BatchID= 30 Loss: nan | Acc: 9.703% (385/3968)
[Train] Epoch= 80  BatchID= 40 Loss: nan | Acc: 9.756% (512/5248)
[Train] Epoch= 80  BatchID= 50 Loss: nan | Acc: 9.942% (649/6528)
[Train] Epoch= 80  BatchID= 60 Loss: nan | Acc: 10.169% (794/7808)
[Train] Epoch= 80  BatchID= 70 Loss: nan | Acc: 10.134% (921/9088)
[Test] Epoch= 80  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 80  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 80  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 80  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 81
[Train] Epoch= 81  BatchID= 0 Loss: nan | Acc: 12.500% (16/128)
[Train] Epoch= 81  BatchID= 10 Loss: nan | Acc: 10.795% (152/1408)
[Train] Epoch= 81  BatchID= 20 Loss: nan | Acc: 10.491% (282/2688)
[Train] Epoch= 81  BatchID= 30 Loss: nan | Acc: 10.635% (422/3968)
[Train] Epoch= 81  BatchID= 40 Loss: nan | Acc: 10.099% (530/5248)
[Train] Epoch= 81  BatchID= 50 Loss: nan | Acc: 9.926% (648/6528)
[Train] Epoch= 81  BatchID= 60 Loss: nan | Acc: 10.067% (786/7808)
[Train] Epoch= 81  BatchID= 70 Loss: nan | Acc: 10.035% (912/9088)
[Test] Epoch= 81  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 81  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 81  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 81  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 82
[Train] Epoch= 82  BatchID= 0 Loss: nan | Acc: 10.156% (13/128)
[Train] Epoch= 82  BatchID= 10 Loss: nan | Acc: 10.795% (152/1408)
[Train] Epoch= 82  BatchID= 20 Loss: nan | Acc: 10.305% (277/2688)
[Train] Epoch= 82  BatchID= 30 Loss: nan | Acc: 9.929% (394/3968)
[Train] Epoch= 82  BatchID= 40 Loss: nan | Acc: 9.699% (509/5248)
[Train] Epoch= 82  BatchID= 50 Loss: nan | Acc: 9.911% (647/6528)
[Train] Epoch= 82  BatchID= 60 Loss: nan | Acc: 9.977% (779/7808)
[Train] Epoch= 82  BatchID= 70 Loss: nan | Acc: 10.156% (923/9088)
[Test] Epoch= 82  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 82  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 82  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 82  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 83
[Train] Epoch= 83  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 83  BatchID= 10 Loss: nan | Acc: 9.659% (136/1408)
[Train] Epoch= 83  BatchID= 20 Loss: nan | Acc: 9.784% (263/2688)
[Train] Epoch= 83  BatchID= 30 Loss: nan | Acc: 9.955% (395/3968)
[Train] Epoch= 83  BatchID= 40 Loss: nan | Acc: 9.775% (513/5248)
[Train] Epoch= 83  BatchID= 50 Loss: nan | Acc: 9.850% (643/6528)
[Train] Epoch= 83  BatchID= 60 Loss: nan | Acc: 9.823% (767/7808)
[Train] Epoch= 83  BatchID= 70 Loss: nan | Acc: 10.090% (917/9088)
[Test] Epoch= 83  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 83  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 83  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 83  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 84
[Train] Epoch= 84  BatchID= 0 Loss: nan | Acc: 5.469% (7/128)
[Train] Epoch= 84  BatchID= 10 Loss: nan | Acc: 9.659% (136/1408)
[Train] Epoch= 84  BatchID= 20 Loss: nan | Acc: 9.933% (267/2688)
[Train] Epoch= 84  BatchID= 30 Loss: nan | Acc: 10.232% (406/3968)
[Train] Epoch= 84  BatchID= 40 Loss: nan | Acc: 10.061% (528/5248)
[Train] Epoch= 84  BatchID= 50 Loss: nan | Acc: 10.340% (675/6528)
[Train] Epoch= 84  BatchID= 60 Loss: nan | Acc: 10.387% (811/7808)
[Train] Epoch= 84  BatchID= 70 Loss: nan | Acc: 10.145% (922/9088)
[Test] Epoch= 84  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 84  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 84  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 84  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 85
[Train] Epoch= 85  BatchID= 0 Loss: nan | Acc: 4.688% (6/128)
[Train] Epoch= 85  BatchID= 10 Loss: nan | Acc: 8.168% (115/1408)
[Train] Epoch= 85  BatchID= 20 Loss: nan | Acc: 9.449% (254/2688)
[Train] Epoch= 85  BatchID= 30 Loss: nan | Acc: 10.081% (400/3968)
[Train] Epoch= 85  BatchID= 40 Loss: nan | Acc: 9.699% (509/5248)
[Train] Epoch= 85  BatchID= 50 Loss: nan | Acc: 10.003% (653/6528)
[Train] Epoch= 85  BatchID= 60 Loss: nan | Acc: 10.233% (799/7808)
[Train] Epoch= 85  BatchID= 70 Loss: nan | Acc: 10.189% (926/9088)
[Test] Epoch= 85  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 85  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 85  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 85  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 86
[Train] Epoch= 86  BatchID= 0 Loss: nan | Acc: 13.281% (17/128)
[Train] Epoch= 86  BatchID= 10 Loss: nan | Acc: 10.582% (149/1408)
[Train] Epoch= 86  BatchID= 20 Loss: nan | Acc: 10.193% (274/2688)
[Train] Epoch= 86  BatchID= 30 Loss: nan | Acc: 10.585% (420/3968)
[Train] Epoch= 86  BatchID= 40 Loss: nan | Acc: 10.252% (538/5248)
[Train] Epoch= 86  BatchID= 50 Loss: nan | Acc: 10.248% (669/6528)
[Train] Epoch= 86  BatchID= 60 Loss: nan | Acc: 10.220% (798/7808)
[Train] Epoch= 86  BatchID= 70 Loss: nan | Acc: 10.222% (929/9088)
[Test] Epoch= 86  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 86  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 86  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 86  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 87
[Train] Epoch= 87  BatchID= 0 Loss: nan | Acc: 11.719% (15/128)
[Train] Epoch= 87  BatchID= 10 Loss: nan | Acc: 10.440% (147/1408)
[Train] Epoch= 87  BatchID= 20 Loss: nan | Acc: 9.933% (267/2688)
[Train] Epoch= 87  BatchID= 30 Loss: nan | Acc: 9.753% (387/3968)
[Train] Epoch= 87  BatchID= 40 Loss: nan | Acc: 9.642% (506/5248)
[Train] Epoch= 87  BatchID= 50 Loss: nan | Acc: 9.988% (652/6528)
[Train] Epoch= 87  BatchID= 60 Loss: nan | Acc: 10.169% (794/7808)
[Train] Epoch= 87  BatchID= 70 Loss: nan | Acc: 10.167% (924/9088)
[Test] Epoch= 87  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 87  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 87  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 87  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 88
[Train] Epoch= 88  BatchID= 0 Loss: nan | Acc: 15.625% (20/128)
[Train] Epoch= 88  BatchID= 10 Loss: nan | Acc: 10.298% (145/1408)
[Train] Epoch= 88  BatchID= 20 Loss: nan | Acc: 10.305% (277/2688)
[Train] Epoch= 88  BatchID= 30 Loss: nan | Acc: 9.980% (396/3968)
[Train] Epoch= 88  BatchID= 40 Loss: nan | Acc: 9.889% (519/5248)
[Train] Epoch= 88  BatchID= 50 Loss: nan | Acc: 9.804% (640/6528)
[Train] Epoch= 88  BatchID= 60 Loss: nan | Acc: 10.156% (793/7808)
[Train] Epoch= 88  BatchID= 70 Loss: nan | Acc: 10.178% (925/9088)
[Test] Epoch= 88  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 88  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 88  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 88  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)

Epoch: 89
[Train] Epoch= 89  BatchID= 0 Loss: nan | Acc: 13.281% (17/128)
[Train] Epoch= 89  BatchID= 10 Loss: nan | Acc: 10.156% (143/1408)
[Train] Epoch= 89  BatchID= 20 Loss: nan | Acc: 10.156% (273/2688)
[Train] Epoch= 89  BatchID= 30 Loss: nan | Acc: 9.829% (390/3968)
[Train] Epoch= 89  BatchID= 40 Loss: nan | Acc: 9.889% (519/5248)
[Train] Epoch= 89  BatchID= 50 Loss: nan | Acc: 10.187% (665/6528)
[Train] Epoch= 89  BatchID= 60 Loss: nan | Acc: 10.131% (791/7808)
[Train] Epoch= 89  BatchID= 70 Loss: nan | Acc: 10.145% (922/9088)
[Test] Epoch= 89  BatchID= 0 Loss: nan | Acc: 100.000% (128/128)
[Test] Epoch= 89  BatchID= 10 Loss: nan | Acc: 27.486% (387/1408)
[Test] Epoch= 89  BatchID= 20 Loss: nan | Acc: 14.397% (387/2688)
[Test] Epoch= 89  BatchID= 30 Loss: nan | Acc: 9.860% (387/3925)
